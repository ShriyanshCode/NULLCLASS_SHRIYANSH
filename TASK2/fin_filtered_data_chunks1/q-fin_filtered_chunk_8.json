[
    {
        "paper_id": 1802.02127,
        "authors": "Duc Thi Luu, Mauro Napoletano, Paolo Barucca and Stefano Battiston",
        "title": "Collateral Unchained: Rehypothecation networks, concentration and\n  systemic effects",
        "comments": "39 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how network structure affects the dynamics of collateral in presence\nof rehypothecation. We build a simple model wherein banks interact via chains\nof repo contracts and use their proprietary collateral or re-use the collateral\nobtained by other banks via reverse repos. In this framework, we show that\ntotal collateral volume and its velocity are affected by characteristics of the\nnetwork like the length of rehypothecation chains, the presence or not of\nchains having a cyclic structure, the direction of collateral flows, the\ndensity of the network. In addition, we show that structures where collateral\nflows are concentrated among few nodes (like in core-periphery networks) allow\nlarge increases in collateral volumes already with small network density.\nFurthermore, we introduce in the model collateral hoarding rates determined\naccording to a Value-at-Risk (VaR) criterion, and we then study the emergence\nof collateral hoarding cascades in different networks. Our results highlight\nthat network structures with highly concentrated collateral flows are also more\nexposed to large collateral hoarding cascades following local shocks. These\nnetworks are therefore characterized by a trade-off between liquidity and\nsystemic risk.\n"
    },
    {
        "paper_id": 1802.02699,
        "authors": "Li Zhou, Lu Qiu, Changgui Gu, and Huijie Yang",
        "title": "Immediate Causality Network of Stock Markets",
        "comments": "7pages, 5figures",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/121/48002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A financial system contains many elements networked by their relationships.\nExtensive works show that topological structure of the network stores rich\ninformation on evolutionary behaviors of the system such as early warning\nsignals of collapses and/or crises. Existing works focus mainly on the network\nstructure within a single stock market, while a collapse/crisis occurs in a\nmacro-scale covering several or even all markets in the world. This mismatch of\nscale leads to unacceptable noise to the topological structure, and lack of\ninformation stored in relationships between different markets. In this work by\nusing the transfer entropy we reconstruct the influential network between ten\ntypical stock markets distributed in the world. Interesting findings include,\nbefore a financial crisis the connection strength reaches a maxima, which can\nact as an early warning signal of financial crises; The markets in America are\nmono-directionally and strongly influenced by that in Europe and act as the\ncenter; Some strongly linked pairs have also close correlations. The findings\nare helpful in understanding the evolution and modelling the dynamical process\nof the global financial system.\n"
    },
    {
        "paper_id": 1802.02939,
        "authors": "Ole Peters and Alexander Adamou",
        "title": "The sum of log-normal variates in geometric Brownian motion",
        "comments": "14 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric Brownian motion (GBM) is a key model for representing\nself-reproducing entities. Self-reproduction may be considered the definition\nof life [5], and the dynamics it induces are of interest to those concerned\nwith living systems from biology to economics. Trajectories of GBM are\ndistributed according to the well-known log-normal density, broadening with\ntime. However, in many applications, what's of interest is not a single\ntrajectory but the sum, or average, of several trajectories. The distribution\nof these objects is more complicated. Here we show two different ways of\nfinding their typical trajectories. We make use of an intriguing connection to\nspin glasses: the expected free energy of the random energy model is an average\nof log-normal variates. We make the mapping to GBM explicit and find that the\nfree energy result gives qualitatively correct behavior for GBM trajectories.\nWe then also compute the typical sum of lognormal variates using Ito calculus.\nThis alternative route is in close quantitative agreement with numerical work.\n"
    },
    {
        "paper_id": 1802.03042,
        "authors": "Hans B\\\"uhler, Lukas Gonon, Josef Teichmann, Ben Wood",
        "title": "Deep Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a framework for hedging a portfolio of derivatives in the presence\nof market frictions such as transaction costs, market impact, liquidity\nconstraints or risk limits using modern deep reinforcement machine learning\nmethods.\n  We discuss how standard reinforcement learning methods can be applied to\nnon-linear reward structures, i.e. in our case convex risk measures. As a\ngeneral contribution to the use of deep learning for stochastic processes, we\nalso show that the set of constrained trading strategies used by our algorithm\nis large enough to $\\epsilon$-approximate any optimal solution.\n  Our algorithm can be implemented efficiently even in high-dimensional\nsituations using modern machine learning tools. Its structure does not depend\non specific market dynamics, and generalizes across hedging instruments\nincluding the use of liquid derivatives. Its computational performance is\nlargely invariant in the size of the portfolio as it depends mainly on the\nnumber of hedging instruments available.\n  We illustrate our approach by showing the effect on hedging under transaction\ncosts in a synthetic market driven by the Heston model, where we outperform the\nstandard \"complete market\" solution.\n"
    },
    {
        "paper_id": 1802.03286,
        "authors": "Rebekka Burkholz, Hans J. Herrmann, Frank Schweitzer",
        "title": "Explicit size distributions of failure cascades redefine systemic risk\n  on finite networks",
        "comments": "systemic risk, finite size effects, cascades, networks",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How big is the risk that a few initial failures of nodes in a network amplify\nto large cascades that span a substantial share of all nodes? Predicting the\nfinal cascade size is critical to ensure the functioning of a system as a\nwhole. Yet, this task is hampered by uncertain or changing parameters and\nmissing information. In infinitely large networks, the average cascade size can\noften be well estimated by established approaches building on local tree\napproximations and mean field approximations. Yet, as we demonstrate, in finite\nnetworks, this average does not even need to be a likely outcome. Instead, we\nfind broad and even bimodal cascade size distributions. This phenomenon\npersists for system sizes up to $10^{7}$ and different cascade models, i.e. it\nis relevant for most real systems. To show this, we derive explicit closed-form\nsolutions for the full probability distribution of the final cascade size. We\nfocus on two topological limit cases, the complete network representing a dense\nnetwork with a very narrow degree distribution, and the star network\nrepresenting a sparse network with a inhomogeneous degree distribution. Those\ntopologies are of great interest, as they either minimize or maximize the\naverage cascade size and are common motifs in many real world networks.\n"
    },
    {
        "paper_id": 1802.03322,
        "authors": "Takashi Shinzato",
        "title": "Replica Approach for Minimal Investment Risk with Cost",
        "comments": null,
        "journal-ref": null,
        "doi": "10.7566/JPSJ.87.064801",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work, the optimal portfolio minimizing the investment risk\nwith cost is discussed analytically, where this objective function is\nconstructed in terms of two negative aspects of investment, the risk and cost.\nWe note the mathematical similarity between the Hamiltonian in the\nmean-variance model and the Hamiltonians in the Hopfield model and the\nSherrington{Kirkpatrick model and show that we can analyze this portfolio\noptimization problem by using replica analysis, and derive the minimal\ninvestment risk with cost and the investment concentration of the optimal\nportfolio. Furthermore, we validate our proposed method through numerical\nsimulations.\n"
    },
    {
        "paper_id": 1802.03376,
        "authors": "Christopher Cameron",
        "title": "Visualizing Treasury Issuance Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce simple cost and risk proxy metrics that can be attached to\nTreasury issuance strategy to complement analysis of the resulting portfolio\nweighted-average maturity (WAM). These metrics are based on mapping issuance\nfractions to their long-term, asymptotic portfolio implications for cost and\nrisk under mechanical debt-rolling dynamics. The resulting mapping enables one\nto visualize tradeoffs involved in contemplated issuance reallocation, and\nidentify an efficient frontier and optimal tenor. Historical Treasury issuance\nstrategy is analyzed empirically using these cost and risk metrics to\nillustrate how changes in issuance needs and strategy have translated into\nstructural shifts in the cost and risk stance of Treasury issuance.\n"
    },
    {
        "paper_id": 1802.03405,
        "authors": "Marius Oltean, Carlos F. Sopuerta, Alessandro D.A.M. Spallicci",
        "title": "Particle-without-Particle: a practical pseudospectral collocation method\n  for linear partial differential equations with distributional sources",
        "comments": "41 pages, 11 figures; v2: references and clarifications added (mostly\n  in the introduction), matches the published version in Journal of Scientific\n  Computing",
        "journal-ref": "Journal of Scientific Computing 79, 827 (2019)",
        "doi": "10.1007/s10915-018-0873-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Partial differential equations with distributional sources---in particular,\ninvolving (derivatives of) delta distributions---have become increasingly\nubiquitous in numerous areas of physics and applied mathematics. It is often of\nconsiderable interest to obtain numerical solutions for such equations, but any\nsingular (\"particle\"-like) source modeling invariably introduces nontrivial\ncomputational obstacles. A common method to circumvent these is through some\nform of delta function approximation procedure on the computational grid;\nhowever, this often carries significant limitations on the efficiency of the\nnumerical convergence rates, or sometimes even the resolvability of the problem\nat all.\n  In this paper, we present an alternative technique for tackling such\nequations which avoids the singular behavior entirely: the\n\"Particle-without-Particle\" method. Previously introduced in the context of the\nself-force problem in gravitational physics, the idea is to discretize the\ncomputational domain into two (or more) disjoint pseudospectral\n(Chebyshev-Lobatto) grids such that the \"particle\" is always at the interface\nbetween them; thus, one only needs to solve homogeneous equations in each\ndomain, with the source effectively replaced by jump (boundary) conditions\nthereon. We prove here that this method yields solutions to any linear PDE the\nsource of which is any linear combination of delta distributions and\nderivatives thereof supported on a one-dimensional subspace of the problem\ndomain. We then implement it to numerically solve a variety of relevant PDEs:\nhyperbolic (with applications to neuroscience and acoustics), parabolic (with\napplications to finance), and elliptic. We generically obtain improved\nconvergence rates relative to typical past implementations relying on delta\nfunction approximations.\n"
    },
    {
        "paper_id": 1802.03593,
        "authors": "Sergio A. Almada Monter, Mykhaylo Shkolnikov, Jiacheng Zhang",
        "title": "Dynamics of observables in rank-based models and performance of\n  functionally generated portfolios",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the seminal work [9], several macroscopic market observables have been\nintroduced, in an attempt to find characteristics capturing the diversity of a\nfinancial market. Despite the crucial importance of such observables for\ninvestment decisions, a concise mathematical description of their dynamics has\nbeen missing. We fill this gap in the setting of rank-based models and expect\nour ideas to extend to other models of large financial markets as well. The\nresults are then used to study the performance of multiplicatively and\nadditively functionally generated portfolios, in particular, over short-term\nand medium-term horizons.\n"
    },
    {
        "paper_id": 1802.03708,
        "authors": "Li Guo, Wolfgang Karl H\\\"ardle, Yubo Tao",
        "title": "A Time-Varying Network for Cryptocurrencies",
        "comments": "Duplicate with arXiv:2108.11921",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies return cross-predictability and technological similarity\nyield information on risk propagation and market segmentation. To investigate\nthese effects, we build a time-varying network for cryptocurrencies, based on\nthe evolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can\nachieve better risk diversification by investing in cryptocurrencies from\ndifferent communities. A cross-sectional portfolio that implements an\ninter-crypto momentum trading strategy earns a 1.08% daily return. By\ndissecting the portfolio returns on behavioral factors, we confirm that our\nresults are not driven by behavioral mechanisms.\n"
    },
    {
        "paper_id": 1802.03735,
        "authors": "Zhentao Shi, Huanhuan Zheng",
        "title": "Structural Estimation of Behavioral Heterogeneity",
        "comments": "add Online Supplement",
        "journal-ref": "Journal of Applied Econometrics 33, no. 5 (2018): 690-707",
        "doi": "10.1002/jae.2640",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a behavioral asset pricing model in which agents trade in a market\nwith information friction. Profit-maximizing agents switch between trading\nstrategies in response to dynamic market conditions. Due to noisy private\ninformation about the fundamental value, the agents form different evaluations\nabout heterogeneous strategies. We exploit a thin set---a small\nsub-population---to pointly identify this nonlinear model, and estimate the\nstructural parameters using extended method of moments. Based on the estimated\nparameters, the model produces return time series that emulate the moments of\nthe real data. These results are robust across different sample periods and\nestimation methods.\n"
    },
    {
        "paper_id": 1802.03756,
        "authors": "Tadeusz Klecha, Daniel Kosiorowski, Dominik Mielczarek, Jerzy P.\n  Rydlewski",
        "title": "New Proposals of a Stress Measure in a Capital and its Robust Estimator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a novel approach for a measurement of stresses in a capital,\nwhich induce the capital flows between economic systems, is proposed. The\nproposals appeal to an apparatus offered by the statistical theory of shape. We\npropose a stress functional basing on a concept of mean shape determined by\nrepresentative particles of a capital carrier. We also propose methods of\ndescribing changes in an amount and a structure of stresses in a capital\nappealing, among others, to a Bookstein's pair of thin plain spline\ndeformation, and a measure of a shape variability. We apply our approach to an\nindirect verification of the hypothesis according to which a capital flow\nbetween economic systems is related to an activity of an inner force related to\nstresses in a capital. We indicate, that the stresses create a phenomenon\nanalogous to the heat, which may be interpreted in terms of a positive economic\nexternal effect, which attracts a capital from environment of a system to the\nsystem. For empirical studies we propose robust approach to estimate the stress\nfunctional basing on the data depth concept. In the empirical research we use\ndata on five branch stock indexes from Warsaw Stock Exchange. The studied\nperiod involves the financial crisis of 2007.\n"
    },
    {
        "paper_id": 1802.04232,
        "authors": "Maxim Bichuch, Zachary Feinstein",
        "title": "Optimization of Fire Sales and Borrowing in Systemic Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a framework for modeling financial contagion in a network\nsubject to fire sales and price impacts, but allowing for firms to borrow to\ncover their shortfall as well. We consider both uncollateralized and\ncollateralized loans. The main results of this work are providing sufficient\nconditions for existence and uniqueness of the clearing solutions (i.e.,\npayments, liquidations, and borrowing); in such a setting any clearing solution\nis the Nash equilibrium of an aggregation game.\n"
    },
    {
        "paper_id": 1802.04413,
        "authors": "Igor Rivin",
        "title": "What is the Sharpe Ratio, and how can everyone get it wrong?",
        "comments": "Four pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Sharpe ratio is the most widely used risk metric in the quantitative\nfinance community - amazingly, essentially everyone gets it wrong. In this\nnote, we will make a quixotic effort to rectify the situation.\n"
    },
    {
        "paper_id": 1802.04774,
        "authors": "Carey Caginalp and Gunduz Caginalp",
        "title": "Asset Price Volatility and Price Extrema",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relationship between price volatilty and a market extremum is examined\nusing a fundamental economics model of supply and demand. By examining\nrandomness through a microeconomic setting, we obtain the implications of\nrandomness in the supply and demand, rather than assuming that price has\nrandomness on an empirical basis. Within a very general setting the volatility\nhas an extremum that precedes the extremum of the price. A key issue is that\nrandomness arises from the supply and demand, and the variance in the\nstochastic differential equation govening the logarithm of price must reflect\nthis. Analogous results are obtained by further assuming that the supply and\ndemand are dependent on the deviation from fundamental value of the asset.\n"
    },
    {
        "paper_id": 1802.04778,
        "authors": "Carey Caginalp and Gunduz Caginalp",
        "title": "The Quotient of Normal Random Variables And Application to Asset Price\n  Fat Tails",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.02.077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The quotient of random variables with normal distributions is examined and\nproven to have have power law decay, with density $f\\left( x\\right) \\simeq\nf_{0}x^{-2}$, with the coefficient depending on the means and variances of the\nnumerator and denominator and their correlation. We also obtain the conditional\nprobability densities for each of the four quadrants given by the signs of the\nnumerator and denominator for arbitrary correlation $\\rho \\in\\lbrack-1,1).$ For\n$\\rho=-1$ we obtain a particularly simple closed form solution for all $x\\in$\n$\\mathbb{R}$. The results are applied to a basic issue in economics and\nfinance, namely the density of relative price changes. Classical finance\nstipulates a normal distribution of relative price changes, though empirical\nstudies suggest a power law at the tail end. By considering the supply and\ndemand in a basic price change model, we prove that the relative price change\nhas density that decays with an $x^{-2}$ power law. Various parameter limits\nare established.\n"
    },
    {
        "paper_id": 1802.04837,
        "authors": "P. Amster and A. P. Mogni",
        "title": "Adapting the CVA model to Leland's framework",
        "comments": "20 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the framework proposed by Burgard and Kjaer (2011) that derives\nthe PDE which governs the price of an option including bilateral counterparty\nrisk and funding. We extend this work by relaxing the assumption of absence of\ntransaction costs in the hedging portfolio by proposing a cost proportional to\nthe amount of assets traded and the traded price. After deriving the nonlinear\nPDE, we prove the existence of a solution for the corresponding\ninitial-boundary value problem. Moreover, we develop a numerical scheme that\nallows to find the solution of the PDE by setting different values for each\nparameter of the model. To understand the impact of each variable within the\nmodel, we analyze the Greeks of the option and the sensitivity of the price to\nchanges in all the risk factors.\n"
    },
    {
        "paper_id": 1802.05016,
        "authors": "Michael B. Giles, Abdul-Lateef Haji-Ali",
        "title": "Multilevel nested simulation for efficient risk estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the problem of computing a nested expectation of the form\n$\\mathbb{P}[\\mathbb{E}[X|Y]\n\\!\\geq\\!0]\\!=\\!\\mathbb{E}[\\textrm{H}(\\mathbb{E}[X|Y])]$ where $\\textrm{H}$ is\nthe Heaviside function. This nested expectation appears, for example, when\nestimating the probability of a large loss from a financial portfolio. We\npresent a method that combines the idea of using Multilevel Monte Carlo (MLMC)\nfor nested expectations with the idea of adaptively selecting the number of\nsamples in the approximation of the inner expectation, as proposed by (Broadie\net al., 2011). We propose and analyse an algorithm that adaptively selects the\nnumber of inner samples on each MLMC level and prove that the resulting MLMC\nmethod with adaptive sampling has an $\\mathcal{O}\\left(\n\\varepsilon^{-2}|\\log\\varepsilon|^2 \\right)$ complexity to achieve a root\nmean-squared error $\\varepsilon$. The theoretical analysis is verified by\nnumerical experiments on a simple model problem. We also present a stochastic\nroot-finding algorithm that, combined with our adaptive methods, can be used to\ncompute other risk measures such as Value-at-Risk (VaR) and Conditional\nValue-at-Risk (CVaR), with the latter being achieved with\n$\\mathcal{O}\\left(\\varepsilon^{-2}\\right)$ complexity.\n"
    },
    {
        "paper_id": 1802.05139,
        "authors": "Sadamori Kojaku, Giulio Cimini, Guido Caldarelli, Naoki Masuda",
        "title": "Structural changes in the interbank market across the financial crisis\n  from multiple core-periphery analysis",
        "comments": "17 pages, 9 figures, 1 table",
        "journal-ref": "Journal of Network Theory in Finance 4(3), 33-51 (2018)",
        "doi": "10.21314/JNTF.2018.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interbank markets are often characterised in terms of a core-periphery\nnetwork structure, with a highly interconnected core of banks holding the\nmarket together, and a periphery of banks connected mostly to the core but not\ninternally. This paradigm has recently been challenged for short time scales,\nwhere interbank markets seem better characterised by a bipartite structure with\nmore core-periphery connections than inside the core. Using a novel\ncore-periphery detection method on the eMID interbank market, we enrich this\npicture by showing that the network is actually characterised by multiple\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\nstructures occurs by shortening the temporal scale of data aggregation. We\nfurther show how the global financial crisis transformed the market, in terms\nof composition, multiplicity and internal organisation of core-periphery pairs.\nBy unveiling such a fine-grained organisation and transformation of the\ninterbank market, our method can find important applications in the\nunderstanding of how distress can propagate over financial networks.\n"
    },
    {
        "paper_id": 1802.05264,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Stock Market Visualization",
        "comments": "103 pages; to appear in Journal of Risk & Control",
        "journal-ref": "Journal of Risk & Control 5(1) (2018) 35-140",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide complete source code for a front-end GUI and its back-end\ncounterpart for a stock market visualization tool. It is built based on the\n\"functional visualization\" concept we discuss, whereby functionality is not\nsacrificed for fancy graphics. The GUI, among other things, displays a\ncolor-coded signal (computed by the back-end code) based on how \"out-of-whack\"\neach stock is trading compared with its peers (\"mean-reversion\"), and the most\nsizable changes in the signal (\"momentum\"). The GUI also allows to efficiently\nfilter/tier stocks by various parameters (e.g., sector, exchange, signal,\nliquidity, market cap) and functionally display them. The tool can be run as a\nweb-based or local application.\n"
    },
    {
        "paper_id": 1802.05326,
        "authors": "Jacky C.K. Chow",
        "title": "Analysis of Financial Credit Risk Using Machine Learning",
        "comments": "MBA Thesis, April 2017, Aston Business School, Aston University,\n  United Kingdom",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.30242.53449",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Corporate insolvency can have a devastating effect on the economy. With an\nincreasing number of companies making expansion overseas to capitalize on\nforeign resources, a multinational corporate bankruptcy can disrupt the world's\nfinancial ecosystem. Corporations do not fail instantaneously; objective\nmeasures and rigorous analysis of qualitative (e.g. brand) and quantitative\n(e.g. econometric factors) data can help identify a company's financial risk.\nGathering and storage of data about a corporation has become less difficult\nwith recent advancements in communication and information technologies. The\nremaining challenge lies in mining relevant information about a company's\nhealth hidden under the vast amounts of data, and using it to forecast\ninsolvency so that managers and stakeholders have time to react. In recent\nyears, machine learning has become a popular field in big data analytics\nbecause of its success in learning complicated models. Methods such as support\nvector machines, adaptive boosting, artificial neural networks, and Gaussian\nprocesses can be used for recognizing patterns in the data (with a high degree\nof accuracy) that may not be apparent to human analysts. This thesis studied\ncorporate bankruptcy of manufacturing companies in Korea and Poland using\nexperts' opinions and financial measures, respectively. Using publicly\navailable datasets, several machine learning methods were applied to learn the\nrelationship between the company's current state and its fate in the near\nfuture. Results showed that predictions with accuracy greater than 95% were\nachievable using any machine learning technique when informative features like\nexperts' assessment were used. However, when using purely financial factors to\npredict whether or not a company will go bankrupt, the correlation is not as\nstrong.\n"
    },
    {
        "paper_id": 1802.05495,
        "authors": "Nassim Nicholas Taleb",
        "title": "How Much Data Do You Need? An Operational, Pre-Asymptotic Metric for\n  Fat-tailedness",
        "comments": null,
        "journal-ref": "International Journal of Forecasting, 35-2, 677-686, 2019",
        "doi": "10.1016/j.ijforecast.2018.10.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note presents an operational measure of fat-tailedness for univariate\nprobability distributions, in $[0,1]$ where 0 is maximally thin-tailed\n(Gaussian) and 1 is maximally fat-tailed. Among others,1) it helps assess the\nsample size needed to establish a comparative $n$ needed for statistical\nsignificance, 2) allows practical comparisons across classes of fat-tailed\ndistributions, 3) helps understand some inconsistent attributes of the\nlognormal, pending on the parametrization of its scale parameter. The\nliterature is rich for what concerns asymptotic behavior, but there is a large\nvoid for finite values of $n$, those needed for operational purposes.\nConventional measures of fat-tailedness, namely 1) the tail index for the power\nlaw class, and 2) Kurtosis for finite moment distributions fail to apply to\nsome distributions, and do not allow comparisons across classes and\nparametrization, that is between power laws outside the Levy-Stable basin, or\npower laws to distributions in other classes, or power laws for different\nnumber of summands. How can one compare a sum of 100 Student T distributed\nrandom variables with 3 degrees of freedom to one in a Levy-Stable or a\nLognormal class? How can one compare a sum of 100 Student T with 3 degrees of\nfreedom to a single Student T with 2 degrees of freedom? We propose an\noperational and heuristic measure that allow us to compare $n$-summed\nindependent variables under all distributions with finite first moment. The\nmethod is based on the rate of convergence of the Law of Large numbers for\nfinite sums, $n$-summands specifically. We get either explicit expressions or\nsimulation results and bounds for the lognormal, exponential, Pareto, and the\nStudent T distributions in their various calibrations --in addition to the\ngeneral Pearson classes.\n"
    },
    {
        "paper_id": 1802.05614,
        "authors": "Damien Lamberton (LAMA, MATHRISK)",
        "title": "On the binomial approximation of the American put",
        "comments": "Applied Mathematics and Optimization, Springer Verlag (Germany), In\n  press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the binomial approximation of the American put price in the\nBlack-Scholes model (with continuous dividend yield). Our main result is that\nthe error of approximation is $O((ln n) $\\alpha$ /n)$ where n is the number of\ntime periods and the exponent $\\alpha$ is a positive number, the value of which\nmay differ according to the respective levels of the interest rate and the\ndividend yield.\n"
    },
    {
        "paper_id": 1802.05993,
        "authors": "Kiyoshi Kanazawa, Takumi Sueshige, Hideki Takayasu, Misako Takayasu",
        "title": "Kinetic Theory for Finance Brownian Motion from Microscopic Dynamics",
        "comments": "36 pages, 15 figures",
        "journal-ref": "Phys. Rev. E 98, 052317 (2018)",
        "doi": "10.1103/PhysRevE.98.052317",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent technological development has enabled researchers to study social\nphenomena scientifically in detail and financial markets has particularly\nattracted physicists since the Brownian motion has played the key role as in\nphysics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.\nLett.), we have presented a microscopic model of trend-following high-frequency\ntraders (HFTs) and its theoretical relation to the dynamics of financial\nBrownian motion, directly supported by a data analysis of tracking trajectories\nof individual HFTs in a financial market. Here we show the mathematical\nfoundation for the HFT model paralleling to the traditional kinetic theory in\nstatistical physics. We first derive the time-evolution equation for the\nphase-space distribution for the HFT model exactly, which corresponds to the\nLiouville equation in conventional analytical mechanics. By a systematic\nreduction of the Liouville equation for the HFT model, the\nBogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for\nfinancial Brownian motion. We then derive the Boltzmann-like and Langevin-like\nequations for the order-book and the price dynamics by making the assumption of\nmolecular chaos. The qualitative behavior of the model is asymptotically\nstudied by solving the Boltzmann-like and Langevin-like equations for the large\nnumber of HFTs, which is numerically validated through the Monte-Carlo\nsimulation. Our kinetic description highlights the parallel mathematical\nstructure between the financial Brownian motion and the physical Brownian\nmotion.\n"
    },
    {
        "paper_id": 1802.06101,
        "authors": "Ismael Lemhadri",
        "title": "Market Impact in a Latent Order Book",
        "comments": "36 pages, 15 figures",
        "journal-ref": "Market Microstructure and Liquidity 2020",
        "doi": "10.1142/S2382626620500045",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The latent order book of \\cite{donier2015fully} is one of the most promising\nagent-based models for market impact. This work extends the minimal model by\nallowing agents to exhibit mean-reversion, a commonly observed pattern in real\nmarkets. This modification leads to new order book dynamics, which we\nexplicitly study and analyze. Underlying our analysis is a mean-field\nassumption that views the order book through its \\textit{average} density. We\nshow how price impact develops in this new model, providing a flexible family\nof solutions that can potentially improve calibration to real data. While no\nclosed-form solution is provided, we complement our theoretical investigation\nwith extensive numerical results, including a simulation scheme for the entire\norder book.\n"
    },
    {
        "paper_id": 1802.0612,
        "authors": "Bruno Bouchard, Johannes Muhle-Karbe",
        "title": "Simple Bounds for Utility Maximization with Small Transaction Costs",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using elementary arguments, we show how to derive $\\mathbf{L}_p$-error bounds\nfor the approximation of frictionless wealth process in markets with\nproportional transaction costs. For utilities with bounded risk aversion, these\nestimates yield lower bounds for the frictional value function, which pave the\nway for its asymptotic analysis using stability results for viscosity\nsolutions. Using tools from Malliavin calculus, we also derive simple\nsufficient conditions for the regularity of frictionless optimal trading\nstrategies, the second main ingredient for the asymptotic analysis of small\ntransaction costs.\n"
    },
    {
        "paper_id": 1802.06386,
        "authors": "Christoph K\\\"uhn",
        "title": "How local in time is the no-arbitrage property under capital gains taxes\n  ?",
        "comments": "30 pages",
        "journal-ref": "Mathematics and Financial Economics 2019 13(3) 329-358",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In frictionless financial markets, no-arbitrage is a local property in time.\nThis means that a discrete time model is arbitrage-free if and only if there\ndoes not exist a one-period-arbitrage. With capital gains taxes, this\nequivalence fails. For a model with a linear tax and one non-shortable risky\nstock, we introduce the concept of robust local no-arbitrage (RLNA) as the\nweakest local condition which guarantees dynamic no-arbitrage. Under a sharp\ndichotomy condition, we prove (RLNA). Since no-one-period-arbitrage is\nnecessary for no-arbitrage, the latter is sandwiched between two local\nconditions, which allows us to estimate its non-locality.\n  Furthermore, we construct a stock price process such that two long positions\nin the same stock hedge each other. This puzzling phenomenon that cannot occur\nin arbitrage-free frictionless markets (or markets with proportional\ntransaction costs) is used to show that no-arbitrage alone does not imply the\nexistence of an equivalent separating measure if the probability space is\ninfinite.\n  Finally, we show that the model with a linear tax on capital gains can be\nwritten as a model with proportional transaction costs by introducing several\nfictitious securities.\n"
    },
    {
        "paper_id": 1802.0652,
        "authors": "Jeonggyu Huh",
        "title": "Pricing Options with Exponential Levy Neural Network",
        "comments": "18 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose the exponential Levy neural network (ELNN) for\noption pricing, which is a new non-parametric exponential Levy model using\nartificial neural networks (ANN). The ELNN fully integrates the ANNs with the\nexponential Levy model, a conventional pricing model. So, the ELNN can improve\nANN-based models to avoid several essential issues such as unacceptable\noutcomes and inconsistent pricing of over-the-counter products. Moreover, the\nELNN is the first applicable non-parametric exponential Levy model by virtue of\noutstanding researches on optimization in the field of ANN. The existing\nnon-parametric models are too vulnerable to be employed in practice. The\nempirical tests with S\\&P 500 option prices show that the ELNN outperforms two\nparametric models, the Merton and Kou models, in terms of fitting performance\nand stability of estimates.\n"
    },
    {
        "paper_id": 1802.0677,
        "authors": "Hardik Rajpal and Deepak Dhar",
        "title": "Achieving perfect coordination amongst agents in the co-action minority\n  game",
        "comments": "Added additional discussion, and a figure",
        "journal-ref": "Games, ( 2018), Vol. 9, p27",
        "doi": "10.3390/g9020027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the strategy that rational agents can use to maximize their\nexpected long-term payoff in the co-action minority game. We argue that the\nagents will try to get into a cyclic state, where each of the $(2N +1)$ agent\nwins exactly $N$ times in any continuous stretch of $(2N+1)$ days. We propose\nand analyse a strategy for reaching such a cyclic state quickly, when any\ndirect communication between agents is not allowed, and only the publicly\navailable common information is the record of total number of people choosing\nthe first restaurant in the past. We determine exactly the average time\nrequired to reach the periodic state for this strategy. We show that it varies\nas $(N/\\ln 2) [1 + \\alpha \\cos (2 \\pi \\log_2 N)$], for large $N$, where the\namplitude $\\alpha$ of the leading term in the log-periodic oscillations is\nfound be $\\frac{8 \\pi^2}{(\\ln 2)^2} \\exp{(- 2 \\pi^2/\\ln 2)} \\approx\n{\\color{blue}7 \\times 10^{-11}}$.\n"
    },
    {
        "paper_id": 1802.07009,
        "authors": "Simon Hochgerner and Florian Gach",
        "title": "Analytical Validation Formulas for Best Estimate Calculation in\n  Traditional Life Insurance",
        "comments": null,
        "journal-ref": "Eur. Actuar. J. (2019) 9: 423",
        "doi": "10.1007/s13385-019-00212-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the context of traditional life insurance, a model-independent\nrelationship about how the market value of assets is attributed to the best\nestimate, the value of in-force business and tax is established. This\nrelationship holds true for any portfolio under run-off assumptions and can be\nused for the validation of models set up for Solvency~II best estimate\ncalculation. Furthermore, we derive a lower bound for the value of future\ndiscretionary benefits. This lower bound formula is applied to publicly\navailable insurance data to show how it can be used for practical validation\npurposes.\n"
    },
    {
        "paper_id": 1802.07312,
        "authors": "Giorgio Locatelli",
        "title": "Why are Megaprojects, Including Nuclear Power Plants, Delivered\n  Overbudget and Late? Reasons and Remedies",
        "comments": null,
        "journal-ref": "Center for Advanced Nuclear Energy Systems (CANES), Massachusetts\n  Institute of Technology, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the first section, this report analyses Nuclear Power Plants (NPPs) in the\ncontext of megaprojects, explaining why they are often delivered over budget\nand late. In the second section, the report discusses how Small Modular\nReactors (SMRs) might address these issues. Megaprojects are extremely risky\nand often implemented after a sub-optimal phase of project planning leading to\nunderestimations of the costs and overestimation of short-term benefits. When\nconsidering adherence to schedule and budget, often megaprojects might be\nconsidered a failure, and optimism bias, strategic mis-rapresentation,\ncomplexity, poor planning, poor risk allocation, poor scope management are all\nreasons to explain their over budget and delay. For megaprojects, especially in\nthe nuclear field, a key strategy to achieve good performances appears to be\nthe standardization. This standardization needs to be twofold: (i) technical\nstandardisation, i.e. the construction of very similar design over and over,\nand (ii) the project delivery chain standardisation, i.e. the same stakeholders\ninvolved in the delivery of a project that is replicable multiple times. Under\nthis perspective, given their size and standardisation potential, SMRs, might\nbe a suitable class of NPP for several countries. Yet, if the economy of scale\nis the only driver considered, SMRs are hardly competitive with large NPPs (or\neven with gas or coal power plants). However, a fleet of standard SMRs might\nbalance the lack of economy of scale with the economy of multiples, and the\ndelivery of several standardised SMR projects might be the key to achieve good\nproject management performances in the nuclear sector. However, the deployment\nof SMRs faces a number of challenges from several perspectives, such as the\nlicencing, supply chain and financing ones. These challenges might be enormous,\nbut so are the potential rewards too.\n"
    },
    {
        "paper_id": 1802.07405,
        "authors": "Teruyoshi Kobayashi, Anna Sapienza, Emilio Ferrara",
        "title": "Extracting the multi-timescale activity patterns of online financial\n  markets",
        "comments": "16 pages, 9 figures",
        "journal-ref": "Scientific Reports 8, 11184 (2018)",
        "doi": "10.1038/s41598-018-29537-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online financial markets can be represented as complex systems where trading\ndynamics can be captured and characterized at different resolutions and time\nscales. In this work, we develop a methodology based on non-negative tensor\nfactorization (NTF) aimed at extracting and revealing the multi-timescale\ntrading dynamics governing online financial systems. We demonstrate the\nadvantage of our strategy first using synthetic data, and then on real-world\ndata capturing all interbank transactions (over a million) occurred in an\nItalian online financial market (e-MID) between 2001 and 2015. Our results\ndemonstrate how NTF can uncover hidden activity patterns that characterize\ngroups of banks exhibiting different trading strategies (normal vs. early vs.\nflash trading, etc.). We further illustrate how our methodology can reveal\n\"crisis modalities\" in trading triggered by endogenous and exogenous system\nshocks: as an example, we reveal and characterize trading anomalies in the\nmidst of the 2008 financial crisis.\n"
    },
    {
        "paper_id": 1802.07422,
        "authors": "Zura Kakushadze and Ronald P. Russo Jr",
        "title": "Blockchain: Data Malls, Coin Economies and Keyless Payments",
        "comments": "32 pages; a few trivial typos corrected; to appear in The Journal of\n  Alternative Investments",
        "journal-ref": "The Journal of Alternative Investments 21(1) (2018) 8-16",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss several uses of blockchain (and, more generally, distributed\nledger) technologies outside of cryptocurrencies with a pragmatic view. We\nmostly focus on three areas: the role of coin economies for what we refer to as\ndata malls (specialized data marketplaces); data provenance (a historical\nrecord of data and its origins); and what we term keyless payments (made\nwithout having to know other users' cryptographic keys). We also discuss voting\nand other areas, and give a sizable list of academic and nonacademic\nreferences.\n"
    },
    {
        "paper_id": 1802.07457,
        "authors": "Anthony D Stephens and David R Walwyn",
        "title": "The Security of the United Kingdom Electricity Imports under Conditions\n  of High European Demand",
        "comments": "13 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy policy in Europe has been driven by the three goals of security of\nsupply, economic competitiveness and environmental sustainability, referred to\nas the energy trilemma. Although there are clear conflicts within the trilemma,\nmember countries have acted to facilitate a fully integrated European\nelectricity market. Interconnection and cross-border electricity trade has been\na fundamental part of such market liberalisation. However, it has been\nsuggested that consumers are exposed to a higher price volatility as a\nconsequence of interconnection. Furthermore, during times of energy shortages\nand high demand, issues of national sovereignty take precedence over\ncooperation. In this article, the unique and somewhat peculiar conditions of\nearly 2017 within France, Germany and the United Kingdom have been studied to\nunderstand how the existing integration arrangements address the energy\ntrilemma. It is concluded that the dominant interests are economic and national\nsecurity; issues of environmental sustainability are neglected or overridden.\nAlthough the optimisation of European electricity generation to achieve a lower\noverall carbon emission is possible, such a goal is far from being realised.\nFurthermore, it is apparent that the United Kingdom, and other countries,\ncannot rely upon imports from other countries during periods of high demand\nand/or limited supply.\n"
    },
    {
        "paper_id": 1802.07741,
        "authors": "Francesca Biagini, Yinglin Zhang",
        "title": "Extended Reduced-Form Framework for Non-Life Insurance",
        "comments": null,
        "journal-ref": "Advances in Applied Probability, 2022, 1-29",
        "doi": "10.1017/apr.2021.60",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a general framework for modeling an insurance\nliability cash flow in continuous time, by generalizing the reduced-form\nframework for credit risk and life insurance. In particular, we assume a\nnontrivial dependence structure between the reference filtration and the\ninsurance internal filtration. We apply these results for pricing and hedging\nnon-life insurance liabilities in hybrid financial and insurance markets, while\ntaking into account the role of inflation under the benchmarked\nrisk-minimization approach. This framework offers at the same time a general\nand flexible structure, and an explicit and treatable pricing-hedging formula.\n"
    },
    {
        "paper_id": 1802.08135,
        "authors": "Nicolas Baradel (CEREMADE, ENSAE), Bruno Bouchard (CEREMADE, PSL),\n  David Evangelista (KAUST), Othmane Mounjid (CMAP)",
        "title": "Optimal inventory management and order book modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the behavior of three agent classes acting dynamically in a limit\norder book of a financial asset. Namely, we consider market makers (MM),\nhigh-frequency trading (HFT) firms, and institutional brokers (IB). Given a\nprior dynamic of the order book, similar to the one considered in the\nQueue-Reactive models [14, 20, 21], the MM and the HFT define their trading\nstrategy by optimizing the expected utility of terminal wealth, while the IB\nhas a prescheduled task to sell or buy many shares of the considered asset. We\nderive the variational partial differential equations that characterize the\nvalue functions of the MM and HFT and explain how almost optimal control can be\ndeduced from them. We then provide a first illustration of the interactions\nthat can take place between these different market participants by simulating\nthe dynamic of an order book in which each of them plays his own (optimal)\nstrategy.\n"
    },
    {
        "paper_id": 1802.08238,
        "authors": "Yiyang Gu",
        "title": "What are the most important factors that influence the changes in London\n  Real Estate Prices? How to quantify them?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, real estate industry has captured government and public\nattention around the world. The factors influencing the prices of real estate\nare diversified and complex. However, due to the limitations and one-sidedness\nof their respective views, they did not provide enough theoretical basis for\nthe fluctuation of house price and its influential factors. The purpose of this\npaper is to build a housing price model to make the scientific and objective\nanalysis of London's real estate market trends from the year 1996 to 2016 and\nproposes some countermeasures to reasonably control house prices. Specifically,\nthe paper analyzes eight factors which affect the house prices from two\naspects: housing supply and demand and find out the factor which is of vital\nimportance to the increase of housing price per square meter. The problem of a\nhigh level of multicollinearity between them is solved by using principal\ncomponents analysis.\n"
    },
    {
        "paper_id": 1802.08358,
        "authors": "Erhan Bayraktar, Jingjie Zhang, Zhou Zhou",
        "title": "Time Consistent Stopping For The Mean-Standard Deviation Problem --- The\n  Discrete Time Case",
        "comments": "Final version. To appear in the SIAM Journal on Financial\n  Mathematics. Keywords: Time-inconsistency, optimal stopping, liquidation\n  strategy, mean-variance problem, subgame perfect Nash equilibrium",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by Strotz's consistent planning strategy, we formulate the infinite\nhorizon mean-variance stopping problem as a subgame perfect Nash equilibrium in\norder to determine time consistent strategies with no regret. Equilibria among\nstopping times or randomized stopping times may not exist. This motivates us to\nconsider the notion of liquidation strategies, which lets the stopping right to\nbe divisible. We then argue that the mean-standard deviation variant of this\nproblem makes more sense for this type of strategies in terms of time\nconsistency. It turns out that an equilibrium liquidation strategy always\nexists. We then analyze whether optimal equilibrium liquidation strategies\nexist and whether they are unique and observe that neither may hold.\n"
    },
    {
        "paper_id": 1802.08502,
        "authors": "Emilio Said, Ahmed Bel Hadj Ayed, Alexandre Husson, Fr\\'ed\\'eric\n  Abergel",
        "title": "Market Impact: A Systematic Study of Limit Orders",
        "comments": null,
        "journal-ref": "Market Microstructure and Liquidity Vol. 03, No. 03n04, (2017)\n  1850008 (33 pages)",
        "doi": "10.1142/S2382626618500089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to the important yet little explored subject of the\nmarket impact of limit orders. Our analysis is based on a proprietary database\nof metaorders - large orders that are split into smaller pieces before being\nsent to the market. We first address the case of aggressive limit orders and\nthen, that of passive limit orders. In both cases, we provide empirical\nevidence of a power law behaviour for the temporary market impact. The\nrelaxation of the price following the end of the metaorder is also studied, and\nthe long-term impact is shown to stabilize at a level of approximately\ntwo-thirds of the maximum impact. Finally, a fair pricing condition during the\nlife cycle of the metaorders is empirically validated.\n"
    },
    {
        "paper_id": 1802.08539,
        "authors": "Stephan Eckstein and Michael Kupper",
        "title": "Computation of optimal transport and related hedging problems via\n  penalization and neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a widely applicable approach to solving (multi-marginal,\nmartingale) optimal transport and related problems via neural networks. The\ncore idea is to penalize the optimization problem in its dual formulation and\nreduce it to a finite dimensional one which corresponds to optimizing a neural\nnetwork with smooth objective function. We present numerical examples from\noptimal transport, martingale optimal transport, portfolio optimization under\nuncertainty and generative adversarial networks that showcase the generality\nand effectiveness of the approach.\n"
    },
    {
        "paper_id": 1802.08575,
        "authors": "Carlo Piccardi, Lucia Tajoli",
        "title": "Complexity, Centralization, and Fragility in Economic Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0208265",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trade networks, across which countries distribute their products, are crucial\ncomponents of the globalized world economy. Their structure is strongly\nheterogeneous across products, given the different features of the countries\nwhich buy and sell goods. By using a diversified pool of indicators from\nnetwork science and product complexity theory, we quantitatively confirm the\nintuition that, overall, products with higher complexity -- i.e., with larger\ntechnological content and number of components -- are traded through a more\ncentralized network -- i.e., with a small number of countries concentrating\nmost of the export flow. Since centralized networks are known to be more\nvulnerable, we argue that the current composition of production and trading is\nassociated to high fragility at the level of the most complex -- thus strategic\n-- products.\n"
    },
    {
        "paper_id": 1802.08987,
        "authors": "Abdulnasser Hatemi-J and Youssef El-Khatib",
        "title": "The Dividend Discount Model with Multiple Growth Rates of Any Order for\n  Stock Evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a general solution for the dividend discount model\nin order to compute the intrinsic value of a common stock that allows for\nmultiple stage growth rates of any predetermined number of periods. A\nmathematical proof is provided for the suggested general solution. A numerical\napplication is also presented. The solution introduced in this paper is\nexpected to improve on the precision of stock valuation, which might be of\nfundamental importance for investors as well as financial institutions.\n"
    },
    {
        "paper_id": 1802.09165,
        "authors": "Sergey Nadtochiy and Thaleia Zariphopoulou",
        "title": "Optimal contract for a fund manager, with capital injections and\n  endogenous trading constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we construct a solution to the optimal contract problem for\ndelegated portfolio management of the fist-best (risk-sharing) type. The\nnovelty of our result is (i) in the robustness of the optimal contract with\nrespect to perturbations of the wealth process (interpreted as capital\ninjections), and (ii) in the more general form of principals objective\nfunction, which is allowed to depend directly on the agents strategy, as\nopposed to being a function of the generated wealth only. In particular, the\nlatter feature allows us to incorporate endogenous trading constraints in the\ncontract. We reduce the optimal contract problem to the following inverse\nproblem: for a given portfolio (defined in a feedback form, as a random field),\nconstruct a stochastic utility whose optimal portfolio coincides with the given\none. We characterize the solution to this problem through a Stochastic Partial\nDifferential Equation (SPDE), prove its well-posedness, and compute the\nsolution explicitly in the Black-Scholes model.\n"
    },
    {
        "paper_id": 1802.09396,
        "authors": "Pak Hung Au, Mark Whitmeyer",
        "title": "Attraction versus Persuasion: Information Provision in Search Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model of oligopolistic competition in a market with search\nfrictions, in which competing firms with products of unknown quality advertise\nhow much information a consumer's visit will glean. In the unique symmetric\nequilibrium of this game, the countervailing incentives of attraction and\npersuasion yield a payoff function for each firm that is linear in the firm's\nrealized effective value. If the expected quality of the products is\nsufficiently high (or competition is sufficiently fierce), this corresponds to\nfull information--firms provide the first-best level of information. If not,\nthis corresponds to information dispersion--firms randomize over signals.\n"
    },
    {
        "paper_id": 1802.09427,
        "authors": "Agnieszka Werpachowska",
        "title": "Forecasting the impact of state pension reforms in post-Brexit England\n  and Wales using microsimulation and deep learning",
        "comments": null,
        "journal-ref": "Proceedings of PenCon 2018 Pensions Conference",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ stochastic dynamic microsimulations to analyse and forecast the\npension cost dependency ratio for England and Wales from 1991 to 2061,\nevaluating the impact of the ongoing state pension reforms and changes in\ninternational migration patterns under different Brexit scenarios. To fully\naccount for the recently observed volatility in life expectancies, we propose\nmortality rate model based on deep learning techniques, which discovers complex\npatterns in data and extrapolated trends. Our results show that the recent\nreforms can effectively stave off the \"pension crisis\" and bring back the\nsystem on a sounder fiscal footing. At the same time, increasingly more workers\ncan expect to spend greater share of their lifespan in retirement, despite the\neligibility age rises. The population ageing due to the observed postponement\nof death until senectitude often occurs with the compression of morbidity, and\nthus will not, perforce, intrinsically strain healthcare costs. To a lesser\ndegree, the future pension cost dependency ratio will depend on the post-Brexit\nrelations between the UK and the EU, with \"soft\" alignment on the free movement\nlowering the relative cost of the pension system compared to the \"hard\" one. In\nthe long term, however, the ratio has a rising tendency.\n"
    },
    {
        "paper_id": 1802.0949,
        "authors": "Ashish R. Hota, Shreyas Sundaram",
        "title": "Controlling Human Utilization of Failure-Prone Systems via Taxes",
        "comments": null,
        "journal-ref": "IEEE Transactions on Automatic Control, 2020",
        "doi": "10.1109/TAC.2020.3042481",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a game-theoretic model where individuals compete over a shared\nfailure-prone system or resource. We investigate the effectiveness of a\ntaxation mechanism in controlling the utilization of the resource at the Nash\nequilibrium when the decision-makers have behavioral risk preferences, captured\nby prospect theory. We first observe that heterogeneous prospect-theoretic risk\npreferences can lead to counter-intuitive outcomes. In particular, for\nresources that exhibit network effects, utilization can increase under taxation\nand there may not exist a tax rate that achieves the socially optimal level of\nutilization. We identify conditions under which utilization is monotone and\ncontinuous, and then characterize the range of utilizations that can be\nachieved by a suitable choice of tax rate. We further show that resource\nutilization is higher when players are charged differentiated tax rates\ncompared to the case when all players are charged an identical tax rate, under\nsuitable assumptions.\n"
    },
    {
        "paper_id": 1802.09611,
        "authors": "Peter Carr and Andrey Itkin",
        "title": "An Expanded Local Variance Gamma model",
        "comments": "38 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes an expanded version of the Local Variance Gamma model of\nCarr and Nadtochiy by adding drift to the governing underlying process. Still\nin this new model it is possible to derive an ordinary differential equation\nfor the option price which plays a role of Dupire's equation for the standard\nlocal volatility model. It is shown how calibration of multiple smiles (the\nwhole local volatility surface) can be done in such a case. Further, assuming\nthe local variance to be a piecewise linear function of strike and piecewise\nconstant function of time this ODE is solved in closed form in terms of\nConfluent hypergeometric functions. Calibration of the model to market smiles\ndoes not require solving any optimization problem and, in contrast, can be done\nterm-by-term by solving a system of non-linear algebraic equations for each\nmaturity, which is fast.\n"
    },
    {
        "paper_id": 1802.09864,
        "authors": "Jean-Philippe Aguilar, Jan Korbel",
        "title": "Option Pricing Models Driven by the Space-Time Fractional Diffusion:\n  Series Representation and Applications",
        "comments": null,
        "journal-ref": "Fractal Fract 2018, 2(1), 15",
        "doi": "10.3390/fractalfract2010015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on option pricing models based on space-time\nfractional diffusion. We briefly revise recent results which show that the\noption price can be represented in the terms of rapidly converging\ndouble-series and apply these results to the data from real markets. We focus\non estimation of model parameters from the market data and estimation of\nimplied volatility within the space-time fractional option pricing models.\n"
    },
    {
        "paper_id": 1802.09911,
        "authors": "Frank Z. Xing and Erik Cambria and Lorenzo Malandri and Carlo\n  Vercellis",
        "title": "Discovering Bayesian Market Views for Intelligent Asset Allocation",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-10997-4_8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Along with the advance of opinion mining techniques, public mood has been\nfound to be a key element for stock market prediction. However, how market\nparticipants' behavior is affected by public mood has been rarely discussed.\nConsequently, there has been little progress in leveraging public mood for the\nasset allocation problem, which is preferred in a trusted and interpretable\nway. In order to address the issue of incorporating public mood analyzed from\nsocial media, we propose to formalize public mood into market views, because\nmarket views can be integrated into the modern portfolio theory. In our\nframework, the optimal market views will maximize returns in each period with a\nBayesian asset allocation model. We train two neural models to generate the\nmarket views, and benchmark the model performance on other popular asset\nallocation strategies. Our experimental results suggest that the formalization\nof market views significantly increases the profitability (5% to 10% annually)\nof the simulated portfolio at a given risk level.\n"
    },
    {
        "paper_id": 1802.09954,
        "authors": "Michail Anthropelos and Constantinos Kardaras",
        "title": "Price Impact Under Heterogeneous Beliefs and Restricted Participation",
        "comments": "Final version, accepted for publication in the Journal of Economic\n  Theory",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market in which traders potentially face restrictions\nin trading some of the available securities. Traders are heterogeneous with\nrespect to their beliefs and risk profiles, and the market is assumed thin:\ntraders strategically trade against their price impacts. We prove existence and\nuniqueness of a corresponding equilibrium, and provide an efficient algorithm\nto numerically obtain the equilibrium prices and allocations given market's\ninputs. We find that restrictions may increase the market's welfare if traders\nhave different views regarding the covariance matrix of securities returns. The\nlatter heterogeneity regarding covariance matrix disagreement is essential in\nmodelling; for instance, when traders agree on the covariance matrix,\nrestricting participation in some securities for some traders leaves\nequilibrium prices unaltered in the unrestricted securities, a certainly\nundesirable model effect.\n"
    },
    {
        "paper_id": 1802.09959,
        "authors": "Carey Caginalp and Gunduz Caginalp",
        "title": "Valuation, Liquidity Price, and Stability of Cryptocurrencies",
        "comments": "Published: Proc. Nat. Acad. of Sci",
        "journal-ref": "Caginalp, C., & Caginalp, G. (2018). Opinion: Valuation, liquidity\n  price, and stability of cryptocurrencies. Proceedings of the National Academy\n  of Sciences, 115(6), 1131-1134",
        "doi": "10.1073/pnas.1722031115",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies are examined through the asset flow equations and\nexperimental asset markets. Since tangible value of a typical cryptocurrency is\nnon-existent, the theory suggests that price will gravitate toward liquidity\nvalue, i.e., the total amount of cash available for purchase of the asset\ndivided by the number of units. Thus it is unlikely that cryptocurrencies in\ntheir current form will be stable in the absence of a mechanism of a link to\nvalue.\n"
    },
    {
        "paper_id": 1802.09999,
        "authors": "Bent Flyvbjerg",
        "title": "Planning Fallacy or Hiding Hand: Which Is the Better Explanation?",
        "comments": null,
        "journal-ref": "World Development, vol. 103, March 2018, pp. 383-386",
        "doi": "10.1016/j.worlddev.2017.10.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper asks and answers the question of whether Kahneman's planning\nfallacy or Hirschman's Hiding Hand best explain performance in capital\ninvestment projects. I agree with my critics that the Hiding Hand exists, i.e.,\nsometimes benefit overruns outweigh cost overruns in project planning and\ndelivery. Specifically, I show this happens in one fifth of projects, based on\nthe best and largest dataset that exists. But that was not the main question I\nset out to answer. My main question was whether the Hiding Hand is \"typical,\"\nas claimed by Hirschman. I show this is not the case, with 80 percent of\nprojects not displaying Hiding Hand behavior. Finally, I agree it would be\nimportant to better understand the circumstances where the Hiding Hand actually\nworks. However, if you want to understand how projects \"typically\" work, as\nHirschman said he did, then the theories of the planning fallacy, optimism\nbias, and strategic misrepresentation - according to which cost overruns and\nbenefit shortfalls are the norm - will serve you significantly better than the\nprinciple of the Hiding Hand. The latter will lead you astray, because it is a\nspecial case instead of a typical one.\n"
    },
    {
        "paper_id": 1802.1,
        "authors": "J. Christopher Westland, Tuan Q. Phan, Tianhui Tan",
        "title": "Private Information, Credit Risk and Graph Structure in P2P Lending\n  Networks",
        "comments": "31 pages, 9 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research investigated the potential for improving Peer-to-Peer (P2P)\ncredit scoring by using \"private information\" about communications and travels\nof borrowers. We found that P2P borrowers' ego networks exhibit scale-free\nbehavior driven by underlying preferential attachment mechanisms that connect\nborrowers in a fashion that can be used to predict loan profitability. The\nprojection of these private networks onto networks of mobile phone\ncommunication and geographical locations from mobile phone GPS potentially give\nloan providers access to private information through graph and location metrics\nwhich we used to predict loan profitability. Graph topology was found to be an\nimportant predictor of loan profitability, explaining over 5.5% of variability.\nNetworks of borrower location information explain an additional 19% of the\nprofitability. Machine learning algorithms were applied to the data set\npreviously analyzed to develop the predictive model and resulted in a 4%\nreduction in mean squared error.\n"
    },
    {
        "paper_id": 1802.10001,
        "authors": "J. Christopher Westland",
        "title": "The Information Content of Sarbanes-Oxley in Predicting Security\n  Breaches",
        "comments": "45 pages, 5 figures, 16 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigated publicly reported security breaches of internal controls in\ncorporate systems to determine whether SOX assessments are information bearing\nwith respect to breaches which can lead to materially significant losses and\nmisstatements. SOX Section 404 adverse decisions on effectiveness of controls\noccurred in 100% of credit card data breaches and around 33% of insider\nbreaches. SOX 404 audits provided a contrarian \"effective\" control decisions on\n88% of situations where there was a control breach concerning a portable\ndevice. We found that management and SOX 404 auditors do not general agree on\nthe underlying internal control situation at any time; instead the SOX 404 team\nwas likely to discover material weaknesses and \"educate\" management and\ninternal audit teams about the importance of these control weaknesses. SOX\nattestations were poor at identifying control weaknesses from unintended\ndisclosures, physical losses, hacking and malware. Hazard and occupancy models\nshowed that both SOX 302 and 404 section audits provided information on the\nfrequency of breaches, with SOX 404 being three times as informative as section\n302 reports. The hazard model found an expected 2.88% reduction in breaches\nwhen SOX 302 controls are effective; management \"material weakness'\nattestations provided no information in this structural model, whereas there\nwould be around a 1% increase in breach occurrence when there are significant\ndeficiencies. SOX 404 attestations were the most informative, and a negative\nSOX 404 attestation is projected to increase the frequency of breaches by\naround 8.5%.\n"
    },
    {
        "paper_id": 1802.10003,
        "authors": "Cainan K. de Oliveira, Henrique G. Menck, Pedro Y. Takito, Eliandro\n  Rodrigues Cirilo, Neyva Maria Lopes Romeiro, \\'Erica R. Takano Natti and\n  Paulo Laerte Natti",
        "title": "Stock management (Gest\\~ao de estoques)",
        "comments": "In Portuguese, 17 pages, 12 figures, 7 tables. Conference SEMAT2017",
        "journal-ref": "In: Applied Production Engineering 2. Chapter4. Ponta Grossa:\n  Atena, 2022, v. 2, p. 46-60",
        "doi": "10.22533/at.ed.8032226044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a great need to stock materials for production, but storing\nmaterials comes at a cost. Lack of organization in the inventory can result in\na very high cost for the final product, in addition to generating other\nproblems in the production chain. In this work we present mathematical and\nstatistical methods applicable to stock management. The stock analysis using\nABC curves serves to identify which are the priority items, the most expensive\nand with the highest turnover (demand), and thus determine, through stock\ncontrol models, the purchase lot size and the periodicity that minimize the\ntotal costs of storing these materials. Using the Economic Order Quantity (EOQ)\nmodel and the (Q,R) model, the inventory costs of a company were minimized. The\ncomparison of the results provided by the models was performed.\n"
    },
    {
        "paper_id": 1802.10117,
        "authors": "Jun Aoyagi, Daisuke Adachi",
        "title": "Economic Implications of Blockchain Platforms",
        "comments": "40 pages, with appendix of 18 pages. This project is presented in\n  conferences including SWET at Otaru University of Commerce and WINDS at\n  University of Pennsylvania. This paper was initially circulated under the\n  title \"Fundamental Values of Cryptocurrencies and Blockchain Technology\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an economy with asymmetric information, the smart contract in the\nblockchain protocol mitigates uncertainty. Since, as a new trading platform,\nthe blockchain triggers segmentation of market and differentiation of agents in\nboth the sell and buy sides of the market, it recomposes the asymmetric\ninformation and generates spreads in asset price and quality between itself and\na traditional platform. We show that marginal innovation and sophistication of\nthe smart contract have non-monotonic effects on the trading value in the\nblockchain platform, its fundamental value, the price of cryptocurrency, and\nconsumers' welfare. Moreover, a blockchain manager who controls the level of\nthe innovation of the smart contract has an incentive to keep it lower than the\nfirst best when the underlying information asymmetry is not severe, leading to\nwelfare loss for consumers.\n"
    },
    {
        "paper_id": 1802.10228,
        "authors": "Damiano Brigo, Cristin Buescu, Marco Francischello, Andrea\n  Pallavicini, Marek Rutkowski",
        "title": "Risk-neutral valuation under differential funding costs, defaults and\n  collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a unified valuation theory that incorporates credit risk\n(defaults), collateralization and funding costs, by expanding the replication\napproach to a generality that has not yet been studied previously and reaching\nvaluation when replication is not assumed. This unifying theoretical framework\nclarifies the relationship between the two valuation approaches: the adjusted\ncash flows approach pioneered for example by Brigo, Pallavicini and co-authors\n([12, 13, 34]) and the classic replication approach illustrated for example by\nBielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this\nwork cover most previous papers where the authors studied specific replication\nmodels.\n"
    },
    {
        "paper_id": 1802.10244,
        "authors": "Yang Wang, Dong Wang, Yaodong Wang, You Zhang",
        "title": "RACORN-K: Risk-Aversion Pattern Matching-based Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio selection is the central task for assets management, but it turns\nout to be very challenging. Methods based on pattern matching, particularly the\nCORN-K algorithm, have achieved promising performance on several stock markets.\nA key shortage of the existing pattern matching methods, however, is that the\nrisk is largely ignored when optimizing portfolios, which may lead to\nunreliable profits, particularly in volatile markets. We present a\nrisk-aversion CORN-K algorithm, RACORN-K, that penalizes risk when searching\nfor optimal portfolios. Experiments on four datasets (DJIA, MSCI, SP500(N),\nHSI) demonstrate that the new algorithm can deliver notable and reliable\nimprovements in terms of return, Sharp ratio and maximum drawdown, especially\non volatile markets.\n"
    },
    {
        "paper_id": 1802.10528,
        "authors": "Miguel Alvarez Texocotitla, M. David Alvarez Hernandez, Shani Alvarez\n  Hernandez",
        "title": "Dimensional Analysis in Economics: A Study of the Neoclassical Economic\n  Growth Model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1177/0260107919845269",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fundamental purpose of the present research article is to introduce the\nbasic principles of Dimensional Analysis in the context of the neoclassical\neconomic theory, in order to apply such principles to the fundamental relations\nthat underlay most models of economic growth. In particular, basic instruments\nfrom Dimensional Analysis are used to evaluate the analytical consistency of\nthe Neoclassical economic growth model. The analysis shows that an adjustment\nto the model is required in such a way that the principle of dimensional\nhomogeneity is satisfied.\n"
    },
    {
        "paper_id": 1803.00261,
        "authors": "Andreas M\\\"uhlbacher and Thomas Guhr",
        "title": "Credit Risk Meets Random Matrices: Coping with Non-Stationary Asset\n  Correlations",
        "comments": "Review of a new random matrix approach to credit risk",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review recent progress in modeling credit risk for correlated assets. We\nstart from the Merton model which default events and losses are derived from\nthe asset values at maturity. To estimate the time development of the asset\nvalues, the stock prices are used whose correlations have a strong impact on\nthe loss distribution, particularly on its tails. These correlations are\nnon-stationary which also influences the tails. We account for the asset\nfluctuations by averaging over an ensemble of random matrices that models the\ntruly existing set of measured correlation matrices. As a most welcome side\neffect, this approach drastically reduces the parameter dependence of the loss\ndistribution, allowing us to obtain very explicit results which show\nquantitatively that the heavy tails prevail over diversification benefits even\nfor small correlations. We calibrate our random matrix model with market data\nand show how it is capable of grasping different market situations.\nFurthermore, we present numerical simulations for concurrent portfolio risks,\ni.e., for the joint probability densities of losses for two portfolios. For the\nconvenience of the reader, we give an introduction to the Wishart random matrix\nmodel.\n"
    },
    {
        "paper_id": 1803.00329,
        "authors": "Gechun Liang, Haodong Sun",
        "title": "Dynkin games with Poisson random intervention times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new class of Dynkin games, where the two players are\nallowed to make their stopping decisions at a sequence of exogenous Poisson\narrival times. The value function and the associated optimal stopping strategy\nare characterized by the solution of a backward stochastic differential\nequation. The paper further applies the model to study the optimal conversion\nand calling strategies of convertible bonds, and their asymptotics when the\nPoisson intensity goes to infinity.\n"
    },
    {
        "paper_id": 1803.00345,
        "authors": "Oliver Braganza",
        "title": "Proxyeconomics, the inevitable corruption of proxy-based competition",
        "comments": "This is a work in progress uploaded for sharing and feedback.\n  Depending on contributions even authorships may change",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When society maintains a competitive system to promote an abstract goal,\ncompetition by necessity relies on imperfect proxy measures. For instance\nprofit is used to measure value to consumers, patient volumes to measure\nhospital performance, or the Journal Impact Factor to measure scientific value.\nHere we note that \\textit{any proxy measure in a competitive societal system\nbecomes a target for the competitors, promoting corruption of the measure},\nsuggesting a general applicability of what is best known as Campbell's or\nGoodhart's Law. Indeed, prominent voices have argued that the scientific\nreproducibility crisis or inaction to the threat of global warming represent\ninstances of such competition induced corruption. Moreover, competing\nindividuals often report that competitive pressures limit their ability to act\naccording to the societal goal, suggesting lock-in. However, despite the\nprofound implications, we lack a coherent theory of such a process. Here we\npropose such a theory, formalized as an agent based model, integrating insights\nfrom complex systems theory, contest theory, behavioral economics and cultural\nevolution theory. The model reproduces empirically observed patterns at\nmultiple levels. It further suggests that any system is likely to converge\ntowards an equilibrium level of corruption determined by i) the information\ncaptured in the proxy and ii) the strength of an intrinsic incentive towards\nthe societal goal. Overall, the theory offers mechanistic insight to subjects\nas diverse as the scientific reproducibility crisis and the threat of global\nwarming.\n"
    },
    {
        "paper_id": 1803.00374,
        "authors": "Matteo Farn\\'e and Angela Montanari",
        "title": "A bootstrap test to detect prominent Granger-causalities across\n  frequencies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10614-021-10112-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Granger-causality in the frequency domain is an emerging tool to analyze the\ncausal relationship between two time series. We propose a bootstrap test on\nunconditional and conditional Granger-causality spectra, as well as on their\ndifference, to catch particularly prominent causality cycles in relative terms.\nIn particular, we consider a stochastic process derived applying independently\nthe stationary bootstrap to the original series. Our null hypothesis is that\neach causality or causality difference is equal to the median across\nfrequencies computed on that process. In this way, we are able to disambiguate\ncausalities which depart significantly from the median one obtained ignoring\nthe causality structure. Our test shows power one as the process tends to\nnon-stationarity, thus being more conservative than parametric alternatives. As\nan example, we infer about the relationship between money stock and GDP in the\nEuro Area via our approach, considering inflation, unemployment and interest\nrates as conditioning variables. We point out that during the period 1999-2017\nthe money stock aggregate M1 had a significant impact on economic output at all\nfrequencies, while the opposite relationship is significant only at high\nfrequencies.\n"
    },
    {
        "paper_id": 1803.00464,
        "authors": "Fabrice Balland, Alexandre Boumezoued, Laurent Devineau, Marine\n  Habart, Tom Popa",
        "title": "Mortality data reliability in an internal model",
        "comments": null,
        "journal-ref": "Ann. actuar. sci. 14 (2020) 420-444",
        "doi": "10.1017/S1748499520000081",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we discuss the impact of some mortality data anomalies on an\ninternal model capturing longevity risk in the Solvency 2 framework. In\nparticular, we are concerned with abnormal cohort effects such as those for\ngenerations 1919 and 1920, for which the period tables provided by the Human\nMortality Database show particularly low and high mortality rates respectively.\nTo provide corrected tables for the three countries of interest here (France,\nItaly and West Germany), we use the approach developed by Boumezoued (2016) for\ncountries for which the method applies (France and Italy), and provide an\nextension of the method for West Germany as monthly fertility histories are not\nsufficient to cover the generations of interest. These mortality tables are\ncrucial inputs to stochastic mortality models forecasting future scenarios,\nfrom which the extreme 0,5% longevity improvement can be extracted, allowing\nfor the calculation of the Solvency Capital Requirement (SCR). More precisely,\nto assess the impact of such anomalies in the Solvency II framework, we use a\nsimplified internal model based on three usual stochastic models to project\nmortality rates in the future combined with a closure table methodology for\nolder ages. Correcting this bias obviously improves the data quality of the\nmortality inputs, which is of paramount importance today, and slightly\ndecreases the capital requirement. Overall, the longevity risk assessment\nremains stable, as well as the selection of the stochastic mortality model. As\na collateral gain of this data quality improvement, the more regular estimated\nparameters allow for new insights and a refined assessment regarding longevity\nrisk.\n"
    },
    {
        "paper_id": 1803.00611,
        "authors": "Hassan Dadashi",
        "title": "Optimal investment-consumption problem: post-retirement with minimum\n  guarantee",
        "comments": "40 pages, 18 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2020.07.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal investment-consumption problem for a member of defined\ncontribution plan during the decumulation phase. For a fixed annuitization\ntime, to achieve higher final annuity, we consider a variable consumption rate.\nMoreover, to have a minimum guarantee for the final annuity, a safety level for\nthe wealth process is considered. To solve the stochastic optimal control\nproblem via dynamic programming, we obtain a Hamilton-Jacobi-Bellman (HJB)\nequation on a bounded domain. The existence and uniqueness of classical\nsolutions are proved through the dual transformation. We apply the finite\ndifference method to find numerical approximations of the solution of the HJB\nequation. Finally, the simulation results for the optimal\ninvestment-consumption strategies, optimal wealth process and the final annuity\nfor different admissible ranges of consumption are given. Furthermore, by\ntaking into account the market present value of the cash flows before and after\nthe annuitization, we compare the outcomes of different scenarios.\n"
    },
    {
        "paper_id": 1803.00957,
        "authors": "Dietmar Pfeifer, Andreas M\\\"andle, Olena Ragulina and C\\^ome Girschig",
        "title": "New copulas based on general partitions-of-unity (part III) - the\n  continuous case (extended version)",
        "comments": "23 pages, 47 figures,5 tables, 9 references",
        "journal-ref": "Dependence Modeling (2019), 181 - 201",
        "doi": "10.1515/demo-2019-0009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss a natural extension of infinite discrete\npartition-of-unity copulas which were recently introduced in the literature to\ncontinuous partition of copulas with possible applications in risk management\nand other fields. We present a general simple algorithm to generate such\ncopulas on the basis of the empirical copula from high-dimensional data sets.\nIn particular, our constructions also allow for an implementation of positive\ntail dependence which sometimes is a desirable property of copula modelling, in\nparticular for internal models under Solvency II.\n"
    },
    {
        "paper_id": 1803.01381,
        "authors": "Zhongzhi Lawrence He",
        "title": "Generalized Information Ratio",
        "comments": "47 pages, 1 figure, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Alpha-based performance evaluation may fail to capture correlated residuals\ndue to model errors. This paper proposes using the Generalized Information\nRatio (GIR) to measure performance under misspecified benchmarks. Motivated by\nthe theoretical link between abnormal returns and residual covariance matrix,\nGIR is derived as alphas scaled by the inverse square root of residual\ncovariance matrix. GIR nests alphas and Information Ratio as special cases,\ndepending on the amount of information used in the residual covariance matrix.\nWe show that GIR is robust to various degrees of model misspecification and\nproduces stable out-of-sample returns. Incorporating residual correlations\nleads to substantial gains that alleviate model error concerns of active\nmanagement.\n"
    },
    {
        "paper_id": 1803.01389,
        "authors": "Zhongzhi Lawrence He",
        "title": "Comparing Asset Pricing Models: Distance-based Metrics and Bayesian\n  Interpretations",
        "comments": "50 pages, 1 figure, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In light of the power problems of statistical tests and undisciplined use of\nalpha-based statistics to compare models, this paper proposes a unified set of\ndistance-based performance metrics, derived as the square root of the sum of\nsquared alphas and squared standard errors. The Bayesian investor views model\nperformance as the shortest distance between his dogmatic belief (model-implied\ndistribution) and complete skepticism (data-based distribution) in the model,\nand favors models that produce low dispersion of alphas with high explanatory\npower. In this view, the momentum factor is a crucial addition to the\nfive-factor model of Fama and French (2015), alleviating his prior concern of\nmodel mispricing by -8% to 8% per annum. The distance metrics complement the\nfrequentist p-values with a diagnostic tool to guard against bad models.\n"
    },
    {
        "paper_id": 1803.01527,
        "authors": "Matheus R. Grasselli and Aditya Maheshwari",
        "title": "A comment on 'Testing Goodwin: growth cycles in ten OECD countries'",
        "comments": "8 pages, Cambridge Journal of Economics 2017",
        "journal-ref": null,
        "doi": "10.1093/cje/bex018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the results of Harvie (2000) and show how correcting for a\nreporting mistake in some of the estimated parameter values leads to\nsignificantly different conclusions, including realistic parameter values for\nthe Philips curve and estimated equilibrium employment rates exhibiting on\naverage one tenth of the relative error of those obtained in Harvie (2000).\n"
    },
    {
        "paper_id": 1803.01536,
        "authors": "Matheus R. Grasselli and Aditya Maheshwari",
        "title": "Testing a Goodwin model with general capital accumulation rate",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform econometric tests on a modified Goodwin model where the capital\naccumulation rate is constant but not necessarily equal to one as in the\noriginal model (Goodwin, 1967). In addition to this modification, we find that\naddressing the methodological and reporting issues in Harvie (2000) leads to\nremarkably better results, with near perfect agreement between the estimates of\nequilibrium employment rates and the corresponding empirical averages, as well\nas significantly improved estimates of equilibrium wage shares. Despite its\nsimplicity and obvious limitations, the performance of the modified Goodwin\nmodel implied by our results show that it can be used as a starting point for\nmore sophisticated models for endogenous growth cycles.\n"
    },
    {
        "paper_id": 1803.02012,
        "authors": "Tomasz R. Bielecki, Igor Cialenco and Shibi Feng",
        "title": "A Dynamic Model of Central Counterparty Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a dynamic model of the default waterfall of derivatives CCPs and\npropose a risk sensitive method for sizing the initial margin (IM), and the\ndefault fund (DF) and its allocation among clearing members. Using a Markovian\nstructure model of joint credit migrations, our evaluation of DF takes into\naccount the joint credit quality of clearing members as they evolve over time.\nAnother important aspect of the proposed methodology is the use of the time\nconsistent dynamic risk measures for computation of IM and DF. We carry out a\ncomprehensive numerical study, where, in particular, we analyze the advantages\nof the proposed methodology and its comparison with the currently prevailing\nmethods used in industry.\n"
    },
    {
        "paper_id": 1803.02019,
        "authors": "Ming-Yuan Yang, Sai-Ping Li, Li-Xin Zhong, Fei Ren",
        "title": "Modelling stock correlations with expected returns from investors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock correlations is crucial to asset pricing, investor decision-making, and\nfinancial risk regulations. However, microscopic explanation based on\nagent-based modeling is still lacking. We here propose a model derived from\nminority game for modeling stock correlations, in which an agent's expected\nreturn for one stock is influenced by the historical return of the other stock.\nEach agent makes a decision based on his expected return with reference to\ninformation dissemination and the historical return of the stock. We find that\nthe returns of the stocks are positively (negatively) correlated when agents'\nexpected returns for one stock are positively (negatively) correlated with the\nhistorical return of the other. We provide both numerical simulations and\nanalytical studies and give explanations to stock correlations for cases with\nagents having either homogeneous or heterogeneous expected returns. The result\nstill holds when other factors such as holding decisions and external events\nare included which broadens the practicability of the model.\n"
    },
    {
        "paper_id": 1803.02171,
        "authors": "Bertram D\\\"uring, Lorenzo Pareschi, Giuseppe Toscani",
        "title": "Kinetic models for optimal control of wealth inequalities",
        "comments": "21 pages, 6 figures",
        "journal-ref": "Eur. Phys. J. B 91(10) (2018), 265",
        "doi": "10.1140/epjb/e2018-90138-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and discuss optimal control strategies for kinetic models for\nwealth distribution in a simple market economy, acting to minimize the variance\nof the wealth density among the population. Our analysis is based on a finite\ntime horizon approximation, or model predictive control, of the corresponding\ncontrol problem for the microscopic agents' dynamic and results in an\nalternative theoretical approach to the taxation and redistribution policy at a\nglobal level. It is shown that in general the control is able to modify the\nPareto index of the stationary solution of the corresponding Boltzmann kinetic\nequation, and that this modification can be exactly quantified. Connections\nbetween previous Fokker-Planck based models and taxation-redistribution\npolicies and the present approach are also discussed.\n"
    },
    {
        "paper_id": 1803.02249,
        "authors": "Damir Filipovi\\'c, Sander Willems",
        "title": "A Term Structure Model for Dividends and Interest Rates",
        "comments": "Forthcoming in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last decade, dividends have become a standalone asset class instead\nof a mere side product of an equity investment. We introduce a framework based\non polynomial jump-diffusions to jointly price the term structures of dividends\nand interest rates. Prices for dividend futures, bonds, and the dividend paying\nstock are given in closed form. We present an efficient moment based\napproximation method for option pricing. In a calibration exercise we show that\na parsimonious model specification has a good fit with Euribor interest rate\nswaps and swaptions, Euro Stoxx 50 index dividend futures and dividend options,\nand Euro Stoxx 50 index options.\n"
    },
    {
        "paper_id": 1803.02486,
        "authors": "John Armstrong, Teemu Pennanen, Udomsak Rakwongwan",
        "title": "Pricing index options by static hedging under finite liquidity",
        "comments": "19 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model for indifference pricing in derivatives markets where\nprice quotes have bid-ask spreads and finite quantities. The model quantifies\nthe dependence of the prices and hedging portfolios on an investor's beliefs,\nrisk preferences and financial position as well as on the price quotes.\nComputational techniques of convex optimisation allow for fast computation of\nthe hedging portfolios and prices as well as sensitivities with respect to\nvarious model parameters. We illustrate the techniques by pricing and hedging\nof exotic derivatives on S&P index using call and put options, forward\ncontracts and cash as the hedging instruments. The optimized static hedges\nprovide good approximations of the options payouts and the spreads between\nindifference selling and buying prices are quite narrow as compared with the\nspread between super- and subhedging prices.\n"
    },
    {
        "paper_id": 1803.02546,
        "authors": "Zuo Quan Xu",
        "title": "Pareto optimal moral-hazard-free insurance contracts in behavioral\n  finance framework",
        "comments": "40 pages; 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates Pareto optimal (PO, for short) insurance contracts in\na behavioral finance framework, in which the insured evaluates contracts by the\nrank-dependent utility (RDU) theory and the insurer by the expected value\npremium principle. The incentive compatibility constraint is taken into\naccount, so the contracts are free of moral hazard. The problem is initially\nformulated as a non-concave maximization problem involving Choquet expectation,\nthen turned into a quantile optimization problem and tackled by calculus of\nvariations method. The optimal contracts are expressed by a double-obstacle\nordinary differential equation for a semi-linear second-order elliptic operator\nwith nonlocal boundary conditions. We provide a simple numerical scheme as well\nas a numerical example to calculate the optimal contracts. Let $\\theta$ and\n$m_0$ denote the relative safety loading and the mass of the potential loss at\n0. We find that every moral-hazard-free contract is optimal for infinitely many\nRDU insureds if $0<\\theta<\\frac{m_0}{1-m_0}$; by contrast, some contracts such\nas the full coverage contract are never optimal for any RDU insured if\n$\\theta>\\frac{m_0}{1-m_0}$. We also derive all the PO contracts when either the\ncompensations or the retentions loss monotonicity.\n"
    },
    {
        "paper_id": 1803.0257,
        "authors": "Thomas Santoli and Christoph Siebenbrunner",
        "title": "An ontological investigation of unimaginable events",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that, under mild assumptions, some unimaginable events - which we\nrefer to as Black Swan events - must necessarily occur. It follows as a\ncorollary of our theorem that any computational model of decision-making under\nuncertainty is incomplete in the sense that not all events that occur can be\ntaken into account. In the context of decision theory we argue that this\nconstitutes a stronger sense of uncertainty than Knightian uncertainty.\n"
    },
    {
        "paper_id": 1803.02872,
        "authors": "Luiz G. A. Alves, Giuseppe Mangioni, Isabella Cingolani, Francisco A.\n  Rodrigues, Pietro Panzarasa, and Yamir Moreno",
        "title": "The nested structural organization of the worldwide trade multi-layer\n  network",
        "comments": "Accepted for publication in Scientific Reports",
        "journal-ref": "Scientific Reports 9, 2866 (2019)",
        "doi": "10.1038/s41598-019-39340-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nestedness has traditionally been used to detect assembly patterns in\nmeta-communities and networks of interacting species. Attempts have also been\nmade to uncover nested structures in international trade, typically represented\nas bipartite networks in which connections can be established between countries\n(exporters or importers) and industries. A bipartite representation of trade,\nhowever, inevitably neglects transactions between industries. To fully capture\nthe organization of the global value chain, we draw on the World Input-Output\nDatabase and construct a multi-layer network in which the nodes are the\ncountries, the layers are the industries, and links can be established from\nsellers to buyers within and across industries. We define the buyers' and\nsellers' participation matrices in which the rows are the countries and the\ncolumns are all possible pairs of industries, and then compute nestedness based\non buyers' and sellers' involvement in transactions between and within\nindustries. Drawing on appropriate null models that preserve the countries' or\nlayers' degree distributions in the original multi-layer network, we uncover\nvariations of country- and transaction-based nestedness over time, and identify\nthe countries and industries that most contributed to nestedness. We discuss\nthe implications of our findings for the study of the international production\nnetwork and other real-world systems.\n"
    },
    {
        "paper_id": 1803.02962,
        "authors": "Yong Jiang, Zhongbao Zhou",
        "title": "Does the time horizon of the return predictive effect of investor\n  sentiment vary with stock characteristics? A Granger causality analysis in\n  the frequency domain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Behavioral theories posit that investor sentiment exhibits predictive power\nfor stock returns, whereas there is little study have investigated the\nrelationship between the time horizon of the predictive effect of investor\nsentiment and the firm characteristics. To this end, by using a Granger\ncausality analysis in the frequency domain proposed by Lemmens et al. (2008),\nthis paper examine whether the time horizon of the predictive effect of\ninvestor sentiment on the U.S. returns of stocks vary with different firm\ncharacteristics (e.g., firm size (Size), book-to-market equity (B/M) rate,\noperating profitability (OP) and investment (Inv)). The empirical results\nindicate that investor sentiment has a long-term (more than 12 months) or\nshort-term (less than 12 months) predictive effect on stock returns with\ndifferent firm characteristics. Specifically, the investor sentiment has strong\npredictability in the stock returns for smaller Size stocks, lower B/M stocks\nand lower OP stocks, both in the short term and long term, but only has a\nshort-term predictability for higher quantile ones. The investor sentiment\nmerely has predictability for the returns of smaller Inv stocks in the short\nterm, but has a strong short-term and long-term predictability for larger Inv\nstocks. These results have important implications for the investors for the\nplanning of the short and the long run stock investment strategy.\n"
    },
    {
        "paper_id": 1803.02974,
        "authors": "Ziping Zhao, Rui Zhou, Zhongju Wang, and Daniel P. Palomar",
        "title": "Optimal Portfolio Design for Statistical Arbitrage in Finance",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the optimal mean-reverting portfolio (MRP) design problem is\nconsidered, which plays an important role for the statistical arbitrage (a.k.a.\npairs trading) strategy in financial markets. The target of the optimal MRP\ndesign is to construct a portfolio from the underlying assets that can exhibit\na satisfactory mean reversion property and a desirable variance property. A\ngeneral problem formulation is proposed by considering these two targets and an\ninvestment leverage constraint. To solve this problem, a successive convex\napproximation method is used. The performance of the proposed model and\nalgorithms are verified by numerical simulations.\n"
    },
    {
        "paper_id": 1803.03088,
        "authors": "Ke Wu, Spencer Wheatley, Didier Sornette",
        "title": "Classification of cryptocurrency coins and tokens by the dynamics of\n  their market capitalisations",
        "comments": null,
        "journal-ref": "Royal Society Open Science 5 (9), 2018",
        "doi": "10.1098/rsos.180381",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically verify that the market capitalisations of coins and tokens in\nthe cryptocurrency universe follow power-law distributions with significantly\ndifferent values, with the tail exponent falling between 0.5 and 0.7 for coins,\nand between 1.0 and 1.3 for tokens. We provide a rationale for this, based on a\nsimple proportional growth with birth & death model previously employed to\ndescribe the size distribution of firms, cities, webpages, etc. We empirically\nvalidate the model and its main predictions, in terms of proportional growth\n(Gibrat's law) of the coins and tokens. Estimating the main parameters of the\nmodel, the theoretical predictions for the power-law exponents of coin and\ntoken distributions are in remarkable agreement with the empirical estimations,\ngiven the simplicity of the model. Our results clearly characterize coins as\nbeing \"entrenched incumbents\" and tokens as an \"explosive immature ecosystem\",\nlargely due to massive and exuberant Initial Coin Offering activity in the\ntoken space. The theory predicts that the exponent for tokens should converge\nto 1 in the future, reflecting a more reasonable rate of new entrants\nassociated with genuine technological innovations.\n"
    },
    {
        "paper_id": 1803.03364,
        "authors": "Keegan Mendonca, Vasileios E. Kontosakos, Athanasios A. Pantelous, and\n  Konstantin M. Zuev",
        "title": "Efficient Pricing of Barrier Options on High Volatility Assets using\n  Subset Simulation",
        "comments": "41 pages, 9 figures, 3 tables, available at SSRN:\n  https://ssrn.com/abstract=3132336",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Barrier options are one of the most widely traded exotic options on stock\nexchanges. In this paper, we develop a new stochastic simulation method for\npricing barrier options and estimating the corresponding execution\nprobabilities. We show that the proposed method always outperforms the standard\nMonte Carlo approach and becomes substantially more efficient when the\nunderlying asset has high volatility, while it performs better than multilevel\nMonte Carlo for special cases of barrier options and underlying assets. These\ntheoretical findings are confirmed by numerous simulation results.\n"
    },
    {
        "paper_id": 1803.03477,
        "authors": "Chris Kenyon and Hayato Iida",
        "title": "Behavioural effects on XVA",
        "comments": "13 pages, 1 figure, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bank behaviour is important for pricing XVA because it links different\ncounterparties and thus breaks the usual XVA pricing assumption of counterparty\nindependence. Consider a typical case of a bank hedging a client trade via a\nCCP. On client default the hedge (effects) will be removed (rebalanced). On the\nother hand, if the hedge counterparty defaults the hedge will be replaced. Thus\nif the hedge required initial margin then the default probability driving MVA\nis from the client not from the hedge counterparty. This is the opposite of\nusual assumptions where counterparty XVAs are computed independent of each\nother. Replacement of the hedge counterparty means multiple CVA costs on the\nhedge side need inclusion. Since hedge trades are generally at riskless mid (or\nworse) these costs are paid on the client side, and must be calculated before\nthe replacement hedge counterparties are known. We call these counterparties\nanonymous counterparties. The effects on CVA and MVA will generally be\nexclusive because MVA largely removes CVA, and CVA is hardly relevant for CCPs.\nEffects on KVA and FVA will resemble those on MVA. We provide a theoretical\nframework, including anonymous counterparties, and numerical examples. Pricing\nXVA by considering counterparties in isolation is inadequate and behaviour must\nbe taken into account.\n"
    },
    {
        "paper_id": 1803.03573,
        "authors": "David Bauder, Taras Bodnar, Nestor Parolya, Wolfgang Schmid",
        "title": "Bayesian mean-variance analysis: Optimal portfolio selection under\n  parameter uncertainty",
        "comments": "21 pages, 5 figures",
        "journal-ref": "Quantitative Finance, 21:2, 221-242, 2021",
        "doi": "10.1080/14697688.2020.1748214",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper solves the problem of optimal portfolio choice when the parameters\nof the asset returns distribution, like the mean vector and the covariance\nmatrix are unknown and have to be estimated by using historical data of the\nasset returns. The new approach employs the Bayesian posterior predictive\ndistribution which is the distribution of the future realization of the asset\nreturns given the observable sample. The parameters of the posterior predictive\ndistributions are functions of the observed data values and, consequently, the\nsolution of the optimization problem is expressed in terms of data only and\ndoes not depend on unknown quantities. In contrast, the optimization problem of\nthe traditional approach is based on unknown quantities which are estimated in\nthe second step leading to a suboptimal solution. We also derive a very useful\nstochastic representation of the posterior predictive distribution whose\napplication leads not only to the solution of the considered optimization\nproblem, but provides the posterior predictive distribution of the optimal\nportfolio return used to construct a prediction interval. A Bayesian efficient\nfrontier, a set of optimal portfolios obtained by employing the posterior\npredictive distribution, is constructed as well. Theoretically and using real\ndata we show that the Bayesian efficient frontier outperforms the sample\nefficient frontier, a common estimator of the set of optimal portfolios known\nto be overoptimistic.\n"
    },
    {
        "paper_id": 1803.03861,
        "authors": "Nils Bertschinger and Iurii Mozzhorin and Sitabhra Sinha",
        "title": "Reality-check for Econophysics: Likelihood-based fitting of\n  physics-inspired market models to empirical data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The statistical description and modeling of volatility plays a prominent role\nin econometrics, risk management and finance. GARCH and stochastic volatility\nmodels have been extensively studied and are routinely fitted to market data,\nalbeit providing a phenomenological description only.\n  In contrast, the field of econophysics starts from the premise that modern\neconomies consist of a vast number of individual actors with heterogeneous\nexpectations and incentives. In turn explaining observed market statistics as\nemerging from the collective dynamics of many actors following heterogeneous,\nyet simple, rather mechanistic rules. While such models generate volatility\ndynamics qualitatively matching several stylized facts and thus illustrate the\npossible role of different mechanisms, such as chartist trading, herding\nbehavior etc., rigorous and quantitative statistical fits are still mostly\nlacking.\n  Here, we show how Stan, a modern probabilistic programming language for\nBayesian modeling, can be used to fit several models from econophysics. In\ncontrast to the method of moment matching, which is currently popular, our fits\nare purely likelihood based with many advantages, including systematic model\ncomparison and principled generation of model predictions conditional on the\nobserved price history. In particular, we investigate models by Vikram & Sinha\nand Franke & Westerhoff, and provide a quantitative comparison with standard\neconometric models.\n"
    },
    {
        "paper_id": 1803.03941,
        "authors": "Julien Hok and Shih-Hau Tan",
        "title": "Calibration of Local Volatility Model with Stochastic Interest Rates by\n  Efficient Numerical PDE Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long maturity options or a wide class of hybrid products are evaluated using\na local volatility type modelling for the asset price S(t) with a stochastic\ninterest rate r(t). The calibration of the local volatility function is usually\ntime-consuming because of the multi-dimensional nature of the problem. In this\npaper, we develop a calibration technique based on a partial differential\nequation (PDE) approach which allows an efficient implementation. The essential\nidea is based on solving the derived forward equation satisfied by P(t; S;\nr)Z(t; S; r), where P(t; S; r) represents the risk neutral probability density\nof (S(t); r(t)) and Z(t; S; r) the projection of the stochastic discounting\nfactor in the state variables (S(t); r(t)). The solution provides effective and\nsufficient information for the calibration and pricing. The PDE solver is\nconstructed by using ADI (Alternative Direction Implicit) method based on an\nextension of the Peaceman-Rachford scheme. Furthermore, an efficient algorithm\nto compute all the corrective terms in the local volatility function due to the\nstochastic interest rates is proposed by using the PDE solutions and grid\npoints. Different numerical experiments are examined and compared to\ndemonstrate the results of our theoretical analysis.\n"
    },
    {
        "paper_id": 1803.03996,
        "authors": "Jarno Talponen",
        "title": "Matching distributions: Recovery of implied physical densities from\n  option prices",
        "comments": "JEL classification: G10, G12, G13, C14, D53, C58",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a non-parametric method to recover physical probability\ndistributions of asset returns based on their European option prices and some\nother sparse parametric information. Thus the main problem is similar to the\none considered foir instance in the Recovery Theorem by Ross (2015), except\nthat here we consider a non-dynamical setting. The recovery of the distribution\nis complete, instead of estimating merely a finite number of its parameters,\nsuch as implied volatility, skew or kurtosis. The technique is based on a\nreverse application of recently introduced Distribution Matching by the author\nand is related to the ideas in Distribution Pricing by Dybvig (1988) as well as\ncomonotonicity.\n"
    },
    {
        "paper_id": 1803.04094,
        "authors": "Philippe Casgrain, Sebastian Jaimungal",
        "title": "Mean Field Games with Partial Information for Algorithmic Trading",
        "comments": "34 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are often driven by latent factors which traders cannot\nobserve. Here, we address an algorithmic trading problem with collections of\nheterogeneous agents who aim to perform optimal execution or statistical\narbitrage, where all agents filter the latent states of the world, and their\ntrading actions have permanent and temporary price impact. This leads to a\nlarge stochastic game with heterogeneous agents. We solve the stochastic game\nby investigating its mean-field game (MFG) limit, with sub-populations of\nheterogeneous agents, and, using a convex analysis approach, we show that the\nsolution is characterized by a vector-valued forward-backward stochastic\ndifferential equation (FBSDE). We demonstrate that the FBSDE admits a unique\nsolution, obtain it in closed-form, and characterize the optimal behaviour of\nthe agents in the MFG equilibrium. Moreover, we prove the MFG equilibrium\nprovides an $\\epsilon$-Nash equilibrium for the finite player game. We conclude\nby illustrating the behaviour of agents using the optimal MFG strategy through\nsimulated examples.\n"
    },
    {
        "paper_id": 1803.04213,
        "authors": "Huy N. Chau and Miklos Rasonyi",
        "title": "Robust utility maximization in markets with transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a continuous-time market with proportional transaction costs.\nUnder appropriate assumptions we prove the existence of optimal strategies for\ninvestors who maximize their worst-case utility over a class of possible\nmodels. We consider utility functions defined either on the positive axis or on\nthe whole real line.\n"
    },
    {
        "paper_id": 1803.04483,
        "authors": "Antoine Jacquier and Konstantinos Spiliopoulos",
        "title": "Pathwise moderate deviations for option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a unifying treatment of pathwise moderate deviations for models\ncommonly used in financial applications, and for related integrated\nfunctionals. Suitable scaling allows us to transfer these results into\nsmall-time, large-time and tail asymptotics for diffusions, as well as for\noption prices and realised variances. In passing, we highlight some intuitive\nrelationships between moderate deviations rate functions and their large\ndeviations counterparts; these turn out to be useful for numerical purposes, as\nlarge deviations rate functions are often difficult to compute.\n"
    },
    {
        "paper_id": 1803.04532,
        "authors": "Naoya Yamaguchi, Maiya Hori, Yoshinari Ideguchi",
        "title": "Minimising the expectation value of the procurement cost in electricity\n  markets based on the prediction error of energy consumption",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1186/s40736-018-0038-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we formulate a method for minimising the expectation value of\nthe procurement cost of electricity in two popular spot markets: {\\it\nday-ahead} and {\\it intra-day}, under the assumption that expectation value of\nunit prices and the distributions of prediction errors for the electricity\ndemand traded in two markets are known. The expectation value of the total\nelectricity cost is minimised over two parameters that change the amounts of\nelectricity. Two parameters depend only on the expected unit prices of\nelectricity and the distributions of prediction errors for the electricity\ndemand traded in two markets. That is, even if we do not know the predictions\nfor the electricity demand, we can determine the values of two parameters that\nminimise the expectation value of the procurement cost of electricity in two\npopular spot markets. We demonstrate numerically that the estimate of two\nparameters often results in a small variance of the total electricity cost, and\nillustrate the usefulness of the proposed procurement method through the\nanalysis of actual data.\n"
    },
    {
        "paper_id": 1803.04585,
        "authors": "David Manheim and Scott Garrabrant",
        "title": "Categorizing Variants of Goodhart's Law",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There are several distinct failure modes for overoptimization of systems on\nthe basis of metrics. This occurs when a metric which can be used to improve a\nsystem is used to an extent that further optimization is ineffective or\nharmful, and is sometimes termed Goodhart's Law. This class of failure is often\npoorly understood, partly because terminology for discussing them is ambiguous,\nand partly because discussion using this ambiguous terminology ignores\ndistinctions between different failure modes of this general type. This paper\nexpands on an earlier discussion by Garrabrant, which notes there are \"(at\nleast) four different mechanisms\" that relate to Goodhart's Law. This paper is\nintended to explore these mechanisms further, and specify more clearly how they\noccur. This discussion should be helpful in better understanding these types of\nfailures in economic regulation, in public policy, in machine learning, and in\nArtificial Intelligence alignment. The importance of Goodhart effects depends\non the amount of power directed towards optimizing the proxy, and so the\nincreased optimization power offered by artificial intelligence makes it\nespecially critical for that field.\n"
    },
    {
        "paper_id": 1803.04591,
        "authors": "Atul Deshpande and B. Ross Barmish",
        "title": "A Generalization of the Robust Positive Expectation Theorem for Stock\n  Trading via Feedback Control",
        "comments": "accepted at the European Control Conference 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The starting point of this paper is the so-called Robust Positive Expectation\n(RPE) Theorem, a result which appears in literature in the context of\nSimultaneous Long-Short stock trading. This theorem states that using a\ncombination of two specially-constructed linear feedback trading controllers,\none long and one short, the expected value of the resulting gain-loss function\nis guaranteed to be robustly positive with respect to a large class of\nstochastic processes for the stock price. The main result of this paper is a\ngeneralization of this theorem. Whereas previous work applies to a single\nstock, in this paper, we consider a pair of stocks. To this end, we make two\nassumptions on their expected returns. The first assumption involves price\ncorrelation between the two stocks and the second involves a bounded non-zero\nmomentum condition. With known uncertainty bounds on the parameters associated\nwith these assumptions, our new version of the RPE Theorem provides necessary\nand sufficient conditions on the positive feedback parameter K of the\ncontroller under which robust positive expectation is assured. We also\ndemonstrate that our result generalizes the one existing for the single-stock\ncase. Finally, it is noted that our results also can be interpreted in the\ncontext of pairs trading.\n"
    },
    {
        "paper_id": 1803.04892,
        "authors": "Mathias Pohl and Alexander Ristig and Walter Schachermayer and Ludovic\n  Tangpi",
        "title": "Theoretical and empirical analysis of trading activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the structure of financial markets deals with suitably\ndetermining the functional relation between financial variables. In this\nrespect, important variables are the trading activity, defined here as the\nnumber of trades $N$, the traded volume $V$, the asset price $P$, the squared\nvolatility $\\sigma^2$, the bid-ask spread $S$ and the cost of trading $C$.\nDifferent reasonings result in simple proportionality relations (\"scaling\nlaws\") between these variables. A basic proportionality is established between\nthe trading activity and the squared volatility, i.e., $N \\sim \\sigma^2$. More\nsophisticated relations are the so called 3/2-law $N^{3/2} \\sim \\sigma P V /C$\nand the intriguing scaling $N \\sim (\\sigma P/S)^2$. We prove that these\n\"scaling laws\" are the only possible relations for considered sets of variables\nby means of a well-known argument from physics: dimensional analysis. Moreover,\nwe provide empirical evidence based on data from the NASDAQ stock exchange\nshowing that the sophisticated relations hold with a certain degree of\nuniversality. Finally, we discuss the time scaling of the volatility $\\sigma$,\nwhich turns out to be more subtle than one might naively expect.\n"
    },
    {
        "paper_id": 1803.04894,
        "authors": "Giuseppe Buccheri, Giacomo Bormetti, Fulvio Corsi and Fabrizio Lillo",
        "title": "A Score-Driven Conditional Correlation Model for Noisy and Asynchronous\n  Data: an Application to High-Frequency Covariance Dynamics",
        "comments": "30 pages, 10 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The analysis of the intraday dynamics of correlations among high-frequency\nreturns is challenging due to the presence of asynchronous trading and market\nmicrostructure noise. Both effects may lead to significant data reduction and\nmay severely underestimate correlations if traditional methods for\nlow-frequency data are employed. We propose to model intraday log-prices\nthrough a multivariate local-level model with score-driven covariance matrices\nand to treat asynchronicity as a missing value problem. The main advantages of\nthis approach are: (i) all available data are used when filtering correlations,\n(ii) market microstructure noise is taken into account, (iii) estimation is\nperformed through standard maximum likelihood methods. Our empirical analysis,\nperformed on 1-second NYSE data, shows that opening hours are dominated by\nidiosyncratic risk and that a market factor progressively emerges in the second\npart of the day. The method can be used as a nowcasting tool for high-frequency\ndata, allowing to study the real-time response of covariances to macro-news\nannouncements and to build intraday portfolios with very short optimization\nhorizons.\n"
    },
    {
        "paper_id": 1803.05002,
        "authors": "Dimitri Kroujiline, Maxim Gusev, Dmitry Ushanov, Sergey V. Sharov,\n  Boris Govorkov",
        "title": "An Endogenous Mechanism of Business Cycles",
        "comments": "54 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper suggests that business cycles may be a manifestation of coupled\nreal economy and stock market dynamics and describes a mechanism that can\ngenerate economic fluctuations consistent with observed business cycles. To\nthis end, we seek to incorporate into the macroeconomic framework a dynamic\nstock market model based on opinion interactions (Gusev et al., 2015). We\nderive this model from microfoundations, provide its empirical verification,\ndemonstrate that it contains the efficient market as a particular regime and\nestablish a link through which macroeconomic models can be attached for the\nstudy of real economy and stock market interaction. To examine key effects, we\nlink it with a simple macroeconomic model (Blanchard, 1981). The coupled system\ngenerates nontrivial endogenous dynamics, which exhibit deterministic and\nstochastic features, producing quasiperiodic fluctuations (business cycles). We\nalso inspect this system's behavior in the phase space. The real economy and\nthe stock market coevolve dynamically along the path governed by a\nstochastically-forced dynamical system with two stable equilibria, one where\nthe economy expands and the other where it contracts, resulting in business\ncycles identified as the coherence resonance phenomenon. Thus, the\nincorporation of stock market dynamics into the macroeconomic framework, as\npresented here, allows the derivation of realistic behaviors in a tractable\nsetting.\n"
    },
    {
        "paper_id": 1803.05075,
        "authors": "Mahsa Ghorbani, Edwin K. P. Chong",
        "title": "Stock Price Prediction using Principle Components",
        "comments": "28 Pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The literature provides strong evidence that stock prices can be predicted\nfrom past price data. Principal component analysis (PCA) is a widely used\nmathematical technique for dimensionality reduction and analysis of data by\nidentifying a small number of principal components to explain the variation\nfound in a data set. In this paper, we describe a general method for stock\nprice prediction using covariance information, in terms of a dimension\nreduction operation based on principle component analysis. Projecting the noisy\nobservation onto a principle subspace leads to a well-conditioned problem. We\nillustrate our method on daily stock price values for five companies in\ndifferent industries. We investigate the results based on mean squared error\nand directional change statistic of prediction, as measures of performance, and\nvolatility of prediction as a measure of risk.\n"
    },
    {
        "paper_id": 1803.05244,
        "authors": "Marco Maggis, Andrea Maran",
        "title": "Stochastic Dynamic Utilities and Inter-Temporal Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an axiomatic approach which economically underpins the\nrepresentation of dynamic preferences in terms of a stochastic utility\nfunction, sensitive to the information available to the decision maker. Our\nconstruction is iterative and based on inter-temporal preference relations,\nwhose characterization is inpired by the original intuition given by Debreu's\nState Dependent Utilities (1960).\n"
    },
    {
        "paper_id": 1803.05663,
        "authors": "Spencer Wheatley, Didier Sornette, Tobias Huber, Max Reppen, and\n  Robert N. Gantner",
        "title": "Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law\n  and the LPPLS Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a strong diagnostic for bubbles and crashes in bitcoin, by\nanalyzing the coincidence (and its absence) of fundamental and technical\nindicators. Using a generalized Metcalfe's law based on network properties, a\nfundamental value is quantified and shown to be heavily exceeded, on at least\nfour occasions, by bubbles that grow and burst. In these bubbles, we detect a\nuniversal super-exponential unsustainable growth. We model this universal\npattern with the Log-Periodic Power Law Singularity (LPPLS) model, which\nparsimoniously captures diverse positive feedback phenomena, such as herding\nand imitation. The LPPLS model is shown to provide an ex-ante warning of market\ninstabilities, quantifying a high crash hazard and probabilistic bracket of the\ncrash time consistent with the actual corrections; although, as always, the\nprecise time and trigger (which straw breaks the camel's back) being exogenous\nand unpredictable. Looking forward, our analysis identifies a substantial but\nnot unprecedented overvaluation in the price of bitcoin, suggesting many months\nof volatile sideways bitcoin prices ahead (from the time of writing, March\n2018).\n"
    },
    {
        "paper_id": 1803.0569,
        "authors": "Charles-Albert Lehalle, Othmane Mounjid and Mathieu Rosenbaum",
        "title": "Optimal liquidity-based trading tactics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an agent who needs to buy (or sell) a relatively small amount of\nasset over some fixed short time interval. We work at the highest frequency\nmeaning that we wish to find the optimal tactic to execute our quantity using\nlimit orders, market orders and cancellations. To solve the agent's control\nproblem, we build an order book model and optimize an expected utility function\nbased on our price impact. We derive the equations satisfied by the optimal\nstrategy and solve them numerically. Moreover, we show that our optimal tactic\nenables us to outperform significantly naive execution strategies.\n"
    },
    {
        "paper_id": 1803.05819,
        "authors": "Ali Al-Aradi, Sebastian Jaimungal",
        "title": "Outperformance and Tracking: Dynamic Asset Allocation for Active and\n  Passive Portfolio Management",
        "comments": "Originally posted Jan 31 2017 on SSRN at\n  http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2908552",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management problems are often divided into two types: active and\npassive, where the objective is to outperform and track a preselected\nbenchmark, respectively. Here, we formulate and solve a dynamic asset\nallocation problem that combines these two objectives in a unified framework.\nWe look to maximize the expected growth rate differential between the wealth of\nthe investor's portfolio and that of a performance benchmark while penalizing\nrisk-weighted deviations from a given tracking portfolio. Using stochastic\ncontrol techniques, we provide explicit closed-form expressions for the optimal\nallocation and we show how the optimal strategy can be related to the growth\noptimal portfolio. The admissible benchmarks encompass the class of\nfunctionally generated portfolios (FGPs), which include the market portfolio,\nas the only requirement is that they depend only on the prevailing asset\nvalues. Finally, some numerical experiments are presented to illustrate the\nrisk-reward profile of the optimal allocation.\n"
    },
    {
        "paper_id": 1803.05831,
        "authors": "Ali Al-Aradi, Alvaro Cartea, Sebastian Jaimungal",
        "title": "Technical Uncertainty in Real Options with Learning",
        "comments": "Originally posted Oct 6 2014 on SSRN at\n  http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2505444",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new approach to incorporate uncertainty into the decision to\ninvest in a commodity reserve. The investment is an irreversible one-off\ncapital expenditure, after which the investor receives a stream of cashflow\nfrom extracting the commodity and selling it on the spot market. The investor\nis exposed to price uncertainty and uncertainty in the amount of available\nresources in the reserves (i.e. technical uncertainty). She does, however,\nlearn about the reserve levels through time, which is a key determinant in the\ndecision to invest. To model the reserve level uncertainty and how she learns\nabout the estimates of the commodity in the reserve, we adopt a continuous-time\nMarkov chain model to value the option to invest in the reserve and investigate\nthe value that learning has prior to investment.\n"
    },
    {
        "paper_id": 1803.05861,
        "authors": "Ludovic Cales, Apostolos Chalkis, Ioannis Z.Emiris, Vissarion\n  Fisikopoulos",
        "title": "Practical volume computation of structured convex bodies, and an\n  application to modeling portfolio dependencies and financial crises",
        "comments": "22 pages, 6 figures, Symposium on Computational Geometry 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine volume computation of general-dimensional polytopes and more\ngeneral convex bodies, defined as the intersection of a simplex by a family of\nparallel hyperplanes, and another family of parallel hyperplanes or a family of\nconcentric ellipsoids. Such convex bodies appear in modeling and predicting\nfinancial crises. The impact of crises on the economy (labor, income, etc.)\nmakes its detection of prime interest. Certain features of dependencies in the\nmarkets clearly identify times of turmoil. We describe the relationship between\nasset characteristics by means of a copula; each characteristic is either a\nlinear or quadratic form of the portfolio components, hence the copula can be\nconstructed by computing volumes of convex bodies. We design and implement\npractical algorithms in the exact and approximate setting, we experimentally\njuxtapose them and study the tradeoff of exactness and accuracy for speed. We\nanalyze the following methods in order of increasing generality: rejection\nsampling relying on uniformly sampling the simplex, which is the fastest\napproach, but inaccurate for small volumes; exact formulae based on the\ncomputation of integrals of probability distribution functions; an optimized\nLawrence sign decomposition method, since the polytopes at hand are shown to be\nsimple; Markov chain Monte Carlo algorithms using random walks based on the\nhit-and-run paradigm generalized to nonlinear convex bodies and relying on new\nmethods for computing a ball enclosed; the latter is experimentally extended to\nnon-convex bodies with very encouraging results. Our C++ software, based on\nCGAL and Eigen and available on github, is shown to be very effective in up to\n100 dimensions. Our results offer novel, effective means of computing portfolio\ndependencies and an indicator of financial crises, which is shown to correctly\nidentify past crises.\n"
    },
    {
        "paper_id": 1803.06223,
        "authors": "Xin-Jian Xu, Kuo Wang, Liucun Zhu, Li-Jie Zhang",
        "title": "Efficient construction of threshold networks of stock markets",
        "comments": "latex, 16 pages, 6 figures",
        "journal-ref": "Physica A 509 (2018) 1080-1086",
        "doi": "10.1016/j.physa.2018.06.083",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although the threshold network is one of the most used tools to characterize\nthe underlying structure of a stock market, the identification of the optimal\nthreshold to construct a reliable stock network remains challenging. In this\npaper, the concept of dynamic consistence between the threshold network and the\nstock market is proposed. The optimal threshold is estimated by maximizing the\nconsistence function. The application of this procedure to stocks belonging to\nStandard \\& Pool's 500 Index from January 2006 to December 2011 yields the\nthreshold value 0.28. In analyzing topological characteristics of the generated\nnetwork, three globally financial crises can be distinguished well from the\nevolutionary perspective.\n"
    },
    {
        "paper_id": 1803.06386,
        "authors": "Sima Siami-Namini and Akbar Siami Namin",
        "title": "Forecasting Economics and Financial Time Series: ARIMA vs. LSTM",
        "comments": "19 pages, 2 figures, 1 diagram, 2 listings",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting time series data is an important subject in economics, business,\nand finance. Traditionally, there are several techniques to effectively\nforecast the next lag of time series data such as univariate Autoregressive\n(AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and\nmore notably Autoregressive Integrated Moving Average (ARIMA) with its many\nvariations. In particular, ARIMA model has demonstrated its outperformance in\nprecision and accuracy of predicting the next lags of time series. With the\nrecent advancement in computational power of computers and more importantly\ndeveloping more advanced machine learning algorithms and approaches such as\ndeep learning, new algorithms are developed to forecast time series data. The\nresearch question investigated in this article is that whether and how the\nnewly developed deep learning-based algorithms for forecasting time series\ndata, such as \"Long Short-Term Memory (LSTM)\", are superior to the traditional\nalgorithms. The empirical studies conducted and reported in this article show\nthat deep learning-based algorithms such as LSTM outperform traditional-based\nalgorithms such as ARIMA model. More specifically, the average reduction in\nerror rates obtained by LSTM is between 84 - 87 percent when compared to ARIMA\nindicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that\nthe number of training times, known as \"epoch\" in deep learning, has no effect\non the performance of the trained forecast model and it exhibits a truly random\nbehavior.\n"
    },
    {
        "paper_id": 1803.0646,
        "authors": "Jize Zhang, Tim Leung and Aleksandr Y. Aravkin",
        "title": "Mean Reverting Portfolios via Penalized OU-Likelihood Estimation",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimization-based approach to con- struct a mean-reverting\nportfolio of assets. Our objectives are threefold: (1) design a portfolio that\nis well-represented by an Ornstein-Uhlenbeck process with parameters estimated\nby maximum likelihood, (2) select portfolios with desirable characteristics of\nhigh mean reversion and low variance, and (3) select a parsimonious portfolio,\ni.e. find a small subset of a larger universe of assets that can be used for\nlong and short positions. We present the full problem formulation, a\nspecialized algorithm that exploits partial minimization, and numerical\nexamples using both simulated and empirical price data.\n"
    },
    {
        "paper_id": 1803.06653,
        "authors": "Jo\\~ao Pedro Rodrigues do Carmo",
        "title": "Modeling stock markets through the reconstruction of market processes",
        "comments": "49 pages, dissertation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  There are two possible ways of interpreting the seemingly stochastic nature\nof financial markets: the Efficient Market Hypothesis (EMH) and a set of\nstylized facts that drive the behavior of the markets. We show evidence for\nsome of the stylized facts such as memory-like phenomena in price volatility in\nthe short term, a power-law behavior and non-linear dependencies on the\nreturns.\n  Given this, we construct a model of the market using Markov chains. Then, we\ndevelop an algorithm that can be generalized for any N-symbol alphabet and\nK-length Markov chain. Using this tool, we are able to show that it's, at\nleast, always better than a completely random model such as a Random Walk. The\ncode is written in MATLAB and maintained in GitHub.\n"
    },
    {
        "paper_id": 1803.06738,
        "authors": "Daniele Bianchi and Kenichiro McAlinn",
        "title": "Large-Scale Dynamic Predictive Regressions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel \"decouple-recouple\" dynamic predictive strategy and\ncontribute to the literature on forecasting and economic decision making in a\ndata-rich environment. Under this framework, clusters of predictors generate\ndifferent latent states in the form of predictive densities that are later\nsynthesized within an implied time-varying latent factor model. As a result,\nthe latent inter-dependencies across predictive densities and biases are\nsequentially learned and corrected. Unlike sparse modeling and variable\nselection procedures, we do not assume a priori that there is a given subset of\nactive predictors, which characterize the predictive density of a quantity of\ninterest. We test our procedure by investigating the predictive content of a\nlarge set of financial ratios and macroeconomic variables on both the equity\npremium across different industries and the inflation rate in the U.S., two\ncontexts of topical interest in finance and macroeconomics. We find that our\npredictive synthesis framework generates both statistically and economically\nsignificant out-of-sample benefits while maintaining interpretability of the\nforecasting variables. In addition, the main empirical results highlight that\nour proposed framework outperforms both LASSO-type shrinkage regressions,\nfactor based dimension reduction, sequential variable selection, and\nequal-weighted linear pooling methodologies.\n"
    },
    {
        "paper_id": 1803.06917,
        "authors": "Justin Sirignano and Rama Cont",
        "title": "Universal features of price formation in financial markets: perspectives\n  from Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a large-scale Deep Learning approach applied to a high-frequency\ndatabase containing billions of electronic market quotes and transactions for\nUS equities, we uncover nonparametric evidence for the existence of a universal\nand stationary price formation mechanism relating the dynamics of supply and\ndemand for a stock, as revealed through the order book, to subsequent\nvariations in its market price. We assess the model by testing its\nout-of-sample predictions for the direction of price moves given the history of\nprice and order flow, across a wide range of stocks and time periods. The\nuniversal price formation model is shown to exhibit a remarkably stable\nout-of-sample prediction accuracy across time, for a wide range of stocks from\ndifferent sectors. Interestingly, these results also hold for stocks which are\nnot part of the training sample, showing that the relations captured by the\nmodel are universal and not asset-specific.\n  The universal model --- trained on data from all stocks --- outperforms, in\nterms of out-of-sample prediction accuracy, asset-specific linear and nonlinear\nmodels trained on time series of any given stock, showing that the universal\nnature of price formation weighs in favour of pooling together financial data\nfrom various stocks, rather than designing asset- or sector-specific models as\ncommonly done. Standard data normalizations based on volatility, price level or\naverage spread, or partitioning the training data into sectors or categories\nsuch as large/small tick stocks, do not improve training results. On the other\nhand, inclusion of price and order flow history over many past observations is\nshown to improve forecasting performance, showing evidence of path-dependence\nin price dynamics.\n"
    },
    {
        "paper_id": 1803.06922,
        "authors": "E. Hashorva",
        "title": "Approximation of Some Multivariate Risk Measures for Gaussian Risks",
        "comments": "To appear in JMVA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gaussian random vectors exhibit the loss of dimension phenomena, which relate\nto their joint survival tail behaviour. Besides, the fact that the components\nof such vectors are light-tailed complicates the approximations of various\nmultivariate risk measures significantly. In this contribution we derive\nprecise approximations of marginal mean excess, marginal expected shortfall and\nmultivariate conditional tail expectation of Gaussian random vectors and\nhighlight links with conditional limit theorems. Our study indicates that\nsimilar results hold for elliptical and Gaussian like multivariate risks.\n"
    },
    {
        "paper_id": 1803.07021,
        "authors": "Luca Spadafora, Francesca Sivero, Nicola Picchiotti",
        "title": "Jumping VaR: Order Statistics Volatility Estimator for Jumps\n  Classification and Market Risk Modeling",
        "comments": "31 pages, 29 figures, source code available at\n  https://github.com/sigmaquadro/VolatilityEstimator",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new integrated variance estimator based on order\nstatistics within the framework of jump-diffusion models. Its ability to\ndisentangle the integrated variance from the total process quadratic variation\nis confirmed by both simulated and empirical tests. For practical purposes, we\nintroduce an iterative algorithm to estimate the time-varying volatility and\nthe occurred jumps of log-return time series. Such estimates enable the\ndefinition of a new market risk model for the Value at Risk forecasting. We\nshow empirically that this procedure outperforms the standard historical\nsimulation method applying standard back-testing approach.\n"
    },
    {
        "paper_id": 1803.07041,
        "authors": "Erwan Koch",
        "title": "Spatial risk measures and rate of spatial diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An accurate assessment of the risk of extreme environmental events is of\ngreat importance for populations, authorities and the\nbanking/insurance/reinsurance industry. Koch (2017) introduced a notion of\nspatial risk measure and a corresponding set of axioms which are well suited to\nanalyze the risk due to events having a spatial extent, precisely such as\nenvironmental phenomena. The axiom of asymptotic spatial homogeneity is of\nparticular interest since it allows one to quantify the rate of spatial\ndiversification when the region under consideration becomes large. In this\npaper, we first investigate the general concepts of spatial risk measures and\ncorresponding axioms further and thoroughly explain the usefulness of this\ntheory for both actuarial science and practice. Second, in the case of a\ngeneral cost field, we give sufficient conditions such that spatial risk\nmeasures associated with expectation, variance, Value-at-Risk as well as\nexpected shortfall and induced by this cost field satisfy the axioms of\nasymptotic spatial homogeneity of order $0$, $-2$, $-1$ and $-1$, respectively.\nLast but not least, in the case where the cost field is a function of a\nmax-stable random field, we provide conditions on both the function and the\nmax-stable field ensuring the latter properties. Max-stable random fields are\nrelevant when assessing the risk of extreme events since they appear as a\nnatural extension of multivariate extreme-value theory to the level of random\nfields. Overall, this paper improves our understanding of spatial risk measures\nas well as of their properties with respect to the space variable and\ngeneralizes many results obtained in Koch (2017).\n"
    },
    {
        "paper_id": 1803.07138,
        "authors": "Igor Rivin",
        "title": "Fear Universality and Doubt in Asset price movements",
        "comments": "13 pages, lots of figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We take a look the changes of different asset prices over variable periods,\nusing both traditional and spectral methods, and discover universality\nphenomena which hold (in some cases) across asset classes.\n"
    },
    {
        "paper_id": 1803.07152,
        "authors": "G\\'abor Petneh\\'azi and J\\'ozsef G\\'all",
        "title": "Exploring the predictability of range-based volatility estimators using\n  RNNs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the predictability of several range-based stock volatility\nestimators, and compare them to the standard close-to-close estimator which is\nmost commonly acknowledged as the volatility. The patterns of volatility\nchanges are analyzed using LSTM recurrent neural networks, which are a state of\nthe art method of sequence learning. We implement the analysis on all current\nconstituents of the Dow Jones Industrial Average index, and report averaged\nevaluation results. We find that changes in the values of range-based\nestimators are more predictable than that of the estimator using daily closing\nvalues only.\n"
    },
    {
        "paper_id": 1803.07216,
        "authors": "David Farahany, Kenneth Jackson, Sebastian Jaimungal",
        "title": "Mixing LSMC and PDE Methods to Price Bermudan Options",
        "comments": "The first version of this paper was submitted to SSRN in November\n  2016: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870962",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a mixed least squares Monte Carlo-partial differential equation\n(LSMC-PDE) method for pricing Bermudan style options on assets whose volatility\nis stochastic. The algorithm is formulated for an arbitrary number of assets\nand volatility processes and we prove the algorithm converges almost surely for\na class of models. We also discuss two methods to improve the algorithm's\ncomputational complexity. Our numerical examples focus on the single ($2d$) and\nmulti-dimensional ($4d$) Heston models and we compare our hybrid algorithm with\nclassical LSMC approaches. In each case, we find that the hybrid algorithm\noutperforms standard LSMC in terms of estimating prices and optimal exercise\nboundaries.\n"
    },
    {
        "paper_id": 1803.07247,
        "authors": "Ziping Zhao, Daniel P. Palomar",
        "title": "Sparse Reduced Rank Regression With Nonconvex Regularization",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the estimation problem for sparse reduced rank regression\n(SRRR) model is considered. The SRRR model is widely used for dimension\nreduction and variable selection with applications in signal processing,\neconometrics, etc. The problem is formulated to minimize the least squares loss\nwith a sparsity-inducing penalty considering an orthogonality constraint.\nConvex sparsity-inducing functions have been used for SRRR in literature. In\nthis work, a nonconvex function is proposed for better sparsity inducing. An\nefficient algorithm is developed based on the alternating minimization (or\nprojection) method to solve the nonconvex optimization problem. Numerical\nsimulations show that the proposed algorithm is much more efficient compared to\nthe benchmark methods and the nonconvex function can result in a better\nestimation accuracy.\n"
    },
    {
        "paper_id": 1803.0759,
        "authors": "Janine Balter and Alexander J. McNeil",
        "title": "On the Basel Liquidity Formula for Elliptical Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A justification of the Basel liquidity formula for risk capital in the\ntrading book is given under the assumption that market risk-factor changes form\na Gaussian white noise process over 10-day time steps and changes to P&L are\nlinear in the risk-factor changes. A generalization of the formula is derived\nunder the more general assumption that risk-factor changes are multivariate\nelliptical. It is shown that the Basel formula tends to be conservative when\nthe elliptical distributions are from the heavier-tailed generalized hyperbolic\nfamily. As a by-product of the analysis a Fourier approach to calculating\nexpected shortfall for general symmetric loss distributions is developed.\n"
    },
    {
        "paper_id": 1803.0772,
        "authors": "Ruimeng Hu",
        "title": "Asymptotic Optimal Portfolio in Fast Mean-reverting Stochastic\n  Environments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the portfolio optimization problem when the investor's\nutility is general and the return and volatility of the risky asset are fast\nmean-reverting, which are important to capture the fast-time scale in the\nmodeling of stock price volatility. Motivated by the heuristic derivation in\n[J.-P. Fouque, R. Sircar and T. Zariphopoulou, \\emph{Mathematical Finance},\n2016], we propose a zeroth order strategy, and show its asymptotic optimality\nwithin a specific (smaller) family of admissible strategies under proper\nassumptions. This optimality result is achieved by establishing a first order\napproximation of the problem value associated to this proposed strategy using\nsingular perturbation method, and estimating the risk-tolerance functions. The\nresults are natural extensions of our previous work on portfolio optimization\nin a slowly varying stochastic environment [J.-P. Fouque and R. Hu, \\emph{SIAM\nJournal on Control and Optimization}, 2017], and together they form a whole\npicture of analyzing portfolio optimization in both fast and slow environments.\n"
    },
    {
        "paper_id": 1803.07843,
        "authors": "Alan White",
        "title": "Pricing Credit Default Swap Subject to Counterparty Risk and\n  Collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a new model for valuing a credit default swap (CDS)\ncontract that is affected by multiple credit risks of the buyer, seller and\nreference entity. We show that default dependency has a significant impact on\nasset pricing. In fact, correlated default risk is one of the most pervasive\nthreats in financial markets. We also show that a fully collateralized CDS is\nnot equivalent to a risk-free one. In other words, full collateralization\ncannot eliminate counterparty risk completely in the CDS market.\n"
    },
    {
        "paper_id": 1803.07904,
        "authors": "Giovanni Paolinelli and Gianni Arioli",
        "title": "A path integral based model for stocks and order dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.07.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model for the short-term dynamics of financial assets based on\nan application to finance of quantum gauge theory, developing ideas of Ilinski.\nWe present a numerical algorithm for the computation of the probability\ndistribution of prices and compare the results with APPLE stocks prices and the\nS&P500 index.\n"
    },
    {
        "paper_id": 1803.0816,
        "authors": "Angelos Dassios, Luting Li",
        "title": "An Economic Bubble Model and Its First Passage Time",
        "comments": "33 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new diffusion process Xt to describe asset prices within an\neconomic bubble cycle. The main feature of the process, which differs from\nexisting models, is the drift term where a mean-reversion is taken based on an\nexponential decay of the scaled price. Our study shows the scaling factor on Xt\nis crucial for modelling economic bubbles as it mitigates the dependence\nstructure between the price and parameters in the model. We prove both the\nprocess and its first passage time are well-defined. An efficient calibration\nscheme, together with the probability density function for the process are\ngiven. Moreover, by employing the perturbation technique, we deduce the\nclosed-form density for the downward first passage time, which therefore can be\nused in estimating the burst time of an economic bubble. The object of this\nstudy is to understand the asset price dynamics when a financial bubble is\nbelieved to form, and correspondingly provide estimates to the bubble crash\ntime. Calibration examples on the US dot-com bubble and the 2007 Chinese stock\nmarket crash verify the effectiveness of the model itself. The example on\nBitCoin prediction confirms that we can provide meaningful estimate on the\ndownward probability for asset prices.\n"
    },
    {
        "paper_id": 1803.08166,
        "authors": "Matteo Basei",
        "title": "Optimal price management in retail energy markets: an impulse control\n  problem with asymptotic estimates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a retailer who buys energy in the wholesale market and resells it\nto final consumers. The retailer has to decide when to intervene to change the\nprice he asks to his customers, in order to maximize his income. We model the\nproblem as an infinite-horizon stochastic impulse control problem. We\ncharacterize an optimal price strategy and provide analytical existence results\nfor the equations involved. We then investigate the dependence on the\nintervention cost. In particular, we prove that the measure of the continuation\nregion is asymptotic to the fourth root of the cost. Finally, we provide some\nnumerical results and consider a suitable extension of the model.\n"
    },
    {
        "paper_id": 1803.08169,
        "authors": "Nils Detering, Thilo Meyer-Brandis, Konstantinos Panagiotou, Daniel\n  Ritter",
        "title": "Financial Contagion in a Generalized Stochastic Block Model",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most defining features of the global financial network is its\ninherent complex and intertwined structure. From the perspective of systemic\nrisk it is important to understand the influence of this network structure on\ndefault contagion. Using sparse random graphs to model the financial network,\nasymptotic methods turned out powerful to analytically describe the contagion\nprocess and to make statements about resilience. So far, however, they have\nbeen limited to so-called {\\em rank one} models in which informally the only\nnetwork parameter is the degree sequence (see (Amini et. al. 2016) and\n(Detering et. al. 2019) for example) and the contagion process can be described\nby a one dimensional fix-point equation. These networks fail to account for a\npronounced block structure such as core/periphery or a network composed of\ndifferent connected blocks for different countries. We present a much more\ngeneral model here, where we distinguish vertices (institutions) of different\ntypes and let edge probabilities and exposures depend on the types of both, the\nreceiving and the sending vertex plus additional parameters. Our main result\nallows to compute explicitly the systemic damage caused by some initial local\nshock event, and we derive a complete characterisation of resilient\nrespectively non-resilient financial systems. This is the first instance that\ndefault contagion is rigorously studied in a model outside the class of rank\none models and several technical challenges arise. Moreover, in contrast to\nprevious work, in which networks could be classified as resilient or non\nresilient, independent of the distribution of the shock, information about the\nshock becomes important in our model and a more refined resilience condition\narises. Among other applications of our theory we derive resilience conditions\nfor the global network based on subnetwork conditions only.\n"
    },
    {
        "paper_id": 1803.0817,
        "authors": "Kevin He",
        "title": "Mislearning from Censored Data: The Gambler's Fallacy and Other\n  Correlational Mistakes in Optimal-Stopping Problems",
        "comments": null,
        "journal-ref": "Theoretical Economics 17(3):1269-1312, 2022",
        "doi": "10.3982/TE4657",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study endogenous learning dynamics for people who misperceive intertemporal\ncorrelations in random sequences. Biased agents face an optimal-stopping\nproblem. They are uncertain about the underlying distribution and learn its\nparameters from predecessors. Agents stop when early draws are \"good enough,\"\nso predecessors' experiences contain negative streaks but not positive streaks.\nWhen agents wrongly expect systematic reversals (the \"gambler's fallacy\"), they\nunderstate the likelihood of consecutive below-average draws, converge to\nover-pessimistic beliefs about the distribution's mean, and stop too early.\nAgents uncertain about the distribution's variance overestimate it to an extent\nthat depends on predecessors' stopping thresholds. I also analyze how other\nmisperceptions of intertemporal correlation interact with endogenous data\ncensoring.\n"
    },
    {
        "paper_id": 1803.08336,
        "authors": "Jin Hyuk Choi, Kasper Larsen, Duane J. Seppi",
        "title": "Equilibrium Effects of Intraday Order-Splitting Benchmarks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a continuous-time model of intraday trading, pricing, and\nliquidity with dynamic TWAP and VWAP benchmarks. The model is solved in\nclosed-form for the competitive equilibrium and also for non-price-taking\nequilibria. The intraday trajectories of TWAP trading targets cause predictable\nintraday patterns of price pressure, and randomness in VWAP target trajectories\ninduces additional randomness in intraday price-pressure patterns. TWAP and\nVWAP trading both reduce market liquidity and increase price volatility\nrelative to just terminal trading targets alone. The model is computationally\ntractable, which lets us provide a number of numerical illustrations.\n"
    },
    {
        "paper_id": 1803.0839,
        "authors": "Kevin Primicerio, Damien Challet",
        "title": "Large large-trader activity weakens the long memory of limit order\n  markets",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using more than 6.7 billions of trades, we explore how the tick-by-tick\ndynamics of limit order books depends on the aggregate actions of large\ninvestment funds on a much larger (quarterly) timescale. In particular, we find\nthat the well-established long memory of market order signs is markedly weaker\nwhen large investment funds trade either in a directional way and even weaker\nwhen their aggregate participation ratio is large. Conversely, we investigate\nto what respect a weaker memory of market order signs predicts that an asset is\nbeing actively traded by large funds. Theoretical arguments suggest two simple\nmechanisms that contribute to the observed effect: a larger number of active\nmeta-orders and a modification of the distribution of size of meta-orders.\nEmpirical evidence suggests that the number of active meta-orders is the most\nimportant contributor to the loss of market order sign memory.\n"
    },
    {
        "paper_id": 1803.08405,
        "authors": "Stjepan Begu\\v{s}i\\'c, Zvonko Kostanj\\v{c}ar, H. Eugene Stanley, and\n  Boris Podobnik",
        "title": "Scaling properties of extreme price fluctuations in Bitcoin markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.06.131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Detection of power-law behavior and studies of scaling exponents uncover the\ncharacteristics of complexity in many real world phenomena. The complexity of\nfinancial markets has always presented challenging issues and provided\ninteresting findings, such as the inverse cubic law in the tails of stock price\nfluctuation distributions. Motivated by the rise of novel digital assets based\non blockchain technology, we study the distributions of cryptocurrency price\nfluctuations. We consider Bitcoin returns over various time intervals and from\nmultiple digital exchanges, in order to investigate the existence of universal\nscaling behavior in the tails, and ascertain whether the scaling exponent\nsupports the presence of a finite second moment. We provide empirical evidence\non slowly decaying tails in the distributions of returns over multiple time\nintervals and different exchanges, corresponding to a power-law. We estimate\nthe scaling exponent and find an asymptotic power-law behavior with 2 <\n{\\alpha} < 2.5 suggesting that Bitcoin returns, in addition to being more\nvolatile, also exhibit heavier tails than stocks, which are known to be around\n3. Our results also imply the existence of a finite second moment, thus\nproviding a fundamental basis for the usage of standard financial theories and\ncovariance-based techniques in risk management and portfolio optimization\nscenarios.\n"
    },
    {
        "paper_id": 1803.08698,
        "authors": "Mario Coccia",
        "title": "Measurement of the evolution of technology: A new perspective",
        "comments": "43 pages; 5 Figure; 7 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A fundamental problem in technological studies is how to measure the\nevolution of technology. The literature has suggested several approaches to\nmeasuring the level of technology (or state-of-the-art) and changes in\ntechnology. However, the measurement of technological advances and\ntechnological evolution is often a complex and elusive topic in science. The\nstudy here starts by establishing a conceptual framework of technological\nevolution based on the theory of technological parasitism, in broad analogy\nwith biology. Then, the measurement of the evolution of technology is modelled\nin terms of morphological changes within complex systems considering the\ninteraction between a host technology and its subsystems of technology. The\ncoefficient of evolutionary growth of the model here indicates the grade and\ntype of the evolutionary route of a technology. This coefficient is quantified\nin real instances using historical data of farm tractor, freight locomotive and\nelectricity generation technology in steam-powered plants and\ninternal-combustion plants. Overall, then, it seems that the approach here is\nappropriate in grasping the typology of evolution of complex systems of\ntechnology and in predicting which technologies are likeliest to evolve\nrapidly.\n"
    },
    {
        "paper_id": 1803.08803,
        "authors": "Jaehyuk Choi and Sungchan Shin",
        "title": "Fast swaption pricing in Gaussian term structure models",
        "comments": null,
        "journal-ref": "Mathematical Finance, 26(4):962-982, 2016",
        "doi": "10.1111/mafi.12077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a fast and accurate numerical method for pricing European\nswaptions in multi-factor Gaussian term structure models. Our method can be\nused to accelerate the calibration of such models to the volatility surface.\nThe pricing of an interest rate option in such a model involves evaluating a\nmulti-dimensional integral of the payoff of the claim on a domain where the\npayoff is positive. In our method, we approximate the exercise boundary of the\nstate space by a hyperplane tangent to the maximum probability point on the\nboundary and simplify the multi-dimensional integration into an analytical\nform. The maximum probability point can be determined using the gradient\ndescent method. We demonstrate that our method is superior to previous methods\nby comparing the results to the price obtained by numerical integration.\n"
    },
    {
        "paper_id": 1803.08831,
        "authors": "Wieger Hinderks, Andreas Wagner, Ralf Korn",
        "title": "A structural Heath-Jarrow-Morton framework for consistent intraday,\n  spot, and futures electricity prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a flexible HJM-type framework that allows for\nconsistent modelling of intraday, spot, futures, and option prices. This\nframework is based on stochastic processes with economic interpretations and\nconsistent with the initial term structure given in the form of a price forward\ncurve. Furthermore, the framework allows for existing day-ahead spot price\nmodels to be used in an HJM setting. We include several explicit examples of\nclassical spot price models but also show how structural models and factor\nmodels can be formulated within the framework.\n"
    },
    {
        "paper_id": 1803.09422,
        "authors": "Yu-Lei Wan (ECUST), Gang-Jin Wang (HNU), Zhi-Qiang Jiang (ECUST),\n  Wen-Jie Xie (ECUST), Wei-Xing Zhou (ECUST)",
        "title": "The cooling-off effect of price limits in the Chinese stock markets",
        "comments": null,
        "journal-ref": "Physica A 505, 153-163 (2018)",
        "doi": "10.1016/j.physa.2018.03.066",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the cooling-off effect (opposite to the magnet\neffect) from two aspects. Firstly, from the viewpoint of dynamics, we study the\nexistence of the cooling-off effect by following the dynamical evolution of\nsome financial variables over a period of time before the stock price hits its\nlimit. Secondly, from the probability perspective, we investigate, with the\nlogit model, the existence of the cooling-off effect through analyzing the\nhigh-frequency data of all A-share common stocks traded on the Shanghai Stock\nExchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the\ntrading period from the opening phase prior to the moment that the stock price\nhits its limits. A comparison is made of the properties between up-limit hits\nand down-limit hits, and the possible difference will also be compared between\nbullish and bearish market state by dividing the whole period into three\nalternating bullish periods and three bearish periods. We find that the\ncooling-off effect emerges for both up-limit hits and down-limit hits, and the\ncooling-off effect of the down-limit hits is stronger than that of the up-limit\nhits. The difference of the cooling-off effect between bullish period and\nbearish period is quite modest. Moreover, we examine the sub-optimal orders\neffect, and infer that the professional individual investors and institutional\ninvestors play a positive role in the cooling-off effects. All these findings\nindicate that the price limit trading rule exerts a positive effect on\nmaintaining the stability of the Chinese stock markets.\n"
    },
    {
        "paper_id": 1803.09432,
        "authors": "Hai-Chuan Xu (ECUST), Wei-Xing Zhou (ECUST), Didier Sornette (ETH\n  Zurich)",
        "title": "Time-dependent lead-lag relationship between the onshore and offshore\n  Renminbi exchange rates",
        "comments": null,
        "journal-ref": "Journal of International Financial Markets, Institutions & Money\n  49, 173-183 (2017)",
        "doi": "10.1016/j.intfin.2017.05.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ the thermal optimal path method to explore both the long-term and\nshort-term interaction patterns between the onshore CNY and offshore CNH\nexchange rates (2012-2015). For the daily data, the CNY and CNH exchange rates\nshow a weak alternate lead-lag structure in most of the time periods. When CNY\nand CNH display a large disparity, the lead-lag relationship is uncertain and\ndepends on the prevailing market factors. The minute-scale interaction pattern\nbetween the CNY and CNH exchange rates change over time according to different\nmarket situations. We find that US dollar appreciation is associated with a\nlead-lag relationship running from offshore to onshore, while a (contrarian)\nRenminbi appreciation is associated with a lead-lag relationship running from\nonshore to offshore. These results are robust with respect to different\nsub-sample analyses and variations of the key smoothing parameter of the TOP\nmethod.\n"
    },
    {
        "paper_id": 1803.09444,
        "authors": "Markus Hess",
        "title": "Cliquet option pricing with Meixner processes",
        "comments": "Published at https://doi.org/10.15559/18-VMSTA96 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 1,\n  81-97",
        "doi": "10.15559/18-VMSTA96",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the pricing of cliquet options in a geometric Meixner model.\nThe considered option is of monthly sum cap style while the underlying stock\nprice model is driven by a pure-jump Meixner--L\\'{e}vy process yielding Meixner\ndistributed log-returns. In this setting, we infer semi-analytic expressions\nfor the cliquet option price by using the probability distribution function of\nthe driving Meixner--L\\'{e}vy process and by an application of Fourier\ntransform techniques. In an introductory section, we compile various facts on\nthe Meixner distribution and the related class of Meixner--L\\'{e}vy processes.\nWe also propose a customized measure change preserving the Meixner distribution\nof any Meixner process.\n"
    },
    {
        "paper_id": 1803.09514,
        "authors": "Charu Sharma (Shiv Nadar University, UP), Amber Habib (Shiv Nadar\n  University, UP), Sunil Bowry (Shiv Nadar University, UP)",
        "title": "Cluster analysis of stocks using price movements of high frequency data\n  from National Stock Exchange",
        "comments": "presented in conference IPECS2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to develop new techniques to describe joint behavior of\nstocks, beyond regression and correlation. For example, we want to identify the\nclusters of the stocks that move together. Our work is based on applying Kernel\nPrincipal Component Analysis(KPCA) and Functional Principal Component\nAnalysis(FPCA) to high frequency data from NSE. Since we dealt with high\nfrequency data with a tick size of 30 seconds, FPCA seems to be an ideal\nchoice. FPCA is a functional variant of PCA where each sample point is\nconsidered to be a function in Hilbert space L^2. On the other hand, KPCA is an\nextension of PCA using kernel methods. Results obtained from FPCA and Gaussian\nKernel PCA seems to be in synergy but with a lag. There were two prominent\nclusters that showed up in our analysis, one corresponding to the banking\nsector and another corresponding to the IT sector. The other smaller clusters\nwere seen from the automobile industry and the energy sector. IT sector was\nseen interacting with these small clusters. The learning gained from these\ninteractions is substantial as one can use it significantly to develop trading\nstrategies for intraday traders.\n"
    },
    {
        "paper_id": 1803.09898,
        "authors": "Francesca Biagini, Jean-Pierre Fouque, Marco Frittelli, Thilo\n  Meyer-Brandis",
        "title": "On Fairness of Systemic Risk Measures",
        "comments": "Keywords}: Systemic risk measures, random allocations, risk\n  allocation, fairness",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our previous paper, \"A Unified Approach to Systemic Risk Measures via\nAcceptance Set\" (\\textit{Mathematical Finance, 2018}), we have introduced a\ngeneral class of systemic risk measures that allow for random allocations to\nindividual banks before aggregation of their risks. In the present paper, we\nprove the dual representation of a particular subclass of such systemic risk\nmeasures and the existence and uniqueness of the optimal allocation related to\nthem. We also introduce an associated utility maximization problem which has\nthe same optimal solution as the systemic risk measure. In addition, the\noptimizer in the dual formulation provides a \\textit{risk allocation} which is\nfair from the point of view of the individual financial institutions. The case\nwith exponential utilities which allows for explicit computation is treated in\ndetails.\n"
    },
    {
        "paper_id": 1803.09935,
        "authors": "Majid Einian, and Farshad Ranjbar Ravasan",
        "title": "A Perfect Specialization Model for Gravity Equation in Bilateral Trade\n  based on Production Structure",
        "comments": "8 pages, 2 figures, 3 tables",
        "journal-ref": "International Economic Studies, Vol. 45, No. 2, 2015, pp. 1-8",
        "doi": "10.22108/IES.2017.79036",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although initially originated as a totally empirical relationship to explain\nthe volume of trade between two partners, gravity equation has been the focus\nof several theoretic models that try to explain it. Specialization models are\nof great importance in providing a solid theoretic ground for gravity equation\nin bilateral trade. Some research papers try to improve specialization models\nby adding imperfect specialization to model, but we believe it is unnecessary\ncomplication. We provide a perfect specialization model based on the phenomenon\nwe call tradability, which overcomes the problems with simpler initial. We\nprovide empirical evidence using estimates on panel data of bilateral trade of\n40 countries over 10 years that support the theoretical model. The empirical\nresults have implied that tradability is the only reason for deviations of data\nfrom basic perfect specialization models.\n"
    },
    {
        "paper_id": 1803.10128,
        "authors": "Tahir Choulli and Sina Yansori",
        "title": "Explicit description of all deflators for market models under random\n  horizon with applications to NFLVR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers an initial market model, specified by its underlying\nassets $S$ and its flow of information $\\mathbb F$, and an arbitrary random\ntime $\\tau$ which might not be an $\\mathbb F$-stopping time. As the death time\nand the default time (that $\\tau$ might represent) can be seen when they occur\nonly, the progressive enlargement of $\\mathbb F$ with $\\tau$ sounds tailor-fit\nfor modelling the new flow of information $\\mathbb G$ that incorporates both\n$\\mathbb F$ and $\\tau$. In this setting of informational market, the first\nprincipal goal resides in describing as explicitly as possible the set of all\ndeflators for $(S^{\\tau}, \\mathbb G)$, while the second principal goal lies in\naddressing the No-Free-Lunch-with-Vanishing-Risk concept (NFLVR hereafter) for\n$(S^{\\tau}, \\mathbb G)$. Besides this direct application to NFLVR, the set of\nall deflators constitutes the dual set of all \"admissible\" wealth processes for\nthe stopped model $(S^{\\tau},\\mathbb G)$, and hence it is vital in many hedging\nand pricing related optimization problems. Thanks to the results of Choulli et\nal. [7], on martingales classification and representation for progressive\nenlarged filtration, our two main goals are fully achieved in different\nversions, when the survival probability never vanishes. The results are\nillustrated on the two particular cases when $(S,\\mathbb F)$ follows the\njump-diffusion model and the discrete-time model.\n"
    },
    {
        "paper_id": 1803.10376,
        "authors": "Axel A. Araneda and Marcelo J. Villena",
        "title": "Computing the CEV option pricing formula using the semiclassical\n  approximation of path integral",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Constant Elasticity of Variance (CEV) model significantly outperforms the\nBlack-Scholes (BS) model in forecasting both prices and options. Furthermore,\nthe CEV model has a marked advantage in capturing basic empirical regularities\nsuch as: heteroscedasticity, the leverage effect, and the volatility smile. In\nfact, the performance of the CEV model is comparable to most stochastic\nvolatility models, but it is considerable easier to implement and calibrate.\nNevertheless, the standard CEV model solution, using the non-central chi-square\napproach, still presents high computational times, specially when: i) the\nmaturity is small, ii) the volatility is low, or iii) the elasticity of the\nvariance tends to zero. In this paper, a new numerical method for computing the\nCEV model is developed. This new approach is based on the semiclassical\napproximation of Feynman's path integral. Our simulations show that the method\nis efficient and accurate compared to the standard CEV solution considering the\npricing of European call options.\n"
    },
    {
        "paper_id": 1803.11161,
        "authors": "Hassan B. Ghassan, Hassan R. Al-Hajhoj, Faruk Balli",
        "title": "Bi-Demographic Changes and Current Account using SVAR Modeling",
        "comments": "50 pages, 18 figures, 7 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper aims to explore the impacts of bi-demographic structure on the\ncurrent account and growth. Using a SVAR modeling, we track the dynamic impacts\nbetween these underlying variables. New insights have been developed about the\ndynamic interrelation between population growth, current account and economic\ngrowth. The long-run net impact on economic growth of the domestic working\npopulation growth and demand labor for emigrants is positive, due to the\npredominant contribution of skilled emigrant workers. Besides, the positive\nlong-run contribution of emigrant workers to the current account growth largely\ncompensates the negative contribution from the native population, because of\nthe predominance of skilled compared to unskilled workforce. We find that a\npositive shock in demand labor for emigrant workers leads to an increasing\neffect on native active age ratio. Thus, the emigrants appear to be more\ncomplements than substitutes for native workers.\n"
    },
    {
        "paper_id": 1803.11309,
        "authors": "Michael Ludkovski and Aditya Maheshwari",
        "title": "Simulation Methods for Stochastic Storage Problems: A Statistical\n  Learning Perspective",
        "comments": "32 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider solution of stochastic storage problems through regression Monte\nCarlo (RMC) methods. Taking a statistical learning perspective, we develop the\ndynamic emulation algorithm (DEA) that unifies the different existing\napproaches in a single modular template. We then investigate the two central\naspects of regression architecture and experimental design that constitute DEA.\nFor the regression piece, we discuss various non-parametric approaches, in\nparticular introducing the use of Gaussian process regression in the context of\nstochastic storage. For simulation design, we compare the performance of\ntraditional design (grid discretization), against space-filling, and several\nadaptive alternatives. The overall DEA template is illustrated with multiple\nexamples drawing from natural gas storage valuation and optimal control of\nback-up generator in a microgrid.\n"
    },
    {
        "paper_id": 1803.11467,
        "authors": "Rongju Zhang, Nicolas Langren\\'e, Yu Tian, Zili Zhu, Fima Klebaner,\n  Kais Hamza",
        "title": "Local Control Regression: Improving the Least Squares Monte Carlo Method\n  for Portfolio Optimization",
        "comments": "10 pages, 4 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The least squares Monte Carlo algorithm has become popular for solving\nportfolio optimization problems. A simple approach is to approximate the value\nfunctions on a discrete grid of portfolio weights, then use control regression\nto generalize the discrete estimates. However, the classical global control\nregression can be expensive and inaccurate. To overcome this difficulty, we\nintroduce a local control regression technique, combined with adaptive grids.\nWe show that choosing a coarse grid for local regression can produce\nsufficiently accurate results.\n"
    },
    {
        "paper_id": 1804.00223,
        "authors": "Claudia Ceci, Katia Colaneri, Alessandra Cretarola",
        "title": "Indifference pricing of pure endowments via BSDEs under partial\n  information",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the pricing problem of a pure endowment contract\nwhen the insurer has a limited information on the mortality intensity of the\npolicyholder. The payoff of this kind of policies depends on the residual life\ntime of the insured as well as the trend of a portfolio traded in the financial\nmarket, where investments in a riskless asset, a risky asset and a longevity\nbond are allowed. We propose a modeling framework that takes into account\nmutual dependence between the financial and the insurance markets via an\nobservable stochastic process, which affects the risky asset and the mortality\nindex dynamics. Since the market is incomplete due to the presence of basis\nrisk, in alternative to arbitrage pricing we use expected utility maximization\nunder exponential preferences as evaluation approach, which leads to the\nso-called indifference price. Under partial information this methodology\nrequires filtering techniques that can reduce the original control problem to\nan equivalent problem in complete information. Using stochastic dynamics\ntechniques, we characterize the indifference price of the insurance derivative\nvia the solutions of suitable backward stochastic differential equations.\n"
    },
    {
        "paper_id": 1804.00442,
        "authors": "Huy N. Chau, Andrea Cosso, Claudio Fontana",
        "title": "The value of informational arbitrage",
        "comments": "36 pages",
        "journal-ref": "Finance and Stochastics, 24: 277-307, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of a general semimartingale model of a complete market, we aim\nat answering the following question: How much is an investor willing to pay for\nlearning some inside information that allows to achieve arbitrage? If such a\nvalue exists, we call it the value of informational arbitrage. In particular,\nwe are interested in the case where the inside information yields arbitrage\nopportunities but not unbounded profits with bounded risk. In the spirit of\nAmendinger et al. (2003, Finance Stoch.), we provide a general answer to the\nabove question by relying on an indifference valuation approach. To this\neffect, we establish some new results on models with inside information and\nstudy optimal investment-consumption problems in the presence of initial\ninformation and arbitrage, also allowing for the possibility of leveraged\npositions. We characterize when the value of informational arbitrage is\nuniversal, in the sense that it does not depend on the preference structure.\nOur results are illustrated with several explicit examples.\n"
    },
    {
        "paper_id": 1804.00764,
        "authors": "Donald Richards and Hein Hundal",
        "title": "Constant Proportion Debt Obligations, Zeno's Paradox, and the\n  Spectacular Financial Crisis of 2008",
        "comments": "32 pages; 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a coin-tossing model used by a ratings agency to justify the sale of\nconstant proportion debt obligations (CPDOs), and prove that it was impossible\nfor CPDOs to achieve in a finite lifetime the Cash-In event of doubling its\ncapital. In the best-case scenario of a two-headed coin, we show that the goal\nof attaining the Cash-In event in a finite lifetime is precisely the goal,\ndescribed more than two thousand years ago in Zeno's Paradox of the Dichotomy,\nof obtaining the sum of an infinite geometric series with only a finite number\nof terms. In the worst-case scenario of a two-tailed coin, we prove that the\nCash-Out event occurs in exactly ten tosses.\n  If the coin is fair, we show that if a CPDO were allowed to toss the coin\nwithout regard for the Cash-Out rule then the CPDO eventually has a high\nprobability of attaining large net capital levels; however, hundreds of\nthousands of tosses may be needed to do so. Moreover, if after many tosses the\nCPDO shows a loss then the probability is high that it will Cash-Out on the\nvery next toss. If a CPDO experiences a tail on the first toss or on an early\ntoss, we show that, with high probability, the CPDO will have capital losses\nthereafter for hundreds of tosses; moreover, its sequence of net capital levels\nis a martingale. When the Cash-Out rule holds, we modify the Cash-In rule to\nmean that the CPDO attains a profit of 90 percent on its capital; then we prove\nthat the CPDO game, almost surely, will end in finitely many tosses and the\nprobability of Cash-Out is at least 89 percent.\n  In light of our results, our fears about the durability of worldwide\nfinancial crises are heightened by the existence of other financial derivatives\nmore arcane than CPDOs. In particular, we view askance all later-generation\nCPDOs that depend mean-reversion assumptions or use a betting strategy similar\nto their first-generation counterparts.\n"
    },
    {
        "paper_id": 1804.0082,
        "authors": "Donald St. P. Richards",
        "title": "Return Optimization Securities and Other Remarkable Structured\n  Investment Products: Indicators of Future Outcomes for U.S. Treasuries?",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze four structured products that have caused severe losses to\ninvestors in recent years. These products are: return optimization securities,\nyield magnet notes, reverse exchangeable securities, and principal-protected\nnotes. We describe the basic structure of these products, analyze them\nprobabilistically using the Law of Total Expectation, and assess the practical\nimplications of buying them in the mid-2000s. By estimating expected rates of\nreturn under various scenarios, we conclude in each case that buyers were\nlikely to experience grave difficulties from the start. By inspecting various\nprospectuses, we detect that many structured products were designed to the\ndetriment of buyers and to the advantage of the issuing banks and\nbroker-dealers. Therefore, we find it difficult to understand why any\ninvestment advisor, in exercising fiduciary care of clients' funds, would have\nadvised a client to purchases these products in the mid-2000's.\n  In light of these results, we fear that the on-going worldwide financial\ncrisis will be lengthened because of these structured products and others even\nmore arcane than the ones considered here. We note that problems caused by\nstructured products have increased investors' fears about the economy and the\nfinancial markets, causing many of them to purchase U.S. Treasury securities at\nnegative real-interest rates. This has caused increases in the prices of U.S.\nTreasuries to record levels, and we fear for the day when this trend reverses.\n"
    },
    {
        "paper_id": 1804.00825,
        "authors": "Gilna K. Samuel and Donald St. P. Richards",
        "title": "A Probabilistic Analysis of Autocallable Optimization Securities",
        "comments": null,
        "journal-ref": "In: Models and Reality: Festschrift For James Robert Thompson (Ed.\n  J. A. Dobelman), pp. , pp. 147-182; T&NO Company, Chicago, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider in this paper some structured financial products, known as\nreverse convertible notes, that resulted in substantial losses to certain\nbuyers of these notes in recent years. We shall focus on specific reverse\nconvertible notes known as \"Autocallable Optimization Securities with\nContingent Protection Linked to the S\\&P 500 Financial Index,\" because these\nnotes are representative of the broad spectrum of reverse convertibles notes.\nTherefore, the analysis provided in this paper is applicable to many other\nreverse convertible notes.\n  We begin by describing the notes in detail and identifying potential areas of\nconfusion in the pricing supplement to the prospectus for the notes. We deduce\ntwo possible interpretations of the payment procedure for the notes and apply\nthe Law of Total Expectation to develop a probabilistic analysis for each\ninterpretation. We also determine the corresponding expected net payments to\nnote-holders under various scenarios for the financial markets and show that,\nunder a broad range of scenarios, note-holders were likely to suffer\nsubstantial losses.\n  As a consequence, we infer that the prospectus is sufficiently complex that\nfinancial advisers generally lacked the mathematical knowledge and expertise to\nunderstand the prospectus completely. Therefore, financial advisers who\nrecommended purchases of the notes did not have the knowledge and expertise\nthat is required by a fiduciary relationship, hence were unable to exercise\nfiduciary duty, and ultimately misguided their clients. We conclude that these\nreverse convertibles notes were designed by financial institutions to insure\nthemselves, against significant declines in the equities markets, at the\nexpense of note-holders.\n"
    },
    {
        "paper_id": 1804.01367,
        "authors": "Juraj Hledik and Riccardo Rastelli",
        "title": "A dynamic network model to measure exposure diversification in the\n  Austrian interbank market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a statistical model for weighted temporal networks capable of\nmeasuring the level of heterogeneity in a financial system. Our model focuses\non the level of diversification of financial institutions; that is, whether\nthey are more inclined to distribute their assets equally among partners, or if\nthey rather concentrate their commitment towards a limited number of\ninstitutions. Crucially, a Markov property is introduced to capture time\ndependencies and to make our measures comparable across time. We apply the\nmodel on an original dataset of Austrian interbank exposures. The temporal span\nencompasses the onset and development of the financial crisis in 2008 as well\nas the beginnings of European sovereign debt crisis in 2011. Our analysis\nhighlights an overall increasing trend for network homogeneity, whereby core\nbanks have a tendency to distribute their market exposures more equally across\ntheir partners.\n"
    },
    {
        "paper_id": 1804.01475,
        "authors": "Andrea Consiglio, Michele Tumminello, Stavros A. Zenios",
        "title": "Pricing sovereign contingent convertible debt",
        "comments": "32 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a pricing model for Sovereign Contingent Convertible bonds\n(S-CoCo) with payment standstills triggered by a sovereign's Credit Default\nSwap (CDS) spread. We model CDS spread regime switching, which is prevalent\nduring crises, as a hidden Markov process, coupled with a mean-reverting\nstochastic process of spread levels under fixed regimes, in order to obtain\nS-CoCo prices through simulation. The paper uses the pricing model in a\nLongstaff-Schwartz American option pricing framework to compute future state\ncontingent S-CoCo prices for risk management. Dual trigger pricing is also\ndiscussed using the idiosyncratic CDS spread for the sovereign debt together\nwith a broad market index. Numerical results are reported using S-CoCo designs\nfor Greece, Italy and Germany with both the pricing and contingent pricing\nmodels.\n"
    },
    {
        "paper_id": 1804.01676,
        "authors": "Arjun R, Suprabha KR",
        "title": "Predictive modeling of stock indices closing from web search trends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study aims to explore the strength of causal relationship between stock\nprice search interest and real stock market outcomes on worldwide equity market\nindices. Such a phenomenon could also be mediated by investor behavior and\nextent of news coverage. The stock-specific internet search trends data and\ncorresponding index close values from different countries stock exchanges are\ncollected and analyzed. Empirical findings show global stock price search\ninterests correlates more with developing economies with fewer effects in south\nasian stock exchanges apart from strong influence in western countries. Finally\nthis study calls for development in expert decision support systems with the\nsynthesis of using big data sources on forecasting market outcomes\n"
    },
    {
        "paper_id": 1804.01764,
        "authors": "Daniel Kinn",
        "title": "Reducing Estimation Risk in Mean-Variance Portfolios with Machine\n  Learning",
        "comments": "Second version: improved readability and corrected typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In portfolio analysis, the traditional approach of replacing population\nmoments with sample counterparts may lead to suboptimal portfolio choices. I\nshow that optimal portfolio weights can be estimated using a machine learning\n(ML) framework, where the outcome to be predicted is a constant and the vector\nof explanatory variables is the asset returns. It follows that ML specifically\ntargets estimation risk when estimating portfolio weights, and that\n\"off-the-shelf\" ML algorithms can be used to estimate the optimal portfolio in\nthe presence of parameter uncertainty. The framework nests the traditional\napproach and recently proposed shrinkage approaches as special cases. By\nrelying on results from the ML literature, I derive new insights for existing\napproaches and propose new estimation methods. Based on simulation studies and\nseveral datasets, I find that ML significantly reduces estimation risk compared\nto both the traditional approach and the equal weight strategy.\n"
    },
    {
        "paper_id": 1804.01825,
        "authors": "Alexei Botchkarev",
        "title": "Evaluating Hospital Case Cost Prediction Models Using Azure Machine\n  Learning Studio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ability for accurate hospital case cost modelling and prediction is critical\nfor efficient health care financial management and budgetary planning. A\nvariety of regression machine learning algorithms are known to be effective for\nhealth care cost predictions. The purpose of this experiment was to build an\nAzure Machine Learning Studio tool for rapid assessment of multiple types of\nregression models. The tool offers environment for comparing 14 types of\nregression models in a unified experiment: linear regression, Bayesian linear\nregression, decision forest regression, boosted decision tree regression,\nneural network regression, Poisson regression, Gaussian processes for\nregression, gradient boosted machine, nonlinear least squares regression,\nprojection pursuit regression, random forest regression, robust regression,\nrobust regression with mm-type estimators, support vector regression. The tool\npresents assessment results arranged by model accuracy in a single table using\nfive performance metrics. Evaluation of regression machine learning models for\nperforming hospital case cost prediction demonstrated advantage of robust\nregression model, boosted decision tree regression and decision forest\nregression. The operational tool has been published to the web and openly\navailable for experiments and extensions.\n"
    },
    {
        "paper_id": 1804.02289,
        "authors": "David Lee",
        "title": "Pricing Financial Derivatives Subject to Counterparty Risk and Credit\n  Value Adjustment",
        "comments": "arXiv admin note: text overlap with arXiv:1803.07843 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a generic model for pricing financial derivatives\nsubject to counterparty credit risk. Both unilateral and bilateral types of\ncredit risks are considered. Our study shows that credit risk should be modeled\nas American style options in most cases, which require a backward induction\nvaluation. To correct a common mistake in the literature, we emphasize that the\nmarket value of a defaultable derivative is actually a risky value rather than\na risk-free value. Credit value adjustment (CVA) is also elaborated. A\npractical framework is developed for pricing defaultable derivatives and\ncalculating their CVAs at a portfolio level.\n"
    },
    {
        "paper_id": 1804.02333,
        "authors": "Oleg Malafeyev, Olga Koroleva, Dmitriy Prusskiy, Olga Zenovich",
        "title": "Corruption-free scheme of entering into contract: mathematical model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of this paper is to formalize the modelling process,\nanalysis and mathematical definition of corruption when entering into a\ncontract between principal agent and producers. The formulation of the problem\nand the definition of concepts for the general case are considered. For\ndefiniteness, all calculations and formulas are given for the case of three\nproducers, one principal agent and one intermediary. Economic analysis of\ncorruption allowed building a mathematical model of interaction between agents.\nFinancial resources distribution problem in a contract with a corrupted\nintermediary is considered.Then proposed conditions for corruption emergence\nand its possible consequences. Optimal non-corruption schemes of financial\nresources distribution in a contract are formed, when principal agent's choice\nis limited first only by asymmetrical information and then also by external\ninfluences.Numerical examples suggesting optimal corruption-free agents'\nbehaviour are presented.\n"
    },
    {
        "paper_id": 1804.0235,
        "authors": "Marco Alberto Javarone and Craig Steven Wright",
        "title": "From Bitcoin to Bitcoin Cash: a network analysis",
        "comments": "CryBlock 18, June 15, 2018, Munich, Germany 5 pages, 3 figures, 1\n  table, ACM Proceeding CryBlock'18 1st Workshop on Cryptocurrencies and\n  Blockchains for Distributed Systems 2018",
        "journal-ref": null,
        "doi": "10.1145/3211933.3211947",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoins and Blockchain technologies are attracting the attention of\ndifferent scientific communities. In addition, their widespread industrial\napplications and the continuous introduction of cryptocurrencies are also\nstimulating the attention of the public opinion. The underlying structure of\nthese technologies constitutes one of their core concepts. In particular, they\nare based on peer-to-peer networks. Accordingly, all nodes lie at the same\nlevel, so that there is no place for privileged actors as, for instance,\nbanking institutions in classical financial networks. In this work, we perform\na preliminary investigation on two kinds of network, i.e. the Bitcoin network\nand the Bitcoin Cash network. Notably, we analyze their global structure and we\ntry to evaluate if they are provided with a small-world behavior. Results\nsuggest that the principle known as 'fittest-gets-richer', combined with a\ncontinuous increasing of connections, might constitute the mechanism leading\nthese networks to reach their current structure. Moreover, further observations\nopen the way to new investigations into this direction.\n"
    },
    {
        "paper_id": 1804.02689,
        "authors": "Alexander Jurisch",
        "title": "An extremal fractional Gaussian with a possible application to\n  option-pricing with skew and smile",
        "comments": "6 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an extremal fractional Gaussian by employing the L\\'evy-Khintchine\ntheorem and L\\'evian noise. With the fractional Gaussian we then generalize the\nBlack-Scholes-Merton option-pricing formula. We obtain an easily applicable and\nexponentially convergent option-pricing formula for fractional markets. We also\ncarry out an analysis of the structure of the implied volatility in this\nsystem.\n"
    },
    {
        "paper_id": 1804.03002,
        "authors": "Jean-Pierre Fouque, Ruimeng Hu",
        "title": "Portfolio Optimization under Fast Mean-reverting and Rough Fractional\n  Stochastic Environment",
        "comments": "arXiv admin note: text overlap with arXiv:1706.03139",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fractional stochastic volatility models have been widely used to capture the\nnon-Markovian structure revealed from financial time series of realized\nvolatility. On the other hand, empirical studies have identified scales in\nstock price volatility: both fast-time scale on the order of days and\nslow-scale on the order of months. So, it is natural to study the portfolio\noptimization problem under the effects of dependence behavior which we will\nmodel by fractional Brownian motions with Hurst index $H$, and in the fast or\nslow regimes characterized by small parameters $\\eps$ or $\\delta$. For the\nslowly varying volatility with $H \\in (0,1)$, it was shown that the first order\ncorrection to the problem value contains two terms of order $\\delta^H$, one\nrandom component and one deterministic function of state processes, while for\nthe fast varying case with $H > \\half$, the same form holds at order\n$\\eps^{1-H}$. This paper is dedicated to the remaining case of a fast-varying\nrough environment ($H < \\half$) which exhibits a different behavior. We show\nthat, in the expansion, only one deterministic term of order $\\sqrt{\\eps}$\nappears in the first order correction.\n"
    },
    {
        "paper_id": 1804.03219,
        "authors": "Ruben van de Geer, Arnoud V. den Boer, Christopher Bayliss, Christine\n  Currie, Andria Ellina, Malte Esders, Alwin Haensel, Xiao Lei, Kyle D.S.\n  Maclean, Antonio Martinez-Sykora, Asbj{\\o}rn Nilsen Riseth, Fredrik\n  {\\O}degaard, Simos Zachariades",
        "title": "Dynamic Pricing and Learning with Competition: Insights from the Dynamic\n  Pricing Challenge at the 2017 INFORMS RM & Pricing Conference",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1057/s41272-018-00164-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the results of the Dynamic Pricing Challenge, held on the\noccasion of the 17th INFORMS Revenue Management and Pricing Section Conference\non June 29-30, 2017 in Amsterdam, The Netherlands. For this challenge,\nparticipants submitted algorithms for pricing and demand learning of which the\nnumerical performance was analyzed in simulated market environments. This\nallows consideration of market dynamics that are not analytically tractable or\ncan not be empirically analyzed due to practical complications. Our findings\nimplicate that the relative performance of algorithms varies substantially\nacross different market dynamics, which confirms the intrinsic complexity of\npricing and learning in the presence of competition.\n"
    },
    {
        "paper_id": 1804.0329,
        "authors": "Rajeshwari Majumdar, Phanuel Mariano, Lowen Peng and Anthony Sisti",
        "title": "A derivation of the Black-Scholes option pricing model using a central\n  limit theorem argument",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Scholes model (sometimes known as the Black-Scholes-Merton model)\ngives a theoretical estimate for the price of European options. The price\nevolution under this model is described by the Black-Scholes formula, one of\nthe most well-known formulas in mathematical finance. For their discovery,\nMerton and Scholes have been awarded the 1997 Nobel prize in Economics. The\nstandard method of deriving the Black-Scholes European call option pricing\nformula involves stochastic differential equations. This approach is out of\nreach for most students learning the model for the first time. We provide an\nalternate derivation using the Lindeberg-Feller central limit theorem under\nsuitable assumptions. Our approach is elementary and can be understood by\nundergraduates taking a standard undergraduate course in probability.\n"
    },
    {
        "paper_id": 1804.03975,
        "authors": "Thomas Gerstner, Bastian Harrach, Daniel Roth",
        "title": "Monte Carlo pathwise sensitivities for barrier options",
        "comments": null,
        "journal-ref": "J. Comput. Finance 23 (5), 75-99, 2020",
        "doi": "10.21314/JCF.2020.385",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Monte Carlo pathwise sensitivities approach is well established for\nsmooth payoff functions. In this work, we present a new Monte Carlo algorithm\nthat is able to calculate the pathwise sensitivities for discontinuous payoff\nfunctions. Our main tool is to combine the one-step survival idea of Glasserman\nand Staum with the stable differentiation approach of Alm, Harrach, Harrach and\nKeller. As an application we use the derived results for a two-dimensional\ncalibration of a CoCo-Bond, which we model with different types of discretely\nmonitored barrier options.\n"
    },
    {
        "paper_id": 1804.0417,
        "authors": "Weston Barger, Matthew Lorig",
        "title": "Optimal liquidation under stochastic price impact",
        "comments": "25 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We assume a continuous-time price impact model similar to Almgren-Chriss but\nwith the added assumption that the price impact parameters are stochastic\nprocesses modeled as correlated scalar Markov diffusions. In this setting, we\ndevelop trading strategies for a trader who desires to liquidate his inventory\nbut faces price impact as a result of his trading. For a fixed trading horizon,\nwe perform coefficient expansion on the Hamilton-Jacobi-Bellman equation\nassociated with the trader's value function. The coefficient expansion yields a\nsequence of partial differential equations that we solve to give closed-form\napproximations to the value function and optimal liquidation strategy. We\nexamine some special cases of the optimal liquidation problem and give\nfinancial interpretations of the approximate liquidation strategies in these\ncases. Finally, we provide numerical examples to demonstrate the effectiveness\nof the approximations.\n"
    },
    {
        "paper_id": 1804.04216,
        "authors": "Thomas Spooner, John Fearnley, Rahul Savani and Andreas Koukorinis",
        "title": "Market Making via Reinforcement Learning",
        "comments": "10 pages, 5 figures, AAMAS2018 Conference Proceedings",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making is a fundamental trading problem in which an agent provides\nliquidity by continually offering to buy and sell a security. The problem is\nchallenging due to inventory risk, the risk of accumulating an unfavourable\nposition and ultimately losing money. In this paper, we develop a high-fidelity\nsimulation of limit order book markets, and use it to design a market making\nagent using temporal-difference reinforcement learning. We use a linear\ncombination of tile codings as a value function approximator, and design a\ncustom reward function that controls inventory risk. We demonstrate the\neffectiveness of our approach by showing that our agent outperforms both simple\nbenchmark strategies and a recent online learning approach from the literature.\n"
    },
    {
        "paper_id": 1804.04283,
        "authors": "Erhan Bayraktar, Xin Zhang, Zhou Zhou",
        "title": "Transport plans with domain constraints",
        "comments": "To appear in Applied Mathematics and Optimization.\n  Keywords:Strassen's Theorem, Kellerer's Theorem, Martingale optimal\n  transport, domain constraints, bounded volatility/quadratic variation,\n  $G$-expectations, Kantorovich duality, monotonicity principle",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on martingale optimal transport problems when the\nmartingales are assumed to have bounded quadratic variation. First, we give a\nresult that characterizes the existence of a probability measure satisfying\nsome convex transport constraints in addition to having given initial and\nterminal marginals. Several applications are provided: martingale measures with\nvolatility uncertainty, optimal transport with capacity constraints, and\nSkorokhod embedding with bounded times. Next, we extend this result to\nmulti-marginal constraints. Finally, we consider an optimal transport problem\nwith constraints and obtain its Kantorovich duality. A corollary of this result\nis a monotonicity principle which gives a geometric way of identifying the\noptimizer.\n"
    },
    {
        "paper_id": 1804.04721,
        "authors": "Victor Olkhov",
        "title": "Econophysics Beyond General Equilibrium: the Business Cycle Model",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Current business cycle theory is an application of the general equilibrium\ntheory. This paper presents the business cycle model without using general\nequilibrium framework. We treat agents risk assessments as their coordinates x\non economic space and establish distribution of all economic agents by their\nrisk coordinates. We suggest aggregation of agents and their variables by\nscales large to compare with risk scales of single agents and small to compare\nwith economic domain on economic space. Such model is alike to transition from\nkinetic description of multi-particle system to hydrodynamic approximation.\nAggregates of agents extensive variables with risk coordinate x determine macro\nvariables as functions of x alike to hydrodynamic variables. Economic and\nfinancial transactions between agents define evolution of their variables.\nAggregation of transactions between agents with risk coordinates x and y\ndetermine macro transactions as functions of x and y and define evolution of\nmacro variables at points x and y. We describe evolution and interactions\nbetween macro transactions by hydrodynamic-like system of economic equations.\nWe show that business cycles are described as consequence of the system of\neconomic equations on macro transactions. As example we describe Credit\ntransactions CL(tax,y) that provide Loans from Creditors at point x to\nBorrowers at point y and Loan-Repayment transactions LR(t,x,y) that describe\nrepayments from Borrowers at point y to Creditors at point x. We use\nhydrodynamic-like economic equations and derive from them the system of\nordinary differential equations that describe business cycle fluctuations of\nmacro Credits C(t) and macro Loan-Repayments LR(t) of the entire economics. The\nnature of business cycle fluctuations is explained as oscillations of \"mean\nrisk\" of economic variables on bounded economic domain of economic space.\n"
    },
    {
        "paper_id": 1804.0487,
        "authors": "Giorgio Ferrari, Patrick Schuhmann",
        "title": "An Optimal Dividend Problem with Capital Injections over a Finite\n  Horizon",
        "comments": "32 pages; improved exposition and added new results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose and solve an optimal dividend problem with capital\ninjections over a finite time horizon. The surplus dynamics obeys a linearly\ncontrolled drifted Brownian motion that is reflected at the origin, dividends\ngive rise to time-dependent instantaneous marginal profits, whereas capital\ninjections are subject to time-dependent instantaneous marginal costs. The aim\nis to maximize the sum of a liquidation value at terminal time and of the total\nexpected profits from dividends, net of the total expected costs for capital\ninjections. Inspired by the study of El Karoui and Karatzas (1989) on reflected\nfollower problems, we relate the optimal dividend problem with capital\ninjections to an optimal stopping problem for a drifted Brownian motion that is\nabsorbed at the origin. We show that whenever the optimal stopping rule is\ntriggered by a time-dependent boundary, the value function of the optimal\nstopping problem gives the derivative of the value function of the optimal\ndividend problem. Moreover, the optimal dividend strategy is also triggered by\nthe moving boundary of the associated stopping problem. The properties of this\nboundary are then investigated in a case study in which instantaneous marginal\nprofits and costs from dividends and capital injections are constants\ndiscounted at a constant rate.\n"
    },
    {
        "paper_id": 1804.04924,
        "authors": "Pierre Cohort, Jacopo Corbetta, Claude Martini and Ismail Laachir",
        "title": "Robust calibration and arbitrage-free interpolation of SSVI slices",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a robust calibration algorithm of a set of SSVI slices (i.e. a\nset of 3 SSVI parameters $\\theta, \\rho, \\varphi$ attached to each option\nmaturity available on the market), which grants that these slices are free of\nButterfly and Calendar-Spread arbitrage. Given such a set of consistent SSVI\nparameters, we show that the most natural interpolation/extrapolation of the\nparameters provides a full continuous volatility surface free of arbitrage. The\nnumerical implementation is straightforward, robust and quick, yielding an\neffective, parsimonious solution to the smile problem, which has the potential\nto become a benchmark one.\n"
    },
    {
        "paper_id": 1804.05103,
        "authors": "Mounira Chniguir, Mohamed Kefi (INAT), Jamel Henchiri",
        "title": "The Determinants of Home Bias in Stock Portfolio: An Emerging and\n  Developed Markets Study",
        "comments": null,
        "journal-ref": "International Journal of Economics and Financial Issues\n  International Journal of Economics and Financial Issues, 2017, 7, pp.182 -\n  191",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to measure the degree of home bias (HB) within\nholdings portfolio and to identify their determining factors. By following\nliterature and an international capital asset pricing model, we have chosen\nquite a number of susceptible factors that impact HB. This model is, hence,\nestimated for 20 countries, with cross-section econometrics, between 2008 and\n2013. Our results show that all countries have recorded a high level of HB in\ntheir holdings portfolio. After that, we test if the HB of the emerging markets\nand that of the developed markets react differently to the determining factors.\nThe volatility of the exchange rate is statistically significant with emerging\nmarkets, while it is hardly remarkable for the developed countries.\nCo-variance, size, distance, language, legal framework and foreign organization\nstocks prevents American investors to invest abroad.\n"
    },
    {
        "paper_id": 1804.05279,
        "authors": "M. Dashti Moghaddam, Zhiyuan Liu, R. A. Serota",
        "title": "Distributions of Historic Market Data -- Implied and Realized Volatility",
        "comments": "28 pages, 40 figures, 16 tables",
        "journal-ref": "Applied Economics and Finance (2019) Vol. 6, No. 5, pp. 104-130,",
        "doi": "10.11114/aef.v6i5.4416",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We undertake a systematic comparison between implied volatility, as\nrepresented by VIX (new methodology) and VXO (old methodology), and realized\nvolatility. We compare visually and statistically distributions of realized and\nimplied variance (volatility squared) and study the distribution of their\nratio. We find that the ratio is best fitted by heavy-tailed -- lognormal and\nfat-tailed (power-law) -- distributions, depending on whether preceding or\nconcurrent month of realized variance is used. We do not find substantial\ndifference in accuracy between VIX and VXO. Additionally, we study the variance\nof theoretical realized variance for Heston and multiplicative models of\nstochastic volatility and compare those with realized variance obtained from\nhistoric market data.\n"
    },
    {
        "paper_id": 1804.05354,
        "authors": "Alessandro Milazzo and Elena Vigna",
        "title": "The Italian Pension Gap: a Stochastic Optimal Control Approach",
        "comments": null,
        "journal-ref": "Risks 2018, 6(2), 48",
        "doi": "10.3390/risks6020048",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the gap between the state pension provided by the Italian pension\nsystem pre-Dini reform and post-Dini reform. The goal is to fill the gap\nbetween the old and the new pension by joining a defined contribution pension\nscheme and adopting an optimal investment strategy that is target-based. We\nfind that it is possible to cover, at least partially, this gap with the\nadditional income of the pension scheme, especially in the presence of late\nretirement and in the presence of stagnant career. Workers with dynamic career\nand workers who retire early are those who are most penalised by the reform.\nResults are intuitive and in line with previous studies on the subject.\n"
    },
    {
        "paper_id": 1804.05454,
        "authors": "Tony Jebara",
        "title": "A refinement of Bennett's inequality with applications to portfolio\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A refinement of Bennett's inequality is introduced which is strictly tighter\nthan the classical bound. The new bound establishes the convergence of the\naverage of independent random variables to its expected value. It also\ncarefully exploits information about the potentially heterogeneous mean,\nvariance, and ceiling of each random variable. The bound is strictly sharper in\nthe homogeneous setting and very often significantly sharper in the\nheterogeneous setting. The improved convergence rates are obtained by\nleveraging Lambert's W function. We apply the new bound in a portfolio\noptimization setting to allocate a budget across investments with heterogeneous\nreturns.\n"
    },
    {
        "paper_id": 1804.05667,
        "authors": "Yingli Wang, Qingpeng Zhang, and Xiaoguang Yang",
        "title": "Evolution of the Chinese Guarantee Network under Financial Crisis and\n  Stimulus Program",
        "comments": "30pages, 8 figures, 1 table",
        "journal-ref": "Nature Communications volume 11, Article number: 2693 (2020)",
        "doi": "10.1038/s41467-020-16535-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our knowledge about the evolution of guarantee network in downturn period is\nlimited due to the lack of comprehensive data of the whole credit system. Here\nwe analyze the dynamic Chinese guarantee network constructed from a\ncomprehensive bank loan dataset that accounts for nearly 80% total loans in\nChina, during 01/2007-03/2012. The results show that, first, during the\n2007-2008 global financial crisis, the guarantee network became smaller, less\nconnected and more stable because of many bankruptcies; second, the stimulus\nprogram encouraged mutual guarantee behaviors, resulting in highly reciprocal\nand fragile network structure; third, the following monetary policy adjustment\nenhanced the resilience of the guarantee network by reducing mutual guarantees.\nInterestingly, our work reveals that the financial crisis made the network more\nresilient, and conversely, the government bailout degenerated network\nresilience. These counterintuitive findings can provide new insight into the\nresilience of real-world credit system under external shocks or rescues.\n"
    },
    {
        "paper_id": 1804.05694,
        "authors": "Erwan Koch",
        "title": "Extremal dependence and spatial risk measures for insured losses due to\n  extreme winds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A meticulous assessment of the risk of impacts associated with extreme wind\nevents is of great necessity for populations, civil authorities as well as the\ninsurance industry. Using the concept of spatial risk measure and related set\nof axioms introduced by Koch (2017, 2019), we quantify the risk of losses due\nto extreme wind speeds. The insured cost due to wind events is proportional to\nthe wind speed at a power ranging typically between 2 and 12. Hence we first\nperform a detailed study of the correlation structure of powers of the\nBrown-Resnick max-stable random fields and look at the influence of the power.\nThen, using the latter results, we thoroughly investigate spatial risk measures\nassociated with variance and induced by powers of max-stable random fields. In\naddition, we show that spatial risk measures associated with several classical\nrisk measures and induced by such cost fields satisfy (at least part of) the\npreviously mentioned axioms under conditions which are generally satisfied for\nthe risk of damaging extreme wind speeds. In particular, we specify the rates\nof spatial diversification in different cases, which is valuable for the\ninsurance industry.\n"
    },
    {
        "paper_id": 1804.05916,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Robert G\\k{e}barowski, Ludovico Minati,\n  Pawe{\\l} O\\'swi\\k{e}cimka, Marcin W\\k{a}torek",
        "title": "Bitcoin market route to maturity? Evidence from return fluctuations,\n  temporal correlations and multiscaling effects",
        "comments": null,
        "journal-ref": "S. Drozdz, R. Gebarowski, L. Minati, P. Oswiecimka, and M.\n  Watorek, Chaos 28, 071101 (2018)",
        "doi": "10.1063/1.5036517",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on 1-minute price changes recorded since year 2012, the fluctuation\nproperties of the rapidly-emerging Bitcoin (BTC) market are assessed over\nchosen sub-periods, in terms of return distributions, volatility\nautocorrelation, Hurst exponents and multiscaling effects. The findings are\ncompared to the stylized facts of mature world markets. While early trading was\naffected by system-specific irregularities, it is found that over the months\npreceding Apr 2018 all these statistical indicators approach the features\nhallmarking maturity. This can be taken as an indication that the Bitcoin\nmarket, and possibly other cryptocurrencies, carry concrete potential of\nimminently becoming a regular market, alternative to the foreign exchange\n(Forex). Since high-frequency price data are available since the beginning of\ntrading, the Bitcoin offers a unique window into the statistical\ncharacteristics of a market maturation trajectory.\n"
    },
    {
        "paper_id": 1804.05979,
        "authors": "Del Rajan (Victoria University of Wellington) and Matt Visser\n  (Victoria University of Wellington)",
        "title": "Quantum Blockchain using entanglement in time",
        "comments": "V1:5 pages; V2: now 7 pages; more discussion; more references; no\n  changes in physics conclusions",
        "journal-ref": "Quantum Reports 1 # 1 (2019) 3--11",
        "doi": "10.3390/quantum1010002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a conceptual design for a quantum blockchain. Our method involves\nencoding the blockchain into a temporal GHZ (Greenberger-Horne-Zeilinger) state\nof photons that do not simultaneously coexist. It is shown that the\nentanglement in time, as opposed to an entanglement in space, provides the\ncrucial quantum advantage. All the subcomponents of this system have already\nbeen shown to be experimentally realized. Furthermore, our encoding procedure\ncan be interpreted as nonclassically influencing the past.\n"
    },
    {
        "paper_id": 1804.06261,
        "authors": "Jan-Christian Gerlach, Guilherme Demos and Didier Sornette",
        "title": "Dissection of Bitcoin's Multiscale Bubble History from January 2012 to\n  February 2018",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a detailed bubble analysis of the Bitcoin to US Dollar price\ndynamics from January 2012 to February 2018. We introduce a robust automatic\npeak detection method that classifies price time series into periods of\nuninterrupted market growth (drawups) and regimes of uninterrupted market\ndecrease (drawdowns). In combination with the Lagrange Regularisation Method\nfor detecting the beginning of a new market regime, we identify 3 major peaks\nand 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin\nprice during the analyzed time period. We explain this classification of long\nand short bubbles by a number of quantitative metrics and graphs to understand\nthe main socio-economic drivers behind the ascent of Bitcoin over this period.\nThen, a detailed analysis of the growing risks associated with the three long\nbubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on\nthe LPPLS Confidence Indicators, defined as the fraction of qualified fits of\nthe LPPLS model over multiple time windows. Furthermore, for various fictitious\n'present' times $t_2$ before the crashes, we employ a clustering method to\ngroup the predicted critical times $t_c$ of the LPPLS fits over different time\nscales, where $t_c$ is the most probable time for the ending of the bubble.\nEach cluster is proposed as a plausible scenario for the subsequent Bitcoin\nprice evolution. We present these predictions for the three long bubbles and\nthe four short bubbles that our time scale of analysis was able to resolve.\nOverall, our predictive scheme provides useful information to warn of an\nimminent crash risk.\n"
    },
    {
        "paper_id": 1804.06598,
        "authors": "Zbigniew Michna",
        "title": "Ruin probabilities for two collaborating insurance companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we find a formula for the supremum distribution of spectrally\npositive or negative L\\'evy processes with a broken linear drift. This gives\nformulas for ruin probabilities in the case when two insurance companies (or\ntwo branches of the same company) divide between them both claims and premia in\nsome specified proportions. As an example we consider gamma L\\'evy process,\n$\\alpha$-stable L\\'evy process and Brownian motion. Moreover we obtain\nidentities for Laplace transform of the distribution for the supremum of L\\'evy\nprocesses with randomly broken drift and on random intervals.\n"
    },
    {
        "paper_id": 1804.06707,
        "authors": "Richard Arnold, Stefanka Chukova, Yu Hayakawa, Sarah Marshall",
        "title": "Warranty Cost Analysis with an Alternating Geometric Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we model the warranty claims process and evaluate the warranty\nservicing costs under non-renewing and renewing free repair warranties. We\nassume that the repair time for rectifying the claims is non-zero and the\nrepair cost is a function of the length of the repair time. To accommodate the\nageing of the product and repair equipment, we use a decreasing geometric\nprocess to model the consecutive operational times and an increasing geometric\nprocess to model the consecutive repair times. We identify and study the\nalternating geometric process (AGP), which is an alternating process with\ncycles consisting of the item's operational time followed by the corresponding\nrepair time. We derive new results for the AGP in finite horizon and use them\nto evaluate the warranty costs over the warranty period and over the life cycle\nof the product under a non-renewing free repair warranty (NRFRW), a renewing\nfree repair warranty (RFRW) and a restricted renewing free repair warranty\n(RRFRW(n)). Properties of the model are demonstrated using a simulation study.\n"
    },
    {
        "paper_id": 1804.06709,
        "authors": "Guillermo Rodriguez-Abitia, Susana Vidrio, Claudia Montiel-Sanchez",
        "title": "Assessing the state of e-Readiness for Small and Medium Companies in\n  Mexico: a Proposed Taxonomy and Adoption Model",
        "comments": null,
        "journal-ref": "AMCIS 2004 Proceedings",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Emerging economies frequently show a large component of their Gross Domestic\nProduct to be dependant on the economic activity of small and medium\nenterprises. Nevertheless, e-business solutions are more likely designed for\nlarge companies. SMEs seem to follow a classical family-based management, used\nto traditional activities, rather than seeking new ways of adding value to\ntheir business strategy. Thus, a large portion of a nations economy may be at\ndisadvantage for competition. This paper aims at assessing the state of\ne-business readiness of Mexican SMEs based on already published e-business\nevolution models and by means of a survey research design. Data is being\ncollected in three cities with differing sizes and infrastructure conditions.\nStatistical results are expected to be presented. A second part of this\nresearch aims at applying classical adoption models to suggest potential causal\nrelationships, as well as more suitable recommendations for development.\n"
    },
    {
        "paper_id": 1804.0671,
        "authors": "Haibo Chen",
        "title": "An Attempt at Analyzing the Information Nature of Money",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Money was invented to address the difficulty in the double coincidence of\nwants between the supply and demand when people exchanged their goods and\nservices. There are two information states in society: one is the initial state\nthat people have goods and services due to division of labor; the other is the\nfinal state that people have different goods and services with the initial\nstate due to exchange of goods and services between them. The process is that\nthe initial state is changed to the final state with the help of money. Because\nthe direct exchanges of goods and services are difficult to achieve the double\ncoincidence of wants in time and space, it can be achieved with the help of\nmoney which is as a medium and bridge. In this paper the changing process of\nthe state information is analyzed through the matrix representation of money,\nand then the nature of money with a kind of information of reliable ledger is\nmore apparently shown. This paper also analyzes the common characteristics of\nphysical money, electronic money and digital currency, that is, reliable ledger\nand explores the future trend of money development from the perspective of\nhistory and security technology of money.\n"
    },
    {
        "paper_id": 1804.06711,
        "authors": "Igor Rivin and Carlo Scevola",
        "title": "The CCI30 Index",
        "comments": "2pp",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the design of the CCI30 cryptocurrency index.\n"
    },
    {
        "paper_id": 1804.0689,
        "authors": "Mike Derksen, Peter Spreij, Sweder van Wijnbergen",
        "title": "Accounting Noise and the Pricing of CoCos",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance 25(7,8),\n  2023",
        "doi": "10.1142/S0219024922500285",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contingent Convertible bonds (CoCos) are debt instruments that convert into\nequity or are written down in times of distress. Existing pricing models assume\nconversion triggers based on market prices and on the assumption that markets\ncan always observe all relevant firm information. But all Cocos issued so far\nhave triggers based on accounting ratios and/or regulatory intervention. We\nincorporate that markets receive information through noisy accounting reports\nissued at discrete time instants, which allows us to distinguish between market\nand accounting values, and between automatic triggers and regulator-mandated\nconversions. Our second contribution is to incorporate that coupon payments are\ncontingent too: their payment is conditional on the Maximum Distributable\nAmount not being exceeded. We examine the impact of CoCo design parameters,\nasset volatility and accounting noise on the price of a CoCo; and investigate\nthe interaction between CoCo design features, the capital structure of the\nissuing bank and their implications for risk taking and investment incentives.\nFinally, we use our model to explain the crash in CoCo prices after Deutsche\nBank's profit warning in February 2016.\n"
    },
    {
        "paper_id": 1804.07022,
        "authors": "Hong Wang, Catherine S. Forbes, Jean-Pierre Fenech and John Vaz",
        "title": "The determinants of bank loan recovery rates in good times and bad - new\n  evidence",
        "comments": "29 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find that factors explaining bank loan recovery rates vary depending on\nthe state of the economic cycle. Our modeling approach incorporates a two-state\nMarkov switching mechanism as a proxy for the latent credit cycle, helping to\nexplain differences in observed recovery rates over time. We are able to\ndemonstrate how the probability of default and certain loan-specific and other\nvariables hold different explanatory power with respect to recovery rates over\n`good' and `bad' times in the credit cycle. That is, the relationship between\nrecovery rates and certain loan characteristics, firm characteristics and the\nprobability of default differs depending on underlying credit market\nconditions. This holds important implications for modelling capital retention,\nparticularly in terms of countercyclicality.\n"
    },
    {
        "paper_id": 1804.07352,
        "authors": "Ya-Chun Gao, Huai-Lin Tang, Shi-Min Cai, Jing-Jing Gao, H. Eugene\n  Stanley",
        "title": "The impact of margin trading on share price evolution: A cascading\n  failure model investigation",
        "comments": "14 pages, 9 figures",
        "journal-ref": "Physica A 505, 69-76 (2018)",
        "doi": "10.1016/j.physa.2018.03.032",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Margin trading in which investors purchase shares with money borrowed from\nbrokers is blamed to be a major cause of the 2015 Chinese stock market crash.\nWe propose a cascading failure model and examine how an increase in margin\ntrading increases share price vulnerability. The model is based on a bipartite\ngraph of investors and shares that includes four margin trading factors, (i)\ninitial margin $k$, (ii) minimum maintenance $r$, (iii) volatility $v$, and\n(iv) diversity $s$. We use our model to simulate margin trading and observe how\nthe share prices are affected by these four factors. The experimental results\nindicate that a stock market can be either vulnerable or stable. A stock market\nis vulnerable when an external shock can cause a cascading failure of its share\nprices. It is stable when its share prices are resilient to external shocks.\nFurthermore, we investigate how the cascading failure of share price is\naffected by these four factors, and find that by increasing $v$ and $r$ or\ndecreasing $k$ we increase the probability that the stock market will\nexperience a phase transition from stable to vulnerable. It is also found that\nincreasing $s$ decreases resilience and increases systematic risk. These\nfindings could be useful to regulators supervising margin trading activities.\n"
    },
    {
        "paper_id": 1804.07384,
        "authors": "Evangelos Melas",
        "title": "Classes of elementary function solutions to the CEV model. I",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The CEV model subsumes some of the previous option pricing models. An\nimportant parameter in the model is the parameter b, the elasticity of\nvolatility. For b=0, b=-1/2, and b=-1 the CEV model reduces respectively to the\nBSM model, the square-root model of Cox and Ross, and the Bachelier model. Both\nin the case of the BSM model and in the case of the CEV model it has become\ntraditional to begin a discussion of option pricing by starting with the\nvanilla European calls and puts. In the case of BSM model simpler solutions are\nthe log and power solutions. These contracts, despite the simplicity of their\nmathematical description, are attracting increasing attention as a trading\ninstrument. Similar simple solutions have not been studied so far in a\nsystematic fashion for the CEV model. We use Kovacic's algorithm to derive, for\nall half-integer values of b, all solutions \"in quadratures\" of the CEV\nordinary differential equation. These solutions give rise, by separation of\nvariables, to simple solutions to the CEV partial differential equation. In\nparticular, when b=...,-5/2,-2,-3/2,-1, 1, 3/2, 2, 5/2,..., we obtain four\nclasses of denumerably infinite elementary function solutions, when b=-1/2 and\nb=1/2 we obtain two classes of denumerably infinite elementary function\nsolutions, whereas, when b=0 we find two elementary function solutions. In the\nderived solutions we have also dispensed with the unnecessary assumption made\nin the the BSM model asserting that the underlying asset pays no dividends\nduring the life of the option.\n"
    },
    {
        "paper_id": 1804.07392,
        "authors": "Peter Bank and Moritz Vo{\\ss}",
        "title": "Optimal investment with transient price impact",
        "comments": "61 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a price impact model which accounts for finite market depth,\ntightness and resilience. Its coupled bid- and ask-price dynamics induce convex\nliquidity costs. We provide existence of an optimal solution to the classical\nproblem of maximizing expected utility from terminal liquidation wealth at a\nfinite planning horizon. In the specific case when market uncertainty is\ngenerated by an arithmetic Brownian motion with drift and the investor exhibits\nconstant absolute risk aversion, we show that the resulting singular optimal\nstochastic control problem readily reduces to a deterministic optimal tracking\nproblem of the optimal frictionless constant Merton portfolio in the presence\nof convex costs. Rather than studying the associated Hamilton-Jacobi-Bellmann\nPDE, we exploit convex analytic and calculus of variations techniques allowing\nus to construct the solution explicitly and to describe the free boundaries of\nthe action- and non-action regions in the underlying state space. As expected,\nit is optimal to trade towards the frictionless Merton position, taking into\naccount the initial bid-ask spread as well as the optimal liquidation of the\naccrued position when approaching terminal time. It turns out that this leads\nto a surprisingly rich phenomenology of possible trajectories for the optimal\nshare holdings.\n"
    },
    {
        "paper_id": 1804.07534,
        "authors": "Kuldip Singh Patel and Mani Mehra",
        "title": "Fourth order compact scheme for option pricing under Merton and Kou\n  jump-diffusion models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, a three-time levels compact scheme is proposed to solve the\npartial integro-differential equation governing the option prices under\njump-diffusion models. In the proposed compact scheme, the second derivative\napproximation of unknowns is approximated by the value of unknowns and their\nfirst derivative approximations which allow us to obtain a tri-diagonal system\nof linear equations for the fully discrete problem. Moreover, consistency and\nstability of the proposed compact scheme are proved. Due to the low regularity\nof typical initial conditions, the smoothing operator is employed to ensure the\nfourth-order convergence rate. Numerical illustrations for pricing European\noptions under Merton and Kou jump-diffusion models are presented to validate\nthe theoretical results.\n"
    },
    {
        "paper_id": 1804.07556,
        "authors": "Martin Keller-Ressel, Thorsten Schmidt, Robert Wardenga",
        "title": "Affine processes beyond stochastic continuity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study time-inhomogeneous affine processes beyond the common\nassumption of stochastic continuity. In this setting times of jumps can be both\ninaccessible and predictable. To this end we develop a general theory of finite\ndimensional affine semimartingales under very weak assumptions. We show that\nthe corresponding semimartingale characteristics have affine form and that the\nconditional characteristic function can be represented with solutions to\nmeasure differential equations of Riccati type. We prove existence of affine\nMarkov processes and affine semimartingales under mild conditions and elaborate\non examples and applications including affine processes in discrete time.\n"
    },
    {
        "paper_id": 1804.07852,
        "authors": "Andre Catalao and Rogerio Rosenfeld",
        "title": "Analytical Path-Integral Pricing of Moving-Barrier Options under\n  non-Gaussian Distributions",
        "comments": "67 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we present an analytical model, based on the path-integral\nformalism of Statistical Mechanics, for pricing options using first-passage\ntime problems involving both fixed and deterministically moving absorbing\nbarriers under possible non-gaussian distributions of the underlying object. We\nadapt to our problem a model originally proposed to describe the formation of\ngalaxies in the universe of De Simone et al (2011), which uses cumulant\nexpansions in terms of the Gaussian distribution, and we generalize it to take\ninto acount drift and cumulants of orders higher than three. From the\nprobability density function, we obtain an analytical pricing model, not only\nfor vanilla options (thus removing the need of volatility smile inherent to the\nBlack-Scholes model), but also for fixed or deterministically moving barrier\noptions. Market prices of vanilla options are used to calibrate the model, and\nbarrier option pricing arising from the model is compared to the price resulted\nfrom the relative entropy model.\n"
    },
    {
        "paper_id": 1804.07978,
        "authors": "Charles Shaw",
        "title": "Conditional heteroskedasticity in crypto-asset returns",
        "comments": "28 pages, 27 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the time series properties of cryptocurrency assets, such\nas Bitcoin, using established econometric inference techniques, namely models\nof the GARCH family. The contribution of this study is twofold. I explore the\ntime series properties of cryptocurrencies, a new type of financial asset on\nwhich there appears to be little or no literature. I suggest an improved\neconometric specification to that which has been recently proposed in Chu et al\n(2017), the first econometric study to examine the price dynamics of the most\npopular cryptocurrencies. Questions regarding the reliability of their study\nstem from the authors mis-diagnosing the distribution of GARCH innovations.\nChecks are performed on whether innovations are Gaussian or GED by using\nKolmogorov type non-parametric tests and Khmaladze's martingale transformation.\nNull of gaussianity is strongly rejected for all GARCH(p,q) models, with $p,q\n\\in \\{1,\\ldots,5 \\}$, for all cryptocurrencies in sample. For tests of\nnormality, I make use of the Gauss-Kronrod quadrature. Parameters of GARCH\nmodels are estimated with generalized error distribution innovations using\nmaximum likelihood. For calculating P-values, the parametric bootstrap method\nis used. Arguing against Chu et al (2017), I show that there is a strong\nempirical argument against modelling innovations under some common assumptions.\n"
    },
    {
        "paper_id": 1804.07997,
        "authors": "Krzysztof Burnecki, Mario Nicol\\'o Giuricich and Zbigniew Palmowski",
        "title": "Valuation of contingent convertible catastrophe bonds - the case for\n  equity conversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the context of the banking-related literature on contingent\nconvertible bonds, we comprehensively formalise the design and features of a\nrelatively new type of insurance-linked security, called a contingent\nconvertible catastrophe bond (CocoCat). We begin with a discussion of its\ndesign and compare its relative merits to catastrophe bonds and\ncatastrophe-equity puts. Subsequently, we derive analytical valuation formulae\nfor index-linked CocoCats under the assumption of independence between natural\ncatastrophe and financial markets risks. We model natural catastrophe losses by\na time-inhomogeneous compound Poisson process, with the interest-rate process\ngoverned by the Longstaff model. By using an exponential change of measure on\nthe loss process, as well as a Girsanov-like transformation to synthetically\nremove the correlation between the share and interest-rate processes, we obtain\nthese analytical formulae. Using selected parameter values in line with earlier\nresearch, we empirically analyse our valuation formulae for index-linked\nCocoCats. An analysis of the results reveals that the CocoCat prices are most\nsensitive to changing interest-rates, conversion fractions and the threshold\nlevels defining the trigger times.\n"
    },
    {
        "paper_id": 1804.08021,
        "authors": "Victor E. Gluzberg and Yuri A. Katz",
        "title": "Planetary boundaries of consumption growth: Declining social discount\n  rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the logistic model of consumption growth, which captures a\nnegative feedback loop preventing an unlimited growth of consumption due to\nfinite biophysical resources of our planet. This simple dynamic model allows\nfor derivation of the expression describing the declining long-term tail of a\nsocial discount curve. The latter plays a critical role in, e.g., climate\nfinance with benefits on current investments deferred to centuries from now.\nThe growth rate of consumption is irregularly evolving in time, which makes\nestimation of an expected term-structure of consumption growth and associated\nsocial discount rates a challenging task. Nonetheless, observations show that\nthe problem at hand is perturbative with the small parameter being the product\nof an average strength of fluctuations in the growth rate and its\nautocorrelation time. This fact permits utilization of the cumulant expansion\nmethod to derive remarkably simple expressions for the term-structure of\nexpected consumption growth and associated discount rates in the bounded\neconomy. Comparison with empirical data shows that the dynamic effect related\nto the planetary resource constrains could become a dominant mechanism\nresponsible for a declining long-term tail of a social discount curve at the\ntime horizon estimated here as about100 years from now (the lower boundary).\nThe derived results can help to shape a more realistic long-term social\ndiscounting policy. Furthermore, with the obvious redefinition of the key\nparameters of the model, obtained results are directly applicable for\ndescription of expected long-term population growth in stochastic environments.\n"
    },
    {
        "paper_id": 1804.08442,
        "authors": "Adriana Ocejo",
        "title": "Explicit solutions to utility maximization problems in a\n  regime-switching market model via Laplace transforms",
        "comments": "Preprint accepted in Nonlinear Analysis: Hybrid Systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of utility maximization from terminal wealth in which an\nagent optimally builds her portfolio by investing in a bond and a risky asset.\nThe asset price dynamics follow a diffusion process with regime-switching\ncoefficients modeled by a continuous-time finite-state Markov chain. We\nconsider an investor with a Constant Relative Risk Aversion (CRRA) utility\nfunction. We deduce the associated Hamilton-Jacobi-Bellman equation to\nconstruct the solution and the optimal trading strategy and verify optimality\nby showing that the value function is the unique constrained viscosity solution\nof the HJB equation. By means of a Laplace transform method, we show how to\nexplicitly compute the value function and illustrate the method with the two-\nand three-states cases. This method is interesting in its own right and can be\nadapted in other applications involving hybrid systems and using other types of\ntransforms with basic properties similar to the Laplace transform.\n"
    },
    {
        "paper_id": 1804.08472,
        "authors": "Liao Zhu, Sumanta Basu, Robert A. Jarrow, Martin T. Wells",
        "title": "High-Dimensional Estimation, Basis Assets, and the Adaptive Multi-Factor\n  Model",
        "comments": null,
        "journal-ref": "The Quarterly Journal of Finance. Vol. 10, No. 04, 2050017 (2020)",
        "doi": "10.1142/S2010139220500172",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a new algorithm for the high-dimensional financial data --\nthe Groupwise Interpretable Basis Selection (GIBS) algorithm, to estimate a new\nAdaptive Multi-Factor (AMF) asset pricing model, implied by the recently\ndeveloped Generalized Arbitrage Pricing Theory, which relaxes the convention\nthat the number of risk-factors is small. We first obtain an adaptive\ncollection of basis assets and then simultaneously test which basis assets\ncorrespond to which securities, using high-dimensional methods. The AMF model,\nalong with the GIBS algorithm, is shown to have a significantly better fitting\nand prediction power than the Fama-French 5-factor model.\n"
    },
    {
        "paper_id": 1804.08904,
        "authors": "Michael Kurz",
        "title": "Closed-form approximations in derivatives pricing: The Kristensen-Mele\n  approach",
        "comments": "Master Thesis for the degree M.Sc. in Economics and Finance at\n  Eberhard Karls University of Tuebingen (Germany)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Kristensen and Mele (2011) developed a new approach to obtain closed-form\napproximations to continuous-time derivatives pricing models. The approach uses\na power series expansion of the pricing bias between an intractable model and\nsome known auxiliary model. Since the resulting approximation formula has\nclosed-form it is straightforward to obtain approximations of greeks. In this\nthesis I will introduce Kristensen and Mele's methods and apply it to a variety\nof stochastic volatility models of European style options as well as a model\nfor commodity futures. The focus of this thesis is the effect of different\nmodel choices and different model parameter values on the numerical stability\nof Kristensen and Mele's approximation.\n"
    },
    {
        "paper_id": 1804.09043,
        "authors": "Kuldip Singh Patel and Mani Mehra",
        "title": "Compact finite difference method for pricing European and American\n  options under jump-diffusion models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1804.07534",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, a compact finite difference method is proposed for pricing\nEuropean and American options under jump-diffusion models. Partial\nintegro-differential equation and linear complementary problem governing\nEuropean and American options respectively are discretized using Crank-Nicolson\nLeap-Frog scheme. In proposed compact finite difference method, the second\nderivative is approximated by the value of unknowns and their first derivative\napproximations which allow us to obtain a tri-diagonal system of linear\nequations for the fully discrete problem. Further, consistency and stability\nfor the fully discrete problem are also proved. Since jump-diffusion models do\nnot have smooth initial conditions, the smoothing operators are employed to\nensure fourth-order convergence rate. Numerical illustrations for pricing\nEuropean and American options under Merton jump-diffusion model are presented\nto validate the theoretical results.\n"
    },
    {
        "paper_id": 1804.09056,
        "authors": "Richard Martin and Yao Ma",
        "title": "Emerging Market Corporate Bonds as First-to-Default Baskets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Emerging market hard-currency bonds are an asset class of growing importance,\nand contain exposure to an EM sovereign and the underlying industry. The\nauthors investigate how to model this as a modification of the well-known\nfirst-to-default (FtD) basket, using the structural model, and find the\napproach feasible.\n"
    },
    {
        "paper_id": 1804.09151,
        "authors": "Michail Anthropelos, Scott Robertson, Konstantinos Spiliopoulos",
        "title": "Optimal Investment, Demand and Arbitrage under Price Impact",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal investment problem with random endowment in an\ninventory-based price impact model with competitive market makers. Our goal is\nto analyze how price impact affects optimal policies, as well as both pricing\nrules and demand schedules for contingent claims. For exponential market makers\npreferences, we establish two effects due to price impact: constrained trading,\nand non-linear hedging costs. To the former, wealth processes in the impact\nmodel are identified with those in a model without impact, but with constrained\ntrading, where the (random) constraint set is generically neither closed nor\nconvex. Regarding hedging, non-linear hedging costs motivate the study of\narbitrage free prices for the claim. We provide three such notions, which\ncoincide in the frictionless case, but which dramatically differ in the\npresence of price impact. Additionally, we show arbitrage opportunities, should\nthey arise from claim prices, can be exploited only for limited position sizes,\nand may be ignored if outweighed by hedging considerations. We also show that\narbitrage inducing prices may arise endogenously in equilibrium, and that\nequilibrium positions are inversely proportional to the market makers'\nrepresentative risk aversion. Therefore, large positions endogenously arise in\nthe limit of either market maker risk neutrality, or a large number of market\nmakers.\n"
    },
    {
        "paper_id": 1804.09253,
        "authors": "Kevin Kuo",
        "title": "DeepTriangle: A Deep Learning Approach to Loss Reserving",
        "comments": "Published version available at https://www.mdpi.com/2227-9091/7/3/97",
        "journal-ref": "Risks 2019, 7(3), 97",
        "doi": "10.3390/risks7030097",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach for loss reserving based on deep neural networks.\nThe approach allows for joint modeling of paid losses and claims outstanding,\nand incorporation of heterogeneous inputs. We validate the models on loss\nreserving data across lines of business, and show that they improve on the\npredictive accuracy of existing stochastic methods. The models require minimal\nfeature engineering and expert input, and can be automated to produce forecasts\nmore frequently than manual workflows.\n"
    },
    {
        "paper_id": 1804.09302,
        "authors": "Miao Yuan, Cheng Yong Tang, Yili Hong, Jian Yang",
        "title": "Disentangling and Assessing Uncertainties in Multiperiod Corporate\n  Default Risk Predictions",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measuring the corporate default risk is broadly important in economics and\nfinance. Quantitative methods have been developed to predictively assess future\ncorporate default probabilities. However, as a more difficult yet crucial\nproblem, evaluating the uncertainties associated with the default predictions\nremains little explored. In this paper, we attempt to fill this blank by\ndeveloping a procedure for quantifying the level of associated uncertainties\nupon carefully disentangling multiple contributing sources. Our framework\neffectively incorporates broad information from historical default data,\ncorporates' financial records, and macroeconomic conditions by a)\ncharacterizing the default mechanism, and b) capturing the future dynamics of\nvarious features contributing to the default mechanism. Our procedure overcomes\nthe major challenges in this large scale statistical inference problem and\nmakes it practically feasible by using parsimonious models, innovative methods,\nand modern computational facilities. By predicting the marketwide total number\nof defaults and assessing the associated uncertainties, our method can also be\napplied for evaluating the aggregated market credit risk level. Upon analyzing\na US market data set, we demonstrate that the level of uncertainties associated\nwith default risk assessments is indeed substantial. More informatively, we\nalso find that the level of uncertainties associated with the default risk\npredictions is correlated with the level of default risks, indicating potential\nfor new scopes in practical applications including improving the accuracy of\ndefault risk assessments.\n"
    },
    {
        "paper_id": 1804.09532,
        "authors": "Antoine Kamiantako Miyamueni, Henry Ngongo Muganza",
        "title": "Chocs technologiques, chocs des prix et fluctuations du ch\\^omage en\n  R\\'epublique D\\'emocratique du Congo",
        "comments": "in French",
        "journal-ref": "Revue congolaise d'\\'economie et de gestion, vol. 12, num\\'ero 12,\n  Universit\\'e Protestante au Congo, avril 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of this research is to analyze the effects of macroeconomic\nshocks on unemployment fluctuations in the Democratic Republic of Congo (DRC).\nUsing the SVECM model on DRC data for the period 1960 to 2014, the conclusion\nis that the high and persistent level of unemployment is mainly explained\npermanently by technological and price shocks.\n"
    },
    {
        "paper_id": 1804.0955,
        "authors": "Mario Coccia and Matteo Bellitto",
        "title": "Critical analysis of human progress: Its negative and positive sides in\n  the late-capitalism",
        "comments": "34 pages, 1 figure, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The concept of progress has characterized human society from millennia.\nHowever, this concept is elusive and too often given for certain. The goal of\nthis paper is to suggest a general definition of human progress that satisfies,\nwhenever possible the conditions of independence, generality, epistemological\napplicability and empirical correctness. This study proposes, within a\npragmatic approach, human progress as an inexhaustible process driven by an\nideal of maximum wellbeing of purposeful people which, on attainment of any of\nits goals or objectives for increasing wellbeing, then seek another\nconsequential goal and objective, endlessly, which more closely approximates\nits ideal fixed in new socioeconomic contexts over time and space. The human\nprogress, in the global, capitalistic, and post-humanistic Era, improves the\nfundamental life-interests represented by health, wealth, expansion of\nknowledge, technology and freedom directed to increase wellbeing throughout the\nsociety. These factors support the acquisition by humanity of better and more\ncomplex forms of life. However, this study shows the inconsistency of the\nequation economic growth= progress because human progress also generates,\nduring its continuous process without limit, negative effects for human being,\nenvironment and society.\n"
    },
    {
        "paper_id": 1804.09565,
        "authors": "Fr\\'ed\\'eric Bucci, Iacopo Mastromatteo, Zolt\\'an Eisler, Fabrizio\n  Lillo, Jean-Philippe Bouchaud and Charles-Albert Lehalle",
        "title": "Co-impact: Crowding effects in institutional trading activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to the important yet unexplored subject of crowding\neffects on market impact, that we call \"co-impact\". Our analysis is based on a\nlarge database of metaorders by institutional investors in the U.S. equity\nmarket. We find that the market chiefly reacts to the net order flow of ongoing\nmetaorders, without individually distinguishing them. The joint co-impact of\nmultiple contemporaneous metaorders depends on the total number of metaorders\nand their mutual sign correlation. Using a simple heuristic model calibrated on\ndata, we reproduce very well the different regimes of the empirical market\nimpact curves as a function of volume fraction $\\phi$: square-root for large\n$\\phi$, linear for intermediate $\\phi$, and a finite intercept $I_0$ when $\\phi\n\\to 0$. The value of $I_0$ grows with the sign correlation coefficient. Our\nstudy sheds light on an apparent paradox: How can a non-linear impact law\nsurvive in the presence of a large number of simultaneously executed\nmetaorders?\n"
    },
    {
        "paper_id": 1804.09752,
        "authors": "Aida Abiad, Sander Gribling, Domenico Lahaye, Matthias Mnich, Guus\n  Regts, Lluis Vena, Gerard Verweij, Peter Zwaneveld",
        "title": "On the complexity of solving a decision problem with flow-depending\n  costs: the case of the IJsselmeer dikes",
        "comments": "23 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a fundamental integer programming (IP) model for cost-benefit\nanalysis flood protection through dike building in the Netherlands, due to\nVerweij and Zwaneveld.\n  Experimental analysis with data for the Ijsselmeer lead to integral optimal\nsolution of the linear programming relaxation of the IP model.\n  This naturally led to the question of integrality of the polytope associated\nwith the IP model.\n  In this paper we first give a negative answer to this question by\nestablishing non-integrality of the polytope.\n  Second, we establish natural conditions that guarantee the linear programming\nrelaxation of the IP model to be integral.\n  We then test the most recent data on flood probabilities, damage and\ninvestment costs of the IJsselmeer for these conditions.\n  Third, we show that the IP model can be solved in polynomial time when the\nnumber of dike segments, or the number of feasible barrier heights, are\nconstant.\n"
    },
    {
        "paper_id": 1804.10264,
        "authors": "David Hartman, Jaroslav Hlinka",
        "title": "Nonlinearity in stock networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock networks, constructed from stock price time series, are a\nwell-established tool for the characterization of complex behavior in stock\nmarkets. Following Mantegna's seminal paper, the linear Pearson's correlation\ncoefficient between pairs of stocks has been the usual way to determine network\nedges. Recently, possible effects of nonlinearity on the graph-theoretical\nproperties of such networks have been demonstrated when using nonlinear\nmeasures such as mutual information instead of linear correlation. In this\npaper, we quantitatively characterize the nonlinearity in stock time series and\nthe effect it has on stock network properties. This is achieved by a systematic\nmulti-step approach that allows us to quantify the nonlinearity of coupling;\ncorrect its effects wherever it is caused by simple univariate non-Gaussianity;\npotentially localize in space and time any remaining strong sources of this\nnonlinearity; and, finally, study the effect nonlinearity has on global network\nproperties. By applying this multi-step approach to stocks included in three\nprominent indices (NYSE100, FTSE100 and SP500), we establish that the apparent\nnonlinearity that has been observed is largely due to univariate\nnon-Gaussianity. Furthermore, strong nonstationarity in a few specific stocks\nmay play a role. In particular, the sharp decrease in some stocks during the\nglobal financial crisis of 2008 gives rise to apparent nonlinear dependencies\namong stocks.\n"
    },
    {
        "paper_id": 1804.10753,
        "authors": "Edward Kim, Tianyang Nie, Marek Rutkowski",
        "title": "Arbitrage-free pricing of American options in nonlinear markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We re-examine and extend the findings from the recent paper by Dumitrescu,\nQuenez and Sulem (2018) who studied American and game options in a particular\nmarket model using the nonlinear arbitrage-free pricing approach developed in\nEl Karoui and Quenez (1997). In the first part, we provide a detailed study of\nunilateral valuation problems for the two counterparties in an American-style\ncontract within the framework of a general nonlinear market. We extend results\nfrom Bielecki and Rutkowski (2015) and Bielecki, Cialenco and Rutkowski (2018)\nwho examined the case of a European-style contract. In the second part, we\npresent a BSDE approach, which is used to establish more explicit pricing,\nhedging and exercising results when solutions to reflected BSDEs have\nadditional desirable properties.\n"
    },
    {
        "paper_id": 1804.10869,
        "authors": "Danish A. Alvi",
        "title": "Application of Probabilistic Graphical Models in Forecasting Crude Oil\n  Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The dissertation investigates the application of Probabilistic Graphical\nModels (PGMs) in forecasting the price of Crude Oil.\n  This research is important because crude oil plays a very pivotal role in the\nglobal economy hence is a very critical macroeconomic indicator of the\nindustrial growth. Given the vast amount of macroeconomic factors affecting the\nprice of crude oil such as supply of oil from OPEC countries, demand of oil\nfrom OECD countries, geopolitical and geoeconomic changes among many other\nvariables - probabilistic graphical models (PGMs) allow us to understand by\nlearning the graphical structure. This dissertation proposes condensing data\nnumerous Crude Oil factors into a graphical model in the attempt of creating a\naccurate forecast of the price of crude oil.\n  The research project experiments with using different libraries in Python in\norder to construct models of the crude oil market. The experiments in this\nthesis investigate three main challenges commonly presented while trading oil\nin the financial markets. The first challenge it investigates is the process of\nlearning the structure of the oil markets; thus allowing crude oil traders to\nunderstand the different physical market factors and macroeconomic indicators\naffecting crude oil markets and how they are \\textit{causally} related. The\nsecond challenge it solves is the exploration and exploitation of the available\ndata and the learnt structure in predicting the behaviour of the oil markets.\nThe third challenge it investigates is how to validate the performance and\nreliability of the constructed model in order for it to be deployed in the\nfinancial markets.\n  A design and implementation of a probabilistic framework for forecasting the\nprice of crude oil is also presented as part of the research.\n"
    },
    {
        "paper_id": 1805.00205,
        "authors": "Yifeng Guo, Xingyu Fu, Yuyan Shi, Mingwen Liu",
        "title": "Robust Log-Optimal Strategy with Reinforcement Learning",
        "comments": "14 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We proposed a new Portfolio Management method termed as Robust Log-Optimal\nStrategy (RLOS), which ameliorates the General Log-Optimal Strategy (GLOS) by\napproximating the traditional objective function with quadratic Taylor\nexpansion. It avoids GLOS's complex CDF estimation process,hence resists the\n\"Butterfly Effect\" caused by estimation error. Besides,RLOS retains GLOS's\nprofitability and the optimization problem involved in RLOS is computationally\nfar more practical compared to GLOS. Further, we combine RLOS with\nReinforcement Learning (RL) and propose the so-called Robust Log-Optimal\nStrategy with Reinforcement Learning (RLOSRL), where the RL agent receives the\nanalyzed results from RLOS and observes the trading environment to make\ncomprehensive investment decisions. The RLOSRL's performance is compared to\nsome traditional strategies on several back tests, where we randomly choose a\nselection of constituent stocks of the CSI300 index as assets under management\nand the test results validate its profitability and stability.\n"
    },
    {
        "paper_id": 1805.00268,
        "authors": "Johannes Bock",
        "title": "Quantifying macroeconomic expectations in stock markets using Google\n  Trends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Among other macroeconomic indicators, the monthly release of U.S.\nunemployment rate figures in the Employment Situation report by the U.S. Bureau\nof Labour Statistics gets a lot of media attention and strongly affects the\nstock markets. I investigate whether a profitable investment strategy can be\nconstructed by predicting the likely changes in U.S. unemployment before the\nofficial news release using Google query volumes for related search terms. I\nfind that massive new data sources of human interaction with the Internet not\nonly improves U.S. unemployment rate predictability, but can also enhance\nmarket timing of trading strategies when considered jointly with macroeconomic\ndata. My results illustrate the potential of combining extensive behavioural\ndata sets with economic data to anticipate investor expectations and stock\nmarket moves.\n"
    },
    {
        "paper_id": 1805.00387,
        "authors": "Fausto Cavalli, Ahmad Naimzada, Nicol\\`o Pecora and Marina Pireddu",
        "title": "Agents' beliefs and economic regimes polarization in interacting markets",
        "comments": "41 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1063/1.5024370",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper a model of a market consisting of real and financial\ninteracting sectors is studied. Agents populating the stock market are assumed\nto be not able to observe the true underlying fundamental, and their beliefs\nare biased by either optimism or pessimism. Depending on the relevance they\ngive to beliefs, they select the best performing strategy in an evolutionary\nperspective. The real side of the economy is described within a\nmultiplier-accelerator framework with a nonlinear, bounded investment function.\nWe show that strongly polarized beliefs in an evolutionary framework can\nintroduce multiplicity of steady states, which, consisting in enhanced or\ndepressed levels of income, reflect and reproduce the optimistic or pessimistic\nnature of the agents' beliefs. The polarization of these steady states, which\ncoexist with an unbiased steady state, positively depends on that of the\nbeliefs and on their relevance. Moreover, with a mixture of analytical and\nnumerical tools, we show that such static characterization is inherited also at\nthe dynamical level, with possibly complex attractors that are characterized by\nendogenously fluctuating pessimistic and optimistic levels of national income\nand price. This framework, when stochastic perturbations are included, is able\nto account for stylized facts commonly observed in real financial markets, such\nas fat tails and excess volatility in the returns distributions, as well as\nbubbles and crashes for stock prices.\n"
    },
    {
        "paper_id": 1805.00558,
        "authors": "Tianyu Ray Li, Anup S. Chamrajnagar, Xander R. Fong, Nicholas R.\n  Rizik, Feng Fu",
        "title": "Sentiment-Based Prediction of Alternative Cryptocurrency Price\n  Fluctuations Using Gradient Boosting Tree Model",
        "comments": "working paper. comments are welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze Twitter signals as a medium for user sentiment to\npredict the price fluctuations of a small-cap alternative cryptocurrency called\n\\emph{ZClassic}. We extracted tweets on an hourly basis for a period of 3.5\nweeks, classifying each tweet as positive, neutral, or negative. We then\ncompiled these tweets into an hourly sentiment index, creating an unweighted\nand weighted index, with the latter giving larger weight to retweets. These two\nindices, alongside the raw summations of positive, negative, and neutral\nsentiment were juxtaposed to $\\sim 400$ data points of hourly pricing data to\ntrain an Extreme Gradient Boosting Regression Tree Model. Price predictions\nproduced from this model were compared to historical price data, with the\nresulting predictions having a 0.81 correlation with the testing data. Our\nmodel's predictive data yielded statistical significance at the $p < 0.0001$\nlevel. Our model is the first academic proof of concept that social media\nplatforms such as Twitter can serve as powerful social signals for predicting\nprice movements in the highly speculative alternative cryptocurrency, or\n\"alt-coin\", market.\n"
    },
    {
        "paper_id": 1805.00785,
        "authors": "Piero Mazzarisi, Fabrizio Lillo, Stefano Marmi",
        "title": "When panic makes you blind: a chaotic route to systemic risk",
        "comments": "24 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an analytical model to study the role of expectation feedbacks and\noverlapping portfolios on systemic stability of financial systems. Building on\n[Corsi et al., 2016], we model a set of financial institutions having Value at\nRisk capital requirements and investing in a portfolio of risky assets, whose\nprices evolve stochastically in time and are endogenously driven by the trading\ndecisions of financial institutions. Assuming that they use adaptive\nexpectations of risk, we show that the evolution of the system is described by\na slow-fast random dynamical system, which can be studied analytically in some\nregimes. The model shows how the risk expectations play a central role in\ndetermining the systemic stability of the financial system and how wrong risk\nexpectations may create panic-induced reduction or over-optimistic expansion of\nbalance sheets. Specifically, when investors are myopic in estimating the risk,\nthe fixed point equilibrium of the system breaks into leverage cycles and\nfinancial variables display a bifurcation cascade eventually leading to chaos.\nWe discuss the role of financial policy and the effects of some market\nfrictions, as the cost of diversification and financial transaction taxes, in\ndetermining the stability of the system in the presence of adaptive\nexpectations of risk.\n"
    },
    {
        "paper_id": 1805.00792,
        "authors": "Foad Shokrollahi",
        "title": "Pricing European option with the short rate under Subdiffusive\n  fractional Brownian motion regime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to analyze the problem of option pricing when\nthe short rate follows subdiffusive fractional Merton model. We incorporate the\nstochastic nature of the short rate in our option valuation model and derive\nexplicit formula for call and put option and discuss the corresponding\nfractional Black-Scholes equation. We present some properties of this pricing\nmodel for the cases of $\\alpha$ and $H$. Moreover, the numerical simulations\nillustrate that our model is flexible and easy to implement.\n"
    },
    {
        "paper_id": 1805.00896,
        "authors": "Alexis Akira Toda",
        "title": "Data-based Automatic Discretization of Nonparametric Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10614-020-10012-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although using non-Gaussian distributions in economic models has become\nincreasingly popular, currently there is no systematic way for calibrating a\ndiscrete distribution from the data without imposing parametric assumptions.\nThis paper proposes a simple nonparametric calibration method based on the\nGolub-Welsch algorithm for Gaussian quadrature. Application to an optimal\nportfolio problem suggests that assuming Gaussian instead of nonparametric\nshocks leads to up to 17% overweighting in the stock portfolio because the\ninvestor underestimates the probability of crashes.\n"
    },
    {
        "paper_id": 1805.00898,
        "authors": "Mariano Zeron Medina Laris, Ignacio Ruiz",
        "title": "Chebyshev Methods for Ultra-efficient Risk Calculations",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial institutions now face the important challenge of having to do\nmultiple portfolio revaluations for their risk computation. The list is almost\nendless: from XVAs to FRTB, stress testing programs, etc. These computations\nrequire from several hundred up to a few million revaluations. The cost of\nimplementing these calculations via a \"brute-force\" full revaluation is\nenormous. There is now a strong demand in the industry for algorithmic\nsolutions to the challenge. In this paper we show a solution based on Chebyshev\ninterpolation techniques. It is based on the demonstrated fact that those\ninterpolants show exponential convergence for the vast majority of pricing\nfunctions that an institution has. In this paper we elaborate on the theory\nbehind it and extend those techniques to any dimensionality. We then approach\nthe problem from a practical standpoint, illustrating how it can be applied to\nmany of the challenges the industry is currently facing. We show that the\ncomputational effort of many current risk calculations can be decreased orders\nof magnitude with the proposed techniques, without compromising accuracy.\nIllustrative examples include XVAs and IMM on exotics, XVA sensitivities,\nInitial Margin Simulations, IMA-FRTB and AAD.\n"
    },
    {
        "paper_id": 1805.01019,
        "authors": "Joel Nishimura",
        "title": "When a `rat race' implies an intergenerational wealth trap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two critical questions about intergenerational outcomes are: one, whether\nsignificant barriers or traps exist between different social or economic\nstrata; and two, the extent to which intergenerational outcomes do (or can be\nused to) affect individual investment and consumption decisions. We develop a\nmodel to explicitly relate these two questions, and prove the first such `rat\nrace' theorem, showing that a fundamental relationship exists between high\nlevels of individual investment and the existence of a wealth trap, which traps\notherwise identical agents at a lower level of wealth. Our simple model of\nintergenerational wealth dynamics involves agents which balance current\nconsumption with investment in a single descendant. Investments then determine\ndescendant wealth via a potentially nonlinear and discontinuous competitiveness\nfunction about which we do not make concavity assumptions. From this model we\ndemonstrate how to infer such a competitiveness function from investments,\nalong with geometric criteria to determine individual decisions. Additionally\nwe investigate the stability of a wealth distribution, both to local\nperturbations and to the introduction of new agents with no wealth.\n"
    },
    {
        "paper_id": 1805.01118,
        "authors": "Shuenn-Jyi Sheu, Li-Hsien Sun, Zheng Zhang",
        "title": "Portfolio Optimization with Delay Factor Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an optimal portfolio problem in the incomplete market where the\nunderlying assets depend on economic factors with delayed effects, such models\ncan describe the short term forecasting and the interaction with time lag among\ndifferent financial markets. The delay phenomenon can be recognized as the\nintegral type and the pointwise type. The optimal strategy is identified\nthrough maximizing the power utility. Due to the delay leading to the\nnon-Markovian structure, the conventional Hamilton-Jacobi-Bellman (HJB)\napproach is no longer applicable. By using the stochastic maximum principle, we\nargue that the optimal strategy can be characterized by the solutions of a\ndecoupled quadratic forward-backward stochastic differential\nequations(QFBSDEs). The optimality is verified via the super-martingale\nargument. The existence and uniqueness of the solution to the QFBSDEs are\nestablished. In addition, if the market is complete, we also provide a\nmartingale based method to solve our portfolio optimization problem, and\ninvestigate its connection with the proposed FBSDE approach. Finally, two\nparticular cases are analyzed where the corresponding FBSDEs can be solved\nexplicitly.\n"
    },
    {
        "paper_id": 1805.02605,
        "authors": "Ernst Eberlein, Christoph Gerhart and Zorana Grbac",
        "title": "Multiple curve L\\'evy forward price model allowing for negative interest\n  rates",
        "comments": "26 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a framework for discretely compounding interest\nrates which is based on the forward price process approach. This approach has a\nnumber of advantages, in particular in the current market environment. Compared\nto the classical as well as the L\\'evy Libor market model, it allows in a\nnatural way for negative interest rates and has superb calibration properties\neven in the presence of extremely low rates. Moreover, the measure changes\nalong the tenor structure are simplified significantly. These properties make\nit an excellent base for a post-crisis multiple curve setup. Two variants for\nmultiple curve constructions are discussed. Time-inhomogeneous L\\'evy processes\nare used as driving processes. An explicit formula for the valuation of caps is\nderived using Fourier transform techniques. Based on the valuation formula, we\ncalibrate the two model variants to market data.\n"
    },
    {
        "paper_id": 1805.02741,
        "authors": "Omar El Euch, Thibaut Mastrolia, Mathieu Rosenbaum, Nizar Touzi",
        "title": "Optimal make-take fees for market making regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an exchange who wishes to set suitable make-take fees to attract\nliquidity on its platform. Using a principal-agent approach, we are able to\ndescribe in quasi-explicit form the optimal contract to propose to a market\nmaker. This contract depends essentially on the market maker inventory\ntrajectory and on the volatility of the asset. We also provide the optimal\nquotes that should be displayed by the market maker. The simplicity of our\nformulas allows us to analyze in details the effects of optimal contracting\nwith an exchange, compared to a situation without contract. We show in\nparticular that it leads to higher quality liquidity and lower trading costs\nfor investors.\n"
    },
    {
        "paper_id": 1805.02909,
        "authors": "Gechun Liang, Zhou Yang",
        "title": "Analysis of the optimal exercise boundary of American put options with\n  delivery lags",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A make-your-mind-up option is an American derivative with delivery lags. We\nshow that its put option can be decomposed as a European put and a new type of\nAmerican-style derivative. The latter is an option for which the investor\nreceives the Greek Theta of the corresponding European option as the running\npayoff, and decides an optimal stopping time to terminate the contract. Based\non this decomposition and using free boundary techniques, we show that the\nassociated optimal exercise boundary exists and is a strictly increasing and\nsmooth curve, and analyze the asymptotic behavior of the value function and the\noptimal exercise boundary for both large maturity and small time lag.\n"
    },
    {
        "paper_id": 1805.03143,
        "authors": "Carey Caginalp",
        "title": "A Dynamical Systems Approach to Cryptocurrency Stability",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, the notion of cryptocurrencies has come to the fore of public\ninterest. These assets that exist only in electronic form, with no underlying\nvalue, offer the owners some protection from tracking or seizure by government\nor creditors. We model these assets from the perspective of asset flow\nequations developed by Caginalp and Balenovich, and investigate their stability\nunder various parameters, as classical finance methodology is inapplicable. By\nutilizing the concept of liquidity price and analyzing stability of the\nresulting system of ordinary differential equations, we obtain conditions under\nwhich the system is linearly stable. We find that trend-based motivations and\nadditional liquidity arising from an uptrend are destabilizing forces, while\nanchoring through value assumed to be fairly recent price history tends to be\nstabilizing.\n"
    },
    {
        "paper_id": 1805.03172,
        "authors": "Jaehyuk Choi",
        "title": "Sum of all Black-Scholes-Merton models: An efficient pricing method for\n  spread, basket, and Asian options",
        "comments": null,
        "journal-ref": "Journal of Futures Markets, 38(6):627-644, 2018",
        "doi": "10.1002/fut.21909",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contrary to the common view that exact pricing is prohibitive owing to the\ncurse of dimensionality, this study proposes an efficient and unified method\nfor pricing options under multivariate Black-Scholes-Merton (BSM) models, such\nas the basket, spread, and Asian options. The option price is expressed as a\nquadrature integration of analytic multi-asset BSM prices under a single\nBrownian motion. Then the state space is rotated in such a way that the\nquadrature requires much coarser nodes than it would otherwise or low varying\ndimensions are reduced. The accuracy and efficiency of the method is\nillustrated through various numerical experiments.\n"
    },
    {
        "paper_id": 1805.03308,
        "authors": "Stefan Feuerriegel, Nicolas Pr\\\"ollochs",
        "title": "Investor Reaction to Financial Disclosures Across Topics: An Application\n  of Latent Dirichlet Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a holistic study of how stock prices vary in their\nresponse to financial disclosures across different topics. Thereby, we\nspecifically shed light into the extensive amount of filings for which no a\npriori categorization of their content exists. For this purpose, we utilize an\napproach from data mining - namely, latent Dirichlet allocation - as a means of\ntopic modeling. This technique facilitates our task of automatically\ncategorizing, ex ante, the content of more than 70,000 regulatory 8-K filings\nfrom U.S. companies. We then evaluate the subsequent stock market reaction. Our\nempirical evidence suggests a considerable discrepancy among various types of\nnews stories in terms of their relevance and impact on financial markets. For\ninstance, we find a statistically significant abnormal return in response to\nearnings results and credit rating, but also for disclosures regarding business\nstrategy, the health sector, as well as mergers and acquisitions. Our results\nyield findings that benefit managers, investors and policy-makers by indicating\nhow regulatory filings should be structured and the topics most likely to\nprecede changes in stock valuations.\n"
    },
    {
        "paper_id": 1805.03347,
        "authors": "Keivan Mallahi-Karai and Pedram Safari",
        "title": "Future exchange rates and Siegel's paradox",
        "comments": "To appear in Global Finance Journal. 9 pages",
        "journal-ref": null,
        "doi": "10.1016/j.gfj.2018.04.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Siegel's paradox is a fundamental question in international finance about\nexchange rates for futures contracts and has puzzled many scholars for over\nforty years. The unorthodox approach presented in this article leads to an\narbitrage-free solution which is invariant under currency re-denominations and\nis symmetric, as explained. We will also give a complete classification of all\nsuch aggregators in the general case. The formula obtained in this setting\ntherefore describes all the negotiated no-arbitrage forward exchange rates in\nterms of a reciprocity function.\n  Keywords: Siegel's paradox, forward exchange rates, discount bias.\n"
    },
    {
        "paper_id": 1805.03492,
        "authors": "Mario Coccia",
        "title": "The laws of the evolution of research fields",
        "comments": "40 pages, 14 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A fundamental question in the field of social studies of science is how\nresearch fields emerge, grow and decline over time and space. This study\nconfronts this question here by developing an inductive analysis of emerging\nresearch fields represented by human microbiome, evolutionary robotics and\nastrobiology. In particular, number of papers from starting years to 2017 of\neach emerging research field is analyzed considering the subject areas (i.e.,\ndisciplines) of authors. Findings suggest some empirical laws of the evolution\nof research fields: the first law states that the evolution of a specific\nresearch field is driven by few scientific disciplines (3- 5) that generate\nmore than 80% of documents (concentration of the scientific production); the\nsecond law states that the evolution of research fields is path-dependent of a\ncritical discipline (it can be a native discipline that has originated the\nresearch field or a new discipline emerged during the social dynamics of\nscience); the third law states that a research field can be driven during its\nevolution by a new discipline originated by a process of specialization within\nscience. The findings here can explain and generalize, whenever possible some\nproperties of the evolution of scientific fields that are due to interaction\nbetween disciplines, convergence between basic and applied research fields and\ninterdisciplinary in scientific research. Overall, then, this study begins the\nprocess of clarifying and generalizing, as far as possible, the properties of\nthe social construction and evolution of science to lay a foundation for the\ndevelopment of sophisticated theories.\n"
    },
    {
        "paper_id": 1805.0389,
        "authors": "Shige Peng, Shuzhen Yang and Jianfeng Yao",
        "title": "Improving Value-at-Risk prediction under model uncertainty",
        "comments": "42 pages, 7 figures, 7 tables",
        "journal-ref": "Journal of Financial Econometrics, 2020/07",
        "doi": "10.1093/jjfinec/nbaa022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several well-established benchmark predictors exist for Value-at-Risk (VaR),\na major instrument for financial risk management. Hybrid methods combining\nAR-GARCH filtering with skewed-$t$ residuals and the extreme value theory-based\napproach are particularly recommended. This study introduces yet another VaR\npredictor, G-VaR, which follows a novel methodology. Inspired by the recent\nmathematical theory of sublinear expectation, G-VaR is built upon the concept\nof model uncertainty, which in the present case signifies that the inherent\nvolatility of financial returns cannot be characterized by a single\ndistribution but rather by infinitely many statistical distributions. By\nconsidering the worst scenario among these potential distributions, the G-VaR\npredictor is precisely identified. Extensive experiments on both the NASDAQ\nComposite Index and S\\&P500 Index demonstrate the excellent performance of the\nG-VaR predictor, which is superior to most existing benchmark VaR predictors.\n"
    },
    {
        "paper_id": 1805.0398,
        "authors": "Jozef Barun\\'ik and Ev\\v{z}en Ko\\v{c}enda",
        "title": "Total, asymmetric and frequency connectedness between oil and forex\n  markets",
        "comments": "arXiv admin note: text overlap with arXiv:1607.08214",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze total, asymmetric and frequency connectedness between oil and\nforex markets using high-frequency, intra-day data over the period 2007 --\n2017. By employing variance decompositions and their spectral representation in\ncombination with realized semivariances to account for asymmetric and frequency\nconnectedness, we obtain interesting results. We show that divergence in\nmonetary policy regimes affects forex volatility spillovers but that adding oil\nto a forex portfolio decreases the total connectedness of the mixed portfolio.\nAsymmetries in connectedness are relatively small. While negative shocks\ndominate forex volatility connectedness, positive shocks prevail when oil and\nforex markets are assessed jointly. Frequency connectedness is largely driven\nby uncertainty shocks and to a lesser extent by liquidity shocks, which impact\nlong-term connectedness the most and lead to its dramatic increase during\nperiods of distress.\n"
    },
    {
        "paper_id": 1805.04325,
        "authors": "Amanah Ramadiah, Domenico Di Gangi, D. Ruggiero Lo Sardo, Valentina\n  Macchiati, Tuan Pham Minh, Francesco Pinotti, Mateusz Wilinski, Paolo Barucca\n  and Giulio Cimini",
        "title": "Network Sensitivity of Systemic Risk",
        "comments": null,
        "journal-ref": "Journal of Network Theory in Finance, 5(3):53-72 (2020)",
        "doi": "10.21314/JNTF.2019.056",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing body of studies on systemic risk in financial markets has\nemphasized the key importance of taking into consideration the complex\ninterconnections among financial institutions. Much effort has been put in\nmodeling the contagion dynamics of financial shocks, and to assess the\nresilience of specific financial markets - either using real network data,\nreconstruction techniques or simple toy networks. Here we address the more\ngeneral problem of how shock propagation dynamics depends on the topological\ndetails of the underlying network. To this end we consider different realistic\nnetwork topologies, all consistent with balance sheets information obtained\nfrom real data on financial institutions. In particular, we consider networks\nof varying density and with different block structures, and diversify as well\nin the details of the shock propagation dynamics. We confirm that the systemic\nrisk properties of a financial network are extremely sensitive to its network\nfeatures. Our results can aid in the design of regulatory policies to improve\nthe robustness of financial markets.\n"
    },
    {
        "paper_id": 1805.0446,
        "authors": "Alexandre Bovet, Carlo Campajola, Jorge F. Lazo, Francesco Mottes,\n  Iacopo Pozzana, Valerio Restocchi, Pietro Saggese, Nicol\\'o Vallarano,\n  Tiziano Squartini, Claudio J. Tessone",
        "title": "Network-based indicators of Bitcoin bubbles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The functioning of the cryptocurrency Bitcoin relies on the open availability\nof the entire history of its transactions. This makes it a particularly\ninteresting socio-economic system to analyse from the point of view of network\nscience. Here we analyse the evolution of the network of Bitcoin transactions\nbetween users. We achieve this by using the complete transaction history from\nDecember 5th 2011 to December 23rd 2013. This period includes three bubbles\nexperienced by the Bitcoin price. In particular, we focus on the global and\nlocal structural properties of the user network and their variation in relation\nto the different period of price surge and decline. By analysing the temporal\nvariation of the heterogeneity of the connectivity patterns we gain insights on\nthe different mechanisms that take place during bubbles, and find that hubs\n(i.e., the most connected nodes) had a fundamental role in triggering the burst\nof the second bubble. Finally, we examine the local topological structures of\ninteractions between users, we discover that the relative frequency of triadic\ninteractions experiences a strong change before, during and after a bubble, and\nsuggest that the importance of the hubs grows during the bubble. These results\nprovide further evidence that the behaviour of the hubs during bubbles\nsignificantly increases the systemic risk of the Bitcoin network, and discuss\nthe implications on public policy interventions.\n"
    },
    {
        "paper_id": 1805.04535,
        "authors": "Levon Avanesyan, Mykhaylo Shkolnikov, Ronnie Sircar",
        "title": "Construction of Forward Performance Processes in Stochastic Factor\n  Models and an Extension of Widder's Theorem",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal portfolio selection under forward\ninvestment performance criteria in an incomplete market. Given multiple traded\nassets, the prices of which depend on multiple observable stochastic factors,\nwe construct a large class of forward performance processes with power-utility\ninitial data, as well as the corresponding optimal portfolios. This is done by\nsolving the associated non-linear parabolic partial differential equations\n(PDEs) posed in the \"wrong\" time direction, for stock-factor correlation\nmatrices with eigenvalue equality (EVE) structure, which we introduce here.\nAlong the way we establish on domains an explicit form of the generalized\nWidder's theorem of Nadtochiy and Tehranchi [NT15, Theorem 3.12] and rely\nhereby on the Laplace inversion in time of the solutions to suitable linear\nparabolic PDEs posed in the \"right\" time direction.\n"
    },
    {
        "paper_id": 1805.04698,
        "authors": "Cuneyt Akcora, Matthew Dixon, Yulia Gel and Murat Kantarcioglu",
        "title": "Bitcoin Risk Modeling with Blockchain Graphs",
        "comments": "JEL Classification: C58, C63, G18",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A key challenge for Bitcoin cryptocurrency holders, such as startups using\nICOs to raise funding, is managing their FX risk. Specifically, a misinformed\ndecision to convert Bitcoin to fiat currency could, by itself, cost USD\nmillions.\n  In contrast to financial exchanges, Blockchain based crypto-currencies expose\nthe entire transaction history to the public. By processing all transactions,\nwe model the network with a high fidelity graph so that it is possible to\ncharacterize how the flow of information in the network evolves over time. We\ndemonstrate how this data representation permits a new form of microstructure\nmodeling - with the emphasis on the topological network structures to study the\nrole of users, entities and their interactions in formation and dynamics of\ncrypto-currency investment risk. In particular, we identify certain sub-graphs\n('chainlets') that exhibit predictive influence on Bitcoin price and\nvolatility, and characterize the types of chainlets that signify extreme\nlosses.\n"
    },
    {
        "paper_id": 1805.04704,
        "authors": "Daniel Guterding and Wolfram Boenkost",
        "title": "The Heston stochastic volatility model with piecewise constant\n  parameters - efficient calibration and pricing of window barrier options",
        "comments": null,
        "journal-ref": "J. Comput. Appl. Math. 343, 353 (2018)",
        "doi": "10.1016/j.cam.2018.04.054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Heston stochastic volatility model is a standard model for valuing\nfinancial derivatives, since it can be calibrated using semi-analytical\nformulas and captures the most basic structure of the market for financial\nderivatives with simple structure in time-direction. However, extending the\nmodel to the case of time-dependent parameters, which would allow for a\nparametrization of the market at multiple timepoints, proves more challenging.\nWe present a simple and numerically efficient approach to the calibration of\nthe Heston stochastic volatility model with piecewise constant parameters. We\nshow that semi-analytical formulas can also be derived in this more complex\ncase and combine them with recent advances in computational techniques for the\nHeston model. Our numerical scheme is based on the calculation of the\ncharacteristic function using Gauss-Kronrod quadrature with an additional\ncontrol variate that stabilizes the numerical integrals. We use our method to\ncalibrate the Heston model with piecewise constant parameters to the foreign\nexchange (FX) options market. Finally, we demonstrate improvements of the\nHeston model with piecewise constant parameters upon the standard Heston model\nin selected cases.\n"
    },
    {
        "paper_id": 1805.04728,
        "authors": "Wonse Kim, Sungjae Jun",
        "title": "Effects of a Price limit Change on Market Stability at the Intraday\n  Horizon in the Korean Stock Market",
        "comments": "Accepted in Applied Economics Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the effects of a price limit change on the volatility\nof the Korean stock market's (KRX) intraday stock price process. Based on the\nmost recent transaction data from the KRX, which experienced a change in the\nprice limit on June 15, 2015, we examine the change in realized variance after\nthe price limit change to investigate the overall effects of the change on the\nintraday market volatility. We then analyze the effects in more detail by\napplying the discrete Fourier transform (DFT) to the data set. We find evidence\nthat the market becomes more volatile in the intraday horizon because of the\nincrease in the amplitudes of the low-frequency components of the price\nprocesses after the price limit change. Therefore, liquidity providers are in a\nworse situation than they were prior to the change.\n"
    },
    {
        "paper_id": 1805.0475,
        "authors": "Zhi-Qiang Jiang (ECUST), Wen-Jie Xie (ECUST), Wei-Xing Zhou (ECUST),\n  Didier Sornette (ETH Zurich)",
        "title": "Multifractal analysis of financial markets",
        "comments": "A review paper contains 145 pages",
        "journal-ref": "Reports on Progress in Physics 82 (12), 125901 (2019)",
        "doi": "10.1088/1361-6633/ab42fb",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multifractality is ubiquitously observed in complex natural and socioeconomic\nsystems. Multifractal analysis provides powerful tools to understand the\ncomplex nonlinear nature of time series in diverse fields. Inspired by its\nstriking analogy with hydrodynamic turbulence, from which the idea of\nmultifractality originated, multifractal analysis of financial markets has\nbloomed, forming one of the main directions of econophysics. We review the\nmultifractal analysis methods and multifractal models adopted in or invented\nfor financial time series and their subtle properties, which are applicable to\ntime series in other disciplines. We survey the cumulating evidence for the\npresence of multifractality in financial time series in different markets and\nat different time periods and discuss the sources of multifractality. The\nusefulness of multifractal analysis in quantifying market inefficiency, in\nsupporting risk management and in developing other applications is presented.\nWe finally discuss open problems and further directions of multifractal\nanalysis.\n"
    },
    {
        "paper_id": 1805.05077,
        "authors": "Jussi Keppo and Max Reppen and H. Mete Soner",
        "title": "Discrete dividend payments in continuous time",
        "comments": "25 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model in which dividend payments occur at regular, deterministic\nintervals in an otherwise continuous model. This contrasts traditional models\nwhere either the payment of continuous dividends is controlled or the dynamics\nare given by discrete time processes. Moreover, between two dividend payments,\nthe structure allows for other types of control; we consider the possibility of\nequity issuance at any point in time. The value is characterized as the fixed\npoint of an optimal control problem with periodic initial and terminal\nconditions. We prove the regularity and uniqueness of the corresponding dynamic\nprogramming equation, and the convergence of an efficient numerical algorithm\nthat we use to study the problem. The model enables us to find the loss caused\nby infrequent dividend payments. We show that under realistic parameter values\nthis loss varies from around 1% to 24% depending on the state of the system,\nand that using the optimal policy from the continuous problem further increases\nthe loss.\n"
    },
    {
        "paper_id": 1805.05259,
        "authors": "Shengzhong Chen, Niushan Gao, Foivos Xanthos",
        "title": "The strong Fatou property of risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore several Fatou-type properties of risk measures. The\npaper continues to reveal that the strong Fatou property, which was introduced\nin [17], seems to be most suitable to ensure nice dual representations of risk\nmeasures. Our main result asserts that every quasiconvex law-invariant\nfunctional on a rearrangement invariant space $\\mathcal{X}$ with the strong\nFatou property is $\\sigma(\\mathcal{X},L^\\infty)$ lower semicontinuous and that\nthe converse is true on a wide range of rearrangement invariant spaces. We also\nstudy inf-convolutions of law-invariant or surplus-invariant risk measures that\npreserve the (strong) Fatou property.\n"
    },
    {
        "paper_id": 1805.05327,
        "authors": "Sergey A. Rashkovskiy",
        "title": "'Bosons' and 'fermions' in social and economic systems",
        "comments": "24 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.09.057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze social and economic systems with a hierarchical structure and show\nthat for such systems, it is possible to construct thermostatistics, based on\nthe intermediate Gentile statistics. We show that in social and economic\nhierarchical systems there are elements that obey the Fermi-Dirac statistics\nand can be called fermions, as well as elements that are approximately subject\nto Bose-Einstein statistics and can be called bosons. We derive the first and\nsecond laws of thermodynamics for the considered economic system and show that\nsuch concepts as temperature, pressure and financial potential (which is an\nanalogue of the chemical potential in thermodynamics) that characterize the\nstate of the economic system as a whole, can be introduced for economic\nsystems.\n"
    },
    {
        "paper_id": 1805.05465,
        "authors": "Youssef Ifleh, Mohamed Lotfi and Mounime Elkabbouri",
        "title": "Rethinking value creation from the resource based view: the case of\n  human capital in moroccan hotels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth of the modern knowledge-based economy is becoming less and less\ndependent on tangible assets and more on intangible ones. In this context, the\nrole of human capital in the value creation process has become central. Despite\nthe large amount of scientific work on human capital phenomena, little research\nhas revealed the role of human capital in the process of creating value. The\npurpose of this article is to evaluate the impact of human capital on value\ncreation within 31 classified hotels in Morocco for the period 2013-2015. This\npaper is organized into four sections. First, we return to the main\nconceptualization of value creation. The goal of this first section is to\nsynthesize prior work on this construct and highlight the main role of the\nresource based view (RBV) in explaining it. This view presents the point that\nlinks value creation to human capital given that this latter concept is one of\nthe most resources of the firm. Next, we present the main definition of human\ncapital. To do so, we make use of concepts from psychology, economy and\nstrategic human resource management. Then, we shed light on the existing\nrelationship between the two concepts of our research. Finally, we present the\nmethodology of this research as well as the results. The required data to\ncalculate value creation is obtained mainly from the annual reports of Moroccan\nhotels. Whereas, human capital is assessed by a questionnaire using the scale\nof Subramaniam and Youndt (2005). Data is examined using linear regression by\nPASW statistics software. The results of this study give a more concrete\npicture on the creation of value in this context and refute any link between\nthese two concepts.\n"
    },
    {
        "paper_id": 1805.05584,
        "authors": "Michele Leonardo Bianchi and Gian Luca Tassinari",
        "title": "Forward-looking portfolio selection with multivariate non-Gaussian\n  models and the Esscher transform",
        "comments": "29 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we suggest a portfolio selection framework based on\noption-implied information and multivariate non-Gaussian models. The proposed\nmodels incorporate skewness, kurtosis and more complex dependence structures\namong stocks log-returns than the simple correlation matrix. The two models\nconsidered are a multivariate extension of the normal tempered stable (NTS)\nmodel and the generalized hyperbolic (GH) model, respectively, and the\nconnection between the historical measure P and the risk-neutral measure Q is\ngiven by the Esscher transform. We consider an estimation method that\nsimultaneously calibrate the time series of univariate log-returns and the\nunivariate observed volatility smile. To calibrate the models, there is no need\nof liquid multivariate derivative quotes. The method is applied to fit a\n50-dimensional series of stock returns, to evaluate widely known portfolio risk\nmeasures and to perform a portfolio selection analysis.\n"
    },
    {
        "paper_id": 1805.05606,
        "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer and Peter\n  Spreij",
        "title": "Nonparametric Bayesian volatility learning under microstructure noise",
        "comments": "22 pages, 9 figures",
        "journal-ref": "Jpn. J. Stat. Data. Sci 6, 551-571 (2023)",
        "doi": "10.1007/s42081-022-00185-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we study the problem of learning the volatility under market\nmicrostructure noise. Specifically, we consider noisy discrete time\nobservations from a stochastic differential equation and develop a novel\ncomputational method to learn the diffusion coefficient of the equation. We\ntake a nonparametric Bayesian approach, where we \\emph{a priori} model the\nvolatility function as piecewise constant. Its prior is specified via the\ninverse Gamma Markov chain. Sampling from the posterior is accomplished by\nincorporating the Forward Filtering Backward Simulation algorithm in the Gibbs\nsampler. Good performance of the method is demonstrated on two representative\nsynthetic data examples. We also apply the method on a EUR/USD exchange rate\ndataset. Finally we present a limit result on the prior distribution.\n"
    },
    {
        "paper_id": 1805.05617,
        "authors": "Huiwen Wang, Shan Lu and Jichang Zhao",
        "title": "Aggregating multiple types of complex data in stock market prediction: A\n  model-independent framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing richness in volume, and especially types of data in the\nfinancial domain provides unprecedented opportunities to understand the stock\nmarket more comprehensively and makes the price prediction more accurate than\nbefore. However, they also bring challenges to classic statistic approaches\nsince those models might be constrained to a certain type of data. Aiming at\naggregating differently sourced information and offering type-free capability\nto existing models, a framework for predicting stock market of scenarios with\nmixed data, including scalar data, compositional data (pie-like) and functional\ndata (curve-like), is established. The presented framework is\nmodel-independent, as it serves like an interface to multiple types of data and\ncan be combined with various prediction models. And it is proved to be\neffective through numerical simulations. Regarding to price prediction, we\nincorporate the trading volume (scalar data), intraday return series\n(functional data), and investors' emotions from social media (compositional\ndata) through the framework to competently forecast whether the market goes up\nor down at opening in the next day. The strong explanatory power of the\nframework is further demonstrated. Specifically, it is found that the intraday\nreturns impact the following opening prices differently between bearish market\nand bullish market. And it is not at the beginning of the bearish market but\nthe subsequent period in which the investors' \"fear\" comes to be indicative.\nThe framework would help extend existing prediction models easily to scenarios\nwith multiple types of data and shed light on a more systemic understanding of\nthe stock market.\n"
    },
    {
        "paper_id": 1805.0608,
        "authors": "Russell Stanley Q. Geronimo",
        "title": "Can Insider Trading Be Committed Without Trading?",
        "comments": null,
        "journal-ref": "Amity Law Review, Vol. 13, No. 1 (2018)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Before a person can be prosecuted and convicted for insider trading, he must\nfirst execute the overt act of trading. If no sale of security is consummated,\nno crime is also consummated. However, through a complex and insidious\ncombination of various financial instruments, one can capture the same amount\nof gains from insider trading without undertaking an actual trade. Since the\ncrime of insider trading involves buying or selling a security, a more\nsophisticated insider can circumvent the language of the Securities Regulation\nCode by replicating the economic equivalent of a sale without consummating a\nsale as defined by law.\n"
    },
    {
        "paper_id": 1805.06126,
        "authors": "Igor Halperin and Ilya Feldshteyn",
        "title": "Market Self-Learning of Signals, Impact and Optimal Trading: Invisible\n  Hand Inference with Free Energy",
        "comments": "56 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple model of a non-equilibrium self-organizing market where\nasset prices are partially driven by investment decisions of a bounded-rational\nagent. The agent acts in a stochastic market environment driven by various\nexogenous \"alpha\" signals, agent's own actions (via market impact), and noise.\nUnlike traditional agent-based models, our agent aggregates all traders in the\nmarket, rather than being a representative agent. Therefore, it can be\nidentified with a bounded-rational component of the market itself, providing a\nparticular implementation of an Invisible Hand market mechanism. In such\nsetting, market dynamics are modeled as a fictitious self-play of such\nbounded-rational market-agent in its adversarial stochastic environment. As\nrewards obtained by such self-playing market agent are not observed from market\ndata, we formulate and solve a simple model of such market dynamics based on a\nneuroscience-inspired Bounded Rational Information Theoretic Inverse\nReinforcement Learning (BRIT-IRL). This results in effective asset price\ndynamics with a non-linear mean reversion - which in our model is generated\ndynamically, rather than being postulated. We argue that our model can be used\nin a similar way to the Black-Litterman model. In particular, it represents, in\na simple modeling framework, market views of common predictive signals, market\nimpacts and implied optimal dynamic portfolio allocations, and can be used to\nassess values of private signals. Moreover, it allows one to quantify a\n\"market-implied\" optimal investment strategy, along with a measure of market\nrationality. Our approach is numerically light, and can be implemented using\nstandard off-the-shelf software such as TensorFlow.\n"
    },
    {
        "paper_id": 1805.06129,
        "authors": "Yoshiaki Nakada",
        "title": "Factor endowment--commodity output relationships in a three-factor\n  two-good general equilibrium trade model: Further analysis",
        "comments": "24 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1711.11429, arXiv:1711.10096",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The position of the EWS (economy-wide substitution)-ratio vector determines\nthe Rybczynski sign pattern, which expresses the factor endowment--commodity\noutput relationships, and the Stolper-Samuelson sign pattern, which expresses\nthe commodity price--factor price relationships in a three-factor two-good\ngeneral equilibrium trade model (see Nakada (2016a)). In this article, we show\nthat the EWS-ratio vector exists on the line segment. Using this relationship,\nwe develop a method to estimate the position of the EWS-ratio vector. We derive\na sufficient condition for extreme factors to be economy-wide complements,\nwhich implies \"a strong Rybczynski result.\" Additionally, we derive a\nsufficient condition for a specific Stolper-Samuelson sign pattern to hold. We\nassume factor-intensity ranking is constant. This article provides a basis for\nfurther applications.\n"
    },
    {
        "paper_id": 1805.06226,
        "authors": "Ben-zhang Yang, Jia Yue, Ming-hui Wang, Nan-jing Huang",
        "title": "Volatility swaps valuation under stochastic volatility with jumps and\n  stochastic intensity",
        "comments": "15PAGES",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a pricing formula for volatility swaps is delivered when the\nunderlying asset follows the stochastic volatility model with jumps and\nstochastic intensity. By using Feynman-Kac theorem, a partial integral\ndifferential equation is obtained to derive the joint moment generating\nfunction of the previous model.\n  Moreover, discrete and continuous sampled volatility swap pricing formulas\nare given by employing transform techniques and the relationship between two\npricing formulas is discussed. Finally, some numerical simulations are reported\nto support the results presented in this paper.\n"
    },
    {
        "paper_id": 1805.06345,
        "authors": "Henryk Gzyl and Alfredo Rios",
        "title": "Which portfolio is better? A discussion of several possible comparison\n  criteria",
        "comments": "Discovered some wrong statements in it. Will be eventually replaced\n  when corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the last few years, there has been an interest in comparing simple or\nheuristic procedures for portfolio selection, such as the naive, equal weights,\nportfolio choice, against more \"sophisticated\" portfolio choices, and in\nexplaining why, in some cases, the heuristic choice seems to outperform the\nsophisticated choice. We believe that some of these results may be due to the\ncomparison criterion used. It is the purpose of this note to analyze some ways\nof comparing the performance of portfolios. We begin by analyzing each\ncriterion proposed on the market line, in which there is only one random\nreturn. Several possible comparisons between optimal portfolios and the naive\nportfolio are possible and easy to establish. Afterwards, we study the case in\nwhich there is no risk free asset. In this way, we believe some basic\ntheoretical questions regarding why some portfolios may seem to outperform\nothers can be clarified.\n"
    },
    {
        "paper_id": 1805.06498,
        "authors": "Shuoqing Deng, Xiaolu Tan, Xiang Yu",
        "title": "Utility maximization with proportional transaction costs under model\n  uncertainty",
        "comments": "Final version, to appear in Mathematics of Operations Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete time financial market with proportional transaction\ncosts under model uncertainty, and study a num\\'eraire-based semi-static\nutility maximization problem with an exponential utility preference. The\nrandomization techniques recently developed in \\cite{BDT17} allow us to\ntransform the original problem into a frictionless counterpart on an enlarged\nspace. By suggesting a different dynamic programming argument than in\n\\cite{bartl2016exponential}, we are able to prove the existence of the optimal\nstrategy and the convex duality theorem in our context with transaction costs.\nIn the frictionless framework, this alternative dynamic programming argument\nalso allows us to generalize the main results in \\cite{bartl2016exponential} to\na weaker market condition. Moreover, as an application of the duality\nrepresentation, some basic features of utility indifference prices are\ninvestigated in our robust setting with transaction costs.\n"
    },
    {
        "paper_id": 1805.06632,
        "authors": "William B. Haskell, Wenjie Huang, Huifu Xu",
        "title": "Preference Elicitation and Robust Optimization with Multi-Attribute\n  Quasi-Concave Choice Functions",
        "comments": "36 pages, 4 figures, submitted to Operations Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decision maker's preferences are often captured by some choice functions\nwhich are used to rank prospects. In this paper, we consider ambiguity in\nchoice functions over a multi-attribute prospect space. Our main result is a\nrobust preference model where the optimal decision is based on the worst-case\nchoice function from an ambiguity set constructed through preference\nelicitation with pairwise comparisons of prospects. Differing from existing\nworks in the area, our focus is on quasi-concave choice functions rather than\nconcave functions and this enables us to cover a wide range of utility/risk\npreference problems including multi-attribute expected utility and $S$-shaped\naspirational risk preferences. The robust choice function is increasing and\nquasi-concave but not necessarily translation invariant, a key property of\nmonetary risk measures. We propose two approaches based respectively on the\nsupport functions and level functions of quasi-concave functions to develop\ntractable formulations of the maximin preference robust optimization model. The\nformer gives rise to a mixed integer linear programming problem whereas the\nlatter is equivalent to solving a sequence of convex risk minimization\nproblems. To assess the effectiveness of the proposed robust preference\noptimization model and numerical schemes, we apply them to a security budget\nallocation problem and report some preliminary results from experiments.\n"
    },
    {
        "paper_id": 1805.06649,
        "authors": "Florian Ziel, Rafal Weron",
        "title": "Day-ahead electricity price forecasting with high-dimensional\n  structures: Univariate vs. multivariate modeling frameworks",
        "comments": null,
        "journal-ref": "Energy Economics, 70 (2018), 396-420",
        "doi": "10.1016/j.eneco.2017.12.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct an extensive empirical study on short-term electricity price\nforecasting (EPF) to address the long-standing question if the optimal model\nstructure for EPF is univariate or multivariate. We provide evidence that\ndespite a minor edge in predictive performance overall, the multivariate\nmodeling framework does not uniformly outperform the univariate one across all\n12 considered datasets, seasons of the year or hours of the day, and at times\nis outperformed by the latter. This is an indication that combining advanced\nstructures or the corresponding forecasts from both modeling approaches can\nbring a further improvement in forecasting accuracy. We show that this indeed\ncan be the case, even for a simple averaging scheme involving only two models.\nFinally, we also analyze variable selection for the best performing\nhigh-dimensional lasso-type models, thus provide guidelines to structuring\nbetter performing forecasting model designs.\n"
    },
    {
        "paper_id": 1805.06682,
        "authors": "Ioane Muni Toke and Nakahiro Yoshida",
        "title": "Analyzing order flows in limit order books with ratios of Cox-type\n  intensities",
        "comments": "38 pages, 9 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.1080/14697688.2019.1637927",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a Cox-type model for relative intensities of orders flows in a\nlimit order book. The model assumes that all intensities share a common\nbaseline intensity, which may for example represent the global market activity.\nParameters can be estimated by quasi likelihood maximization, without any\ninterference from the baseline intensity. Consistency and asymptotic behavior\nof the estimators are given in several frameworks, and model selection is\ndiscussed with information criteria and penalization. The model is well-suited\nfor high-frequency financial data: fitted models using easily interpretable\ncovariates show an excellent agreement with empirical data. Extensive\ninvestigation on tick data consequently helps identifying trading signals and\nimportant factors determining the limit order book dynamics. We also illustrate\nthe potential use of the framework for out-of-sample predictions.\n"
    },
    {
        "paper_id": 1805.06829,
        "authors": "Kiran Sharma, Anindya S. Chakrabarti and Anirban Chakraborti",
        "title": "Multi-layered Network Structure: Relationship Between Financial and\n  Macroeconomic Dynamics",
        "comments": "43 pages, 7 figures, 31 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate using multi-layered networks, the existence of an empirical\nlinkage between the dynamics of the financial network constructed from the\nmarket indices and the macroeconomic networks constructed from macroeconomic\nvariables such as trade, foreign direct investments, etc. for several countries\nacross the globe. The temporal scales of the dynamics of the financial\nvariables and the macroeconomic fundamentals are very different, which make the\nempirical linkage even more interesting and significant. Also, we find that\nthere exist in the respective networks, core-periphery structures (determined\nthrough centrality measures) that are composed of the similar set of countries\n-- a result that may be related through the `gravity model' of the\ncountry-level macroeconomic networks. Thus, from a multi-lateral openness\nperspective, we elucidate that for individual countries, larger trade\nconnectivity is positively associated with higher financial return\ncorrelations. Furthermore, we show that the Economic Complexity Index and the\nequity markets have a positive relationship among themselves, as is the case\nfor Gross Domestic Product. The data science methodology using network theory,\ncoupled with standard econometric techniques constitute a new approach to\nstudying multi-level economic phenomena in a comprehensive manner.\n"
    },
    {
        "paper_id": 1805.06929,
        "authors": "Adams Vallejos, Ignacio Ormazabal, Felix A. Borotto and Hernan F.\n  Astudillo",
        "title": "A new $\\kappa$-deformed parametric model for the size distribution of\n  wealth",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.09.060",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been pointed out by Patriarca et al. (2005) that the power-law tailed\nequilibrium distribution in heterogeneous kinetic exchange models with a\ndistributed saving parameter can be resolved as a mixture of Gamma\ndistributions corresponding to particular subsets of agents. Here, we propose a\nnew four-parameter statistical distribution which is a $\\kappa$-deformation of\nthe Generalized Gamma distribution with a power-law tail, based on the deformed\nexponential and logarithm functions introduced by Kaniadakis(2001). We found\nthat this new distribution is also an extension to the $\\kappa$-Generalized\ndistribution proposed by Clementi et al. (2007), with an additional shape\nparameter $\\nu$, and properly reproduces the whole range of the distribution of\nwealth in such heterogeneous kinetic exchange models. We also provide various\nassociated statistical measures and inequality measures.\n"
    },
    {
        "paper_id": 1805.07134,
        "authors": "Paul Jusselin and Mathieu Rosenbaum",
        "title": "No-arbitrage implies power-law market impact and rough volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market impact is the link between the volume of a (large) order and the price\nmove during and after the execution of this order. We show that under\nno-arbitrage assumption, the market impact function can only be of power-law\ntype. Furthermore, we prove that this implies that the macroscopic price is\ndiffusive with rough volatility, with a one-to-one correspondence between the\nexponent of the impact function and the Hurst parameter of the volatility.\nHence we simply explain the universal rough behavior of the volatility as a\nconsequence of the no-arbitrage property. From a mathematical viewpoint, our\nstudy relies in particular on new results about hyper-rough stochastic Volterra\nequations.\n"
    },
    {
        "paper_id": 1805.07194,
        "authors": "Viet Anh Nguyen and Daniel Kuhn and Peyman Mohajerin Esfahani",
        "title": "Distributionally Robust Inverse Covariance Estimation: The Wasserstein\n  Shrinkage Estimator",
        "comments": "30 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a distributionally robust maximum likelihood estimation model\nwith a Wasserstein ambiguity set to infer the inverse covariance matrix of a\n$p$-dimensional Gaussian random vector from $n$ independent samples. The\nproposed model minimizes the worst case (maximum) of Stein's loss across all\nnormal reference distributions within a prescribed Wasserstein distance from\nthe normal distribution characterized by the sample mean and the sample\ncovariance matrix. We prove that this estimation problem is equivalent to a\nsemidefinite program that is tractable in theory but beyond the reach of\ngeneral purpose solvers for practically relevant problem dimensions $p$. In the\nabsence of any prior structural information, the estimation problem has an\nanalytical solution that is naturally interpreted as a nonlinear shrinkage\nestimator. Besides being invertible and well-conditioned even for $p>n$, the\nnew shrinkage estimator is rotation-equivariant and preserves the order of the\neigenvalues of the sample covariance matrix. These desirable properties are not\nimposed ad hoc but emerge naturally from the underlying distributionally robust\noptimization model. Finally, we develop a sequential quadratic approximation\nalgorithm for efficiently solving the general estimation problem subject to\nconditional independence constraints typically encountered in Gaussian\ngraphical models.\n"
    },
    {
        "paper_id": 1805.07403,
        "authors": "Petteri Piiroinen, Lassi Roininen, Tobias Schoden, Martin Simon",
        "title": "Asset Price Bubbles: An Option-based Indicator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a statistical indicator for the detection of short-term asset\nprice bubbles based on the information content of bid and ask market quotes for\nplain vanilla put and call options. Our construction makes use of the\nmartingale theory of asset price bubbles and the fact that such scenarios where\nthe price for an asset exceeds its fundamental value can in principle be\ndetected by analysis of the asymptotic behavior of the implied volatility\nsurface. For extrapolating this implied volatility, we choose the SABR model,\nmainly because of its decent fit to real option market quotes for a broad range\nof maturities and its ease of calibration. As main theoretical result, we show\nthat under lognormal SABR dynamics, we can compute a simple yet powerful\nclosed-form martingale defect indicator by solving an ill-posed inverse\ncalibration problem. In order to cope with the ill-posedness and to quantify\nthe uncertainty which is inherent to such an indicator, we adopt a Bayesian\nstatistical parameter estimation perspective. We probe the resulting posterior\ndensities with a combination of optimization and adaptive Markov chain Monte\nCarlo methods, thus providing a full-blown uncertainty estimation of all the\nunderlying parameters and the martingale defect indicator. Finally, we provide\nreal-market tests of the proposed option-based indicator with focus on tech\nstocks due to increasing concerns about a tech bubble 2.0.\n"
    },
    {
        "paper_id": 1805.07478,
        "authors": "Son Le",
        "title": "Algorithmic Trading with Fitted Q Iteration and Heston Model",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the use of the fitted Q iteration in algorithmic trading. We show\nthat the fitted Q iteration helps alleviate the dimension problem that the\nbasic Q-learning algorithm faces in application to trading. Furthermore, we\nintroduce a procedure including model fitting and data simulation to enrich\ntraining data as the lack of data is often a problem in realistic application.\nWe experiment our method on both simulated environment that permits arbitrage\nopportunity and real-world environment by using prices of 450 stocks. In the\nformer environment, the method performs well, implying that our method works in\ntheory. To perform well in the real-world environment, the agents trained might\nrequire more training (iteration) and more meaningful variables with predictive\nvalue.\n"
    },
    {
        "paper_id": 1805.07532,
        "authors": "Yu-Jui Huang, Saeed Khalili",
        "title": "Optimal Consumption in the Stochastic Ramsey Problem without Boundedness\n  Constraints",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization, Vol. 57 (2019), No. 2,\n  pp 783-809",
        "doi": "10.1137/18M1188410",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates optimal consumption in the stochastic Ramsey problem\nwith the Cobb-Douglas production function. Contrary to prior studies, we allow\nfor general consumption processes, without any a priori boundedness constraint.\nA non-standard stochastic differential equation, with neither Lipschitz\ncontinuity nor linear growth, specifies the dynamics of the controlled state\nprocess. A mixture of probabilistic arguments are used to construct the state\nprocess, and establish its non-explosiveness and strict positivity. This leads\nto the optimality of a feedback consumption process, defined in terms of the\nvalue function and the state process. Based on additional viscosity solutions\ntechniques, we characterize the value function as the unique classical solution\nto a nonlinear elliptic equation, among an appropriate class of functions. This\ncharacterization involves a condition on the limiting behavior of the value\nfunction at the origin, which is the key to dealing with unbounded\nconsumptions. Finally, relaxing the boundedness constraint is shown to\nincrease, strictly, the expected utility at all wealth levels.\n"
    },
    {
        "paper_id": 1805.08454,
        "authors": "James Paulin, Anisoara Calinescu and Michael Wooldridge",
        "title": "Understanding Flash Crash Contagion and Systemic Risk: A Micro-Macro\n  Agent-Based Approach",
        "comments": "37 pages, 9 figures",
        "journal-ref": "Journal of Economic Dynamics and Control 100 (2019) p.200-229",
        "doi": "10.1016/j.jedc.2018.12.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to advance the understanding of the conditions\nthat give rise to flash crash contagion, particularly with respect to\noverlapping asset portfolio crowding. To this end, we designed, implemented,\nand assessed a hybrid micro-macro agent-based model, where price impact arises\nendogenously through the limit order placement activity of algorithmic traders.\nOur novel hybrid microscopic and macroscopic model allows us to quantify\nsystemic risk not just in terms of system stability, but also in terms of the\nspeed of financial distress propagation over intraday timescales. We find that\nsystemic risk is strongly dependent on the behaviour of algorithmic traders, on\nleverage management practices, and on network topology. Our results demonstrate\nthat, for high-crowding regimes, contagion speed is a non-monotone function of\nportfolio diversification. We also find the surprising result that, in certain\ncircumstances, increased portfolio crowding is beneficial to systemic\nstability. We are not aware of previous studies that have exhibited this\nphenomenon, and our results establish the importance of considering non-uniform\nasset allocations in future studies. Finally, we characterise the time window\navailable for regulatory interventions during the propagation of flash crash\ndistress, with results suggesting ex ante precautions may have higher efficacy\nthan ex post reactions.\n"
    },
    {
        "paper_id": 1805.08544,
        "authors": "Tathagata Banerjee and Zachary Feinstein",
        "title": "Impact of Contingent Payments on Systemic Risk in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the implications of contingent payments on the\nclearing wealth in a network model of financial contagion. We consider an\nextension of the Eisenberg-Noe financial contagion model in which the nominal\ninterbank obligations depend on the wealth of the firms in the network. We\nfirst consider the problem in a static framework and develop conditions for\nexistence and uniqueness of solutions as long as no firm is speculating on the\nfailure of other firms. In order to achieve existence and uniqueness under more\ngeneral conditions, we introduce a dynamic framework. We demonstrate how this\ndynamic framework can be applied to problems that were ill-defined in the\nstatic framework.\n"
    },
    {
        "paper_id": 1805.0855,
        "authors": "Laura Alessandretti, Abeer ElBahrawy, Luca Maria Aiello, Andrea\n  Baronchelli",
        "title": "Anticipating cryptocurrency prices using machine learning",
        "comments": "Complexity, 2018",
        "journal-ref": null,
        "doi": "10.1155/2018/8983590",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning and AI-assisted trading have attracted growing interest for\nthe past few years. Here, we use this approach to test the hypothesis that the\ninefficiency of the cryptocurrency market can be exploited to generate abnormal\nprofits. We analyse daily data for $1,681$ cryptocurrencies for the period\nbetween Nov. 2015 and Apr. 2018. We show that simple trading strategies\nassisted by state-of-the-art machine learning algorithms outperform standard\nbenchmarks. Our results show that nontrivial, but ultimately simple,\nalgorithmic mechanisms can help anticipate the short-term evolution of the\ncryptocurrency market.\n"
    },
    {
        "paper_id": 1805.08653,
        "authors": "Richard Gerlach, Chao Wang",
        "title": "Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk\n  Forecasting, Incorporating Realized Measures",
        "comments": "36 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1612.08488",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression\nmodel of Taylor (2017) is extended via incorporating a realized measure, to\ndrive the tail risk dynamics, as a potentially more efficient driver than daily\nreturns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte\nCarlo method are employed for estimation, whose properties are assessed and\ncompared via a simulation study; results favour the Bayesian approach, which is\nsubsequently employed in a forecasting study of seven market indices and two\nindividual assets. The proposed models are compared to a range of parametric,\nnon-parametric and semi-parametric models, including GARCH, Realized-GARCH and\nthe joint VaR and ES quantile regression models in Taylor (2017). The\ncomparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected\nShortfall forecasts, over a long forecast sample period that includes the\nglobal financial crisis in 2007-2008. The results favor the proposed models\nincorporating a realized measure, especially when employing the sub-sampled\nRealized Variance and the sub-sampled Realized Range.\n"
    },
    {
        "paper_id": 1805.09014,
        "authors": "Ludovic Tangpi",
        "title": "Concentration of dynamic risk measures in a Brownian filtration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by liquidity risk in mathematical finance, D. Lacker introduced\nconcentration inequalities for risk measures, i.e. upper bounds on the\n\\emph{liquidity risk profile} of a financial loss. We derive these inequalities\nin the case of time-consistent dynamic risk measures when the filtration is\nassumed to carry a Brownian motion. The theory of backward stochastic\ndifferential equations (BSDEs) and their dual formulation plays a crucial role\nin our analysis. Natural by-products of concentration of risk measures are a\ndescription of the tail behavior of the financial loss and transport-type\ninequalities in terms of the generator of the BSDE, which in the present case\ncan grow arbitrarily fast.\n"
    },
    {
        "paper_id": 1805.09068,
        "authors": "Thai Nguyen and Mitja Stadje",
        "title": "Optimal investment for participating insurance contracts under\n  VaR-Regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a Value-at-Risk (VaR)-regulated optimal portfolio problem\nof the equity holders of a participating life insurance contract. In a setting\nwith unhedgeable mortality risk and complete financial market, the optimal\nsolution is given explicitly for contracts with mortality risk using a\nmartingale approach for constrained non-concave optimization problems. We show\nthat regulatory VaR constraints for participating insurance contracts lead to\nmore prudent investment than in the case of no regulation. This result is\ncontrary to the situation where the insurer maximizes the utility of the total\nwealth of the company (without distinguishing between contributions of equity\nholders and policyholders), in which case a VaR constraint may induce the\ninsurer to take excessive risks leading to higher losses than in the case of no\nregulation. Compared to the unregulated problem, the VaR-constrained strategy\nleads to a higher expected utility for the policyholders, highlighting the\npotential usefulness of a VaR-regulation in the context of insurance. The\nprudent investment behavior is more significant if a VaR-type regulation is\nreplaced by a portfolio insurance (PI)-type regulation. Furthermore, a stricter\nregulation (a smaller allowed default probability in the VaR problem or a\nhigher minimum guarantee level in the PI problem) enhances the benefit of the\npolicyholder but deteriorates that of the insurer. For both types of\nregulation, the gains in terms of expected utility are greater for higher\nparticipation rates, while being smaller for higher bonus rates. We also extend\nour analysis to frameworks where dividend and premature death benefit payments\nare made at an intermediate time date.\n"
    },
    {
        "paper_id": 1805.09427,
        "authors": "Nabil Kahale",
        "title": "General multilevel Monte Carlo methods for pricing discretely monitored\n  Asian options",
        "comments": "22 pages. Presented at the 35th Spring International Conference of\n  the French Finance Association, May 2018",
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2020.04.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe general multilevel Monte Carlo methods that estimate the price of\nan Asian option monitored at $m$ fixed dates. Our approach yields unbiased\nestimators with standard deviation $O(\\epsilon)$ in $O(m + (1/\\epsilon)^{2})$\nexpected time for a variety of processes including the Black-Scholes model,\nMerton's jump-diffusion model, the Square-Root diffusion model, Kou's double\nexponential jump-diffusion model, the variance gamma and NIG exponential Levy\nprocesses and, via the Milstein scheme, processes driven by scalar stochastic\ndifferential equations. Using the Euler scheme, our approach estimates the\nAsian option price with root mean square error $O(\\epsilon)$ in\n$O(m+(\\ln(\\epsilon)/\\epsilon)^{2})$ expected time for processes driven by\nmultidimensional stochastic differential equations. Numerical experiments\nconfirm that our approach outperforms the conventional Monte Carlo method by a\nfactor of order $m$.\n"
    },
    {
        "paper_id": 1805.09686,
        "authors": "O.A. Malafeyev, V.E. Onishenko, I.V. Zaytseva",
        "title": "Forecasting the sustainable status of the labor market in agriculture",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, a game-theoretic model is constructed that is related to the\nproblem of optimal assignments. Examples are considered. A compromise point is\nfound, the Nash equilibriums and the decision of the Nash arbitration scheme\nare constructed.\n"
    },
    {
        "paper_id": 1805.09763,
        "authors": "Daniel Fraiman",
        "title": "A self-organized criticality participative pricing mechanism for selling\n  zero-marginal cost products",
        "comments": "21 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In today's economy, selling a new zero-marginal cost product is a real\nchallenge, as it is difficult to determine a product's \"correct\" sales price\nbased on its profit and dissemination. As an example, think of the price of a\nnew app or video game. New sales mechanisms for selling this type of product\nneed to be designed, in particular ones that consider consumer preferences and\nreality. Current auction mechanisms establish a time deadline for the auction\nto take place. This deadline is set to increase the number of bidders and thus\nthe final offering price. Consumers want to obtain the product as quickly as\npossible from the moment they become interested in it, and this time does not\nalways coincide with the seller's deadline. Naturally, consumers also want to\npay a price they consider \"fair\". Here we introduce an auction model where\nbuyers continuously place bids and the challenge is to decide quickly whether\nor not to accept them. The model does not include a deadline for placing bids,\nand exhibits self-organized criticality; it presents a critical price from\nwhich a bid is accepted with probability one, and avalanches of sales above\nthis value are observed. This model is of particular interest for startup\ncompanies interested in profit as well as making the product known on the\nmarket.\n"
    },
    {
        "paper_id": 1805.09996,
        "authors": "Michele Leonardo Bianchi",
        "title": "Are multi-factor Gaussian term structure models still useful? An\n  empirical analysis on Italian BTPs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we empirically study models for pricing Italian sovereign\nbonds under a reduced form framework, by assuming different dynamics for the\nshort-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek\nmulti-factor models, with a focus on optimization algorithms applied in the\ncalibration exercise. The Kalman filter algorithm together with a maximum\nlikelihood estimation method are considered to fit the Italian term-structure\nover a 12-year horizon, including the global financial crisis and the euro area\nsovereign debt crisis. Analytic formulas for the gradient vector and the\nHessian matrix of the likelihood function are provided.\n"
    },
    {
        "paper_id": 1805.10128,
        "authors": "Carey Caginalp, Gunduz Caginalp",
        "title": "Cryptocurrency Equilibria Through Game Theoretic Optimization",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimization methods are used to determine equilibria of investment in\ncryptocurrencies. The basic assumptions involve existence of a core group (the\n\"wealthy\") that fears the loss of substantial assets through government\nseizure. Speculators constitute another group that tends to introduce\nvolatility and risk for the wealthy. The wealthy must divide their assets\nbetween the home currency and the cryptocurrency, while the government decides\non the probability of seizing a fraction the assets of this group. Under the\nassumption that each group exhibits risk aversion through a utility function,\nwe establish the existence and uniqueness of Nash equilibrium. Also examined is\nthe more realistic optimization problem in which the government policy cannot\nbe reversed, while the wealthy can adjust their allocation in reaction to the\ngovernment's designation of probability. The methodology leads to an\nunderstanding the equilibrium market capitalization of cryptocurrencies.\n"
    },
    {
        "paper_id": 1805.11036,
        "authors": "Torsten Trimborn",
        "title": "A Macroscopic Portfolio Model: From Rational Agents to Bounded\n  Rationality",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1711.03291",
        "journal-ref": null,
        "doi": "10.1007/s11579-019-00235-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a microscopic model of interacting financial agents, where each\nagent is characterized by two portfolios; money invested in bonds and money\ninvested in stocks. Furthermore, each agent is faced with an optimization\nproblem in order to determine the optimal asset allocation. The stock price\nevolution is driven by the aggregated investment decision of all agents. In\nfact, we are faced with a differential game since all agents aim to invest\noptimal. Mathematically such a problem is ill posed and we introduce the\nconcept of Nash equilibrium solutions to ensure the existence of a solution.\nEspecially, we denote an agent who solves this Nash equilibrium exactly a\nrational agent. As next step we use model predictive control to approximate the\ncontrol problem. This enables us to derive a precise mathematical\ncharacterization of the degree of rationality of a financial agent. This is a\nnovel concept in portfolio optimization and can be regarded as a general\napproach. In a second step we consider the case of a fully myopic agent, where\nwe can solve the optimal investment decision of investors analytically. We\nselect the running cost to be the expected missed revenue of an agent and we\nassume quadratic transaction costs. More precisely the expected revenues are\ndetermined by a combination of a fundamentalist or chartist strategy. Then we\nderive the mean field limit of the microscopic model in order to obtain a\nmacroscopic portfolio model. The novelty in comparison to existent\nmacroeconomic models in literature is that our model is derived from\nmicroeconomic dynamics. The resulting portfolio model is a three dimensional\nODE system which enables us to derive analytical results. Simulations reveal,\nthat our model is able to replicate the most prominent features of financial\nmarkets, namely booms and crashes.\n"
    },
    {
        "paper_id": 1805.11317,
        "authors": "Yue-Gang Song, Yu-Long Zhou, Ren-Jie Han",
        "title": "Neural networks for stock price prediction",
        "comments": "13 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the extremely volatile nature of financial markets, it is commonly\naccepted that stock price prediction is a task full of challenge. However in\norder to make profits or understand the essence of equity market, numerous\nmarket participants or researchers try to forecast stock price using various\nstatistical, econometric or even neural network models. In this work, we survey\nand compare the predictive power of five neural network models, namely, back\npropagation (BP) neural network, radial basis function (RBF) neural network,\ngeneral regression neural network (GRNN), support vector machine regression\n(SVMR), least squares support vector machine regresssion (LS-SVMR). We apply\nthe five models to make price prediction of three individual stocks, namely,\nBank of China, Vanke A and Kweichou Moutai. Adopting mean square error and\naverage absolute percentage error as criteria, we find BP neural network\nconsistently and robustly outperforms the other four models.\n"
    },
    {
        "paper_id": 1805.11562,
        "authors": "Nana Kwame Akosah, Francis W. Loloh, Maurice Omane-Adjepong",
        "title": "Justifying the Adoption and Relevance of Inflation Targeting Framework:\n  A Time-Varying Evidence from Ghana",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper scrutinizes the rationale for the adoption of inflation targeting\n(IT) by Bank of Ghana in 2002. In this case, we determine the stability or\notherwise of the relationship between money supply and inflation in Ghana over\nthe period 1970M1-2016M3 using battery of econometric methods. The empirical\nresults show an unstable link between inflation and monetary growth in Ghana,\nwhile the final state coefficient of inflation elasticity to money growth is\npositive but statistically insignificant. We find that inflation elasticity to\nmonetary growth has continued to decline since the 1970s, showing a waning\nimpact of money growth on inflation in Ghana. Notably, there is also evidence\nof negative inflation elasticity to monetary growth between 2001 and 2004,\nlending support to the adoption of IT framework in Ghana in 2002. We emphasized\nthat the unprecedented 31-months of single-digit inflation (June 2010-December\n2012), despite the observed inflationary shocks in 2010 and 2012, reinforces\nthe immense contribution of the IT framework in anchoring inflation\nexpectations, with better inflation outcomes and inflation variability in\nGhana. The paper therefore recommends the continuous pursuance and\nstrengthening of the IT framework in Ghana, as it embodies a more eclectic\napproach to policy formulation and implementation.\n"
    },
    {
        "paper_id": 1805.11804,
        "authors": "Vilislav Boutchaktchiev",
        "title": "A Markov Chain Model for the Cure Rate of Non-Performing Loans",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3175475",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Markov-chain model is developed for the purpose estimation of the cure rate\nof non-performing loans. The technique is performed collectively, on portfolios\nand it can be applicable in the process of calculation of credit impairment. It\nis efficient in terms of data manipulation costs which makes it accessible even\nto smaller financial institutions. In addition, several other applications to\nportfolio optimization are suggested.\n"
    },
    {
        "paper_id": 1805.11844,
        "authors": "Tahir Choulli, Catherine Daveloose, Mich\\`ele Vanmaele",
        "title": "Mortality/longevity Risk-Minimization with or without securitization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the risk-minimization problem, with and without\nmortality securitization, \\`a la F\\\"ollmer-Sondermann for a large class of\nequity-linked mortality contracts when no model for the death time is\nspecified. This framework includes the situation where the correlation between\nthe market model and the time of death is arbitrary general, and hence leads to\nthe case of a market model where there are two levels of information. The\npublic information which is generated by the financial assets, and a larger\nflow of information that contains additional knowledge about a death time of an\ninsured. By enlarging the filtration, the death uncertainty and its entailed\nrisk are fully considered without any mathematical restriction.\n  Our key tool lies in our optional martingale representation that states that\nany martingale in the large filtration stopped at the death time can be\ndecomposed into precise orthogonal local martingales. This allows us to derive\nthe dynamics of the value processes of the mortality/longevity securities used\nfor the securitization, and to decompose any mortality/longevity liability into\nthe sum of orthogonal risks by means of a risk basis. The first main\ncontribution of this paper resides in quantifying, as explicit as possible, the\neffect of mortality uncertainty on the risk-minimizing strategy by determining\nthe optimal strategy in the enlarged filtration in terms of strategies in the\nsmaller filtration. Our second main contribution consists of finding\nrisk-minimizing strategies with insurance securitization by investing in stocks\nand one (or more) mortality/longevity derivatives such as longevity bonds. This\ngeneralizes the existing literature on risk-minimization using mortality\nsecuritization in many directions.\n"
    },
    {
        "paper_id": 1805.11909,
        "authors": "Rafal Rak and Dariusz Grech",
        "title": "Quantitative approach to multifractality induced by correlations and\n  broad distribution of data",
        "comments": "27 pages",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 508,\n  15 October 2018, Pages 48-66",
        "doi": "10.1016/j.physa.2018.05.059",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze quantitatively the effect of spurious multifractality induced by\nthe presence of fat-tailed symmetric and asymmetric probability distributions\nof fluctuations in time series. In the presented approach different kinds of\nsymmetric and asymmetric broad probability distributions of synthetic data are\nexamined starting from Levy regime up to those with finite variance. We use\nnonextensive Tsallis statistics to construct all considered data in order to\nhave good analytical description of frequencies of fluctuations in the whole\nrange of their magnitude and simultaneously the full control over exponent of\npower-law decay for tails of probability distribution. The semi-analytical\ncompact formulas are then provided to express the level of spurious\nmultifractality generated by the presence of fat tails in terms of Tsallis\nparameter $\\tilde{q}$ and the scaling exponent $\\beta$ of the asymptotic decay\nof cumulated probability density function (CDF).The results are presented in\nHurst and H\\\"{o}lder languages - more often used in study of multifractal\nphenomena. According to the provided semi-analytical relations, it is argued\nhow one can make a clear quantitative distinction for any real data between\ntrue multifractality caused by the presence of nonlinear correlations, spurious\nmultifractality generated by fat-tailed shape of distributions - eventually\nwith their asymmetry, and the correction due to linear autocorrelations in\nanalyzed time series of finite length. In particular, the spurious multifractal\neffect of fat tails is found basic for proper quantitative estimation of all\nspurious multifractal effects. Examples from stock market data are presented to\nsupport these findings.\n"
    },
    {
        "paper_id": 1805.11932,
        "authors": "Mario Coccia",
        "title": "How do public research labs use funding for research? A case study",
        "comments": "33 pages; 4 tables; 7 figures (4 in text + 3 in appendix)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper discusses how public research organizations consume funding for\nresearch, applying a new approach based on economic metabolism of research\nlabs, in a broad analogy with biology. This approach is applied to a case study\nin Europe represented by one of the biggest European public research\norganizations, the National Research council of Italy. Results suggest that\nfunding for research (state subsidy and public contracts) of this public\nresearch organization is mainly consumed for the cost of personnel. In\naddition, the analysis shows a disproportionate growth of the cost of personnel\nin public research labs in comparison with total revenue from government. In\nthe presence of shrinking public research lab budgets, this organizational\nbehavior generates inefficiencies and stress. R&D management and public policy\nimplications are suggested for improving economic performance of public\nresearch organizations in turbulent markets.\n"
    },
    {
        "paper_id": 1805.11954,
        "authors": "Yu-Long Zhou, Ren-Jie Han, Qian Xu, Wei-Ke Zhang",
        "title": "Long Short-Term Memory Networks for CSI300 Volatility Prediction with\n  Baidu Search Volume",
        "comments": "7 pages, 3 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1512.04916 by other authors",
        "journal-ref": null,
        "doi": "10.1002/cpe.4721",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intense volatility in financial markets affect humans worldwide. Therefore,\nrelatively accurate prediction of volatility is critical. We suggest that\nmassive data sources resulting from human interaction with the Internet may\noffer a new perspective on the behavior of market participants in periods of\nlarge market movements. First we select 28 key words, which are related to\nfinance as indicators of the public mood and macroeconomic factors. Then those\n28 words of the daily search volume based on Baidu index are collected\nmanually, from June 1, 2006 to October 29, 2017. We apply a Long Short-Term\nMemory neural network to forecast CSI300 volatility using those search volume\ndata. Compared to the benchmark GARCH model, our forecast is more accurate,\nwhich demonstrates the effectiveness of the LSTM neural network in volatility\nforecasting.\n"
    },
    {
        "paper_id": 1805.11981,
        "authors": "Tim Xiao",
        "title": "A New Model for Pricing Collateralized Financial Derivatives",
        "comments": "arXiv admin note: text overlap with arXiv:1803.07843,\n  arXiv:1804.02289 by other authors",
        "journal-ref": "Journal of Derivatives, Institutional Investor Inc., 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a new model for pricing financial derivatives subject to\ncollateralization. It allows for collateral arrangements adhering to bankruptcy\nlaws. As such, the model can back out the market price of a collateralized\ncontract. This framework is very useful for valuing outstanding derivatives.\nUsing a unique dataset, we find empirical evidence that credit risk alone is\nnot overly important in determining credit-related spreads. Only accounting for\nboth collateral posting and credit risk can sufficiently explain unsecured\ncredit costs. This finding suggests that failure to properly account for\ncollateralization may result in significant mispricing of derivatives. We also\nempirically gauge the impact of collateral agreements on risk measurements. Our\nfindings indicate that there are important interactions between market and\ncredit risk.\n"
    },
    {
        "paper_id": 1805.12035,
        "authors": "Tiziano De Angelis",
        "title": "Optimal dividends with partial information and stopping of a degenerate\n  reflecting diffusion",
        "comments": "42 pages; improved Prop. 6.2 and Cor. 6.3",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal dividend problem for a firm's manager who has partial\ninformation on the profitability of the firm. The problem is formulated as one\nof singular stochastic control with partial information on the drift of the\nunderlying process and with absorption. In the Markovian formulation, we have a\n2-dimensional degenerate diffusion, whose first component is singularly\ncontrolled and it is absorbed as it hits zero. The free boundary problem (FBP)\nassociated to the value function of the control problem is challenging from the\nanalytical point of view due to the interplay of degeneracy and absorption. We\nfind a probabilistic way to show that the value function of the dividend\nproblem is a smooth solution of the FBP and to construct an optimal dividend\nstrategy. Our approach establishes a new link between multidimensional singular\nstochastic control problems with absorption and problems of optimal stopping\nwith `creation'. One key feature of the stopping problem is that creation\noccurs at a state-dependent rate of the `local-time' of an auxiliary\n2-dimensional reflecting diffusion.\n"
    },
    {
        "paper_id": 1805.12066,
        "authors": "Irina Georgescu",
        "title": "The effect of prudence on the optimal allocation in possibilistic and\n  mixed models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper two portfolio choice models are studied: a purely possibilistic\nmodel, in which the return of a risky asset is a fuzzy number, and a mixed\nmodel in which a probabilistic background risk is added. For the two models an\napproximate formula of the optimal allocation is computed, with respect to the\npossibilistic moments associated with fuzzy numbers and the indicators of the\ninvestor risk preferences (risk aversion, prudence).\n"
    },
    {
        "paper_id": 1805.12083,
        "authors": "Ivan P. Yamshchikov, Sharwin Rezagholi",
        "title": "Elephants, Donkeys, and Colonel Blotto",
        "comments": null,
        "journal-ref": "In Proceedings of the 3rd International Conference on Complexity,\n  Future Information Systems and Risk - Volume 1: COMPLEXIS, pages 113-119\n  (2018)",
        "doi": "10.5220/0006761601130119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper employs a novel method for the empirical analysis of political\ndiscourse and develops a model that demonstrates dynamics comparable with the\nempirical data. Applying a set of binary text classifiers based on\nconvolutional neural networks, we label statements in the political programs of\nthe Democratic and the Republican Party in the United States. Extending the\nframework of the Colonel Blotto game by a stochastic activation structure, we\nshow that, under a simple learning rule, the simulated game exhibits dynamics\nthat resemble the empirical data.\n"
    },
    {
        "paper_id": 1805.12101,
        "authors": "Paridhi Choudhary, Aniket Jain, Rahul Baijal",
        "title": "Unravelling Airbnb Predicting Price for New Listing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes Airbnb listings in the city of San Francisco to better\nunderstand how different attributes such as bedrooms, location, house type\namongst others can be used to accurately predict the price of a new listing\nthat optimal in terms of the host's profitability yet affordable to their\nguests. This model is intended to be helpful to the internal pricing tools that\nAirbnb provides to its hosts. Furthermore, additional analysis is performed to\nascertain the likelihood of a listings availability for potential guests to\nconsider while making a booking. The analysis begins with exploring and\nexamining the data to make necessary transformations that can be conducive for\na better understanding of the problem at large while helping us make\nhypothesis. Moving further, machine learning models are built that are\nintuitive to use to validate the hypothesis on pricing and availability and run\nexperiments in that context to arrive at a viable solution. The paper then\nconcludes with a discussion on the business implications, associated risks and\nfuture scope.\n"
    },
    {
        "paper_id": 1805.12102,
        "authors": "Ran Huang",
        "title": "A Physical Review on Currency",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A theoretical self-sustainable economic model is established based on the\nfundamental factors of production, consumption, reservation and reinvestment,\nwhere currency is set as a unconditional credit symbol serving as transaction\nequivalent and stock means. Principle properties of currency are explored in\nthis ideal economic system. Physical analysis reveals some facts that were not\naddressed by traditional monetary theory, and several basic principles of ideal\ncurrency are concluded: 1. The saving-replacement is a more primary function of\ncurrency than the transaction equivalents; 2. The ideal efficiency of currency\ncorresponds to the least practical value; 3. The contradiction between constant\nface value of currency and depreciable goods leads to intrinsic inflation.\n"
    },
    {
        "paper_id": 1805.12105,
        "authors": "Tingting Ye and Liangliang Zhang",
        "title": "A Convergent Linear Regression Method for Forward-Backward Stochastic\n  Differential Equations with Jumps",
        "comments": "Potential Mistakes Found",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a large class of convergent numerical methods,\nbased on (linear) basis function regression technique, to approximate the\nsolution to a forward-backward stochastic differential equation with jumps\n(FBSDEJ hereafter). Numerical experiment shows good applicability of the\nproposed method.\n"
    },
    {
        "paper_id": 1805.12106,
        "authors": "Bent Flyvbjerg and Alexander Budzier",
        "title": "Report for the Edinburgh Tram Inquiry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report reviews the Edinburgh tram project's risk management. Projects\nfrequently overrun their cost and timelines and fall short on intended\nbenefits. Cost, schedule, and benefit risk of projects need to be carefully\nconsidered to avoid this. The report describes and evaluates risk assessment\nand management for the Edinburgh tram. The report was produced as part of the\nEdinburgh Tram Inquiry.\n  Keywords: risk assessment, risk management, infrastructure, megaprojects,\noptimism bias, strategic misrepresentation, planning fallacy, behavioral\nscience.\n"
    },
    {
        "paper_id": 1805.12107,
        "authors": "V. I. Gorelov",
        "title": "Information Technologies in Public Administration",
        "comments": "10 pages, 4 tables, 18 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are visible changes in the world organization, environment and health\nof national conscience that create a background for discussion on possible\nredefinition of global, state and regional management goals. The author applies\nthe sustainable development criteria to a hierarchical management scheme that\nis to lead the world community to non-contradictory growth. Concrete\ndefinitions are discussed in respect of decision-making process representing\nthe state mostly. With the help of systems analysis it is highlighted how to\nunderstand who would carry the distinctive sign of world leadership in the\nnearest future.\n"
    },
    {
        "paper_id": 1805.12108,
        "authors": "Herv\\'e Lebret",
        "title": "Are Biotechnology Startups Different?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the domain of technology startups, biotechnology has often been considered\nas specific. Their unique technology content, the type of founders and managers\nthey have, the amount of venture capital they raise, the time it takes them to\nreach an exit as well as the technology clusters they belong to are seen as\nsuch unique features. Based on extensive research from new databases, the\nauthor claims that the biotechnology startups are not as different as it might\nhave been claimed: the amount of venture capital raised, the time to exit,\ntheir geography are indeed similar and even their equity structure to founders\nand managers have similarities. The differences still exist, for example the\nexperience of the founders, the revenue and profit level at exit.\n"
    },
    {
        "paper_id": 1805.12109,
        "authors": "Thi Huong Tran",
        "title": "Critical factors and enablers of food quality and safety compliance risk\n  management in the Vietnamese seafood supply chain",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5121/ijmvsc.2018.9101",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, along with the emergence of food scandals, food supply chains have\nto face with ever-increasing pressure from compliance with food quality and\nsafety regulations and standards. This paper aims to explore critical factors\nof compliance risk in food supply chain with an illustrated case in Vietnamese\nseafood industry. To this end, this study takes advantage of both primary and\nsecondary data sources through a comprehensive literature research of\nindustrial and scientific papers, combined with expert interview. Findings\nshowed that there are three main critical factor groups influencing on\ncompliance risk including challenges originating from Vietnamese food supply\nchain itself, characteristics of regulation and standards, and business\nenvironment. Furthermore, author proposed enablers to eliminate compliance\nrisks to food supply chain managers as well as recommendations to government\nand other influencers and supporters.\n"
    },
    {
        "paper_id": 1805.1211,
        "authors": "Sina Aghaei",
        "title": "A Data-Driven Approach for Modeling Stochasticity in Oil Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Global oil price is an important factor in determining many economic\nvariables in the world's economy. It is generally modeled as a stochastic\nprocess and have been studied through different techniques by comparing the\nhistoric time series of demand, supply and the price itself. However, there are\nmany historic events where the demand or supply changes are not sufficient in\nexplaining the price changes. In such cases, it is the expectations on the\nfuture changes of demand or supply that causes heavy and quick influences on\nthe price. There are many parameters and variables that shape these\nexpectations, and are usually neglected in traditional models. In this paper,\nwe have proposed a model based on System Dynamics approach that takes into\naccount these non-traditional factors. The validity of the proposed model is\nthen evaluated using real and potential scenarios in which the proposed model\nfollows the trend of the real data.\n"
    },
    {
        "paper_id": 1805.12111,
        "authors": "Zhengyang Dong",
        "title": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies",
        "comments": "This is the latest version published in Plos ONE",
        "journal-ref": "PLOS ONE 14(2): e0212487 (2019)",
        "doi": "10.1371/journal.pone.0212487",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trend prediction is a challenging task due to the market's noise, and\nmachine learning techniques have recently been successful in coping with this\nchallenge. In this research, we create a novel framework for stock prediction,\nDynamic Advisor-Based Ensemble (dynABE). dynABE explores domain-specific areas\nbased on the companies of interest, diversifies the feature set by creating\ndifferent \"advisors\" that each handles a different area, follows an effective\nmodel ensemble procedure for each advisor, and combines the advisors together\nin a second-level ensemble through an online update strategy we developed.\ndynABE is able to adapt to price pattern changes of the market during the\nactive trading period robustly, without needing to retrain the entire model. We\ntest dynABE on three cobalt-related companies, and it achieves the best-case\nmisclassification error of 31.12% and an annualized absolute return of 359.55%\nwith zero maximum drawdown. dynABE also consistently outperforms the baseline\nmodels of support vector machine, neural network, and random forest in all case\nstudies.\n"
    },
    {
        "paper_id": 1805.12112,
        "authors": "Chris Kirrane",
        "title": "Lessons from the History of European EMU",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the history of previous examples of EMU from the\nviewpoint that state actors make decisions about whether to participate in a\nmonetary union based on rational self-interest concerning costs and benefits to\ntheir national economies. Illustrative examples are taken from nineteenth\ncentury German, Italian and Japanese attempts at monetary integration with\nearly twentieth century ones from the Latin Monetary Union and the Scandinavian\nMonetary Union and contemporary ones from the West African Monetary Union and\nthe European Monetary System. Lessons learned from the historical examples will\nbe used to identify issues that could arise with the move towards closer EMU in\nEurope.\n"
    },
    {
        "paper_id": 1805.12113,
        "authors": "Chris Kirrane",
        "title": "Implications of EMU for the European Community",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monetary integration has both costs and benefits. Europeans have a strong\naversion to exchange rate instability. From this perspective, the EMS has shown\nits limits and full monetary union involving a single currency appears to be a\nnecessity. This is the goal of the EMU project contained in the Maastricht\nTreaty. This paper examines the pertinent choices: independence of the Central\nBank, budgetary discipline and economic policy coordination. Therefore, the\nimplications of EMU for the economic policy of France will be examined. If the\nexternal force disappears, the public sector still cannot circumvent its\nsolvency constraint. The instrument of national monetary policy will not be\navailable so the absorption of asymmetric shocks will require greater wage\nflexibility and fiscal policy will play a greater role. The paper includes\nthree parts. The first concerns the economic foundations of monetary union and\nthe costs it entails. The second is devoted to the institutional arrangements\nunder the Treaty of Maastricht. The third examines the consequences of monetary\nunion for the economy and the economic policy of France.\n"
    },
    {
        "paper_id": 1805.12217,
        "authors": "Florian Huber, Gregor Kastner, Michael Pfarrhofer",
        "title": "Introducing shrinkage in heavy-tailed state space models to predict\n  equity excess returns",
        "comments": null,
        "journal-ref": "Empirical Economics (2023)",
        "doi": "10.1007/s00181-023-02437-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We forecast S&P 500 excess returns using a flexible Bayesian econometric\nstate space model with non-Gaussian features at several levels. More precisely,\nwe control for overparameterization via novel global-local shrinkage priors on\nthe state innovation variances as well as the time-invariant part of the state\nspace model. The shrinkage priors are complemented by heavy tailed state\ninnovations that cater for potential large breaks in the latent states.\nMoreover, we allow for leptokurtic stochastic volatility in the observation\nequation. The empirical findings indicate that several variants of the proposed\napproach outperform typical competitors frequently used in the literature, both\nin terms of point and density forecasts.\n"
    },
    {
        "paper_id": 1805.12222,
        "authors": "Ariah Klages-Mundt, Andreea Minca",
        "title": "Cascading Losses in Reinsurance Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1287/mnsc.2019.3389",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model for contagion in reinsurance networks by which primary\ninsurers' losses are spread through the network. Our model handles general\nreinsurance contracts, such as typical excess of loss contracts. We show that\nsimpler models existing in the literature--namely proportional\nreinsurance--greatly underestimate contagion risk. We characterize the fixed\npoints of our model and develop efficient algorithms to compute contagion with\nguarantees on convergence and speed under conditions on network structure. We\ncharacterize exotic cases of problematic graph structure and nonlinearities,\nwhich cause network effects to dominate the overall payments in the system. We\nlastly apply our model to data on real world reinsurance networks. Our\nsimulations demonstrate the following: (1) Reinsurance networks face extreme\nsensitivity to parameters. A firm can be wildly uncertain about its losses even\nunder small network uncertainty. (2) Our sensitivity results reveal a new\nincentive for firms to cooperate to prevent fraud, as even small cases of fraud\ncan have outsized effect on the losses across the network. (3) Nonlinearities\nfrom excess of loss contracts obfuscate risks and can cause excess costs in a\nreal world system.\n"
    },
    {
        "paper_id": 1805.12587,
        "authors": "Callegaro Giorgia and Grasselli Martino and Pag\\`es Gilles",
        "title": "Fast Hybrid Schemes for Fractional Riccati Equations (Rough is not so\n  Tough)",
        "comments": "48 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve a family of fractional Riccati differential equations with constant\n(possibly complex) coefficients. These equations arise, e.g., in fractional\nHeston stochastic volatility models, that have received great attention in the\nrecent financial literature thanks to their ability to reproduce a rough\nvolatility behavior. We first consider the case of a zero initial value\ncorresponding to the characteristic function of the log-price. Then we\ninvestigate the case of a general starting value associated to a transform also\ninvolving the volatility process. The solution to the fractional Riccati\nequation takes the form of power series, whose convergence domain is typically\nfinite. This naturally suggests a hybrid numerical algorithm to explicitly\nobtain the solution also beyond the convergence domain of the power series\nrepresentation. Our numerical tests show that the hybrid algorithm turns out to\nbe extremely fast and stable. When applied to option pricing, our method\nlargely outperforms the only available alternative in the literature, based on\nthe Adams method.\n"
    },
    {
        "paper_id": 1806.00529,
        "authors": "Blake C. Stacey, Yaneer Bar-Yam",
        "title": "The Stock Market Has Grown Unstable Since February 2018",
        "comments": "3 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On the fifth of February, 2018, the Dow Jones Industrial Average dropped\n1,175.21 points, the largest single-day fall in history in raw point terms.\nThis followed a 666-point loss on the second, and another drop of over a\nthousand points occurred three days later. It is natural to ask whether these\nevents indicate a transition to a new regime of market behavior, particularly\ngiven the dramatic fluctuations --- both gains and losses --- in the weeks\nsince. To illuminate this matter, we can apply a model grounded in the science\nof complex systems, a model that demonstrated considerable success at\nunraveling the stock-market dynamics from the 1980s through the 2000s. By using\nlarge-scale comovement of stock prices as an early indicator of unhealthy\nmarket dynamics, this work found that abrupt drops in a certain parameter $U$\nprovide an early warning of single-day panics and economic crises. Decreases in\n$U$ indicate regimes of \"high co-movement\", a market behavior that is not the\nsame as volatility, though market volatility can be a component of co-movement.\nApplying the same analysis to stock-price data from the beginning of 2016 until\nnow, we find that the $U$ value for the period since 5 February is\nsignificantly lower than for the period before. This decrease entered the\n\"danger zone\" in the last week of May, 2018.\n"
    },
    {
        "paper_id": 1806.00605,
        "authors": "Yuichi Ikeda and Hiroshi Iyetomi",
        "title": "Trade Network Reconstruction and Simulation with Changes in Trade Policy",
        "comments": "submitted to Evolutionary and Institutional Economics Review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interdependent nature of the global economy has become stronger with\nincreases in international trade and investment. We propose a new model to\nreconstruct the international trade network and associated cost network by\nmaximizing entropy based on local information about inward and outward trade.\nWe show that the trade network can be successfully reconstructed using the\nproposed model. In addition to this reconstruction, we simulated structural\nchanges in the international trade network caused by changing trade tariffs in\nthe context of the government's trade policy. The simulation for the FOOD\ncategory shows that import of FOOD from the U.S. to Japan increase drastically\nby halving the import cost. Meanwhile, the simulation for the MACHINERY\ncategory shows that exports from Japan to the U.S. decrease drastically by\ndoubling the export cost, while exports to the EU increased.\n"
    },
    {
        "paper_id": 1806.00799,
        "authors": "Tembo Nakamoto and Yuichi Ikeda",
        "title": "Identification of Conduit Countries and Community Structures in the\n  Withholding Tax Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to economic globalization, each country's economic law, including tax\nlaws and tax treaties, has been forced to work as a single network. However,\neach jurisdiction (country or region) has not made its economic law under the\nassumption that its law functions as an element of one network, so it has\nbrought unexpected results. We thought that the results are exactly\ninternational tax avoidance. To contribute to the solution of international tax\navoidance, we tried to investigate which part of the network is vulnerable.\nSpecifically, focusing on treaty shopping, which is one of international tax\navoidance methods, we attempt to identified which jurisdiction are likely to be\nused for treaty shopping from tax liabilities and the relationship between\njurisdictions which are likely to be used for treaty shopping and others. For\nthat purpose, based on withholding tax rates imposed on dividends, interest,\nand royalties by jurisdictions, we produced weighted multiple directed graphs,\ncomputed the centralities and detected the communities. As a result, we\nclarified the jurisdictions that are likely to be used for treaty shopping and\npointed out that there are community structures. The results of this study\nsuggested that fewer jurisdictions need to introduce more regulations for\nprevention of treaty abuse worldwide.\n"
    },
    {
        "paper_id": 1806.00817,
        "authors": "Marcel Nutz, Jaime San Martin, Xiaowei Tan",
        "title": "Convergence to the Mean Field Game Limit: A Case Study",
        "comments": "Forthcoming in 'Annals of Applied Probability'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the convergence of Nash equilibria in a game of optimal stopping. If\nthe associated mean field game has a unique equilibrium, any sequence of\n$n$-player equilibria converges to it as $n\\to\\infty$. However, both the finite\nand infinite player versions of the game often admit multiple equilibria. We\nshow that mean field equilibria satisfying a transversality condition are limit\npoints of $n$-player equilibria, but we also exhibit a remarkable class of mean\nfield equilibria that are not limits, thus questioning their interpretation as\n\"large $n$\" equilibria.\n"
    },
    {
        "paper_id": 1806.00898,
        "authors": "Sander Heinsalu",
        "title": "Competitive pricing despite search costs if lower price signals quality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show that firms price almost competitively and consumers can infer product\nquality from prices in markets where firms differ in quality and production\ncost, and learning prices is costly. Bankruptcy risk or regulation links higher\nquality to lower cost. If high-quality firms have lower cost, then they can\nsignal quality by cutting prices. Then the low-quality firms must cut prices to\nretain customers. This price-cutting race to the bottom ends in a separating\nequilibrium in which the low-quality firms charge their competitive price and\nthe high-quality firms charge slightly less.\n"
    },
    {
        "paper_id": 1806.00997,
        "authors": "Federico Flore and Giovanna Nappo",
        "title": "A Feynman-Kac type formula for a fixed delay CIR model",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic delay differential equations (SDDE's) have been used for financial\nmodeling. In this article, we study a SDDE obtained by the equation of a CIR\nprocess, with an additional fixed delay term in drift; in particular, we prove\nthat there exists a unique strong solution (positive and integrable) which we\ncall fixed delay CIR process. Moreover, for the fixed delay CIR process, we\nderive a Feynman-Kac type formula, leading to a generalized exponential-affine\nformula, which is used to determine a bond pricing formula when the interest\nrate follows the delay's equation. It turns out that, for each maturity time T,\nthe instantaneous forward rate is an affine function (with time dependent\ncoefficients) of the rate process and of an auxiliary process (also depending\non T). The coefficients satisfy a system of deterministic delay differential\nequations.\n"
    },
    {
        "paper_id": 1806.0107,
        "authors": "Jos\\'e Igor Morlanes",
        "title": "Non-linear Time Series and Artificial Neural Networks of Red Hat\n  Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the empirical results published in article \"Empirical Evidence on\nArbitrage by Changing the Stock Exchange\" by means of machine learning and\nadvanced econometric methodologies based on Smooth Transition Regression models\nand Artificial Neural Networks.\n"
    },
    {
        "paper_id": 1806.01166,
        "authors": "Fei Sun, Jingchao Li, Jieming Zhou",
        "title": "Dynamic risk measures with fluctuation of market volatility under\n  Bochne-Lebesgue space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Starting from the global financial crisis to the more recent disruptions\nbrought about by geopolitical tensions and public health crises, the volatility\nof risk in financial markets has increased significantly. This underscores the\nnecessity for comprehensive risk measures capable of capturing the complexity\nand heightened fluctuations in market volatility. This need is crucial not only\nfor new financial assets but also for the traditional financial market in the\nface of a rapidly changing financial environment and global landscape. In this\npaper, we consider the risk measures on a special space $L^{p(\\cdot)}$, where\nthe variable exponent $p(\\cdot)$ is no longer a given real number as in the\nconventional risk measure space $L^{p}$, but rather a random variable\nreflecting potential fluctuations in volatility within financial markets.\nThrough further development of axioms related to this class of risk measures,\nwe also establish dual representations for them.\n"
    },
    {
        "paper_id": 1806.01172,
        "authors": "Antonis Papapantoleon, Dylan Possamai, Alexandros Saplaouras",
        "title": "Stability results for martingale representations: the general case",
        "comments": "55 pages, revised version",
        "journal-ref": null,
        "doi": "10.1090/tran/7880",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain stability results for martingale representations in\na very general framework. More specifically, we consider a sequence of\nmartingales each adapted to its own filtration, and a sequence of random\nvariables measurable with respect to those filtrations. We assume that the\nterminal values of the martingales and the associated filtrations converge in\nthe extended sense, and that the limiting martingale is quasi--left--continuous\nand admits the predictable representation property. Then, we prove that each\ncomponent in the martingale representation of the sequence converges to the\ncorresponding component of the martingale representation of the limiting random\nvariable relative to the limiting filtration, under the Skorokhod topology.\nThis extends in several directions earlier contributions in the literature, and\nhas applications to stability results for backward SDEs with jumps and to\ndiscretisation schemes for stochastic systems.\n"
    },
    {
        "paper_id": 1806.01223,
        "authors": "Matteo Brachetta and Claudia Ceci",
        "title": "Optimal proportional reinsurance and investment for stochastic factor\n  models",
        "comments": "35 pages, 7 figures",
        "journal-ref": "Insurance Mathematics and Economics (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we investigate the optimal proportional reinsurance-investment\nstrategy of an insurance company which wishes to maximize the expected\nexponential utility of its terminal wealth in a finite time horizon. Our goal\nis to extend the classical Cramer-Lundberg model introducing a stochastic\nfactor which affects the intensity of the claims arrival process, described by\na Cox process, as well as the insurance and reinsurance premia. Using the\nclassical stochastic control approach based on the Hamilton-Jacobi-Bellman\nequation we characterize the optimal strategy and provide a verification result\nfor the value function via classical solutions of two backward partial\ndifferential equations. Existence and uniqueness of these solutions are\ndiscussed. Results under various premium calculation principles are illustrated\nand a new premium calculation rule is proposed in order to get more realistic\nstrategies and to better fit our stochastic factor model. Finally, numerical\nsimulations are performed to obtain sensitivity analyses.\n"
    },
    {
        "paper_id": 1806.01495,
        "authors": "Kerem Ugurlu",
        "title": "Dynamic optimal contract under parameter uncertainty with risk averse\n  agent and principal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a continuous time Principal-Agent model on a finite time horizon,\nwhere we look for the existence of an optimal contract both parties agreed on.\nContrary to the main stream, where the principal is modelled as risk-neutral,\nwe assume that both the principal and the agent have exponential utility, and\nare risk averse with same risk awareness level. Moreover, the agent's quality\nis unknown and modelled as a filtering term in the problem, which is revealed\nas time passes by. The principal can not observe the agent's real action, but\ncan only recommend action levels to the agent. Hence, we have a \\textit{moral\nhazard} problem. In this setting, we give an explicit solution to the optimal\ncontract problem.\n"
    },
    {
        "paper_id": 1806.01616,
        "authors": "Ladislav Kristoufek",
        "title": "Power-law cross-correlations: Issues, solutions and future challenges",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Analysis of long-range dependence in financial time series was one of the\ninitial steps of econophysics into the domain of mainstream finance and\nfinancial economics in the 1990s. Since then, many different financial series\nhave been analyzed using the methods standardly used outside of finance to\ndeliver some important stylized facts of the financial markets. In the late\n2000s, these methods have started being generalized to bivariate settings so\nthat the relationship between two series could be examined in more detail. It\nwas then only a single step from bivariate long-range dependence towards\nscale-specific correlations and regressions as well as power-law coherency as a\nunique relationship between power-law correlated series. Such rapid development\nin the field has brought some issues and challenges that need further\ndiscussion and attention. We shortly review the development and historical\nsteps from long-range dependence to bivariate generalizations and connected\nmethods, focus on its technical aspects and discuss problematic parts and\nchallenges for future directions in this specific subfield of econophysics.\n"
    },
    {
        "paper_id": 1806.01728,
        "authors": "Francesca Biagini, Andrea Mazzon, Thilo Meyer-Brandis",
        "title": "Financial asset bubbles in banking networks",
        "comments": "33 pages, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a banking network represented by a system of stochastic\ndifferential equations coupled by their drift. We assume a core-periphery\nstructure, and that the banks in the core hold a bubbly asset. The banks in the\nperiphery have not direct access to the bubble, but can take initially\nadvantage from its increase by investing on the banks in the core. Investments\nare modeled by the weight of the links, which is a function of the robustness\nof the banks. In this way, a preferential attachment mechanism towards the core\ntakes place during the growth of the bubble. We then investigate how the bubble\ndistort the shape of the network, both for finite and infinitely large systems,\nassuming a non vanishing impact of the core on the periphery. Due to the\ninfluence of the bubble, the banks are no longer independent, and the law of\nlarge numbers cannot be directly applied at the limit. This results in a term\nin the drift of the diffusions which does not average out, and that increases\nsystemic risk at the moment of the burst. We test this feature of the model by\nnumerical simulations.\n"
    },
    {
        "paper_id": 1806.01731,
        "authors": "Greg Kirczenow, Ali Fathi, Matt Davison",
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to\n  Illiquid Corporate Bonds (Preliminary Draft)",
        "comments": "5 figures, Presented in CAIMS 2018-Toronto, CA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the application of machine learning in extracting the\nmarket implied features from historical risk neutral corporate bond yields. We\nconsider the example of a hypothetical illiquid fixed income market. After\nchoosing a surrogate liquid market, we apply the Denoising Autoencoder\nalgorithm from the field of computer vision and pattern recognition to learn\nthe features of the missing yield parameters from the historically implied data\nof the instruments traded in the chosen liquid market. The results of the\ntrained machine learning algorithm are compared with the outputs of a point in-\ntime 2 dimensional interpolation algorithm known as the Thin Plate Spline.\nFinally, the performances of the two algorithms are compared.\n"
    },
    {
        "paper_id": 1806.01734,
        "authors": "P. P. Osei and A. Jasra",
        "title": "Estimating option prices using multilevel particle filters",
        "comments": "$19$ pages, $18$ figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Option valuation problems are often solved using standard Monte Carlo (MC)\nmethods. These techniques can often be enhanced using several strategies\nespecially when one discretizes the dynamics of the underlying asset, of which\nwe assume follows a diffusion process. We consider the combination of two\nmethodologies in this direction. The first is the well-known multilevel Monte\nCarlo (MLMC) method, which is known to reduce the computational effort to\nachieve a given level of mean square error relative to MC in some cases.\nSequential Monte Carlo (or the particle filter (PF)) methods have also been\nshown to be beneficial in many option pricing problems potentially reducing\nvariances by large magnitudes (relative to MC). We propose a multilevel\nparticle filter (MLPF) as an alternative approach to price options. The\ncomputational savings obtained in using MLPF over PF for pricing both vanilla\nand exotic options is demonstrated via numerical simulations.\n"
    },
    {
        "paper_id": 1806.01743,
        "authors": "XingYu Fu and JinHong Du and YiFeng Guo and MingWen Liu and Tao Dong\n  and XiuWen Duan",
        "title": "A Machine Learning Framework for Stock Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper demonstrates how to apply machine learning algorithms to\ndistinguish good stocks from the bad stocks. To this end, we construct 244\ntechnical and fundamental features to characterize each stock, and label stocks\naccording to their ranking with respect to the return-to-volatility ratio.\nAlgorithms ranging from traditional statistical learning methods to recently\npopular deep learning method, e.g. Logistic Regression (LR), Random Forest\n(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the\nclassification task. Genetic Algorithm (GA) is also used to implement feature\nselection. The effectiveness of the stock selection strategy is validated in\nChinese stock market in both statistical and practical aspects, showing that:\n1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic\nAlgorithm picks a subset of 114 features and the prediction performances of all\nmodels remain almost unchanged after the selection procedure, which suggests\nsome features are indeed redundant; 3) LR and DNN are radical models; RF is\nrisk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios\nconstructed by our models outperform market average in back tests.\n"
    },
    {
        "paper_id": 1806.01781,
        "authors": "Abhishta and Reinoud Joosten and Lambert J.M. Nieuwenhuis",
        "title": "Comparing Alternatives to Measure the Impact of DDoS Attack\n  Announcements on Target Stock Prices",
        "comments": "The final version of this paper has been published in Journal of\n  Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications\n  (JoWUA), Volume 8, Number 4",
        "journal-ref": "2017, Volume 8, Number 4, Journal of Wireless Mobile Networks,\n  Ubiquitous Computing, and Dependable Applications (JoWUA)",
        "doi": "10.22667/JOWUA.2017.12.31.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The attack intensity of distributed denial of service (DDoS) attacks is\nincreasing every year. Botnets based on internet of things (IOT) devices are\nnow being used to conduct DDoS attacks. The estimation of direct and indirect\neconomic damages caused by these attacks is a complex problem. One of the\nindirect damage of a DDoS attack can be on the market value of the victim firm.\nIn this article we analyze the impact of 45 different DDoS attack announcements\non victim's stock prices. We find that previous studies have a mixed conclusion\non the impact of DDoS attack announcements on the victim's stock price. Hence,\nin this article we evaluate this impact using three different approaches and\ncompare the results. In the first approach, we use the assume the cumulative\nabnormal returns to be normally distributed and test the hypothesis that a DDoS\nattack announcement has no impact on the victim's stock price. In the latter\ntwo methods, we do not assume a distribution and use the empirical distribution\nof cumulative abnormal returns to test the hypothesis. We find that the\nassumption of cumulative abnormal returns being normally distributed leads to\noverestimation/underestimation of the impact. Finally, we analyze the impact of\nDDoS attack announcement on victim's stock price in each of the 45 cases and\npresent our results.\n"
    },
    {
        "paper_id": 1806.01924,
        "authors": "Alain B\\'elanger, Ndoun\\'e Ndoun\\'e, Roland Pongou",
        "title": "Dark Markets with Multiple Assets: Segmentation, Asymptotic Stability,\n  and Equilibrium Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a generalization of the model of a dark market due to\nDuffie-G\\^arleanu- Pedersen [6]. Our market is segmented and involves multiple\nassets. We show that this market has a unique asymptotically stable\nequilibrium. In order to establish this result, we use a novel approach\ninspired by a theory due to McKenzie and Hawkins-Simon. Moreover, we obtain a\nclosed form solution for the price of each asset at which investors trade at\nequilibrium. We conduct a comparative statics analysis which shows, among other\nsensitivities, how equilibrium prices respond to the level of interactions\nbetween investors.\n"
    },
    {
        "paper_id": 1806.02083,
        "authors": "B. A. Surya",
        "title": "Parisian excursion below a fixed level from the last record maximum of\n  Levy insurance risk process",
        "comments": "15",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents some new results on Parisian ruin under Levy insurance\nrisk process, where ruin occurs when the process has gone below a fixed level\nfrom the last record maximum, also known as the high-water mark or drawdown,\nfor a fixed consecutive periods of time. The law of ruin-time and the position\nat ruin is given in terms of their joint Laplace transforms. Identities are\npresented semi-explicitly in terms of the scale function and the law of the\nLevy process. They are established using recent developments on fluctuation\ntheory of drawdown of spectrally negative Levy process. In contrast to the\nParisian ruin of Levy process below a fixed level, ruin under drawdown occurs\nin finite time with probability one.\n"
    },
    {
        "paper_id": 1806.02912,
        "authors": "Tolulope Fadina, Ariel Neufeld, Thorsten Schmidt",
        "title": "Affine processes under parameter uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a one-dimensional notion of affine processes under parameter\nuncertainty, which we call non-linear affine processes. This is done as\nfollows: given a set of parameters for the process, we construct a\ncorresponding non-linear expectation on the path space of continuous processes.\nBy a general dynamic programming principle we link this non-linear expectation\nto a variational form of the Kolmogorov equation, where the generator of a\nsingle affine process is replaced by the supremum over all corresponding\ngenerators of affine processes with parameters in the parameter set. This\nnon-linear affine process yields a tractable model for Knightian uncertainty,\nespecially for modelling interest rates under ambiguity.\n  We then develop an appropriate Ito-formula, the respective term-structure\nequations and study the non-linear versions of the Vasicek and the\nCox-Ingersoll-Ross (CIR) model. Thereafter we introduce the non-linear\nVasicek-CIR model. This model is particularly suitable for modelling interest\nrates when one does not want to restrict the state space a priori and hence the\napproach solves this modelling issue arising with negative interest rates.\n"
    },
    {
        "paper_id": 1806.02991,
        "authors": "Po-Keng Cheng (SAF), Fr\\'ed\\'eric Planchet (SAF)",
        "title": "Stochastic Deflator for an Economic Scenario Generator with Five Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we implement a stochastic deflator with five economic and\nfinancial risk factors: interest rates, market price of risk, stock prices,\ndefault intensities, and convenience yields. We examine the deflator with\ndifferent financial assets, such as stocks, zero-coupon bonds, vanilla options,\nand corporate coupon bonds. We find required regularity conditions to implement\nour stochastic deflator. Our numerical results show the reliability of the\ndeflator approach in pricing financial derivatives.\n"
    },
    {
        "paper_id": 1806.03153,
        "authors": "Damir Filipovic, Martin Larsson, Anders B. Trolle",
        "title": "On the Relation Between Linearity-Generating Processes and\n  Linear-Rational Models",
        "comments": "Forthcoming in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review the notion of a linearity-generating (LG) process introduced by\nGabaix (2007) and relate LG processes to linear-rational (LR) models studied by\nFilipovic, Larsson, and Trolle (2017). We show that every LR model can be\nrepresented as an LG process and vice versa. We find that LR models have two\nbasic properties which make them an important representation of LG processes.\nFirst, LR models can be easily specified and made consistent with nonnegative\ninterest rates. Second, LR models go naturally with the long-term risk\nfactorization due to Alvarez and Jermann (2005), Hansen and Scheinkman (2009),\nand Qin and Linetsky (2017). Every LG process under the long forward measure\ncan be represented as a lower dimensional LR model.\n"
    },
    {
        "paper_id": 1806.03294,
        "authors": "Rajbir-Singh Nirwan and Nils Bertschinger",
        "title": "Applications of Gaussian Process Latent Variable Models in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating covariances between financial assets plays an important role in\nrisk management. In practice, when the sample size is small compared to the\nnumber of variables, the empirical estimate is known to be very unstable. Here,\nwe propose a novel covariance estimator based on the Gaussian Process Latent\nVariable Model (GP-LVM). Our estimator can be considered as a non-linear\nextension of standard factor models with readily interpretable parameters\nreminiscent of market betas. Furthermore, our Bayesian treatment naturally\nshrinks the sample covariance matrix towards a more structured matrix given by\nthe prior and thereby systematically reduces estimation errors. Finally, we\ndiscuss some financial applications of the GP-LVM.\n"
    },
    {
        "paper_id": 1806.03496,
        "authors": "Zbigniew Palmowski, {\\L}ukasz Stettner, and Anna Sulima",
        "title": "Optimal portfolio selection in an It\\^o-Markov additive market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a portfolio selection problem in a continuous-time It\\^o-Markov\nadditive market with prices of financial assets described by Markov additive\nprocesses which combine L\\'evy processes and regime switching models. Thus the\nmodel takes into account two sources of risk: the jump diffusion risk and the\nregime switching risk. For this reason the market is incomplete. We complete\nthe market by enlarging it with the use of a set of Markovian jump securities,\nMarkovian power-jump securities and impulse regime switching securities.\nMoreover, we give conditions under which the market is\nasymptotic-arbitrage-free. We solve the portfolio selection problem in the\nIt\\^o-Markov additive market for the power utility and the logarithmic utility.\n"
    },
    {
        "paper_id": 1806.03543,
        "authors": "Sergey Badikov, Mark H.A. Davis, Antoine Jacquier",
        "title": "Perturbation analysis of sub/super hedging problems",
        "comments": "26 pages. Forthcoming in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the links between various no-arbitrage conditions and the\nexistence of pricing functionals in general markets, and prove the Fundamental\nTheorem of Asset Pricing therein. No-arbitrage conditions, either in this\nabstract setting or in the case of a market consisting of European Call\noptions, give rise to duality properties of infinite-dimensional sub- and\nsuper-hedging problems. With a view towards applications, we show how duality\nis preserved when reducing these problems over finite-dimensional bases. We\nfinally perform a rigorous perturbation analysis of those linear programming\nproblems, and highlight numerically the influence of smile extrapolation on the\nbounds of exotic options.\n"
    },
    {
        "paper_id": 1806.03624,
        "authors": "Weiping Wu and Jianjun Gao and Junguo Lu and Xun Li",
        "title": "Optimal Control of Constrained Stochastic Linear-Quadratic Model with\n  Applications",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a class of continuous-time scalar-state stochastic\nLinear-Quadratic (LQ) optimal control problem with the linear control\nconstraints. Applying the state separation theorem induced from its special\nstructure, we develop the explicit solution for this class of problem. The\nrevealed optimal control policy is a piece-wise affine function of system\nstate. This control policy can be computed efficiently by solving two Riccati\nequations off-line. Under some mild conditions, the stationary optimal control\npolicy can be also derived for this class of problem with infinite horizon.\nThis result can be used to solve the constrained dynamic mean-variance\nportfolio selection problem. Examples shed light on the solution procedure of\nimplementing our method.\n"
    },
    {
        "paper_id": 1806.03683,
        "authors": "Giuseppe Orlando, Rosa Maria Mininni, Michele Bufalo",
        "title": "On The Calibration of Short-Term Interest Rates Through a CIR Model",
        "comments": "Research Article",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that the Cox-Ingersoll-Ross (CIR) stochastic model to study\nthe term structure of interest rates, as introduced in 1985, is inadequate for\nmodelling the current market environment with negative short interest rates.\nMoreover, the diffusion term in the rate dynamics goes to zero when short rates\nare small; both volatility and long-run mean do not change with time; they do\nnot fit with the skewed (fat tails) distribution of the interest rates, etc.\nThe aim of the present work is to suggest a new framework, which we call the\nCIR\\# model, that well fits the term structure of short interest rates so that\nthe market volatility structure is preserved as well as the analytical\ntractability of the original CIR model.\n"
    },
    {
        "paper_id": 1806.03758,
        "authors": "Emanuele Crosato and Ramil Nigmatullin and Mikhail Prokopenko",
        "title": "On critical dynamics and thermodynamic efficiency of urban\n  transformations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urban transformations within large and growing metropolitan areas often\ngenerate critical dynamics affecting social interactions, transport\nconnectivity and income flow distribution. We develop a statistical-mechanical\nmodel of urban transformations, exemplified for Greater Sydney, and derive a\nthermodynamic description highlighting critical regimes. We consider urban\ndynamics at two time scales: fast dynamics for the distribution of population\nand income, modelled via the maximum entropy principle, and slower dynamics\nevolving the urban structure under spatially distributed competition. We\nidentify phase transitions between dispersed and polycentric phases, induced by\nvarying the social disposition---a factor balancing the suburbs'\nattractiveness---in contrast with the travel impedance. Using the Fisher\ninformation we identify critical thresholds and quantify the thermodynamic cost\nof urban transformation, as the minimal work required to vary the underlying\nparameter. Finally, we introduce the notion of thermodynamic efficiency of\nurban transformation, as the ratio of the order gained during a change to the\namount of required work, showing that this measure is maximised at criticality.\n"
    },
    {
        "paper_id": 1806.03887,
        "authors": "Mar\\'ia Fernanda del Carmen Agoitia Hurtado and Thorsten Schmidt",
        "title": "Time-inhomogeneous polynomial processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time homogeneous polynomial processes are Markov processes whose moments can\nbe calculated easily through matrix exponentials. In this work, we develop a\nnotion of time inhomogeneous polynomial processes where the coeffiecients of\nthe process may depend on time. A full characterization of this model class is\ngiven by means of their semimartingale characteristics. We show that in\ngeneral, the computation of moments by matrix exponentials is no longer\npossible. As an alternative we explore a connection to Magnus series for fast\nnumerical approximations.\n  Time-inhomogeneity is important in a number of applications: in\nterm-structure models, this allows a perfect calibration to available prices.\nIn electricity markets, seasonality comes naturally into play and have to be\ncaptured by the used models. The model class studied in this work extends\nexisting models, for example Sato processes and time-inhomogeneous affine\nprocesses.\n"
    },
    {
        "paper_id": 1806.04025,
        "authors": "Yushi Hamaguchi",
        "title": "BSDEs driven by cylindrical martingales with application to approximate\n  hedging in bond markets",
        "comments": "27 pages",
        "journal-ref": "Japan J. Indust. Appl. Math., 38, pp:425--453 (2021)",
        "doi": "10.1007/s13160-020-00442-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider Lipschitz-type backward stochastic differential equations (BSDEs)\ndriven by cylindrical martingales on the space of continuous functions. We show\nthe existence and uniqueness of the solution of such infinite-dimensional BSDEs\nand prove that the sequence of solutions of corresponding finite-dimensional\nBSDEs approximates the original solution. We also consider the hedging problem\nin bond markets and prove that, for an approximately attainable contingent\nclaim, the sequence of locally risk-minimizing strategies based on small\nmarkets converges to the generalized hedging strategy.\n"
    },
    {
        "paper_id": 1806.04347,
        "authors": "Yingli Wang and Xiaoguang Yang",
        "title": "Asymmetric response to PMI announcements in China's stock returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Considered an important macroeconomic indicator, the Purchasing Managers'\nIndex (PMI) on Manufacturing generally assumes that PMI announcements will\nproduce an impact on stock markets. International experience suggests that\nstock markets react to negative PMI news. In this research, we empirically\ninvestigate the stock market reaction towards PMI in China. The asymmetric\neffects of PMI announcements on the stock market are observed: no market\nreaction is generated towards negative PMI announcements, while a positive\nreaction is generally generated for positive PMI news. We further find that the\npositive reaction towards the positive PMI news occurs 1 day before the\nannouncement and lasts for nearly 3 days, and the positive reaction is observed\nin the context of expanding economic conditions. By contrast, the negative\nreaction towards negative PMI news is prevalent during downward economic\nconditions for stocks with low market value, low institutional shareholding\nratios or high price earnings. Our study implies that China's stock market\nfavors risk to a certain extent given the vast number of individual investors\nin the country, and there may exist information leakage in the market.\n"
    },
    {
        "paper_id": 1806.04351,
        "authors": "Yingli Wang, Qingpeng Zhang, and Xiaoguang Yang",
        "title": "Network Subgraphs of the heterogeneous Chinese credit system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we investigate the evolution of Chinese guarantee networks\nfrom the angle of sub-patterns. First, we find that the mutual, 2-out-stars and\ntriangle sub-patterns are motifs in 2- and 3-node subgraphs. Considering the\nheterogeneous financial characteristics of nodes, we find that small firms tend\nto form a mutual guarantee relationship and large firms are likely to be the\nguarantors in 2-out-stars sub-patterns.\n"
    },
    {
        "paper_id": 1806.04363,
        "authors": "Jae Woo Lee, Ashadun Nobi",
        "title": "State and Network Structures of Stock Markets around the Global\n  Financial Crisis",
        "comments": "18pages, 5figures",
        "journal-ref": "Computational Economics, 51, 195-210 (2018)",
        "doi": "10.1007/s10614-017-9672-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the effects of the 2008 global financial crisis on the global\nstock market before, during, and after the crisis. We generate complex networks\nfrom a cross-correlation matrix such as the threshold network (TN) and the\nminimal spanning tree (MST). In the threshold network, we assign a threshold\nvalue by using the mean and standard deviation of cross-correlation\ncoefficients. When the threshold is equal to the mean of these coefficients, we\nobserve a giant cluster composed of three economic zones in all three periods.\nWe find that during the crisis, the countries in the Asian zone were weakly\nconnected and those in the American zone were tightly linked to the countries\nin the European zone. At a large threshold, the three economic zones were\nfragmented. The European countries connected tightly, but the Asian countries\nbound weakly. The MST constructed from the distance matrix. In the MST, France\nremained a hub node in all three periods. The size of the MST shrank slightly\nduring the crisis. We observe a scaling relation between the network distance\nof nodes from the central hub (France) and the geometrical distance. We observe\nthe topological change of the financial network structure during the global\nfinancial crisis. The TN and MST are complementary roles to understand the\nconnecting structure of financial complex networks. The TN reveals to observe\nthe clustering effects and robustness of the cluster during the financial\ncrisis. The MST shows the central hub and connecting node among the economic\nzones.\n"
    },
    {
        "paper_id": 1806.0446,
        "authors": "Alvaro Cartea, Sebastian Jaimungal, Jamie Walton",
        "title": "Foreign Exchange Markets with Last Look",
        "comments": "40 pages, 7 figures",
        "journal-ref": "Mathematics and Financial Economics, Forthcoming, 2018",
        "doi": "10.1007/s11579-018-0218-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the Foreign Exchange (FX) spot price spreads with and without Last\nLook on the transaction. We assume that brokers are risk-neutral and they quote\nspreads so that losses to latency arbitrageurs (LAs) are recovered from other\ntraders in the FX market. These losses are reduced if the broker can reject,\nex-post, loss-making trades by enforcing the Last Look option which is a\nfeature of some trading venues in FX markets. For a given rejection threshold\nthe risk-neutral broker quotes a spread to the market so that her expected\nprofits are zero. When there is only one venue, we find that the Last Look\noption reduces quoted spreads. If there are two venues we show that the market\nreaches an equilibrium where traders have no incentive to migrate. The\nequilibrium can be reached with both venues coexisting, or with only one venue\nsurviving. Moreover, when one venue enforces Last Look and the other one does\nnot, counterintuitively, it may be the case that the Last Look venue quotes\nlarger spreads.\n"
    },
    {
        "paper_id": 1806.04472,
        "authors": "Philippe Casgrain, Sebastian Jaimungal",
        "title": "Trading algorithms with learning in latent alpha models",
        "comments": "42 pages, 5 figures",
        "journal-ref": "Mathematical Finance, Forthcoming, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Alpha signals for statistical arbitrage strategies are often driven by latent\nfactors. This paper analyses how to optimally trade with latent factors that\ncause prices to jump and diffuse. Moreover, we account for the effect of the\ntrader's actions on quoted prices and the prices they receive from trading.\nUnder fairly general assumptions, we demonstrate how the trader can learn the\nposterior distribution over the latent states, and explicitly solve the latent\noptimal trading problem. We provide a verification theorem, and a methodology\nfor calibrating the model by deriving a variation of the\nexpectation-maximization algorithm. To illustrate the efficacy of the optimal\nstrategy, we demonstrate its performance through simulations and compare it to\nstrategies which ignore learning in the latent factors. We also provide\ncalibration results for a particular model using Intel Corporation stock as an\nexample.\n"
    },
    {
        "paper_id": 1806.05028,
        "authors": "Mario Coccia",
        "title": "Socioeconomic driving forces of scientific research",
        "comments": "35 pages, 6 figures, 3 tables. Keywords: Science Progress, Scientific\n  Research, Social Power, Social Dynamics of Science, R&D Investments,\n  Knowledge Spillovers, Scientific Knowledge, Productivity",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Why do nations produce scientific research? This is a fundamental problem in\nthe field of social studies of science. The paper confronts this question here\nby showing vital determinants of science to explain the sources of social power\nand wealth creation by nations. Firstly, this study suggests a new general\ndefinition of science and scientific research that synthetizes previous\nconcepts and endeavors to extend them: Science discovers the root causes of\nphenomena to explain and predict them in a context of adaptation of life to new\neconomic and social bases, whereas scientific research is a systematic process,\napplying methods of scientific inquiry, to solve consequential problems, to\nsatisfy human wants, to take advantage of important opportunities and/or to\ncope with environmental threats. In particular, science and scientific research\nare driven by an organized social effort that inevitably reflect the concerns\nand interests of nations to achieve advances and discoveries that are spread to\nthe rest of humankind. This study reveals that scientific research is produced\nfor social and economic interests of nations (power, wealth creation,\ntechnological superiority, etc.), rather than philosophical inquiries. A main\nimplication of this study is that the immense growth of science in modern\nsociety is not only due to activity of scientists but rather to general social\nefforts of nations to take advantage of important opportunities and/or to cope\nwith environmental threats, such as war. Empirical evidence endeavors to\nsupport the sources of scientific research for nations, described here.\nFinally, relationships between R&D investment and productivity, and research\npolicy implications are discussed.\n"
    },
    {
        "paper_id": 1806.05101,
        "authors": "Xiaofei Lu, Fr\\'ed\\'eric Abergel",
        "title": "Order-book modelling and market making strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making is one of the most important aspects of algorithmic trading,\nand it has been studied quite extensively from a theoretical point of view. The\npractical implementation of so-called \"optimal strategies\" however suffers from\nthe failure of most order book models to faithfully reproduce the behaviour of\nreal market participants.\n  This paper is twofold. First, some important statistical properties of order\ndriven markets are identified, advocating against the use of purely Markovian\norder book models. Then, market making strategies are designed and their\nperformances are compared, based on simulation as well as backtesting. We find\nthat incorporating some simple non-Markovian features in the limit order book\ngreatly improves the performances of market making strategies in a realistic\ncontext.\n"
    },
    {
        "paper_id": 1806.0516,
        "authors": "Ludovico Latmiral",
        "title": "Weak Correlations of Stocks Future Returns",
        "comments": "8 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze correlations among stock returns via a series of widely adopted\nparameters which we refer to as explanatory variables. We subsequently exploit\nthe results to propose a long only quantitative adaptive technique to construct\na profitable portfolio of assets which exhibits minor drawdowns and higher\nrecoveries than both an equally weighted and an efficient frontier portfolio.\n"
    },
    {
        "paper_id": 1806.05262,
        "authors": "Venkat Venkatasubramanian and Yu Luo",
        "title": "How much income inequality is fair? Nash bargaining solution and its\n  connection to entropy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The question about fair income inequality has been an important open question\nin economics and in political philosophy for over two centuries with only\nqualitative answers such as the ones suggested by Rawls, Nozick, and Dworkin.\nWe provided a quantitative answer recently, for an ideal free-market society,\nby developing a game-theoretic framework that proved that the ideal inequality\nis a lognormal distribution of income at equilibrium. In this paper, we develop\nanother approach, using the Nash Bargaining Solution (NBS) framework, which\nalso leads to the same conclusion. Even though the conclusion is the same, the\nnew approach, however, reveals the true nature of NBS, which has been of\nconsiderable interest for several decades. Economists have wondered about the\neconomic meaning or purpose of the NBS. While some have alluded to its fairness\nproperty, we show more conclusively that it is all about fairness. Since the\nessence of entropy is also fairness, we see an interesting connection between\nthe Nash product and entropy for a large population of rational economic\nagents.\n"
    },
    {
        "paper_id": 1806.05293,
        "authors": "Tim Byrnes, Tristan Barnett",
        "title": "Generalized framework for applying the Kelly criterion to stock markets",
        "comments": "6 pages, 2 figures, accepted for publication in International Journal\n  of Theoretical and Applied Finance",
        "journal-ref": "International Journal of Theoretical and Applied Finance 21,\n  1850033 (2018)",
        "doi": "10.1142/S0219024918500334",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general framework for applying the Kelly criterion to stock\nmarkets. By supplying an arbitrary probability distribution modeling the future\nprice movement of a set of stocks, the Kelly fraction for investing each stock\ncan be calculated by inverting a matrix involving only first and second\nmoments. The framework works for one or a portfolio of stocks and the Kelly\nfractions can be efficiently calculated. For a simple model of geometric\nBrownian motion of a single stock we show that our calculated Kelly fraction\nagrees with existing results. We demonstrate that the Kelly fractions can be\ncalculated easily for other types of probabilities such as the Gaussian\ndistribution and correlated multivariate assets.\n"
    },
    {
        "paper_id": 1806.05387,
        "authors": "Karol Gellert and Erik Schl\\\"ogl",
        "title": "Parameter Learning and Change Detection Using a Particle Filter With\n  Accelerated Adaptation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the construction of a particle filter, which incorporates\nelements inspired by genetic algorithms, in order to achieve accelerated\nadaptation of the estimated posterior distribution to changes in model\nparameters. Specifically, the filter is designed for the situation where the\nsubsequent data in online sequential filtering does not match the model\nposterior filtered based on data up to a current point in time. The examples\nconsidered encompass parameter regime shifts and stochastic volatility. The\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\nfor distinguishing between regime shifts and stochastic volatility, even though\nthe model dynamics assumed by the filter exhibit neither of those features.\n"
    },
    {
        "paper_id": 1806.05401,
        "authors": "Masahiro Fujimoto",
        "title": "The Theoretical Price of a Share-Based Payment with Performance\n  Conditions and Implications for the Current Accounting Standards",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although the growth of share-based payments with performance conditions\n(hereafter, SPPC) is prominent today, the theoretical price of SPPC has not\nbeen sufficiently studied. Reflecting such a situation, the current accounting\nstandards for share-based payments issued in 2004 have had many problems. This\npaper develops a theoretical SPPC price model with a framework for a marginal\nutility-based price, which previous studies proposed is the price of contingent\nclaims in an incomplete market. This paper's contribution is fivefold. First,\nwe restricted the stochastic process to a certain class to demonstrate how to\nconsistently change all variables' probability distributions, which affect the\nSPPC payoff. Second, we explicitly indicated not only the stochastic processes\nof the stock price process and performance variables under the changed\nprobability, but also how the changes in the performance variables' drift\ncoefficients related to stock betas. Third, we proposed a convenient model in\napplication that uses only a few parameters. Fourth, we provided a method to\nestimate the parameters and improve the estimation of both the price and\nparameters. Fifth, we illustrated the problems in current accounting standards\nand indicated how the theoretical price model can significantly improve them.\n"
    },
    {
        "paper_id": 1806.05542,
        "authors": "Jan E. Snellman, Gerardo I\\~niguez, J\\'anos Kert\\'esz, R. A. Barrio\n  and Kimmo K. Kaski",
        "title": "Status maximization as a source of fairness in a networked dictator game",
        "comments": "Submitted to Journal of Complex Networks, 14 pages, 9 figures",
        "journal-ref": "J. Compl. Netw., cny022 (2018)",
        "doi": "10.1093/comnet/cny022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Human behavioural patterns exhibit selfish or competitive, as well as\nselfless or altruistic tendencies, both of which have demonstrable effects on\nhuman social and economic activity. In behavioural economics, such effects have\ntraditionally been illustrated experimentally via simple games like the\ndictator and ultimatum games. Experiments with these games suggest that, beyond\nrational economic thinking, human decision-making processes are influenced by\nsocial preferences, such as an inclination to fairness. In this study we\nsuggest that the apparent gap between competitive and altruistic human\ntendencies can be bridged by assuming that people are primarily maximising\ntheir status, i.e., a utility function different from simple profit\nmaximisation. To this end we analyse a simple agent-based model, where\nindividuals play the repeated dictator game in a social network they can\nmodify. As model parameters we consider the living costs and the rate at which\nagents forget infractions by others. We find that individual strategies used in\nthe game vary greatly, from selfish to selfless, and that both of the above\nparameters determine when individuals form complex and cohesive social\nnetworks.\n"
    },
    {
        "paper_id": 1806.05557,
        "authors": "Nicholas S. Gonchar",
        "title": "Martingales and Super-martingales Relative to a Convex Set of Equivalent\n  Measures",
        "comments": "29 pages. arXiv admin note: substantial text overlap with\n  arXiv:1611.09062",
        "journal-ref": "Advances in Pure Mathematics, Vol.8 No.4, April 2018, 428-462.\n  http://www.scirp.org/Journal/PaperInformation.aspx?PaperID=83938",
        "doi": "10.4236/apm.2018.84025",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the paper, the martingales and super-martingales relative to a convex set\nof equivalent measures are systematically studied. The notion of local regular\nsuper-martingale relative to a convex set of equivalent measures is introduced\nand the necessary and sufficient conditions of the local regularity of it in\nthe discrete case are founded. The description of all local regular\nsuper-martingales relative to a convex set of equivalent measures is presented.\nThe notion of the complete set of equivalent measures is introduced. We prove\nthat every bounded in some sense super-martingale relative to the complete set\nof equivalent measures is local regular. A new definition of the fair price of\ncontingent claim in an incomplete market is given and the formula for the fair\nprice of Standard Option of European type is found. The proved Theorems are the\ngeneralization of the famous Doob decomposition for super-martingale onto the\ncase of super-martingales relative to a convex set of equivalent measures.\n"
    },
    {
        "paper_id": 1806.05561,
        "authors": "Joseph D. O'Brien, Mark E. Burke, and Kevin Burke",
        "title": "A Generalized Framework for Simultaneous Long-Short Feedback Trading",
        "comments": "12 pages, 12 figures. To appear in IEEE Transactions on Automatic\n  Control",
        "journal-ref": null,
        "doi": "10.1109/TAC.2020.3011914",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a generalization of the Simultaneous Long-Short (SLS) trading\nstrategy described in recent control literature wherein we allow for different\nparameters across the short and long sides of the controller; we refer to this\nnew strategy as Generalized SLS (GSLS). Furthermore, we investigate the\nconditions under which positive gain can be assured within the GSLS setup for\nboth deterministic stock price evolution and geometric Brownian motion. In\ncontrast to existing literature in this area (which places little emphasis on\nthe practical application of SLS strategies), we suggest optimization\nprocedures for selecting the control parameters based on historical data, and\nwe extensively test these procedures across a large number of real stock price\ntrajectories (495 in total). We find that the implementation of such\noptimization procedures greatly improves the performance compared with fixing\ncontrol parameters, and, indeed, the GSLS strategy outperforms the simpler SLS\nstrategy in general.\n"
    },
    {
        "paper_id": 1806.05579,
        "authors": "Kathrin Glau, Mirco Mahlstedt, Christian P\\\"otz",
        "title": "A new approach for American option pricing: The Dynamic Chebyshev method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new method to price American options based on Chebyshev\ninterpolation. In each step of a dynamic programming time-stepping we\napproximate the value function with Chebyshev polynomials. The key advantage of\nthis approach is that it allows to shift the model-dependent computations into\nan offline phase prior to the time-stepping. In the offline part a family of\ngeneralised (conditional) moments is computed by an appropriate numerical\ntechnique such as a Monte Carlo, PDE or Fourier transform based method. Thanks\nto this methodological flexibility the approach applies to a large variety of\nmodels. Online, the backward induction is solved on a discrete Chebyshev grid,\nand no (conditional) expectations need to be computed. For each time step the\nmethod delivers a closed form approximation of the price function along with\nthe options' delta and gamma. Moreover, the same family of (conditional)\nmoments yield multiple outputs including the option prices for different\nstrikes, maturities and different payoff profiles. We provide a theoretical\nerror analysis and find conditions that imply explicit error bounds for a\nvariety of stock price models. Numerical experiments confirm the fast\nconvergence of prices and sensitivities. An empirical investigation of accuracy\nand runtime also shows an efficiency gain compared with the least-square\nMonte-Carlo method introduced by Longstaff and Schwartz (2001).\n"
    },
    {
        "paper_id": 1806.05849,
        "authors": "Xuefeng Gao and Yunhan Wang",
        "title": "Optimal Market Making in the Presence of Latency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies optimal market making for large-tick assets in the\npresence of latency. We consider a random walk model for the asset price, and\nformulate the market maker's optimization problem using Markov Decision\nProcesses (MDP). We characterize the value of an order and show that it plays\nthe role of one-period reward in the MDP model. Based on this characterization,\nwe provide explicit criteria for assessing the profitability of market making\nwhen there is latency. Under our model, we show that a market maker can earn a\npositive expected profit if there are sufficient uninformed market orders\nhitting the market maker's limit orders compared with the rate of price jumps,\nand the trading horizon is sufficiently long. In addition, our theoretical and\nnumerical results suggest that latency can be an additional source of risk and\nlatency impacts negatively the performance of market makers.\n"
    },
    {
        "paper_id": 1806.05876,
        "authors": "Carlos Pedro Gon\\c{c}alves",
        "title": "Financial Risk and Returns Prediction with Modular Networked Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.\n"
    },
    {
        "paper_id": 1806.06061,
        "authors": "Bilgi Yilmaz",
        "title": "Computation of option greeks under hybrid stochastic volatility models\n  via Malliavin calculus",
        "comments": "Published at https://doi.org/10.15559/18-VMSTA100 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 2,\n  145-165",
        "doi": "10.15559/18-VMSTA100",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study introduces computation of option sensitivities (Greeks) using the\nMalliavin calculus under the assumption that the underlying asset and interest\nrate both evolve from a stochastic volatility model and a stochastic interest\nrate model, respectively. Therefore, it integrates the recent developments in\nthe Malliavin calculus for the computation of Greeks: Delta, Vega, and Rho and\nit extends the method slightly. The main results show that Malliavin calculus\nallows a running Monte Carlo (MC) algorithm to present numerical\nimplementations and to illustrate its effectiveness. The main advantage of this\nmethod is that once the algorithms are constructed, they can be used for\nnumerous types of option, even if their payoff functions are not\ndifferentiable.\n"
    },
    {
        "paper_id": 1806.06105,
        "authors": "Moustapha Pemy",
        "title": "Explicit Solutions for Optimal Resource Extraction Problems under Regime\n  Switching L\\'evy Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of optimally extracting nonrenewable natural\nresources. Taking into account the fact that the market values of the main\nnatural resources i.e. oil, natural gas, copper,..., etc, fluctuate randomly\nfollowing global and seasonal macroeconomic parameters, the prices of natural\nresources are modeled using Markov switching L\\'evy processes. We formulate\nthis optimal extraction problem as an infinite-time horizon optimal control\nproblem. We derive closed-form solutions for the value function as well as the\noptimal extraction policy. Numerical examples are presented to illustrate these\nresults.\n"
    },
    {
        "paper_id": 1806.06148,
        "authors": "Jozef Barun\\'ik and Mat\\v{e}j Nevrla",
        "title": "Quantile Spectral Beta: A Tale of Tail Risks, Investment Horizons, and\n  Asset Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how two important sources of risk -- market tail risk\nand extreme market volatility risk -- are priced into the cross-section of\nasset returns across various investment horizons. To identify such risks, we\npropose a quantile spectral beta representation of risk based on the\ndecomposition of covariance between indicator functions that capture\nfluctuations over various frequencies. We study the asymptotic behavior of the\nproposed estimators of such risk. Empirically, we find that tail risk is a\nshort-term phenomenon, whereas extreme volatility risk is priced by investors\nin the long term when pricing a cross-section of individual stocks. In\naddition, we study popular industry, size and value, profit, investment or\nbook-to-market portfolios, as well as portfolios constructed from various asset\nclasses, portfolios sorted on cash flow duration and other strategies. These\nresults reveal that tail-dependent and horizon-specific risks are priced\nheterogeneously across datasets and are important sources of risk for\ninvestors.\n"
    },
    {
        "paper_id": 1806.06632,
        "authors": "Andrew Burnie",
        "title": "Exploring the Interconnectedness of Cryptocurrencies using Correlation\n  Networks",
        "comments": "Conference Paper presented at The Cryptocurrency Research Conference\n  2018, 24 May 2018, Anglia Ruskin University Lord Ashcroft International\n  Business School Centre for Financial Research, Cambridge, UK",
        "journal-ref": "Andrew Burnie, 2018. Exploring the Interconnectedness of\n  Cryptocurrencies using Correlation Networks. In Cryptocurrency Research\n  Conference 2018 (Anglia Ruskin University, 2018). Anglia Ruskin University,\n  Cambridge, UK",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Correlation networks were used to detect characteristics which, although\nfixed over time, have an important influence on the evolution of prices over\ntime. Potentially important features were identified using the websites and\nwhitepapers of cryptocurrencies with the largest userbases. These were assessed\nusing two datasets to enhance robustness: one with fourteen cryptocurrencies\nbeginning from 9 November 2017, and a subset with nine cryptocurrencies\nstarting 9 September 2016, both ending 6 March 2018. Separately analysing the\nsubset of cryptocurrencies raised the number of data points from 115 to 537,\nand improved robustness to changes in relationships over time. Excluding USD\nTether, the results showed a positive association between different\ncryptocurrencies that was statistically significant. Robust, strong positive\nassociations were observed for six cryptocurrencies where one was a fork of the\nother; Bitcoin / Bitcoin Cash was an exception. There was evidence for the\nexistence of a group of cryptocurrencies particularly associated with Cardano,\nand a separate group correlated with Ethereum. The data was not consistent with\na token's functionality or creation mechanism being the dominant determinants\nof the evolution of prices over time but did suggest that factors other than\nspeculation contributed to the price.\n"
    },
    {
        "paper_id": 1806.06657,
        "authors": "John G. Thistle",
        "title": "The Origin and the Resolution of Nonuniqueness in Linear Rational\n  Expectations",
        "comments": "41 pages, 1 figure. v2: 43 pages, 1 figure. Extended discussion,\n  minor corrections and revisions (arguments unchanged). v3: Added result on\n  uniqueness of solution with least-square forecast errors (other results\n  unchanged); used a standard New Keynesian example; added introductory example\n  (of Taylor's); generally reorganized, and edited prose",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The nonuniqueness of rational expectations is explained: in the stochastic,\ndiscrete-time, linear, constant-coefficients case, the associated free\nparameters are coefficients that determine the public's most immediate\nreactions to shocks. The requirement of model-consistency may leave these\nparameters completely free, yet when their values are appropriately specified,\na unique solution is determined. In a broad class of models, the requirement of\nleast-square forecast errors determines the parameter values, and therefore\ndefines a unique solution. This approach is independent of dynamical stability,\nand generally does not suppress model dynamics.\n  Application to a standard New Keynesian example shows that the traditional\nsolution suppresses precisely those dynamics that arise from rational\nexpectations. The uncovering of those dynamics reveals their incompatibility\nwith the new I-S equation and the expectational Phillips curve.\n"
    },
    {
        "paper_id": 1806.06883,
        "authors": "Aur\\'elien Alfonsi, David Krief and Peter Tankov",
        "title": "Long-time large deviations for the multi-asset Wishart stochastic\n  volatility model and option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a large deviations principle for the class of multidimensional\naffine stochastic volatility models considered in (Gourieroux, C. and Sufana,\nR., J. Bus. Econ. Stat., 28(3), 2010), where the volatility matrix is modelled\nby a Wishart process. This class extends the very popular Heston model to the\nmultivariate setting, thus allowing to model the joint behaviour of a basket of\nstocks or several interest rates. We then use the large deviation principle to\nobtain an asymptotic approximation for the implied volatility of basket options\nand to develop an asymptotically optimal importance sampling algorithm, to\nreduce the number of simulations when using Monte-Carlo methods to price\nderivatives.\n"
    },
    {
        "paper_id": 1806.06941,
        "authors": "Tiziano Squartini, Guido Caldarelli, Giulio Cimini, Andrea Gabrielli,\n  Diego Garlaschelli",
        "title": "Reconstruction methods for networks: the case of economic and financial\n  systems",
        "comments": "79 pages, 6 figures, 3 tables",
        "journal-ref": "Phys. Rep. 757, 1-47 (2018)",
        "doi": "10.1016/j.physrep.2018.06.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When studying social, economic and biological systems, one has often access\nto only limited information about the structure of the underlying networks. An\nexample of paramount importance is provided by financial systems: information\non the interconnections between financial institutions is privacy-protected,\ndramatically reducing the possibility of correctly estimating crucial systemic\nproperties such as the resilience to the propagation of shocks. The need to\ncompensate for the scarcity of data, while optimally employing the available\ninformation, has led to the birth of a research field known as network\nreconstruction. Since the latter has benefited from the contribution of\nresearchers working in disciplines as different as mathematics, physics and\neconomics, the results achieved so far are still scattered across heterogeneous\npublications. Most importantly, a systematic comparison of the network\nreconstruction methods proposed up to now is currently missing. This review\naims at providing a unifying framework to present all these studies, mainly\nfocusing on their application to economic and financial networks.\n"
    },
    {
        "paper_id": 1806.06947,
        "authors": "JongRoul Woo, Christopher L. Magee",
        "title": "Forecasting the value of battery electric vehicles compared to internal\n  combustion engine vehicles: the influence of driving range and battery\n  technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Battery electric vehicles (BEVs) are now clearly a promising candidate in\naddressing the environmental problems associated with conventional internal\ncombustion engine vehicles (ICEVs). However, BEVs, unlike ICEVs, are still not\nwidely accepted in the automobile market but continuing technological change\ncould overcome this barrier. The aim of this study is to assess and forecast\nwhether and when design changes and technological improvements related to major\nchallenges in driving range and battery cost will make the user value of BEVs\ngreater than the user value of ICEVs. Specifically, we estimate the relative\nuser value of BEVs and ICEVs resulting after design modifications to achieve\ndifferent driving ranges by considering the engineering trade-offs based on a\nvehicle simulation. Then, we analyze when the relative user value of BEVs is\nexpected to exceed ICEVs as the energy density and cost of batteries improve\nbecause of ongoing technological change. Our analysis demonstrates that the\nrelative value of BEVs is lower than that of ICEVs because BEVs have high\nbattery cost and high cost of time spent recharging despite high torque, high\nfuel efficiency, and low fuel cost. Moreover, we found the relative value\ndifferences between BEVs and ICEVs are found to be less in high performance\nlarge cars than in low performance compact cars because BEVs can achieve high\nacceleration performance more easily than ICEVs. In addition, this study\npredicts that in approximately 2050, high performance large BEVs could have\nhigher relative value than high performance large ICEVs because of\ntechnological improvements in batteries; however low performance compact BEVs\nare still very likely to have significantly lower user value than comparable\nICEVs until well beyond 2050.\n"
    },
    {
        "paper_id": 1806.07175,
        "authors": "Lijun Bo and Agostino Capponi",
        "title": "Portfolio Choice with Market-Credit Risk Dependencies",
        "comments": "38 pages, 12 figures, Forthcoming in SIAM Journal on Control and\n  Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal investment/consumption problem in a model capturing\nmarket and credit risk dependencies. Stochastic factors drive both the default\nintensity and the volatility of the stocks in the portfolio. We use the\nmartingale approach and analyze the recursive system of nonlinear\nHamilton-Jacobi-Bellman equations associated with the dual problem. We\ntransform such a system into an equivalent system of semi-linear PDEs, for\nwhich we establish existence and uniqueness of a bounded global classical\nsolution. We obtain explicit representations for the optimal strategy,\nconsumption path and wealth process, in terms of the solution to the recursive\nsystem of semi-linear PDEs. We numerically analyze the sensitivity of the\noptimal investment strategies to risk aversion, default risk and volatility.\n"
    },
    {
        "paper_id": 1806.07203,
        "authors": "Masahiko Hattori, Atsuhiro Satoh and Yasuhito Tanaka",
        "title": "Minimax theorem and Nash equilibrium of symmetric multi-players zero-sum\n  game with two strategic variables",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a symmetric multi-players zero-sum game with two strategic\nvariables. There are $n$ players, $n\\geq 3$. Each player is denoted by $i$. Two\nstrategic variables are $t_i$ and $s_i$, $i\\in \\{1, \\dots, n\\}$. They are\nrelated by invertible functions. Using the minimax theorem by \\cite{sion} we\nwill show that Nash equilibria in the following states are equivalent.\n  1. All players choose $t_i,\\ i\\in \\{1, \\dots, n\\}$, (as their strategic\nvariables). 2. Some players choose $t_i$'s and the other players choose\n$s_i$'s. 3. All players choose $s_i,\\ i\\in \\{1, \\dots, n\\}$.\n"
    },
    {
        "paper_id": 1806.07436,
        "authors": "Anthony D Stephens and David R Walwyn",
        "title": "Two Different Methods for Modelling the Likely Upper Economic Limit of\n  the Future United Kingdom Wind Fleet",
        "comments": "20 pages; 13 figures; 7 tables; 2 models",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Methods for predicting the likely upper economic limit for the wind fleet in\nthe United Kingdom should be simple to use whilst being able to cope with\nevolving technologies, costs and grid management strategies. This paper present\ntwo such models, both of which use data on historical wind patterns but apply\ndifferent approaches to estimating the extent of wind shedding as a function of\nthe size of the wind fleet. It is clear from the models that as the wind fleet\nincreases in size, wind shedding will progressively increase, and as a result\nthe overall economic efficiency of the wind fleet will be reduced. The models\nprovide almost identical predictions of the efficiency loss and suggest that\nthe future upper economic limit of the wind fleet will be mainly determined by\nthe wind fleet Headroom, a concept described in some detail in the paper. The\nresults, which should have general applicability, are presented in graphical\nform, and should obviate the need for further modelling using the primary data.\nThe paper also discusses the effectiveness of the wind fleet in decarbonising\nthe grid, and the growing competition between wind and solar fleets as sources\nof electrical energy for the United Kingdom.\n"
    },
    {
        "paper_id": 1806.07499,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Optimal Dividend Distribution Under Drawdown and Ratcheting Constraints\n  on Dividend Rates",
        "comments": "To appear in SIAM J. Financial Mathematics, 34 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal dividend problem under a habit formation constraint\nthat prevents the dividend rate to fall below a certain proportion of its\nhistorical maximum, the so-called drawdown constraint. This is an extension of\nthe optimal Duesenberry's ratcheting consumption problem, studied by Dybvig\n(1995) [Review of Economic Studies 62(2), 287-313], in which consumption is\nassumed to be nondecreasing. Our problem differs from Dybvig's also in that the\ntime of ruin could be finite in our setting, whereas ruin was impossible in\nDybvig's work. We formulate our problem as a stochastic control problem with\nthe objective of maximizing the expected discounted utility of the dividend\nstream until bankruptcy, in which risk preferences are embodied by power\nutility. We semi-explicitly solve the corresponding Hamilton-Jacobi-Bellman\nvariational inequality, which is a nonlinear free-boundary problem. The optimal\n(excess) dividend rate $c^*_t$ - as a function of the company's current surplus\n$X_t$ and its historical running maximum of the (excess) dividend rate $z_t$ -\nis as follows: There are constants $0 < w_{\\alpha} < w_0 < w^*$ such that (1)\nfor $0 < X_t \\le w_{\\alpha} z_t$, it is optimal to pay dividends at the lowest\nrate $\\alpha z_t$, (2) for $w_{\\alpha} z_t < X_t < w_0 z_t$, it is optimal to\ndistribute dividends at an intermediate rate $c^*_t \\in (\\alpha z_t, z_t)$, (3)\nfor $w_0 z_t < X_t < w^* z_t$, it is optimal to distribute dividends at the\nhistorical peak rate $z_t$, (4) for $X_t > w^* z_t$, it is optimal to increase\nthe dividend rate above $z_t$, and (5) it is optimal to increase $z_t$ via\nsingular control as needed to keep $X_t \\le w^* z_t$. Because, the maximum\n(excess) dividend rate will eventually be proportional to the running maximum\nof the surplus, \"mountains will have to move\" before we increase the dividend\nrate beyond its historical maximum.\n"
    },
    {
        "paper_id": 1806.07556,
        "authors": "Marco Neffelli, Marina Resta",
        "title": "Is VIX still the investor fear gauge? Evidence for the US and BRIC\n  markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the relationships of the VIX with US and BRIC markets. In\ndetail, we pick up the analysis from the point left off by (Sarwar, 2012), and\nwe focus on the period: Jan 2007 - Feb 2018, thus capturing the relations\nbefore, during and after the 2008 financial crisis. Results pinpoint frequent\nstructural breaks in the VIX and suggest an enhancement around 2008 of the fear\ntransmission in response to negative market moves; largely depending on\noverlaps in trading hours, this has become even stronger post-crisis for the\nUS, while for BRIC countries has gone back towards pre-crisis levels.\n"
    },
    {
        "paper_id": 1806.07604,
        "authors": "Xin-Lan Fu, Xing-Lu Gao, Zheng Shan, Zhi-Qiang Jiang, and Wei-Xing\n  Zhou (ECUST)",
        "title": "Multifractal characteristics and return predictability in the Chinese\n  stock markets",
        "comments": "2 figures, 5 tables, and 12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By adopting Multifractal detrended fluctuation (MF-DFA) analysis methods, the\nmultifractal nature is revealed in the high-frequency data of two typical\nindexes, the Shanghai Stock Exchange Composite 180 Index (SH180) and the\nShenzhen Stock Exchange Composite Index (SZCI). The characteristics of the\ncorresponding multifractal spectra are defined as a measurement of market\nvolatility. It is found that there is a statistically significant relationship\nbetween the stock index returns and the spectral characteristics, which can be\napplied to forecast the future market return. The in-sample and out-of-sample\ntests on the return predictability of multifractal characteristics indicate the\nspectral width $\\Delta {\\alpha}$ is a significant and positive excess return\npredictor. Our results shed new lights on the application of multifractal\nnature in asset pricing.\n"
    },
    {
        "paper_id": 1806.07623,
        "authors": "Jamal Bouoiyour (CATT), Refk Selmi (CATT), Mark Wohar",
        "title": "Measuring the response of gold prices to uncertainty: An analysis beyond\n  the mean",
        "comments": "Economic Modelling, Elsevier, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides an innovative perspective on the role of gold as a hedge\nand safe haven. We use a quantile-on-quantile regression approach to capture\nthe dependence structure between gold returns and changes in uncertainty under\ndifferent gold market conditions, while considering the nuances of uncertainty\nlevels. To capture the core uncertainty effects on gold returns, a dynamic\nfactor model is used. This technique allows summarizing the impact of six\ndifferent indexes (namely economic, macroeconomic, microeconomic, monetary\npolicy, financial and political uncertainties) within one aggregate measure of\nuncertainty. In doing so, we show that the gold's role as a hedge and safe\nhaven cannot be assumed to hold at all times. This ability seems to be\nsensitive to the gold's various market states (bearish, normal or bullish) and\nto whether the uncertainty is low, middle or high. Interestingly, we find a\npositive and strong relationship between gold returns and the uncertainty\ncomposite indicator when the uncertainty attains its highest level and under\nnormal gold market scenario. This suggests that holding a diversified portfolio\ncomposed of gold could help protecting against exposure to uncertain risks.\n"
    },
    {
        "paper_id": 1806.07626,
        "authors": "Takeru Matsuda, Akimichi Takemura",
        "title": "Game-theoretic derivation of upper hedging prices of multivariate\n  contingent claims and submodularity",
        "comments": null,
        "journal-ref": "Japan Journal of Industrial and Applied Mathematics, 37, 213--248,\n  2020",
        "doi": "10.1007/s13160-019-00394-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate upper and lower hedging prices of multivariate contingent\nclaims from the viewpoint of game-theoretic probability and submodularity. By\nconsidering a game between \"Market\" and \"Investor\" in discrete time, the\npricing problem is reduced to a backward induction of an optimization over\nsimplexes. For European options with payoff functions satisfying a\ncombinatorial property called submodularity or supermodularity, this\noptimization is solved in closed form by using the Lov\\'asz extension and the\nupper and lower hedging prices can be calculated efficiently. This class\nincludes the options on the maximum or the minimum of several assets. We also\nstudy the asymptotic behavior as the number of game rounds goes to infinity.\nThe upper and lower hedging prices of European options converge to the\nsolutions of the Black-Scholes-Barenblatt equations. For European options with\nsubmodular or supermodular payoff functions, the Black-Scholes-Barenblatt\nequation is reduced to the linear Black-Scholes equation and it is solved in\nclosed form. Numerical results show the validity of the theoretical results.\n"
    },
    {
        "paper_id": 1806.07667,
        "authors": "Ola Hammarlid, Marta Leniec",
        "title": "Credit Value Adjustment for Counterparties with Illiquid CDS",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Value Adjustment (CVA) is the difference between the value of the\ndefault-free and credit-risky derivative portfolio, which can be regarded as\nthe cost of the credit hedge. Default probabilities are therefore needed, as\ninput parameters to the valuation. When liquid CDS are available, then implied\nprobabilities of default can be derived and used. However, in small markets,\nlike the Nordic region of Europe, there are practically no CDS to use. We study\nthe following problem: given that no liquid contracts written on the default\nevent are available, choose a model for the default time and estimate the model\nparameters. We use the minimum variance hedge to show that we should use the\nreal-world probabilities, first in a discrete time setting and later in the\ncontinuous time setting. We also argue that this approach should fulfil the\nrequirements of IFRS 13, which means it could be used in accounting as well. We\nalso present a method that can be used to estimate the real-world probabilities\nof default, making maximal use of market information (IFRS requirement).\n"
    },
    {
        "paper_id": 1806.07791,
        "authors": "Luis Carlos Garc\\'ia del Molino, Iacopo Mastromatteo, Michael\n  Benzaquen and Jean-Philippe Bouchaud",
        "title": "The Multivariate Kyle model: More is different",
        "comments": "30 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reconsider the multivariate Kyle model in a risk-neutral setting with a\nsingle, perfectly informed rational insider and a rational competitive market\nmaker, setting the price of n correlated securities. We prove the unicity of a\nsymmetric, positive definite solution for the impact matrix and provide\ninsights on its interpretation. We explore its implications from the\nperspective of empirical market microstructure, and argue that it provides a\nsensible inference procedure to cure some pathologies encountered in recent\nattempts to calibrate cross-impact matrices. As an illustration, we determine\nthe empirical cross impact matrix of US. Treasuries, and compare the results\nwith recent alternative calibration methods.\n"
    },
    {
        "paper_id": 1806.07829,
        "authors": "Lin Chen, Ping Li, Qiang Li",
        "title": "The evolving networks of debtor-creditor relationships with addition and\n  deletion of nodes: a case of P2P lending",
        "comments": "14 pages,14 figures,5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  P2P lending activities have grown rapidly and have caused the huge and\ncomplex networks of debtor-creditor relationships. The aim of this study was to\nstudy the underlying structural characteristics of networks formed by\ndebtor-creditor relationships. According attributes of P2P lending, this paper\nmodel the networks of debtor-creditor relationships as an evolving networks\nwith addition and deletion of nodes. It was found that networks of\ndebtor-creditor relationships are scale-free networks. Moreover, the exponent\nof power-law was calculated by an empirical study. In addition, this paper\nstudy what factors impact on the exponent of power-law besides the number of\nnodes. It was found that the both interest rate and term have significantly\ninfluence on the exponent of power-law. Interest rate is negatively correlated\nwith the exponent of power-law and term is positively correlated with the\nexponent of power-law. Our results enriches the application of complex networks\n"
    },
    {
        "paper_id": 1806.0783,
        "authors": "Mario Coccia",
        "title": "National debts and government deficits within European Monetary Union:\n  Statistical evidence of economic issues",
        "comments": "36 pages, 4 tables +1 in Appendix, 6 figures + 3 in Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study analyzes public debts and deficits between European countries. The\nstatistical evidence here seems in general to reveal that sovereign debts and\ngovernment deficits of countries within European Monetary Unification-in\naverage- are getting worse than countries outside European Monetary\nUnification, in particular after the introduction of Euro currency. This\nsocioeconomic issue might be due to Maastricht Treaty, the Stability and Growth\nPact, the new Fiscal Compact, strict Balanced-Budget Rules, etc. In fact, this\neconomic policy of European Union, in phases of economic recession, may\ngenerate delay and rigidity in the application of prompt counter-cycle (or\nacyclical) interventions to stimulate the economy when it is in a downturn\nwithin countries. Some implications of economic policy are discussed.\n"
    },
    {
        "paper_id": 1806.07983,
        "authors": "Will Hicks",
        "title": "Nonlocal Diffusions and The Quantum Black-Scholes Equation: Modelling\n  the Market Fear Factor",
        "comments": "21 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we establish a link between quantum stochastic processes, and\nnonlocal diffusions. We demonstrate how the non-commutative Black-Scholes\nequation of Accardi & Boukas (Luigi Accardi, Andreas Boukas, 'The Quantum\nBlack-Scholes Equation', Jun 2007, available at arXiv:0706.1300v1) can be\nwritten in integral form. This enables the application of the Monte-Carlo\nmethods adapted to McKean stochastic differential equations (H. P. McKean, 'A\nclass of Markov processes associated with nonlinear parabolic equations', Proc.\nNatl. Acad. Sci. U.S.A., 56(6):1907-1911, 1966) for the simulation of\nsolutions. We show how unitary transformations can be applied to classical\nBlack-Scholes systems to introduce novel quantum effects. These have a simple\neconomic interpretation as a market `fear factor', whereby recent market\nturbulence causes an increase in volatility going forward, that is not linked\nto either the local volatility function or an additional stochastic variable.\nLastly, we extend this system to 2 variables, and consider Quantum models for\nbid-offer spread dynamics.\n"
    },
    {
        "paper_id": 1806.08005,
        "authors": "Taras Bodnar, Dmytro Ivasiuk, Nestor Parolya and Wofgang Schmid",
        "title": "Mean-Variance Efficiency of Optimal Power and Logarithmic Utility\n  Portfolios",
        "comments": "31 pages, 4 figures, UPDATE: the log-normal approximation is now\n  appropriately motivated analytically",
        "journal-ref": "Mathematics and Financial Economics 14, 675-698, 2020",
        "doi": "10.1007/s11579-020-00270-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive new results related to the portfolio choice problem for power and\nlogarithmic utilities. Assuming that the portfolio returns follow an\napproximate log-normal distribution, the closed-form expressions of the optimal\nportfolio weights are obtained for both utility functions. Moreover, we prove\nthat both optimal portfolios belong to the set of mean-variance feasible\nportfolios and establish necessary and sufficient conditions such that they are\nmean-variance efficient. Furthermore, an application to the stock market is\npresented and the behavior of the optimal portfolio is discussed for different\nvalues of the relative risk aversion coefficient. It turns out that the\nassumption of log-normality does not seem to be a strong restriction.\n"
    },
    {
        "paper_id": 1806.08107,
        "authors": "Erik Schl\\\"ogl",
        "title": "Arbitrage-Free Interpolation in Models of Market Observable Interest\n  Rates",
        "comments": null,
        "journal-ref": "Schl\\\"ogl, E. (2002), Arbitrage-Free Interpolation in Models of\n  Market Observable Interest Rates, in K. Sandmann and P. Sch\\\"onbucher (eds),\n  Advances in Finance and Stochastics, Springer-Verlag",
        "doi": "10.1007/978-3-662-04790-3_11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models which postulate lognormal dynamics for interest rates which are\ncompounded according to market conventions, such as forward LIBOR or forward\nswap rates, can be constructed initially in a discrete tenor framework.\nInterpolating interest rates between maturities in the discrete tenor structure\nis equivalent to extending the model to continuous tenor. The present paper\nsets forth an alternative way of performing this extension; one which preserves\nthe Markovian properties of the discrete tenor models and guarantees the\npositivity of all interpolated rates.\n"
    },
    {
        "paper_id": 1806.08161,
        "authors": "Angelos Dassios and Luting Li",
        "title": "Explicit Asymptotics on First Passage Times of Diffusion Processes",
        "comments": "31 pages, 16 figures",
        "journal-ref": "Adv. Appl. Probab. 52 (2020) 681-704",
        "doi": "10.1017/apr.2020.13",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a unified framework for solving first passage times of\ntime-homogeneous diffusion processes. According to the killed version potential\ntheory and the perturbation theory, we are able to deduce closed-form solutions\nfor probability densities of single-sided level crossing problem. The framework\nis applicable to diffusion processes with continuous drift functions, and a\nrecursive system in the frequency domain has been provided. Besides, we derive\na probabilistic representation for error estimation. The representation can be\nused to evaluate deviations in perturbed density functions. In the present\npaper, we apply the framework to Ornstein-Uhlenbeck and Bessel processes to\nfind closed-form approximations for their first passage times; another\nsuccessful application is given by the exponential-Shiryaev process. Numerical\nresults are provided at the end of this paper.\n"
    },
    {
        "paper_id": 1806.08386,
        "authors": "Chengyi Tu, Paolo DOdorico, Samir Suweis",
        "title": "Critical slowing down associated with critical transition and risk of\n  collapse in cryptocurrency",
        "comments": "14 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The year 2017 saw the rise and fall of the crypto-currency market, followed\nby high variability in the price of all crypto-currencies. In this work, we\nstudy the abrupt transition in crypto-currency residuals, which is associated\nwith the critical transition (the phenomenon of critical slowing down) or the\nstochastic transition phenomena. We find that, regardless of the specific\ncrypto-currency or rolling window size, the autocorrelation always fluctuates\naround a high value, while the standard deviation increases monotonically.\nTherefore, while the autocorrelation does not display signals of critical\nslowing down, the standard deviation can be used to anticipate critical or\nstochastic transitions. In particular, we have detected two sudden jumps in the\nstandard deviation, in the second quarter of 2017 and at the beginning of 2018,\nwhich could have served as early warning signals of two majors price collapses\nthat have happened in the following periods. We finally propose a mean-field\nphenomenological model for the price of crypto-currency to show how the use of\nthe standard deviation of the residuals is a better leading indicator of the\ncollapse in price than the time series' autocorrelation. Our findings represent\na first step towards a better diagnostic of the risk of critical transition in\nthe price and/or volume of crypto-currencies.\n"
    },
    {
        "paper_id": 1806.08444,
        "authors": "Yves-Laurent Kom Samo, Dieter Hendricks",
        "title": "What Makes An Asset Useful?",
        "comments": "To keep abreast of how this work is being used by Pit.AI Technologies\n  follow us on Medium (https://medium.com/pit-ai-technologies) and watch the\n  Devise GitHub repo (https://github.com/devisechain/Devise)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a new candidate asset represented as a time series of returns, how\nshould a quantitative investment manager be thinking about assessing its\nusefulness? This is a key qualitative question inherent to the investment\nprocess which we aim to make precise. We argue that the usefulness of an asset\ncan only be determined relative to a reference universe of assets and/or\nbenchmarks the investment manager already has access to or would like to\ndiversify away from, for instance, standard risk factors, common trading styles\nand other assets. We identify four features that the time series of returns of\nan asset should exhibit for the asset to be useful to an investment manager,\ntwo primary and two secondary. As primary criteria, we propose that the new\nasset should provide sufficient incremental diversification to the reference\nuniverse of assets/benchmarks, and its returns time series should be\nsufficiently predictable. As secondary criteria, we propose that the new asset\nshould mitigate tail risk, and the new asset should be suitable for passive\ninvestment (e.g. buy-and-hold or short-and-hold). We discuss how to quantify\nincremental diversification, returns predictability, impact on tail risk, and\nsuitability for passive investment, and for each criterion, we provide a\nscalable algorithmic test of usefulness.\n"
    },
    {
        "paper_id": 1806.08701,
        "authors": "Fei Sun, Yijun Hu",
        "title": "Quasiconvex risk measures with markets volatility",
        "comments": "arXiv admin note: text overlap with arXiv:1806.01166",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the quasiconvex risk measures is a bigger class than the well known\nconvex risk measures, the study of quasiconvex risk measures makes sense\nespecially in the financial markets with volatility. In this paper, we will\nstudy the quasiconvex risk measures defined on a special space $L^{p(\\cdot)}$\nwhere the variable exponent $p(\\cdot)$ is no longer a given real number like\nthe space $L^{p}$, but a random variable, which reflects the possible\nvolatility of the financial markets. The dual representation for this\nquasiconvex risk measures will also provided.\n"
    },
    {
        "paper_id": 1806.09198,
        "authors": "Hovik Tumasyan",
        "title": "A Second Look at Post Crisis Pricing of Derivatives - Part I: A Note on\n  Money Accounts and Collateral",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper reviews origins of the approach to pricing derivatives post-crisis\nby following three papers that have received wide acceptance from practitioners\nas the theoretical foundations for it - [Piterbarg 2010], [Burgard and Kjaer\n2010] and [Burgard and Kjaer 2013].\n  The review reveals several conceptual and technical inconsistencies with the\napproaches taken in these papers. In particular, a key component of the\napproach - prescription of cost components to a risk-free money account,\ngenerates derivative prices that are not cleared by the markets that trade the\nderivative and its underlying securities. It also introduces several risk-free\npositions (accounts) that accrue at persistently non-zero spreads with respect\nto each other and the risk free rate. In the case of derivatives with\ncounterparty default risk [Burgard and Kjaer 2013] introduces an approach\nreferred to as semi-replication, which through the choice of cost components in\nthe money account results in derivative prices that carry arbitrage\nopportunities in the form of holding portfolio of counterparty's bonds versus a\nderivative position with it.\n  This paper derives no-arbitrage expressions for default-risky derivative\ncontracts with and without collateral, avoiding these inconsistencies.\n"
    },
    {
        "paper_id": 1806.09216,
        "authors": "Jos\\'e-Luis P\\'erez, Kazutoshi Yamazaki, Alain Bensoussan",
        "title": "Optimal periodic replenishment policies for spectrally positive L\\'evy\n  demand processes",
        "comments": "27 pages, 3 figures. Forthcoming in SIAM Journal on Control and\n  Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a version of the stochastic inventory control problem for a\nspectrally positive L\\'evy demand process, in which the inventory can only be\nreplenished at independent exponential times. We show the optimality of a\nperiodic barrier replenishment policy that restocks any shortage below a\ncertain threshold at each replenishment opportunity. The optimal policies and\nvalue functions are concisely written in terms of the scale functions.\nNumerical results are also provided.\n"
    },
    {
        "paper_id": 1806.09302,
        "authors": "Yuecai Han, Qingshuo Song, Gu Wang",
        "title": "Exit problem as the generalized solution of Dirichlet problem",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates sufficient conditions for a Feynman-Kac functional up\nto an exit time to be the generalized viscosity solution of a Dirichlet\nproblem. The key ingredient is to find out the continuity of exit operator\nunder Skorokhod topology, which reveals the intrinsic connection between\noverfitting Dirichlet boundary and fine topology. As an application, we\nestablish the sub and supersolutions for a class of non-stationary HJB\n(Hamilton-Jacobi-Bellman) equations with fractional Laplacian operator via\nFeynman-Kac functionals associated to $\\alpha$-stable processes, which help\nverify the solvability of the original HJB equation.\n"
    },
    {
        "paper_id": 1806.09906,
        "authors": "Nandita Das, Bernadette Ruf, Swarn Chatterjee, and Aman Sunder",
        "title": "Fund Characteristics and Performances of Socially Responsible Mutual\n  Funds: Do ESG Ratings Play a Role?",
        "comments": "Forthcoming in the Journal of Accounting and Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the risk-adjusted performance and differential fund flows\nfor socially responsible mutual funds (SRMF). The results show that SRMF rated\nhigh on ESG, perform better than lower rated ESG funds during the period of\neconomic crisis. The findings also show that low ESG rated SRMF had higher\ndifferential cash-flows than high rated ESG funds except for the period of\neconomic down turn. The findings are of interest to financial advisors,\ninvestors, mutual fund managers, and researchers on how SRMF performance\nresponds to periods of economic downturn and expansion\n"
    },
    {
        "paper_id": 1806.10452,
        "authors": "Nicholas I Fisher and Raymond E Kordupleski",
        "title": "What is Wrong with Net Promoter Score",
        "comments": "20 pages, 5 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Net-Promoter Score (NPS) is now ubiquitous as an easily-collected market\nresearch metric, having displaced many serious market research processes.\nUnfortunately, this has been its sole success. It possesses few, if any, of the\ncharacteristics that might be regarded as highly desirable in a high-level\nmarket research metric; on the contrary, it has done considerable damage both\nto companies and to their customers.\n"
    },
    {
        "paper_id": 1806.10935,
        "authors": "Marcel Ausloos (University of Leicester), Roy Cerqueti (University of\n  Macerata), Tariq A. Mir (Bhabha Atomic Research Center, Srinagar)",
        "title": "Data on the annual aggregated income taxes of the Italian municipalities\n  over the quinquennium 2007-2011",
        "comments": "4 pages, 9 references",
        "journal-ref": "Data in Brief 18 (2018) 156-159",
        "doi": "10.1016/j.dib.2018.02.082",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This dataset contains the annual aggregated income taxes of all the Italian\nmunicipalities over the years 2007-2011. Data are clustered over the Italian\nregions and provinces. The source of the data is the Italian Ministry of\nEconomics and Finance. The administrative variations in Italy over the\nquinquennium have been taken into account. Data are useful to understand the\neconomic structure of Italy at the microscopic level of municipalities. They\ncan serve also for making comparisons between economical aspects and other\nfeatures of the Italian cities.\n"
    },
    {
        "paper_id": 1806.10981,
        "authors": "Gabriela Kov\\'a\\v{c}ov\\'a, Birgit Rudloff",
        "title": "Time consistency of the mean-risk problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Choosing a portfolio of risky assets over time that maximizes the expected\nreturn at the same time as it minimizes portfolio risk is a classical problem\nin Mathematical Finance and is referred to as the dynamic Markowitz problem\n(when the risk is measured by variance) or more generally, the dynamic\nmean-risk problem. In most of the literature, the mean-risk problem is\nscalarized and it is well known that this scalarized problem does not satisfy\nthe (scalar) Bellman's principle. Thus, the classical dynamic programming\nmethods are not applicable. For the purpose of this paper we focus on the\ndiscrete time setup, and we will use a time consistent dynamic convex risk\nmeasure to evaluate the risk of a portfolio. We will show that when we do not\nscalarize the problem, but leave it in its original form as a vector\noptimization problem, the upper images, whose boundary contains the efficient\nfrontier, recurse backwards in time under very mild assumptions. Thus, the\ndynamic mean-risk problem does satisfy a Bellman's principle, but a more\ngeneral one, that seems more appropriate for a vector optimization problem: a\nset-valued Bellman's principle. We will present conditions under which this\nrecursion can be exploited directly to compute a solution in the spirit of\ndynamic programming. Numerical examples illustrate the proposed method. The\nobtained results open the door for a new branch in mathematics: dynamic\nmultivariate programming.\n"
    },
    {
        "paper_id": 1806.1129,
        "authors": "Lioudmila Vostrikova (LAREMA), J\\'er\\^ome Spielmann (LAREMA)",
        "title": "On The Ruin Problem With Investment When The Risky Asset Is A\n  Semimartingale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the ruin problem with investment in a general\nframework where the business part X is a L{\\'e}vy process and the return on\ninvestment R is a semimartingale. We obtain upper bounds on the finite and\ninfinite time ruin probabilities that decrease as a power function when the\ninitial capital increases. When R is a L{\\'e}vy process, we retrieve the\nwell-known results. Then, we show that these bounds are asymptotically optimal\nin the finite time case, under some simple conditions on the characteristics of\nX. Finally, we obtain a condition for ruin with probability one when X is a\nBrownian motion with negative drift and express it explicitly using the\ncharacteristics of R.\n"
    },
    {
        "paper_id": 1806.11348,
        "authors": "Obryan Poyser",
        "title": "Herding behavior in cryptocurrency markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are no solid arguments to sustain that digital currencies are the\nfuture of online payments or the disruptive technology that some of its former\nparticipants declared when used to face critiques. This paper aims to solve the\ncryptocurrency puzzle from a behavioral finance perspective by finding the\nparallelism between biases present in financial markets that could be applied\nto cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are\ndriven by herding, hence this study test herding behavior under asymmetric and\nsymmetric conditions and the existence of different herding regimes by\nemploying the Markov-Switching approach.\n"
    },
    {
        "paper_id": 1807.00568,
        "authors": "J\\\"orn Sass, Dorothee Westphal, Ralf Wunderlich",
        "title": "Diffusion Approximations for Expert Opinions in a Financial Market with\n  Gaussian Drift",
        "comments": "Update with changes in notation, 38 pages",
        "journal-ref": "Journal of Applied Probability 58 (1), 2021, pp. 197-216",
        "doi": "10.1017/jpr.2020.82",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a financial market where returns depend on an\nunobservable Gaussian drift process. While the observation of returns yields\ninformation about the underlying drift, we also incorporate discrete-time\nexpert opinions as an external source of information.\n  For estimating the hidden drift it is crucial to consider the conditional\ndistribution of the drift given the available observations, the so-called\nfilter. For an investor observing both the return process and the discrete-time\nexpert opinions, we investigate in detail the asymptotic behavior of the filter\nas the frequency of the arrival of expert opinions tends to infinity. In our\nsetting, a higher frequency of expert opinions comes at the cost of accuracy,\nmeaning that as the frequency of expert opinions increases, the variance of\nexpert opinions becomes larger. We consider a model where information dates are\ndeterministic and equidistant and another model where the information dates\narrive randomly as the jump times of a Poisson process. In both cases we derive\nlimit theorems stating that the information obtained from observing the\ndiscrete-time expert opinions is asymptotically the same as that from observing\na certain diffusion process which can be interpreted as a continuous-time\nexpert.\n  We use our limit theorems to derive so-called diffusion approximations of the\nfilter for high-frequency discrete-time expert opinions. These diffusion\napproximations are extremely helpful for deriving simplified approximate\nsolutions of utility maximization problems.\n"
    },
    {
        "paper_id": 1807.00573,
        "authors": "Damien Challet",
        "title": "Strategic behaviour and indicative price diffusion in Paris Stock\n  Exchange auctions",
        "comments": "11 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report statistical regularities of the opening and closing auctions of\nFrench equities, focusing on the diffusive properties of the indicative auction\nprice. Two mechanisms are at play as the auction end time nears: the typical\nprice change magnitude decreases, favoring underdiffusion, while the rate of\nthese events increases, potentially leading to overdiffusion. A third\nmechanism, caused by the strategic behavior of traders, is needed to produce\nnearly diffusive prices: waiting to submit buy orders until sell orders have\ndecreased the indicative price and vice-versa.\n"
    },
    {
        "paper_id": 1807.00939,
        "authors": "Sheikh Rabiul Islam, Sheikh Khaled Ghafoor, William Eberle",
        "title": "Mining Illegal Insider Trading of Stocks: A Proactive Approach",
        "comments": "Accepted in IEEE BigData 2018",
        "journal-ref": "2018 IEEE International Conference on Big Data (Big Data)",
        "doi": "10.1109/BigData.2018.8622303",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Illegal insider trading of stocks is based on releasing non-public\ninformation (e.g., new product launch, quarterly financial report, acquisition\nor merger plan) before the information is made public. Detecting illegal\ninsider trading is difficult due to the complex, nonlinear, and non-stationary\nnature of the stock market. In this work, we present an approach that detects\nand predicts illegal insider trading proactively from large heterogeneous\nsources of structured and unstructured data using a deep-learning based\napproach combined with discrete signal processing on the time series data. In\naddition, we use a tree-based approach that visualizes events and actions to\naid analysts in their understanding of large amounts of unstructured data.\nUsing existing data, we have discovered that our approach has a good success\nrate in detecting illegal insider trading patterns.\n"
    },
    {
        "paper_id": 1807.01186,
        "authors": "Wing Fung Chong, Gechun Liang",
        "title": "Optimal investment and consumption with forward preferences and\n  uncertain parameters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies robust forward investment and consumption preferences\nwithin a zero-volatility context. Different from previous works, we consider an\nincomplete financial market model due to general investment portfolio\nconstraints. We provide a new PDE characterization and a novel semi-explicit\nsaddle-point construction of forward preferences and optimal strategies. We\nfurther present a more detailed construction of forward preferences and optimal\nstrategies under constant relative risk aversion (CRRA). Key findings emphasize\nthe necessity of a specific relationship between the initial investment\npreference and the forward consumption preference, indicating a long-term\ndecreasing trend in forward consumption preference behavior.\n"
    },
    {
        "paper_id": 1807.01428,
        "authors": "Alvaro Cartea, Luhui Gan, Sebastian Jaimungal",
        "title": "Trading Cointegrated Assets with Price Impact",
        "comments": "32 pages,4 figures, 11 tables. First appeared on SSRN Oct 2015 at\n  http://ssrn.com/abstract=2681309",
        "journal-ref": "Mathematical Finance, Forthcoming 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Executing a basket of co-integrated assets is an important task facing\ninvestors. Here, we show how to do this accounting for the informational\nadvantage gained from assets within and outside the basket, as well as for the\npermanent price impact of market orders (MOs) from all market participants, and\nthe temporary impact that the agent's MOs have on prices. The execution problem\nis posed as an optimal stochastic control problem and we demonstrate that,\nunder some mild conditions, the value function admits a closed-form solution,\nand prove a verification theorem. Furthermore, we use data of five stocks\ntraded in the Nasdaq exchange to estimate the model parameters and use\nsimulations to illustrate the performance of the strategy. As an example, the\nagent liquidates a portfolio consisting of shares in Intel Corporation (INTC)\nand Market Vectors Semiconductor ETF (SMH). We show that including the\ninformation provided by three additional assets, FARO Technologies (FARO),\nNetApp (NTAP) and Oracle Corporation (ORCL), considerably improves the\nstrategy's performance; for the portfolio we execute, it outperforms the\nmulti-asset version of Almgren-Chriss by approximately 4 to 4.5 basis points.\n"
    },
    {
        "paper_id": 1807.01756,
        "authors": "Lasko Basnarkov, Viktor Stojkoski, Zoran Utkovski and Ljupco Kocarev",
        "title": "Option Pricing with Heavy-Tailed Distributions of Logarithmic Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing body of literature suggests that heavy tailed distributions\nrepresent an adequate model for the observations of log returns of stocks.\nMotivated by these findings, here we develop a discrete time framework for\npricing of European options. Probability density functions of log returns for\ndifferent periods are conveniently taken to be convolutions of the Student's\nt-distribution with three degrees of freedom. The supports of these\ndistributions are truncated in order to obtain finite values for the options.\nWithin this framework, options with different strikes and maturities for one\nstock rely on a single parameter -- the standard deviation of the Student's\nt-distribution for unit period. We provide a study which shows that the\ndistribution support width has weak influence on the option prices for certain\nrange of values of the width. It is furthermore shown that such family of\ntruncated distributions approximately satisfies the no-arbitrage principle and\nthe put-call parity. The relevance of the pricing procedure is empirically\nverified by obtaining remarkably good match of the numerically computed values\nby our scheme to real market data.\n"
    },
    {
        "paper_id": 1807.01785,
        "authors": "Ken Seng Tan, Wei Wei, Xun Yu Zhou",
        "title": "Failure of Smooth Pasting Principle and Nonexistence of Equilibrium\n  Stopping Rules under Time-Inconsistency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a time-inconsistent stopping problem in which the\ninconsistency arises from non-constant time preference rates. We show that the\nsmooth pasting principle, the main approach that has been used to construct\nexplicit solutions for conventional time-consistent optimal stopping problems,\nmay fail under time-inconsistency. Specifically, we prove that the smooth\npasting principle solves a time-inconsistent problem within the intra-personal\ngame theoretic framework if and only if a certain inequality on the model\nprimitives is satisfied. We show that the violation of this inequality can\nhappen even for very simple non-exponential discount functions. Moreover, we\ndemonstrate that the stopping problem does not admit any intra-personal\nequilibrium whenever the smooth pasting principle fails. The \"negative\" results\nin this paper caution blindly extending the classical approaches for\ntime-consistent stopping problems to their time-inconsistent counterparts.\n"
    },
    {
        "paper_id": 1807.01816,
        "authors": "Ying Hu, Gechun Liang, Shanjian Tang",
        "title": "Systems of ergodic BSDEs arising in regime switching forward performance\n  processes",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and solve a new type of quadratic backward stochastic\ndifferential equation systems defined in an infinite time horizon, called\n\\emph{ergodic BSDE systems}. Such systems arise naturally as candidate\nsolutions to characterize forward performance processes and their associated\noptimal trading strategies in a regime switching market. In addition, we\ndevelop a connection between the solution of the ergodic BSDE system and the\nlong-term growth rate of classical utility maximization problems, and use the\nergodic BSDE system to study the large time behavior of PDE systems with\nquadratic growth Hamiltonians.\n"
    },
    {
        "paper_id": 1807.01934,
        "authors": "Jaros{\\l}aw Klamut, Tomasz Gubiec",
        "title": "Directed Continuous-Time Random Walk with memory",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1140/epjb/e2019-90453-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new Directed Continuous-Time Random Walk (CTRW) model with\nmemory. As CTRW trajectory consists of spatial jumps preceded by waiting times,\nin Directed CTRW, we consider the case with only positive spatial jumps.\nMoreover, we consider the memory in the model as each spatial jump depends on\nthe previous one. Our model is motivated by the financial application of the\nCTRW presented in [Phys. Rev. E 82:046119][Eur. Phys. J. B 90:50]. As CTRW can\nsuccessfully describe the short term negative autocorrelation of returns in\nhigh-frequency financial data (caused by the bid-ask bounce phenomena), we\nasked ourselves to what extent the observed long-term autocorrelation of\nabsolute values of returns can be explained by the same phenomena. It turned\nout that the bid-ask bounce can be responsible only for the small fraction of\nthe memory observed in the high-frequency financial data.\n"
    },
    {
        "paper_id": 1807.01977,
        "authors": "Marcelo Brutti Righi",
        "title": "A theory for combinations of risk measures",
        "comments": null,
        "journal-ref": "Journal of Risk 25, 25-60 (2023)",
        "doi": "10.21314/JOR.2022.054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study combinations of risk measures under no restrictive assumption on the\nset of alternatives. We develop and discuss results regarding the preservation\nof properties and acceptance sets for the combinations of risk measures. One of\nthe main results is the representation of resulting risk measures from the\nproperties of both alternative functionals and combination functions. We build\non developing a dual representation for an arbitrary mixture of convex risk\nmeasures. In this case, we obtain a penalty that recalls the notion of\ninf-convolution under theoretical measure integration. We develop results\nrelated to this specific context. We also explore features of individual\ninterest generated by our frameworks, such as the preservation of continuity\nproperties and the representation of worst-case risk measures.\n"
    },
    {
        "paper_id": 1807.01979,
        "authors": "Marco Piccirilli and Tiziano Vargiolu",
        "title": "Optimal Portfolio in Intraday Electricity Markets Modelled by\n  L\\'evy-Ornstein-Uhlenbeck Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal portfolio problem designed for an agent operating in\nintraday electricity markets. The investor is allowed to trade in a single\nrisky asset modelling the continuously traded power and aims to maximize the\nexpected terminal utility of his wealth. We assume a mean-reverting additive\nprocess to drive the power prices. In the case of logarithmic utility, we\nreduce the fully non-linear Hamilton-Jacobi-Bellman equation to a linear\nparabolic integro-differential equation, for which we explicitly exhibit a\nclassical solution in two cases of modelling interest. The optimal strategy is\ngiven implicitly as the solution of an integral equation, which is possible to\nsolve numerically as well as to describe analytically. An analysis of two\ndifferent approximations for the optimal policy is provided. Finally, we\nperform a numerical test by adapting the parameters of a popular electricity\nspot price model.\n"
    },
    {
        "paper_id": 1807.02015,
        "authors": "Sergey Nadtochiy, Mykhaylo Shkolnikov",
        "title": "Mean field systems on networks, with singular interaction through\n  hitting times",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Building on the line of work [DIRT15a], [DIRT15b], [NS17a], [DT17], [HLS18],\n[HS18] we continue the study of particle systems with singular interaction\nthrough hitting times. In contrast to the previous research, we (i) consider\nvery general driving processes and interaction functions, (ii) allow for\ninhomogeneous connection structures, and (iii) analyze a game in which the\nparticles determine their connections strategically. Hereby, we uncover two\ncompletely new phenomena. First, we characterize the \"times of fragility\" of\nsuch systems (e.g., the times when a macroscopic part of the population\ndefaults or gets infected simultaneously, or when the neuron cells\n\"synchronize\") explicitly in terms of the dynamics of the driving processes,\nthe current distribution of the particles' values, and the topology of the\nunderlying network (represented by its Perron-Frobenius eigenvalue). Second, we\nuse such systems to describe a dynamic credit-network game and show that, in\nequilibrium, the system regularizes: i.e., the times of fragility never occur,\nas the particles avoid them by adjusting their connections strategically. Two\nauxiliary mathematical results, useful in their own right, are uncovered during\nour investigation: a generalization of Schauder's fixed-point theorem for the\nSkorokhod space with the M1 topology, and the application of the max-plus\nalgebra to the equilibrium version of the network flow problem.\n"
    },
    {
        "paper_id": 1807.02227,
        "authors": "David A. Goldberg and Yilun Chen",
        "title": "Polynomial time algorithm for optimal stopping with fixed accuracy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of high-dimensional path-dependent optimal stopping (OS) is\nimportant to multiple academic communities and applications. Modern OS tasks\noften have a large number of decision epochs, and complicated non-Markovian\ndynamics, making them especially challenging. Standard approaches, often\nrelying on ADP, duality, deep learning and other heuristics, have shown strong\nempirical performance, yet have limited rigorous guarantees (which may scale\nexponentially in the problem parameters and/or require previous knowledge of\nbasis functions or additional continuity assumptions). Although past work has\nplaced these problems in the framework of computational complexity and\npolynomial-time approximability, those analyses were limited to simple\none-dimensional problems. For long-horizon complex OS problems, is a polynomial\ntime solution even theoretically possible? We prove that given access to an\nefficient simulator of the underlying information process, and fixed accuracy\nepsilon, there exists an algorithm that returns an epsilon-optimal solution\n(both stopping policies and approximate optimal values) with computational\ncomplexity scaling polynomially in the time horizon and underlying dimension.\nLike the first polynomial-time (approximation) algorithms for several other\nwell-studied problems, our theoretical guarantees are polynomial yet\nimpractical. Our approach is based on a novel expansion for the optimal value\nwhich may be of independent interest.\n"
    },
    {
        "paper_id": 1807.02243,
        "authors": "Jian Sun",
        "title": "Generalization of Doob's Inequality and A Tighter Estimate on Look-back\n  Option Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short note, we will strengthen the classic Doob's $L^p$ inequality\nfor sub-martingale processes. Because this inequality is of fundamental\nimportance to the theory of stochastic process, we believe this generalization\nwill find many interesting applications.\n"
    },
    {
        "paper_id": 1807.02422,
        "authors": "Chao Wang, Richard Gerlach, Qian Chen",
        "title": "A Semi-parametric Realized Joint Value-at-Risk and Expected Shortfall\n  Regression Framework",
        "comments": "45 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1805.08653, arXiv:1612.08488, arXiv:1707.03715",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A new realized conditional autoregressive Value-at-Risk (VaR) framework is\nproposed, through incorporating a measurement equation into the original\nquantile regression model. The framework is further extended by employing\nvarious Expected Shortfall (ES) components, to jointly estimate and forecast\nVaR and ES. The measurement equation models the contemporaneous dependence\nbetween the realized measure (i.e., Realized Variance and Realized Range) and\nthe latent conditional ES. An adaptive Bayesian Markov Chain Monte Carlo method\nis employed for estimation and forecasting, the properties of which are\nassessed and compared with maximum likelihood through a simulation study. In a\ncomprehensive forecasting study on 1% and 2.5 % quantile levels, the proposed\nmodels are compared to a range of parametric, non-parametric and\nsemi-parametric models, based on 7 market indices and 7 individual assets.\nOne-day-ahead VaR and ES forecasting results favor the proposed models,\nespecially when incorporating the sub-sampled Realized Variance and the\nsub-sampled Realized Range in the model.\n"
    },
    {
        "paper_id": 1807.02711,
        "authors": "Zachary Feinstein",
        "title": "Capital Regulation under Price Impacts and Dynamic Financial Contagion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a continuous time model for price-mediated contagion\nprecipitated by a common exogenous stress to the banking book of all firms in\nthe financial system. In this setting, firms are constrained so as to satisfy a\nrisk-weight based capital ratio requirement. We use this model to find\nanalytical bounds on the risk-weights for assets as a function of the market\nliquidity. Under these appropriate risk-weights, we find existence and\nuniqueness for the joint system of firm behavior and the asset prices. We\nfurther consider an analytical bound on the firm liquidations, which allows us\nto construct exact formulas for stress testing the financial system with\ndeterministic or random stresses. Numerical case studies are provided to\ndemonstrate various implications of this model and analytical bounds.\n"
    },
    {
        "paper_id": 1807.02787,
        "authors": "Chien Yi Huang",
        "title": "Financial Trading as a Game: A Deep Reinforcement Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An automatic program that generates constant profit from the financial market\nis lucrative for every market practitioner. Recent advance in deep\nreinforcement learning provides a framework toward end-to-end training of such\ntrading agent. In this paper, we propose an Markov Decision Process (MDP) model\nsuitable for the financial trading task and solve it with the state-of-the-art\ndeep recurrent Q-network (DRQN) algorithm. We propose several modifications to\nthe existing learning algorithm to make it more suitable under the financial\ntrading setting, namely 1. We employ a substantially small replay memory (only\na few hundreds in size) compared to ones used in modern deep reinforcement\nlearning algorithms (often millions in size.) 2. We develop an action\naugmentation technique to mitigate the need for random exploration by providing\nextra feedback signals for all actions to the agent. This enables us to use\ngreedy policy over the course of learning and shows strong empirical\nperformance compared to more commonly used epsilon-greedy exploration. However,\nthis technique is specific to financial trading under a few market assumptions.\n3. We sample a longer sequence for recurrent neural network training. A side\nproduct of this mechanism is that we can now train the agent for every T steps.\nThis greatly reduces training time since the overall computation is down by a\nfactor of T. We combine all of the above into a complete online learning\nalgorithm and validate our approach on the spot foreign exchange market.\n"
    },
    {
        "paper_id": 1807.02923,
        "authors": "Chandrashekar Kuyyamudi, Anindya S. Chakrabarti and Sitabhra Sinha",
        "title": "Emergence of frustration signals systemic risk",
        "comments": "6 pages, 4 figures",
        "journal-ref": "Phys. Rev. E 99, 052306 (2019)",
        "doi": "10.1103/PhysRevE.99.052306",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the emergence of systemic risk in complex systems can be\nunderstood from the evolution of functional networks representing interactions\ninferred from fluctuation correlations between macroscopic observables.\nSpecifically, we analyze the long-term collective dynamics of the New York\nStock Exchange between 1926-2016, showing that periods marked by systemic\ncrisis, viz., around the Great Depression of 1929-33 and the Great Recession of\n2007-09, are associated with emergence of frustration indicated by the loss of\nstructural balance in the interaction networks. During these periods the\ndominant eigenmodes characterizing the collective behavior exhibit\ndelocalization leading to increased coherence in the dynamics. The topological\nstructure of the networks exhibits a slowly evolving trend marked by the\nemergence of a prominent core-periphery organization around both of the crisis\nperiods.\n"
    },
    {
        "paper_id": 1807.03192,
        "authors": "Sid Ghoshal, Stephen J. Roberts",
        "title": "Thresholded ConvNet Ensembles: Neural Networks for Technical Forecasting",
        "comments": "9 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Much of modern practice in financial forecasting relies on technicals, an\numbrella term for several heuristics applying visual pattern recognition to\nprice charts. Despite its ubiquity in financial media, the reliability of its\nsignals remains a contentious and highly subjective form of 'domain knowledge'.\nWe investigate the predictive value of patterns in financial time series,\napplying machine learning and signal processing techniques to 22 years of US\nequity data. By reframing technical analysis as a poorly specified, arbitrarily\npreset feature-extractive layer in a deep neural network, we show that better\nconvolutional filters can be learned directly from the data, and provide visual\nrepresentations of the features being identified. We find that an ensemble of\nshallow, thresholded CNNs optimised over different resolutions achieves\nstate-of-the-art performance on this domain, outperforming technical methods\nwhile retaining some of their interpretability.\n"
    },
    {
        "paper_id": 1807.03229,
        "authors": "Christa Cuchiero, Martin Larsson, Sara Svaluto-Ferro",
        "title": "Probability measure-valued polynomial diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of probability measure-valued diffusions, coined\npolynomial, of which the well-known Fleming--Viot process is a particular\nexample. The defining property of finite dimensional polynomial processes\nconsidered by Cuchiero et al. (2012) and Filipovic and Larsson (2016) is\ntransferred to this infinite dimensional setting. This leads to a\nrepresentation of conditional marginal moments via a finite dimensional linear\nPDE, whose spatial dimension corresponds to the degree of the moment. As a\nresult, the tractability of finite dimensional polynomial processes are\npreserved in this setting. We also obtain a representation of the corresponding\nextended generators, and prove well-posedness of the associated martingale\nproblems. In particular, uniqueness is obtained from the duality relationship\nwith the PDEs mentioned above.\n"
    },
    {
        "paper_id": 1807.03364,
        "authors": "Anthony J. Webster",
        "title": "A global consumer-led strategy to tackle climate change",
        "comments": "One table included at end of text",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A successful response to climate change needs vast investments in low-carbon\nresearch, energy, and sustainable development. Governments can drive research,\nprovide environmental regulation, and accelerate global development, but the\nnecessary low-carbon investments of 2-3% GDP have yet to materialise. A new\nstrategy to tackle climate change through consumer and government action is\noutlined. It relies on ethical investments for sustainable development and\nlow-carbon energy, and a voluntarily financed low-carbon fund for adaptation to\nclimate change. Together these enable a global response through individual\nactions and investments. With OECD savings exceeding 5% of disposable household\nincome, ethical savings alone have considerable potential.\n"
    },
    {
        "paper_id": 1807.03813,
        "authors": "Xiangge Luo and Alexander Schied",
        "title": "Nash equilibrium for risk-averse investors in a market impact game with\n  transient price impact",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S238262662050001X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market impact game for $n$ risk-averse agents that are\ncompeting in a market model with linear transient price impact and additional\ntransaction costs. For both finite and infinite time horizons, the agents aim\nto minimize a mean-variance functional of their costs or to maximize the\nexpected exponential utility of their revenues. We give explicit\nrepresentations for corresponding Nash equilibria and prove uniqueness in the\ncase of mean-variance optimization. A qualitative analysis of these Nash\nequilibria is conducted by means of numerical analysis.\n"
    },
    {
        "paper_id": 1807.03882,
        "authors": "Samuel N. Cohen and Martin Tegn\\'er",
        "title": "European Option Pricing with Stochastic Volatility models under\n  Parameter Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider stochastic volatility models under parameter uncertainty and\ninvestigate how model derived prices of European options are affected. We let\nthe pricing parameters evolve dynamically in time within a specified region,\nand formalise the problem as a control problem where the control acts on the\nparameters to maximise/minimise the option value. Through a dual representation\nwith backward stochastic differential equations, we obtain explicit equations\nfor Heston's model and investigate several numerical solutions thereof. In an\nempirical study, we apply our results to market data from the S&P 500 index\nwhere the model is estimated to historical asset prices. We find that the\nconservative model-prices cover 98% of the considered market-prices for a set\nof European call options.\n"
    },
    {
        "paper_id": 1807.03893,
        "authors": "Liangchen Li, Michael Ludkovski",
        "title": "Stochastic Switching Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study nonzero-sum stochastic switching games. Two players compete for\nmarket dominance through controlling (via timing options) the discrete-state\nmarket regime $M$. Switching decisions are driven by a continuous stochastic\nfactor $X$ that modulates instantaneous revenue rates and switching costs. This\ngenerates a competitive feedback between the short-term fluctuations due to $X$\nand the medium-term advantages based on $M$. We construct threshold-type\nFeedback Nash Equilibria which characterize stationary strategies describing\nlong-run dynamic equilibrium market organization. Two sequential approximation\nschemes link the switching equilibrium to (i) constrained optimal switching,\n(ii) multi-stage timing games. We provide illustrations using an\nOrnstein-Uhlenbeck $X$ that leads to a recurrent equilibrium $M^\\ast$ and a\nGeometric Brownian Motion $X$ that makes $M^\\ast$ eventually \"absorbed\" as one\nplayer eventually gains permanent advantage. Explicit computations and\ncomparative statics regarding the emergent macroscopic market equilibrium are\nalso provided.\n"
    },
    {
        "paper_id": 1807.04211,
        "authors": "Jan Obloj, Johannes Wiesel",
        "title": "Robust estimation of superhedging prices",
        "comments": "This work will appear in the Annals of Statistics. The above version\n  merges the main paper to appear in print and its online supplement",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider statistical estimation of superhedging prices using historical\nstock returns in a frictionless market with d traded assets. We introduce a\nplugin estimator based on empirical measures and show it is consistent but\nlacks suitable robustness. To address this we propose novel estimators which\nuse a larger set of martingale measures defined through a tradeoff between the\nradius of Wasserstein balls around the empirical measure and the allowed norm\nof martingale densities. We establish consistency and robustness of these\nestimators and argue that they offer a superior performance relative to the\nplugin estimator. We generalise the results by replacing the superhedging\ncriterion with acceptance relative to a risk measure. We further extend our\nstudy, in part, to the case of markets with traded options, to a multiperiod\nsetting and to settings with model uncertainty. We also study convergence rates\nof estimators and convergence of superhedging strategies.\n"
    },
    {
        "paper_id": 1807.04393,
        "authors": "Milan Kumar Das and Anindya Goswami",
        "title": "Testing of Binary Regime Switching Models using Squeeze Duration\n  Analysis",
        "comments": "12 pages, 7 figures, 2 tables",
        "journal-ref": "Int. J. Financ. Eng. 6 (2019), no. 1, 1950006 (20 pages)",
        "doi": "10.1142/S2424786319500063",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed a statistical technique to test the model assumption of\nbinary regime switching extension of the geometric Brownian motion (GBM) model\nby proposing a new discriminating statistics. Given a time series data, we have\nidentified an admissible class of the regime switching candidate models for the\nstatistical inference. By performing several systematic experiments, we have\nsuccessfully shown that the sampling distribution of the test statistics\ndiffers drastically, if the model assumption changes from GBM to Markov\nmodulated GBM, or to semi-Markov modulated GBM. Furthermore, we have\nimplemented this statistics for testing the regime switching hypothesis with\nIndian sectoral indices.\n"
    },
    {
        "paper_id": 1807.04612,
        "authors": "Julien Baptiste, Laurence Carassus and Emmanuel L\\'epinette",
        "title": "Pricing without martingale measure",
        "comments": "33 pages 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For several decades, the no-arbitrage (NA) condition and the martingale\nmeasures have played a major role in the financial asset's pricing theory. We\npropose a new approach for estimating the super-replication cost based on\nconvex duality instead of martingale measures duality: Our prices will be\nexpressed using Fenchel conjugate and bi-conjugate. The super-hedging problem\nleads endogenously to a weak condition of NA called Absence of Immediate Profit\n(AIP). We propose several characterizations of AIP and study the relation with\nthe classical notions of no-arbitrage. We also give some promising numerical\nillustrations.\n"
    },
    {
        "paper_id": 1807.05015,
        "authors": "S. Valeyre, D. S. Grebenkov, and S. Aboura",
        "title": "Emergence of correlations between securities at short time scales",
        "comments": null,
        "journal-ref": "Physica A 526, 121026 (2019)",
        "doi": "10.1016/j.physa.2019.04.262",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The correlation matrix is the key element in optimal portfolio allocation and\nrisk management. In particular, the eigenvectors of the correlation matrix\ncorresponding to large eigenvalues can be used to identify the market mode,\nsectors and style factors. We investigate how these eigenvalues depend on the\ntime scale of securities returns in the U.S. market. For this purpose,\none-minute returns of the largest 533 U.S. stocks are aggregated at different\ntime scales and used to estimate the correlation matrix and its spectral\nproperties. We propose a simple lead-lag factor model to capture and reproduce\nthe observed time-scale dependence of eigenvalues. We reveal the emergence of\nseveral dominant eigenvalues as the time scale increases. This important\nfinding evidences that the underlying economic and financial mechanisms\ndetermining the correlation structure of securities depend as well on time\nscales.\n"
    },
    {
        "paper_id": 1807.05126,
        "authors": "Sean Ledger and Andreas Sojmark",
        "title": "At the Mercy of the Common Noise: Blow-ups in a Conditional\n  McKean--Vlasov Problem",
        "comments": "47 pages, 1 simulation, 2 figures, updated proof of Lemma 3.13",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend a model of positive feedback and contagion in large mean-field\nsystems, by introducing a common source of noise driven by Brownian motion.\nAlthough the driving dynamics are continuous, the positive feedback effect can\nlead to `blow-up' phenomena whereby solutions develop jump-discontinuities. Our\nmain results are twofold and concern the conditional McKean--Vlasov formulation\nof the model. First and foremost, we show that there are global solutions to\nthis McKean--Vlasov problem, which can be realised as limit points of a\nmotivating particle system with common noise. Furthermore, we derive results on\nthe occurrence of blow-ups, thereby showing how these events can be triggered\nor prevented by the pathwise realisations of the common noise.\n"
    },
    {
        "paper_id": 1807.05265,
        "authors": "Chung-Han Hsieh, John A. Gubner, B. Ross Barmish",
        "title": "Rebalancing Frequency Considerations for Kelly-Optimal Stock Portfolios\n  in a Control-Theoretic Framework",
        "comments": "To appear in the Proceedings of the IEEE Conference on Decision and\n  Control, Miami Beach, FL, 2018",
        "journal-ref": null,
        "doi": "10.1109/CDC.2018.8619189",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, motivated by the celebrated work of Kelly, we consider the\nproblem of portfolio weight selection to maximize expected logarithmic growth.\nGoing beyond existing literature, our focal point here is the rebalancing\nfrequency which we include as an additional parameter in our analysis. The\nproblem is first set in a control-theoretic framework, and then, the main\nquestion we address is as follows: In the absence of transaction costs, does\nhigh-frequency trading always lead to the best performance? Related to this is\nour prior work on betting, also in the Kelly context, which examines the impact\nof making a wager and letting it ride. Our results on betting frequency can be\ninterpreted in the context of weight selection for a two-asset portfolio\nconsisting of one risky asset and one riskless asset. With regard to the\nquestion above, our prior results indicate that it is often the case that there\nare no performance benefits associated with high-frequency trading. In the\npresent paper, we generalize the analysis to portfolios with multiple risky\nassets. We show that if there is an asset satisfying a new condition which we\ncall dominance, then an optimal portfolio consists of this asset alone; i.e.,\nthe trader has \"all eggs in one basket\" and performance becomes a constant\nfunction of rebalancing frequency. Said another way, the problem of rebalancing\nis rendered moot. The paper also includes simulations which address practical\nconsiderations associated with real stock prices and the dominant asset\ncondition.\n"
    },
    {
        "paper_id": 1807.0536,
        "authors": "Shinji Kakinaka and Ken Umeno",
        "title": "Characterizing Cryptocurrency market with Levy's stable distributions",
        "comments": null,
        "journal-ref": "J. Phys. Soc. Jpn. 89, 024802 (2020)",
        "doi": "10.7566/JPSJ.89.024802",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent emergence of cryptocurrencies such as Bitcoin and Ethereum has\nposed possible alternatives to global payments as well as financial assets\naround the globe, making investors and financial regulators aware of the\nimportance of modeling them correctly. The Levy's stable distribution is one of\nthe attractive distributions that well describes the fat tails and scaling\nphenomena in economic systems. In this paper, we show that the behaviors of\nprice fluctuations in emerging cryptocurrency markets can be characterized by a\nnon-Gaussian Levy's stable distribution with $\\alpha \\simeq 1.4$ under certain\nconditions on time intervals ranging roughly from 30 minutes to 4 hours. Our\narguments are developed under quantitative valuation defined as a distance\nfunction using the Parseval's relation in addition to the theoretical\nbackground of the General Central Limit Theorem (GCLT). We also discuss the fit\nwith the Levy's stable distribution compared to the fit with other\ndistributions by employing the method based on likelihood ratios. Our approach\ncan be extended for further analysis of statistical properties and contribute\nto developing proper applications for financial modeling.\n"
    },
    {
        "paper_id": 1807.05396,
        "authors": "Elisa Al\\`os and Michael Coulon",
        "title": "On the optimal choice of strike conventions in exchange option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important but rarely-addressed option pricing question is how to choose\nappropriate strikes for implied volatility inputs when pricing more exotic\nmulti-asset derivatives. By means of Malliavin Calculus we construct an optimal\nlog-linear strikevconvention for exchange options under stochastic volatility\nmodels. This novel approach allows us to minimize the difference between the\ncorresponding Margrabe computed price and the true option price. We show that\nthis optimal convention does not depend on the specific stochastic volatility\nmodel chosen. Numerical examples are given which provide strong support to the\nnew methodology.\n"
    },
    {
        "paper_id": 1807.05448,
        "authors": "Tianyang Nie and Edward Kim and Marek Rutkowski",
        "title": "Arbitrage-Free Pricing of Game Options in Nonlinear Markets",
        "comments": "arXiv admin note: text overlap with arXiv:1804.10753",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal is to re-examine and extend the findings from the recent paper by\nDumitrescu, Quenez and Sulem (2017) who studied game options within the\nnonlinear arbitrage-free pricing approach developed in El Karoui and Quenez\n(1997). We consider the setup introduced in Kim, Nie and Rutkowski (2018) where\ncontracts of an American style were examined. We give a detailed study of\nunilateral pricing, hedging and exercising problems for the counterparties\nwithin a general nonlinear setup. We also present a BSDE approach, which is\nused to obtain more explicit results under suitable assumptions about solutions\nto doubly reflected BSDEs.\n"
    },
    {
        "paper_id": 1807.05513,
        "authors": "Lijun Bo, Huafu Liao, Yongjin Wang",
        "title": "Optimal Credit Investment and Risk Control for an Insurer with\n  Regime-Switching",
        "comments": "30 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investment and risk control problem for an\ninsurer with default contagion and regime-switching. The insurer in our model\nallocates his/her wealth across multi-name defaultable stocks and a riskless\nbond under regime-switching risk. Default events have an impact on the distress\nstate of the surviving stocks in the portfolio. The aim of the insurer is to\nmaximize the expected utility of the terminal wealth by selecting optimal\ninvestment and risk control strategies. We characterize the optimal trading\nstrategy of defaultable stocks and risk control for the insurer. By developing\na truncation technique, we analyze the existence and uniqueness of global\n(classical) solutions to the recursive HJB system. We prove the verification\ntheorem based on the (classical) solutions of the recursive HJB system.\n"
    },
    {
        "paper_id": 1807.05692,
        "authors": "Lesiba Ch. Galane, Rafa{\\l} M. {\\L}ochowski and Farai J. Mhlanga",
        "title": "On SDEs with Lipschitz coefficients, driven by continuous, model-free\n  martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We prove the existence and uniqueness of solutions of SDEs with Lipschitz\ncoefficients, driven by continuous, model-free martingales. The main tool in\nour reasoning is Picard's iterative procedure and a model-free version of the\nBurkholder-Davis-Gundy inequality for integrals driven by model-free,\ncontinuous martingales. We work with a new outer measure which assigns zero\nvalue exactly to those properties which are instantly blockable.\n"
    },
    {
        "paper_id": 1807.05737,
        "authors": "Kota Ogasawara",
        "title": "Consumption smoothing in the working-class households of interwar Japan",
        "comments": null,
        "journal-ref": "The Journal of Economic History 2024, Volume 84, Issue 1, 111-148",
        "doi": "10.1017/S0022050724000019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I analyze factory worker households in the early 1920s in Osaka to examine\nidiosyncratic income shocks and consumption. Using the household-level monthly\npanel dataset, I find that while households could not fully cope with\nidiosyncratic income shocks at that time, they mitigated fluctuations in\nindispensable consumption during economic hardship. In terms of risk-coping\nmechanisms, I find suggestive evidence that savings institutions helped\nmitigate vulnerabilities and that both using borrowing institutions and\nadjusting labor supply served as risk-coping strategies among households with\nless savings.\n"
    },
    {
        "paper_id": 1807.05773,
        "authors": "Kerem Ugurlu",
        "title": "Portfolio Optimization with Nondominated Priors and Unbounded Parameters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider classical Merton problem of terminal wealth maximization in\nfinite horizon. We assume that the drift of the stock is following\nOrnstein-Uhlenbeck process and the volatility of it is following GARCH(1)\nprocess. In particular, both mean and volatility are unbounded. We assume that\nthere is Knightian uncertainty on the parameters of both mean and volatility.\nWe take that the investor has logarithmic utility function, and solve the\ncorresponding utility maximization problem explicitly. To the best of our\nknowledge, this is the first work on utility maximization with unbounded mean\nand volatility in Knightian uncertainty under nondominated priors.\n"
    },
    {
        "paper_id": 1807.05836,
        "authors": "Pier Francesco Procacci and Tomaso Aste",
        "title": "Forecasting market states",
        "comments": "13 pages, 5 figures",
        "journal-ref": "Quantitative Finance 19 (2019) 1491-1498",
        "doi": "10.1080/14697688.2019.1622313",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel methodology to define, analyze and forecast market states.\nIn our approach market states are identified by a reference sparse precision\nmatrix and a vector of expectation values. In our procedure, each multivariate\nobservation is associated with a given market state accordingly to a\nminimization of a penalized Mahalanobis distance. The procedure is made\ncomputationally very efficient and can be used with a large number of assets.\nWe demonstrate that this procedure is successful at clustering different states\nof the markets in an unsupervised manner. In particular, we describe an\nexperiment with one hundred log-returns and two states in which the methodology\nautomatically associates states prevalently to pre- and post- crisis periods\nwith one state gathering periods with average positive returns and the other\nstate periods with average negative returns, therefore discovering\nspontaneously the common classification of `bull' and `bear' markets. In\nanother experiment, with again one hundred log-returns and two states, we\ndemonstrate that this procedure can be efficiently used to forecast off-sample\nfuture market states with significant prediction accuracy. This methodology\nopens the way to a range of applications in risk management and trading\nstrategies in the context where the correlation structure plays a central role.\n"
    },
    {
        "paper_id": 1807.05837,
        "authors": "Vladimir Soloviev, Andrey Belinskiy",
        "title": "Methods of nonlinear dynamics and the construction of cryptocurrency\n  crisis phenomena precursors",
        "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article demonstrates the possibility of constructing indicators of\ncritical and crisis phenomena in the volatile market of cryptocurrency. For\nthis purpose, the methods of the theory of complex systems such as recurrent\nanalysis of dynamic systems and the calculation of permutation entropy are\nused. It is shown that it is possible to construct dynamic measures of\ncomplexity, both recurrent and entropy, which behave in a proper way during\nactual pre-crisis periods. This fact is used to build predictors of crisis\nphenomena on the example of the main five crises recorded in the time series of\nthe key cryptocurrency bitcoin, the effectiveness of the proposed\nindicators-precursors of crises has been identified.\n"
    },
    {
        "paper_id": 1807.05917,
        "authors": "Dirk Becherer and Todor Bilarev",
        "title": "Hedging with physical or cash settlement under transient multiplicative\n  price impact",
        "comments": "to appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve the superhedging problem for European options in an illiquid\nextension of the Black-Scholes model, in which transactions have transient\nprice impact and the costs and the strategies for hedging are affected by\nphysical or cash settlement requirements at maturity. Our analysis is based on\na convenient choice of reduced effective coordinates of magnitudes at\nliquidation for geometric dynamic programming. The price impact is transient\nover time and multiplicative, ensuring non-negativity of underlying asset\nprices while maintaining an arbitrage-free model. The basic (log-)linear\nexample is a Black-Scholes model with relative price impact being proportional\nto the volume of shares traded, where the transience for impact on log-prices\nis being modelled like in Obizhaeva-Wang \\cite{ObizhaevaWang13} for nominal\nprices. More generally, we allow for non-linear price impact and resilience\nfunctions. The viscosity solutions describing the minimal superhedging price\nare governed by the transient character of the price impact and by the physical\nor cash settlement specifications. Pricing equations under illiquidity extend\nno-arbitrage pricing a la Black-Scholes for complete markets in a\nnon-paradoxical way (cf.\\ {\\c{C}}etin, Soner and Touzi\n\\cite{CetinSonerTouzi10}) even without additional frictions, and can recover it\nin base cases.\n"
    },
    {
        "paper_id": 1807.06449,
        "authors": "Tahir Choulli and Sina Yansori",
        "title": "Log-optimal portfolio without NFLVR: existence, complete\n  characterization, and duality",
        "comments": "arXiv admin note: text overlap with arXiv:1803.10128",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the log-optimal portfolio for a general semimartingale\nmodel. The most advanced literature on the topic elaborates existence and\ncharacterization of this portfolio under no-free-lunch-with-vanishing-risk\nassumption (NFLVR). There are many financial models violating NFLVR, while\nadmitting the log-optimal portfolio on the one hand. On the other hand, for\nfinancial markets under progressively enlargement of filtration, NFLVR remains\ncompletely an open issue, and hence the literature can be applied to these\nmodels. Herein, we provide a complete characterization of log-optimal portfolio\nand its associated optimal deflator, necessary and sufficient conditions for\ntheir existence, and we elaborate their duality as well without NFLVR.\n"
    },
    {
        "paper_id": 1807.06546,
        "authors": "Gayatri Pradhan",
        "title": "Analysis of Advisor Portfolio using Multivariate Time Series and Cosine\n  Similarity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In mutual fund, an investment adviser gives advice to clients about investing\nin securities such as stocks, bonds, mutual funds, or exchange traded funds.\nSome investment advisers manage portfolios of securities. In this paper, we\nanalyze advisor portfolio for each advisor so as to recognize the pattern in\neach adviser's portfolio. Such analysis helps the sales people to sell the fund\ncompany products to the suitable advisors desirable to the nature of the\nproduct they want to sell. This is done by analyzing the kind of products\nadvisors have been interested in which will help to boost the sales of the\nproducts as sales people will be reaching the appropriate advisors.\n"
    },
    {
        "paper_id": 1807.06622,
        "authors": "Haojie Wang, Han Chen, Agus Sudjianto, Richard Liu, Qi Shen",
        "title": "Deep Learning-Based BSDE Solver for Libor Market Model with Application\n  to Bermudan Swaption Pricing and Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Libor market model is a mainstay term structure model of interest rates\nfor derivatives pricing, especially for Bermudan swaptions, and other exotic\nLibor callable derivatives. For numerical implementation the pricing of\nderivatives with Libor market models is mainly carried out with Monte Carlo\nsimulation. The PDE grid approach is not particularly feasible due to Curse of\nDimensionality. The standard Monte Carlo method for American/Bermudan swaption\npricing more or less uses regression to estimate expected value as a linear\ncombination of basis functions (Longstaff and Schwartz). However, Monte Carlo\nmethod only provides the lower bound for American option price. Another\ncomplexity is the computation of the sensitivities of the option, the so-called\nGreeks, which are fundamental for a trader's hedging activity. Recently, an\nalternative numerical method based on deep learning and backward stochastic\ndifferential equations appeared in quite a few researches. For European style\noptions the feedforward deep neural networks (DNN) show not only feasibility\nbut also efficiency to obtain both prices and numerical Greeks. In this paper,\na new backward DNN solver is proposed for Bermudan swaptions. Our approach is\nrepresenting financial pricing problems in the form of high dimensional\nstochastic optimal control problems, FBSDEs, or equivalent PDEs. We demonstrate\nthat using backward DNN the high-dimension Bermudan swaption pricing and\nhedging can be solved effectively and efficiently. A comparison between Monte\nCarlo simulation and the new method for pricing vanilla interest rate options\nmanifests the superior performance of the new method. We then use this method\nto calculate prices and Greeks of Bermudan swaptions as a prelude for other\nLibor callable derivatives.\n"
    },
    {
        "paper_id": 1807.06824,
        "authors": "Stefan Feuerriegel and Helmut Prendinger",
        "title": "News-based trading strategies",
        "comments": null,
        "journal-ref": "Feuerriegel, Stefan, and Helmut Prendinger. \"News-based trading\n  strategies.\" Decision Support Systems 90 (2016): 65-74",
        "doi": "10.1016/j.dss.2016.06.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The marvel of markets lies in the fact that dispersed information is\ninstantaneously processed and used to adjust the price of goods, services and\nassets. Financial markets are particularly efficient when it comes to\nprocessing information; such information is typically embedded in textual news\nthat is then interpreted by investors. Quite recently, researchers have started\nto automatically determine news sentiment in order to explain stock price\nmovements. Interestingly, this so-called news sentiment works fairly well in\nexplaining stock returns. In this paper, we design trading strategies that\nutilize textual news in order to obtain profits on the basis of novel\ninformation entering the market. We thus propose approaches for automated\ndecision-making based on supervised and reinforcement learning. Altogether, we\ndemonstrate how news-based data can be incorporated into an investment system.\n"
    },
    {
        "paper_id": 1807.06892,
        "authors": "Yuxia Huang, Chuancun Yin",
        "title": "A unifying approach to constrained and unconstrained optimal reinsurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study two classes of optimal reinsurance models from\nperspectives of both insurers and reinsurers by minimizing their convex\ncombination where the risk is measured by a distortion risk measure and the\npremium is given by a distortion premium principle. Firstly, we show that how\noptimal reinsurance models for the unconstrained optimization problem and\nconstrained optimization problems can be formulated in a unified way. Secondly,\nwe propose a geometric approach to solve optimal reinsurance problems directly.\nThis paper considers a class of increasing convex ceded loss functions and\nderives the explicit solutions of the optimal reinsurance which can be in forms\nof quota-share, stop-loss, change-loss, the combination of quota-share and\nchange-loss or the combination of change-loss and change-loss with different\nretentions. Finally, we consider two specific cases: Value at Risk (VaR) and\nTail Value at Risk (TVaR).\n"
    },
    {
        "paper_id": 1807.07036,
        "authors": "Marcello Rambaldi, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy",
        "title": "Disentangling and quantifying market participant volatility\n  contributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thanks to the access to labeled orders on the Cac40 index future provided by\nEuronext, we are able to quantify market participants contributions to the\nvolatility in the diffusive limit. To achieve this result we leverage the\nbranching properties of Hawkes point processes. We find that fast\nintermediaries (e.g., market maker type agents) have a smaller footprint on the\nvolatility than slower, directional agents. The branching structure of Hawkes\nprocesses allows us to examine also the degree of endogeneity of each agent\nbehavior. We find that high-frequency traders are more endogenously driven than\nother types of agents.\n"
    },
    {
        "paper_id": 1807.07328,
        "authors": "Abdolrahman Khoshrou and Eric J. Pauwels",
        "title": "Quantifying Volatility Reduction in German Day-ahead Spot Market in the\n  Period 2006 through 2016",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/PESGM.2018.8586020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Europe, Germany is taking the lead in the switch from the conventional to\nrenewable energy. This poses new challenges as wind and solar energy are\nfundamentally intermittent, weather-dependent and less predictable. It is\ntherefore of considerable interest to investigate the evolution of price\nvolatility in this post-transition era. There are a number of reasons, however,\nthat makes the practical studies difficult. For instance, EPEX prices can be\nzero or negative. Consequently, the standard approach in financial time series\nanalysis to switch to logarithmic measures is inapplicable. Furthermore, in\ncontrast to the stock market prices which are only available for trading days,\nEPEX prices cover the whole year, including weekends and holidays. Accordingly,\nthere is a lot of underlying variability in the data which has nothing to do\nwith volatility, but simply reflects diurnal activity patterns. An important\ndistinction of the present work is the application of matrix decomposition\ntechniques, namely the singular value decomposition (SVD), for defining an\nalternative notion of volatility. This approach is systematically more robust\ntoward outliers and also the diurnal patterns. Our observations show that the\nday-ahead market is becoming less volatile in recent years.\n"
    },
    {
        "paper_id": 1807.08081,
        "authors": "Linlin Tian, Xiaoyi Zhang",
        "title": "Optimal Dividend of Compound Poisson Process under a Stochastic Interest\n  Rate",
        "comments": "16 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we assume the insurance wealth process is driven by the\ncompound Poisson process. The discounting factor is modelled as a geometric\nBrownian motion at first and then as an exponential function of an integrated\nOrnstein-Uhlenbeck process. The objective is to maximize the cumulated value of\nexpected discounted dividends up to the time of ruin. We give an explicit\nexpression of the value function and the optimal strategy in the case of\ninterest rate following a geometric Brownian motion. For the case of the\nVasicek model, we explore some properties of the value function. Since we can\nnot find an explicit expression for the value function in the second case, we\nprove that the value function is the viscosity solution of the corresponding\nHJB equation.\n"
    },
    {
        "paper_id": 1807.08222,
        "authors": "Andrew Papanicolaou",
        "title": "Backward SDEs for Control with Partial Information",
        "comments": "Part of this research was performed while the author was visiting the\n  Institute for Pure and Applied Mathematics (IPAM), which is supported by the\n  National Science Foundation, Mathematical Finance (2018)",
        "journal-ref": null,
        "doi": "10.1111/mafi.12174",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a non-Markov control problem arising in a financial\nmarket where asset returns depend on hidden factors. The problem is non-Markov\nbecause nonlinear filtering is required to make inference on these factors, and\nhence the associated dynamic program effectively takes the filtering\ndistribution as one of its state variables. This is of significant difficulty\nbecause the filtering distribution is a stochastic probability measure of\ninfinite dimension, and therefore the dynamic program has a state that cannot\nbe differentiated in the traditional sense. This lack of differentiability\nmeans that the problem cannot be solved using a Hamilton-Jacobi-Bellman (HJB)\nequation. This paper will show how the problem can be analyzed and solved using\nbackward stochastic differential equations (BSDEs), with a key tool being the\nproblem's dual formulation.\n"
    },
    {
        "paper_id": 1807.08278,
        "authors": "Peter Bank, Ibrahim Ekren, Johannes Muhle-Karbe",
        "title": "Liquidity in Competitive Dealer Markets",
        "comments": "29 pages, 3 figures, forthcoming in 'Mathematical Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous-time version of the intermediation model of Grossman\nand Miller (1988). To wit, we solve for the competitive equilibrium prices at\nwhich liquidity takers' demands are absorbed by dealers with quadratic\ninventory costs, who can in turn gradually transfer these positions to an\nexogenous open market with finite liquidity. This endogenously leads to\ntransient price impact in the dealer market. Smooth, diffusive, and discrete\ntrades all incur finite but nontrivial liquidity costs, and can arise naturally\nfrom the liquidity takers' optimization.\n"
    },
    {
        "paper_id": 1807.0839,
        "authors": "Bal\\'azs Csan\\'ad Cs\\'aji",
        "title": "Score Permutation Based Finite Sample Inference for Generalized\n  AutoRegressive Conditional Heteroskedasticity (GARCH) Models",
        "comments": "19th International Conference on Artificial Intelligence and\n  Statistics (AISTATS)",
        "journal-ref": "Proceedings of Machine Learning Research, Volume 51, 2016, pp.\n  296-304",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A standard model of (conditional) heteroscedasticity, i.e., the phenomenon\nthat the variance of a process changes over time, is the Generalized\nAutoRegressive Conditional Heteroskedasticity (GARCH) model, which is\nespecially important for economics and finance. GARCH models are typically\nestimated by the Quasi-Maximum Likelihood (QML) method, which works under mild\nstatistical assumptions. Here, we suggest a finite sample approach, called\nScoPe, to construct distribution-free confidence regions around the QML\nestimate, which have exact coverage probabilities, despite no additional\nassumptions about moments are made. ScoPe is inspired by the recently developed\nSign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH\ncase. ScoPe works by perturbing the score function using randomly permuted\nresiduals. This produces alternative samples which lead to exact confidence\nregions. Experiments on simulated and stock market data are also presented, and\nScoPe is compared with the asymptotic theory and bootstrap approaches.\n"
    },
    {
        "paper_id": 1807.08404,
        "authors": "John Stachurski and Alexis Akira Toda",
        "title": "An Impossibility Theorem for Wealth in Heterogeneous-agent Models with\n  Limited Heterogeneity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jet.2019.04.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been conjectured that canonical Bewley--Huggett--Aiyagari\nheterogeneous-agent models cannot explain the joint distribution of income and\nwealth. The results stated below verify this conjecture and clarify its\nimplications under very general conditions. We show in particular that if (i)\nagents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have\nconstant discount factors, then the wealth distribution inherits the tail\nbehavior of income shocks (e.g., light-tailedness or the Pareto exponent). Our\nrestrictions on utility require only that relative risk aversion is bounded,\nand a large variety of income processes are admitted. Our results show\nconclusively that it is necessary to go beyond standard models to explain the\nempirical fact that wealth is heavier-tailed than income. We demonstrate\nthrough examples that relaxing any of the above three conditions can generate\nPareto tails.\n"
    },
    {
        "paper_id": 1807.08644,
        "authors": "James A. Liu",
        "title": "Atomic Swaptions: Cryptocurrency Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The atomic swap protocol allows for the exchange of cryptocurrencies on\ndifferent blockchains without the need to trust a third-party. However, market\nparticipants who desire to hold derivative assets such as options or futures\nwould also benefit from trustless exchange. In this paper I propose the atomic\nswaption, which extends the atomic swap to allow for such exchanges. Crucially,\natomic swaptions do not require the use of oracles. I also introduce the margin\ncontract, which provides the ability to create leveraged and short positions.\nLastly, I discuss how atomic swaptions may be routed on the Lightning Network.\n"
    },
    {
        "paper_id": 1807.08982,
        "authors": "Lioudmila Vostrikova (LAREMA), Yuchao Dong",
        "title": "Utility maximization for L{\\'e}vy switching models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is devoted to the maximisation of HARA utilities of L{\\'e}vy\nswitching process on finite time interval via dual method. We give the\ndescription of all f-divergence minimal martingale measures in initially\nenlarged filtration, the expression of their Radon-Nikodym densities involving\nHellinger and Kulback-Leibler processes, the expressions of the optimal\nstrategies in progressively enlarged filtration for the maximisation of HARA\nutilities as well as the values of the corresponding maximal expected\nutilities. The example of Brownian switching model is presented to give the\nfinancial interpretation of the results.\n"
    },
    {
        "paper_id": 1807.09346,
        "authors": "Roy Cerqueti (Macerata), Giulia Rotundo (Roma), and Marcel Ausloos\n  (Leicester)",
        "title": "Investigating the configurations in cross-shareholding: a joint\n  copula-entropy approach",
        "comments": "36 pages, 45 references, 16 figures; abstract size hereby reduced to\n  less than 1920 characters",
        "journal-ref": "Entropy 20 (2018) 134",
        "doi": "10.3390/e20020134",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  --- the companies populating a Stock market, along with their connections,\ncan be effectively modeled through a directed network, where the nodes\nrepresent the companies, and the links indicate the ownership. This paper deals\nwith this theme and discusses the concentration of a market. A\ncross-shareholding matrix is considered, along with two key factors: the node\nout-degree distribution which represents the diversification of investments in\nterms of the number of involved companies, and the node in-degree distribution\nwhich reports the integration of a company due to the sales of its own shares\nto other companies. While diversification is widely explored in the literature,\nintegration is most present in literature on contagions. This paper captures\nsuch quantities of interest in the two frameworks and studies the stochastic\ndependence of diversification and integration through a copula approach. We\nadopt entropies as measures for assessing the concentration in the market. The\nmain question is to assess the dependence structure leading to a better\ndescription of the data or to market polarization (minimal entropy) or market\nfairness (maximal entropy). In so doing, we derive information on the way in\nwhich the in- and out-degrees should be connected in order to shape the market.\nThe question is of interest to regulators bodies, as witnessed by specific\nalert threshold published on the US mergers guidelines for limiting the\npossibility of acquisitions and the prevalence of a single company on the\nmarket. Indeed, all countries and the EU have also rules or guidelines in order\nto limit concentrations, in a country or across borders, respectively. The\ncalibration of copulas and model parameters on the basis of real data serves as\nan illustrative application of the theoretical proposal.\n"
    },
    {
        "paper_id": 1807.09423,
        "authors": "Stephan Schwill",
        "title": "Entropy Analysis of Financial Time Series",
        "comments": "Doctoral Thesis, Alliance Manchester Business School, The University\n  of Manchester, 2015. 137 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis applies entropy as a model independent measure to address three\nresearch questions concerning financial time series. In the first study we\napply transfer entropy to drawdowns and drawups in foreign exchange rates, to\nstudy their correlation and cross correlation. When applied to daily and hourly\nEUR/USD and GBP/USD exchange rates, we find evidence of dependence among the\nlargest draws (i.e. 5% and 95% quantiles), but not as strong as the correlation\nbetween the daily returns of the same pair of FX rates. In the second study we\nuse state space models (Hidden Markov Models) of volatility to investigate\nvolatility spill overs between exchange rates. Among the currency pairs, the\nco-movement of EUR/USD and CHF/USD volatility states show the strongest\nobserved relationship. With the use of transfer entropy, we find evidence for\ninformation flows between the volatility state series of AUD, CAD and BRL. The\nthird study uses the entropy of S&P realised volatility in detecting changes of\nvolatility regime in order to re-examine the theme of market volatility timing\nof hedge funds. A one-factor model is used, conditioned on information about\nthe entropy of market volatility, to measure the dynamic of hedge funds equity\nexposure. On a cross section of around 2500 hedge funds with a focus on the US\nequity markets we find that, over the period from 2000 to 2014, hedge funds\nadjust their exposure dynamically in response to changes in volatility regime.\nThis adds to the literature on the volatility timing behaviour of hedge fund\nmanager, but using entropy as a model independent measure of volatility regime.\n"
    },
    {
        "paper_id": 1807.09424,
        "authors": "Andres Gomez-Lievano, Vladislav Vysotsky, Jose Lobo",
        "title": "Artificial Increasing Returns to Scale and the Problem of Sampling from\n  Lognormals",
        "comments": "29 pages, 9 figures, 2 tables. Environment and Planning B: Urban\n  Analytics and City Science (2020)",
        "journal-ref": null,
        "doi": "10.1177/2399808320942366",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how increasing returns to scale in urban scaling can artificially\nemerge, systematically and predictably, without any sorting or positive\nexternalities. We employ a model where individual productivities are\nindependent and identically distributed lognormal random variables across all\ncities. We use extreme value theory to demonstrate analytically the paradoxical\nemergence of increasing returns to scale when the variance of log-productivity\nis larger than twice the log-size of the population size of the smallest city\nin a cross-sectional regression. Our contributions are to derive an analytical\nprediction for the artificial scaling exponent arising from this mechanism and\nto develop a simple statistical test to try to tell whether a given estimate is\nreal or an artifact. Our analytical results are validated analyzing simulations\nand real microdata of wages across municipalities in Colombia. We show how an\nartificial scaling exponent emerges in the Colombian data when the sizes of\nrandom samples of workers per municipality are $1\\%$ or less of their total\nsize.\n"
    },
    {
        "paper_id": 1807.09475,
        "authors": "Carl Duisberg",
        "title": "CAP and Monetary Policy",
        "comments": "8",
        "journal-ref": "European Economic Integration Review: Volume 15 January 1997",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the importance of CAP-related agricultural market regulation\nmechanisms within Europe, the agricultural sectors in European countries retain\na degree of sensitivity to macroeconomic activity and policies. This reality\nnow raises the question of the effects to be expected from the implementation\nof the single monetary policy on these agricultural sectors within the Monetary\nUnion.\n"
    },
    {
        "paper_id": 1807.09577,
        "authors": "Martin Shubik",
        "title": "Apologia Pro Vita Sua: The Vanishing of the White Whale in the Mists",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are many analogies among fortune hunting in business, politics, and\nscience. The prime task of the gold digger was to go to the Klondikes, find the\nright mine and mine the richest veins. This task requires motivation, sense of\npurpose and ability. Techniques and equipment must be developed. Fortune\nhunting in New England was provided at one time by hunting for whales. One went\nto a great whalers' station such as New Bedford and joined the whale hunters.\nThe hunt in academic research is similar. A single-minded passion is called\nfor. These notes here are the wrap-up comments containing some terminal\nobservations of mine on a hunt for a theory money and financial institutions.\n"
    },
    {
        "paper_id": 1807.09583,
        "authors": "Marcel Ausloos, Roy Cerqueti, Francesca Bartolacci, and Nicola G.\n  Castellano",
        "title": "SME investment best strategies. Outliers for assessing how to optimize\n  performance",
        "comments": "23 pages, 5 Tables, 6 Figures, 29 references; prepared for a Physica\n  A Special Issue",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.06.039",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Any research on strategies for reaching business excellence aims at revealing\nthe appropriate course of actions any executive should consider. Thus,\ndiscussions take place on how effective a performance measurement system can be\nestimated, or/and validated. Can one find an adequate measure (i) on the\nperformance result due to whatever level of investment, and (ii) on the timing\nof such investments? We argue that extreme value statistics provide the answer.\nWe demonstrate that the level and timing of investments allow to be forecasting\nsmall and medium size enterprises (SME) performance, - at financial crisis\ntimes. The \"investment level\" is taken as the yearly total tangible asset\n(TTA). The financial/economic performance indicators defining growth are the\nsales or total assets variations; profitability is defined from returns on\ninvestments or returns on sales. Companies on the Italian Stock Exchange STAR\nMarket serve as example. It is found from the distributions extreme values that\noutlier companies (with positive performance) are those with the lowest but\ngrowing TTA. In contrast, the SME with low TTA, but which did not increase its\nTTA, before the crisis, became a negative outlier. The outcome of these\nstatistical findings should suggest strategies to SME board members.\n"
    },
    {
        "paper_id": 1807.09595,
        "authors": "Burin Gumjudpai (IF Naresuan, ThEP, NIDA)",
        "title": "Towards equation of state for a market: A thermodynamical paradigm of\n  economics",
        "comments": "4 pages, 4 figures",
        "journal-ref": "Journal of Physics: Conference Series (2018) 1144: 012181",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Foundations of equilibrium thermodynamics are the equation of state (EoS) and\nfour postulated laws of thermodynamics. We use equilibrium thermodynamics\nparadigms in constructing the EoS for microeconomics system that is a market.\nThis speculation is hoped to be first step towards whole pictures of\nthermodynamical paradigm of economics.\n"
    },
    {
        "paper_id": 1807.0966,
        "authors": "Sayed El-Houshy",
        "title": "Hospitality Students' Perceptions towards Working in Hotels: a case\n  study of the faculty of tourism and hotels in Alexandria University",
        "comments": "TOURISM IN A CHANGING WORLD: OPPORTUNITIES & CHALLENGES The 6th\n  International Scientific Conference Faculty of Tourism and Hotels Alexandria\n  UniversityAt: Alexandria, Egypt",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The tourism and hospitality industry worldwide has been confronted with the\nproblem of attracting and retaining quality employees. If today's students are\nto become the effective practitioners of tomorrow, it is fundamental to\nunderstand their perceptions of tourism employment. Therefore, this research\naims at investigating the perceptions of hospitality students at the Faculty of\nTourism in Alexandria University towards the industry as a career choice. A\nself-administrated questionnaire was developed to rate the importance of 20\nfactors in influencing career choice, and the extent to which hospitality as a\ncareer offers these factors. From the results, it is clear that students\ngenerally do not believe that the hospitality career will offer them the\nfactors they found important. However, most of respondents (70.6%) indicated\nthat they would work in the industry after graduation. Finally, a set of\nspecific remedial actions that hospitality stakeholders could initiate to\nimprove the perceptions of hospitality career are discussed.\n"
    },
    {
        "paper_id": 1807.09864,
        "authors": "Eric Benhamou and Beatrice Guez",
        "title": "Incremental Sharpe and other performance ratios",
        "comments": "18 pages",
        "journal-ref": "Journal of Statistical and Econometric Methods, vol.7, no.4, 2018,\n  19-37",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new methodology of computing incremental contribution for\nperformance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling\nratios. Using Euler's homogeneous function theorem, we are able to decompose\nthese performance ratios as a linear combination of individual modified\nperformance ratios. This allows understanding the drivers of these performance\nratios as well as deriving a condition for a new asset to provide incremental\nperformance for the portfolio. We provide various numerical examples of this\nperformance ratio decomposition.\n"
    },
    {
        "paper_id": 1807.09873,
        "authors": "Mnacho Echenim, Herv\\'e Guiol, Nicolas Peltier",
        "title": "Formalizing the Cox-Ross-Rubinstein pricing of European derivatives in\n  Isabelle/HOL",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formalize in the proof assistant Isabelle essential basic notions and\nresults in financial mathematics. We provide generic formal definitions of\nconcepts such as markets, portfolios, derivative products, arbitrages or fair\nprices, and we show that, under the usual no-arbitrage condition, the existence\nof a replicating portfolio for a derivative implies that the latter admits a\nunique fair price. Then, we provide a formalization of the Cox-Rubinstein model\nand we show that the market is complete in this model, i.e., that every\nderivative product admits a replicating portfolio. This entails that in this\nmodel, every derivative product admits a unique fair price.\n"
    },
    {
        "paper_id": 1807.09919,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Betas, Benchmarks and Beating the Market",
        "comments": "36 pages; to appear in The Journal of Trading",
        "journal-ref": "The Journal of Trading 13(3) (2018) 44-66",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit formulaic algorithm and source code for building\nlong-only benchmark portfolios and then using these benchmarks in long-only\nmarket outperformance strategies. The benchmarks (or the corresponding betas)\ndo not involve any principal components, nor do they require iterations.\nInstead, we use a multifactor risk model (which utilizes multilevel industry\nclassification or clustering) specifically tailored to long-only benchmark\nportfolios to compute their weights, which are explicitly positive in our\nconstruction.\n"
    },
    {
        "paper_id": 1807.09967,
        "authors": "Xinyi Liu and Artit Wangperawong",
        "title": "A Collaborative Approach to Angel and Venture Capital Investment\n  Recommendations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Matrix factorization was used to generate investment recommendations for\ninvestors. An iterative conjugate gradient method was used to optimize the\nregularized squared-error loss function. The number of latent factors, number\nof iterations, and regularization values were explored. Overfitting can be\naddressed by either early stopping or regularization parameter tuning. The\nmodel achieved the highest average prediction accuracy of 13.3%. With a similar\nmodel, the same dataset was used to generate investor recommendations for\ncompanies undergoing fundraising, which achieved highest prediction accuracy of\n11.1%.\n"
    },
    {
        "paper_id": 1807.10114,
        "authors": "Wally Tzara",
        "title": "The Evolution of Security Prices Is Not Stochastic but Governed by a\n  Physicomathematical Law",
        "comments": "In order to limit the file size of this document, figures in the\n  appendices are of reduced image quality. For a version with better quality\n  images (30 MB file), please contact the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since Bachelier's thesis in 1900 (laying the foundation of the stochastic\nprocess, or Brownian motion, as a model of stock price changes), attempts at\nunderstanding the nature of prices and at predicting them have failed.\nStatistical methods have only found minor regularities/anomalies, and other\nmathematical and physical approaches do not work. This leads researchers to\nconsider that the evolution of security prices is basically random, and, thus,\ninherently not predictable. We show that the evolution of security prices is\nnot a stochastic process but largely deterministic and governed by a physical\nlaw. The law takes the form of a physicomathematical theory centered around a\npurely mathematical function, unrelated to models and statistical methods. It\ncan be described as an \"isodense\" network of moving regression curves of an\norder greater than or equal to 1. The salient aspect of the function is that,\nwhen inputting a time series of any security into the function, new\nmathematical objects emerge spontaneously, and these objects exhibit the unique\nproperty of attracting and repelling the quantity. The graphical representation\nof the function is called a \"topological network\" due to the preeminence of\nshapes over metrics, and the emergent objects are called \"characteristic\nfigures\" (mainly \"cords\"). The attraction and repulsion of the price by the\ncords results in the price bouncing from cord to cord. Thus, the price has to\nbe considered as driven by the cords in a semi-deterministic manner (leaning\ntowards deterministic). With a function that describes the evolution of the\nprice, we now understand the reason behind each price movement and can also\npredict prices both qualitatively and quantitatively. The function is\nuniversal, does not rely on any fitting, and, due to its extreme sensitivity,\nreveals the hidden order in financial time series data that existing research\nnever uncovered.\n"
    },
    {
        "paper_id": 1807.10115,
        "authors": "Fuad Aleskerov, Natalia Meshcheryakova, Alisa Nikitina, Sergey Shvydun",
        "title": "Key Borrowers Detection by Long-Range Interactions",
        "comments": null,
        "journal-ref": "National research university Higher School of economics. Series WP\n  BRP \"Basic research program\". 2016. No. 56",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for assessing agents' influence in financial network\nstructures, which takes into consideration the intensity of interactions. A\ndistinctive feature of this approach is that it considers not only direct\ninteractions of agents of the first level and indirect interactions of the\nsecond level, but also long-range indirect interactions. At the same time we\ntake into account the attributes of agents as well as the possibility of impact\nto a single agent from a group of other agents. This approach helps us to\nidentify systemically important elements which cannot be detected by classical\ncentrality measures or other indices. The proposed method was used to analyze\nthe banking foreign claims for the end of 1Q 2015. Under the approach, two\ntypes of key borrowers were detected: a) major players with high ratings and\npositive credit history; b) intermediary players, which have a great scale of\nfinancial activities through the organization of favorable investment\nconditions and positive business climate.\n"
    },
    {
        "paper_id": 1807.10276,
        "authors": "Vito D. P. Servedio, Paolo Butt\\`a, Dario Mazzilli, Andrea Tacchella,\n  Luciano Pietronero",
        "title": "A new and stable estimation method of country economic fitness and\n  product complexity",
        "comments": "12 pages, 8 figures",
        "journal-ref": "Entropy 2018, 20(10), 783",
        "doi": "10.3390/e20100783",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new metric estimating fitness of countries and complexity of\nproducts by exploiting a non-linear non-homogeneous map applied to the publicly\navailable information on the goods exported by a country. The non homogeneous\nterms guarantee both convergence and stability. After a suitable rescaling of\nthe relevant quantities, the non homogeneous terms are eventually set to zero\nso that this new metric is parameter free. This new map almost reproduces the\nresults of the original homogeneous metrics already defined in literature and\nallows for an approximate analytic solution in case of actual binarized\nmatrices based on the Revealed Comparative Advantage (RCA) indicator. This\nsolution is connected with a new quantity describing the neighborhood of nodes\nin bipartite graphs, representing in this work the relations between countries\nand exported products. Moreover, we define the new indicator of country\nnet-efficiency quantifying how a country efficiently invests in capabilities\nable to generate innovative complex high quality products. Eventually, we\ndemonstrate analytically the local convergence of the algorithm involved.\n"
    },
    {
        "paper_id": 1807.10464,
        "authors": "Aur\\'elien Hazan (LISSI)",
        "title": "A maximum entropy network reconstruction of macroeconomic models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.12.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article the problem of reconstructing the pattern of connection\nbetween agents from partial empirical data in a macro-economic model is\naddressed, given a set of behavioral equations. This systemic point of view\nputs the focus on distributional and network effects, rather than\ntime-dependence. Using the theory of complex networks we compare several models\nto reconstruct both the topology and the flows of money of the different types\nof monetary transactions, while imposing a series of constraints related to\nnational accounts, and to empirical network sparsity. Some properties of\nreconstructed networks are compared with their empirical counterpart.\n"
    },
    {
        "paper_id": 1807.10694,
        "authors": "Zachary Feinstein, Birgit Rudloff",
        "title": "Scalar multivariate risk measures with a single eligible asset",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present results on scalar risk measures in markets with\ntransaction costs. Such risk measures are defined as the minimal capital\nrequirements in the cash asset. First, some results are provided on the dual\nrepresentation of such risk measures, with particular emphasis given on the\nspace of dual variables as (equivalent) martingale measures and prices\nconsistent with the market model. Then, these dual representations are used to\nobtain the main results of this paper on time consistency for scalar risk\nmeasures in markets with frictions. It is well known from the superhedging risk\nmeasure in markets with transaction costs, as in Jouini and Kallal (1995), Roux\nand Zastawniak (2016), and Loehne and Rudloff (2014), that the usual scalar\nconcept of time consistency is too strong and not satisfied. We will show that\na weaker notion of time consistency can be defined, which corresponds to the\nusual scalar time consistency but under any fixed consistent pricing process.\nWe will prove the equivalence of this weaker notion of time consistency and a\ncertain type of backward recursion with respect to the underlying risk measure\nwith a fixed consistent pricing process. Several examples are given, with\nspecial emphasis on the superhedging risk measure.\n"
    },
    {
        "paper_id": 1807.10793,
        "authors": "M. Dashti Moghaddam and R. A. Serota",
        "title": "Combined Mutiplicative-Heston Model for Stochastic Volatility",
        "comments": "10 pages, 7 figures",
        "journal-ref": "Physica A 561, 1 January 2021, 125263",
        "doi": "10.1016/j.physa.2020.125263",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model of stochastic volatility which combines features of the\nmultiplicative model for large volatilities and of the Heston model for small\nvolatilities. The steady-state distribution in this model is a Beta Prime and\nis characterized by the power-law behavior at both large and small\nvolatilities. We discuss the reasoning behind using this model as well as\nconsequences for our recent analyses of distributions of stock returns and\nrealized volatility.\n"
    },
    {
        "paper_id": 1807.10924,
        "authors": "Antti Vauhkonen",
        "title": "Corrected XVA Modelling Framework and Formulae for KVA and MVA",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss and clarify the XVA modelling framework specified in the paper\n\"MVA by replication and regression\" (Risk Magazine, May 2015) for including\nbilateral credit risk and funding costs in derivative pricing, and in doing so\nwe rectify two key errors in the valuation adjustments accounting for costs of\ncapital and initial margin, and present corrected formulae for KVA and MVA.\n"
    },
    {
        "paper_id": 1807.11381,
        "authors": "Natalie Packham and Fabian Woebbeking",
        "title": "A factor-model approach for correlation scenarios and correlation\n  stress-testing",
        "comments": null,
        "journal-ref": "Journal of Banking and Finance, 101 (2019), 92-103",
        "doi": "10.1016/j.jbankfin.2019.01.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2012, JPMorgan accumulated a USD~6.2 billion loss on a credit derivatives\nportfolio, the so-called `London Whale', partly as a consequence of\nde-correlations of non-perfectly correlated positions that were supposed to\nhedge each other. Motivated by this case, we devise a factor model for\ncorrelations that allows for scenario-based stress testing of correlations. We\nderive a number of analytical results related to a portfolio of homogeneous\nassets. Using the concept of Mahalanobis distance, we show how to identify\nadverse scenarios of correlation risk. In addition, we demonstrate how\ncorrelation and volatility stress tests can be combined. As an example, we\napply the factor-model approach to the \"London Whale\" portfolio and determine\nthe value-at-risk impact from correlation changes. Since our findings are\nparticularly relevant for large portfolios, where even small correlation\nchanges can have a large impact, a further application would be to stress test\nportfolios of central counterparties, which are of systemically relevant size.\n"
    },
    {
        "paper_id": 1807.11477,
        "authors": "Alexander J. Stewart, Nolan McCarty and Joanna J. Bryson",
        "title": "Polarization under rising inequality and economic decline",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1126/sciadv.abd4201",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social and political polarization is a significant source of conflict and\npoor governance in many societies. Thus, understanding its causes has become a\npriority of scholars across many disciplines. Here we demonstrate that shifts\nin socialization strategies analogous to political polarization and identity\npolitics can arise as a locally-beneficial response to both rising wealth\ninequality and economic decline. Adopting a perspective of cultural evolution,\nwe develop a framework to study the emergence of polarization under shifting\neconomic environments. In many contexts, interacting with diverse out-groups\nconfers benefits from innovation and exploration greater than those that arise\nfrom interacting exclusively with a homogeneous in-group. However, when the\neconomic environment favors risk-aversion, a strategy of seeking low-risk\ninteractions can be important to maintaining individual solvency. To capture\nthis dynamic, we assume that in-group interactions have a lower expected\noutcome, but a more certain one. Thus in-group interactions are less risky than\nout-group interactions. Our model shows that under conditions of economic\ndecline or increasing wealth inequality, some members of the population benefit\nfrom adopting a risk-averse, in-group favoring strategy. Moreover, we show that\nsuch in-group polarization can spread rapidly to the whole population and\npersist even when the conditions that produced it have reversed. Finally we\noffer empirical support for the role of income inequality as a driver of\naffective polarization in the United States, mirroring findings on a panel of\ndeveloped democracies. Our work provides a framework for studying how disparate\nforces interplay, via cultural evolution, to shape patterns of identity, and\nunifies what are often seen as conflicting explanations for political\npolarization: identity threat versus economic anxiety.\n"
    },
    {
        "paper_id": 1807.11703,
        "authors": "Yuri Kifer",
        "title": "Shortfall Minimization for Game Options in Discrete Time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove existence of a self-financing strategy which minimizes shortfall for\ngame options in discrete time\n"
    },
    {
        "paper_id": 1807.11743,
        "authors": "Jarek Duda, Ma{\\l}gorzata Snarska",
        "title": "Modeling joint probability distribution of yield curve parameters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  US Yield curve has recently collapsed to its most flattened level since\nsubprime crisis and is close to the inversion. This fact has gathered attention\nof investors around the world and revived the discussion of proper modeling and\nforecasting yield curve, since changes in interest rate structure are believed\nto represent investors expectations about the future state of economy and have\nforeshadowed recessions in the United States. While changes in term structure\nof interest rates are relatively easy to interpret they are however very\ndifficult to model and forecast due to no proper economic theory underlying\nsuch events. Yield curves are usually represented by multivariate sparse time\nseries, at any point in time infinite dimensional curve is portrayed via\nrelatively few points in a multivariate space of data and as a consequence\nmultimodal statistical dependencies behind these curves are relatively hard to\nextract and forecast via typical multivariate statistical methods.We propose to\nmodel yield curves via reconstruction of joint probability distribution of\nparameters in functional space as a high degree polynomial. Thanks to adoption\nof an orthonormal basis, the MSE estimation of coefficients of a given function\nis an average over a data sample in the space of functions. Since such\npolynomial coefficients are independent and have cumulant-like interpretation\nie.describe corresponding perturbation from an uniform joint distribution, our\napproach can also be extended to any d-dimensional space of yield curve\nparameters (also in neighboring times) due to controllable accuracy. We believe\nthat this approach to modeling of local behavior of a sparse multivariate\ncurved time series can complement prediction from standard models like ARIMA,\nthat are using long range dependencies, but provide only inaccurate prediction\nof probability distribution, often as just Gaussian with constant width.\n"
    },
    {
        "paper_id": 1807.11751,
        "authors": "Adam Majewski, Stefano Ciliberti and Jean-Philippe Bouchaud",
        "title": "Co-existence of Trend and Value in Financial Markets: Estimating an\n  Extended Chiarella Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trend and Value are pervasive anomalies, common to all financial markets. We\naddress the problem of their co-existence and interaction within the framework\nof Heterogeneous Agent Based Models (HABM). More specifically, we extend the\nChiarella (1992) model by adding noise traders and a non-linear demand of\nfundamentalists. We use Bayesian filtering techniques to calibrate the model on\ntime series of prices across a variety of asset classes since 1800. The\nfundamental value is an output of the calibration, and does not require the use\nof an external pricing model. Our extended model reproduces many empirical\nobservations, including the non-monotonic relation between past trends and\nfuture returns. The destabilizing activity of trend-followers leads to a\nqualitative change of mispricing distribution, from unimodal to bimodal,\nmeaning that some markets tend to be over- (or under-) valued for long periods\nof time.\n"
    },
    {
        "paper_id": 1807.11823,
        "authors": "Franti\\v{s}ek \\v{C}ech and Jozef Barun\\'ik",
        "title": "Panel quantile regressions for estimating and predicting the\n  Value--at--Risk of commodities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how realized and option implied volatilities are\nrelated to the future quantiles of commodity returns. Whereas realized\nvolatility measures ex-post uncertainty, volatility implied by option prices\nreveals the market's expectation and is often used as an ex-ante measure of the\ninvestor sentiment. Using a flexible panel quantile regression framework, we\nshow how the future conditional quantiles of commodities returns depend on both\nex-post and ex-ante uncertainty measures. Empirical analysis of the most liquid\ncommodities covering main sectors including energy, food, agricultural,\nprecious and industrial metals reveal several important stylized facts about\nthe data. We document common patterns of the dependence between future quantile\nreturns and ex-post as well as ex-ante volatilities. We further show that\nconditional returns distribution is platykurtic and time-invariant. The\napproach can serve as a useful risk management tools for investors interested\nin commodity future contracts.\n"
    },
    {
        "paper_id": 1807.11835,
        "authors": "Christopher P Barrington-Leigh",
        "title": "The econometrics of happiness: Are we underestimating the returns to\n  education and income?",
        "comments": null,
        "journal-ref": "Journal of Public Economics, 2024",
        "doi": "10.1016/j.jpubeco.2023.105052",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a fundamental and empirically conspicuous problem\ninherent to surveys of human feelings and opinions in which subjective\nresponses are elicited on numerical scales. The paper also proposes a solution.\nThe problem is a tendency by some individuals -- particularly those with low\nlevels of education -- to simplify the response scale by considering only a\nsubset of possible responses such as the lowest, middle, and highest. In\nprinciple, this ``focal value rounding'' (FVR) behavior renders invalid even\nthe weak ordinality assumption often used in analysis of such data. With\n``happiness'' or life satisfaction data as an example, descriptive methods and\na multinomial logit model both show that the effect is large and that education\nand, to a lesser extent, income level are predictors of FVR behavior.\n  A model simultaneously accounting for the underlying wellbeing and for the\ndegree of FVR is able to estimate the latent subjective wellbeing, i.e.~the\ncounterfactual full-scale responses for all respondents, the biases associated\nwith traditional estimates, and the fraction of respondents who exhibit FVR.\nAddressing this problem helps to resolve a longstanding puzzle in the life\nsatisfaction literature, namely that the returns to education, after adjusting\nfor income, appear to be small or negative. Due to the same econometric\nproblem, the marginal utility of income in a subjective wellbeing sense has\nbeen consistently underestimated.\n"
    },
    {
        "paper_id": 1808.0016,
        "authors": "Alejandro Noriega-Campero, Alex Rutherford, Oren Lederman, Yves A. de\n  Montjoye, and Alex Pentland",
        "title": "Mapping the Privacy-Utility Tradeoff in Mobile Phone Data for\n  Development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Today's age of data holds high potential to enhance the way we pursue and\nmonitor progress in the fields of development and humanitarian action. We study\nthe relation between data utility and privacy risk in large-scale behavioral\ndata, focusing on mobile phone metadata as paradigmatic domain. To measure\nutility, we survey experts about the value of mobile phone metadata at various\nspatial and temporal granularity levels. To measure privacy, we propose a\nformal and intuitive measure of reidentification risk$\\unicode{x2014}$the\ninformation ratio$\\unicode{x2014}$and compute it at each granularity level. Our\nresults confirm the existence of a stark tradeoff between data utility and\nreidentifiability, where the most valuable datasets are also most prone to\nreidentification. When data is specified at ZIP-code and hourly levels, outside\nknowledge of only 7% of a person's data suffices for reidentification and\nretrieval of the remaining 93%. In contrast, in the least valuable dataset,\nspecified at municipality and daily levels, reidentification requires on\naverage outside knowledge of 51%, or 31 data points, of a person's data to\nretrieve the remaining 49%. Overall, our findings show that coarsening data\ndirectly erodes its value, and highlight the need for using data-coarsening,\nnot as stand-alone mechanism, but in combination with data-sharing models that\nprovide adjustable degrees of accountability and security.\n"
    },
    {
        "paper_id": 1808.00421,
        "authors": "Archil Gulisashvili",
        "title": "Gaussian stochastic volatility models: Scaling regimes, large\n  deviations, and moment explosions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we establish sample path large and moderate deviation\nprinciples for log-price processes in Gaussian stochastic volatility models,\nand study the asymptotic behavior of exit probabilities, call pricing\nfunctions, and the implied volatility. In addition, we prove that if the\nvolatility function in an uncorrelated Gaussian model grows faster than\nlinearly, then, for the asset price process, all the moments of order greater\nthan one are infinite. Similar moment explosion results are obtained for\ncorrelated models.\n"
    },
    {
        "paper_id": 1808.00515,
        "authors": "Christoph Belak, Johannes Muhle-Karbe, Kevin Ou",
        "title": "Optimal Trading with General Signals and Liquidation in Target Zone\n  Models",
        "comments": "8 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal trading in an Almgren-Chriss model with running and terminal\ninventory costs and general predictive signals about price changes. As a\nspecial case, this allows to treat optimal liquidation in \"target zone models\":\nasset prices with a reflecting boundary enforced by regulatory interventions.\nIn this case, the optimal liquidation rate is the \"theta\" of a lookback option,\nleading to explicit formulas for Bachelier or Black-Scholes dynamics.\n"
    },
    {
        "paper_id": 1808.00656,
        "authors": "Yuecai Han and Chunyang Liu",
        "title": "Asian Option Pricing under Uncertain Volatility Model",
        "comments": "19pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the asymptotic behavior of Asian option prices in the\nworst case scenario under an uncertain volatility model. We give a procedure to\napproximate the Asian option prices with a small volatility interval. By\nimposing additional conditions on the boundary condition and cutting the\nobtained Black-Scholes-Barenblatt equation into two Black-Scholes-like\nequations, we obtain an approximation method to solve the fully nonlinear PDE.\n"
    },
    {
        "paper_id": 1808.00821,
        "authors": "Fabio Bellini, Pablo Koch-Medina, Cosimo Munari, Gregor Svindland",
        "title": "Law-invariant functionals on general spaces of random variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish general versions of a variety of results for quasiconvex,\nlower-semicontinuous, and law-invariant functionals. Our results extend\nwell-known results from the literature to a large class of spaces of random\nvariables. We sometimes obtain sharper versions, even for the well-studied case\nof bounded random variables. Our approach builds on two fundamental structural\nresults for law-invariant functionals: the equivalence of law invariance and\nSchur convexity, i.e., monotonicity with respect to the convex stochastic\norder, and the fact that a law-invariant functional is fully determined by its\nbehaviour on bounded random variables. We show how to apply these results to\nprovide a unifying perspective on the literature on law-invariant functionals,\nwith special emphasis on quantile-based representations, including Kusuoka\nrepresentations, dilatation monotonicity, and infimal convolutions.\n"
    },
    {
        "paper_id": 1808.00866,
        "authors": "Enrico Ferri",
        "title": "Infinite dimensional portfolio representation as applied to model points\n  selection in life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of seeking an optimal set of model points associated\nto a fixed portfolio of life insurance policies. Such an optimal set is\ncharacterized by minimizing a certain risk functional, which gauges the average\ndiscrepancy with the fixed portfolio in terms of the fluctuation of the\ninterest rate term structure within a given time horizon. We prove a\nrepresentation theorem which provides two alternative formulations of the risk\nfunctional and which may be understood in connection with the standard\napproaches for the portfolio immunization based on sensitivity analysis. For\nthis purpose, a general framework concerning some techniques of stochastic\nintegration in Banach space and Malliavin calculus is introduced. A numerical\nexample is discussed when considering a portfolio of whole life policies.\n"
    },
    {
        "paper_id": 1808.00982,
        "authors": "Stefania Corsaro and Valentina De Simone",
        "title": "Adaptive l1-regularization for short-selling control in portfolio\n  selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the l1-regularized Markowitz model, where a l1-penalty term is\nadded to the objective function of the classical mean-variance one to stabilize\nthe solution process, promoting sparsity in the solution. The l1-penalty term\ncan also be interpreted in terms of short sales, on which several financial\nmarkets have posed restrictions. The choice of the regularization parameter\nplays a key role to obtain optimal portfolios that meet the financial\nrequirements. We propose an updating rule for the regularization parameter in\nBregman iteration to control both the sparsity and the number of short\npositions. We show that the modified scheme preserves the properties of the\noriginal one. Numerical tests are reported, which show the effectiveness of the\napproach.\n"
    },
    {
        "paper_id": 1808.01205,
        "authors": "Lori Beaman, Ariel BenYishay, Jeremy Magruder, Ahmed Mushfiq Mobarak",
        "title": "Can Network Theory-based Targeting Increase Technology Adoption?",
        "comments": "61 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to induce farmers to adopt a productive new agricultural technology,\nwe apply simple and complex contagion diffusion models on rich social network\ndata from 200 villages in Malawi to identify seed farmers to target and train\non the new technology. A randomized controlled trial compares these\ntheory-driven network targeting approaches to simpler strategies that either\nrely on a government extension worker or an easily measurable proxy for the\nsocial network (geographic distance between households) to identify seed\nfarmers. Our results indicate that technology diffusion is characterized by a\ncomplex contagion learning environment in which most farmers need to learn from\nmultiple people before they adopt themselves. Network theory based targeting\ncan out-perform traditional approaches to extension, and we identify methods to\nrealize these gains at low cost to policymakers.\n  Keywords: Social Learning, Agricultural Technology Adoption, Complex\nContagion, Malawi\n  JEL Classification Codes: O16, O13\n"
    },
    {
        "paper_id": 1808.01237,
        "authors": "C. Jara-Figueroa, Bogang Jun, Edward Glaeser, and Cesar Hidalgo",
        "title": "The role of industry, occupation, and location specific knowledge in the\n  survival of new firms",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.1800475115",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do regions acquire the knowledge they need to diversify their economic\nactivities? How does the migration of workers among firms and industries\ncontribute to the diffusion of that knowledge? Here we measure the industry,\noccupation, and location-specific knowledge carried by workers from one\nestablishment to the next using a dataset summarizing the individual work\nhistory for an entire country. We study pioneer firms--firms operating in an\nindustry that was not present in a region--because the success of pioneers is\nthe basic unit of regional economic diversification. We find that the growth\nand survival of pioneers increase significantly when their first hires are\nworkers with experience in a related industry, and with work experience in the\nsame location, but not with past experience in a related occupation. We compare\nthese results with new firms that are not pioneers and find that\nindustry-specific knowledge is significantly more important for pioneer than\nnon-pioneer firms. To address endogeneity we use Bartik instruments, which\nleverage national fluctuations in the demand for an activity as shocks for\nlocal labor supply. The instrumental variable estimates support the finding\nthat industry-related knowledge is a predictor of the survival and growth of\npioneer firms. These findings expand our understanding of the micro-mechanisms\nunderlying regional economic diversification events.\n"
    },
    {
        "paper_id": 1808.01261,
        "authors": "Jun Zhang and Fei-Yue Wang and Siyuan Chen",
        "title": "Token Economics in Energy Systems: Concept, Functionality and\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional centralized energy systems have the disadvantages of difficult\nmanagement and insufficient incentives. Blockchain is an emerging technology,\nwhich can be utilized in energy systems to enhance their management and\ncontrol. Integrating token economy and blockchain technology, token economic\nsystems in energy possess the characteristics of strong incentives and low\ncost, facilitating integrating renewable energy and demand side management, and\nproviding guarantees for improving energy efficiency and reducing emission.\nThis article describes the concept and functionality of token economics, and\nthen analyzes the feasibility of applying token economics in the energy\nsystems, and finally discuss the applications of token economics with an\nexample in integrated energy systems.\n"
    },
    {
        "paper_id": 1808.0156,
        "authors": "Hyeong Kyu Choi",
        "title": "Stock Price Correlation Coefficient Prediction with ARIMA-LSTM Hybrid\n  Model",
        "comments": "I'd appreciate any kind of comments on my work. Feel free to email\n  me!",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the price correlation of two assets for future time periods is\nimportant in portfolio optimization. We apply LSTM recurrent neural networks\n(RNN) in predicting the stock price correlation coefficient of two individual\nstocks. RNNs are competent in understanding temporal dependencies. The use of\nLSTM cells further enhances its long term predictive properties. To encompass\nboth linearity and nonlinearity in the model, we adopt the ARIMA model as well.\nThe ARIMA model filters linear tendencies in the data and passes on the\nresidual value to the LSTM model. The ARIMA LSTM hybrid model is tested against\nother traditional predictive financial models such as the full historical\nmodel, constant correlation model, single index model and the multi group\nmodel. In our empirical study, the predictive ability of the ARIMA-LSTM model\nturned out superior to all other financial models by a significant scale. Our\nwork implies that it is worth considering the ARIMA LSTM model to forecast\ncorrelation coefficient for portfolio optimization.\n"
    },
    {
        "paper_id": 1808.01926,
        "authors": "Aurelio F. Bariviera, Luciano Zunino, Osvaldo A. Rosso",
        "title": "An analysis of high-frequency cryptocurrencies prices dynamics using\n  permutation-information-theory quantifiers",
        "comments": "17 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1508.04748",
        "journal-ref": "Chaos 28, 075511 (2018)",
        "doi": "10.1063/1.5027153",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the dynamics of intraday prices of twelve\ncryptocurrencies during last months' boom and bust. The importance of this\nstudy lies on the extended coverage of the cryptoworld, accounting for more\nthan 90\\% of the total daily turnover. By using the complexity-entropy\ncausality plane, we could discriminate three different dynamics in the data\nset. Whereas most of the cryptocurrencies follow a similar pattern, there are\ntwo currencies (ETC and ETH) that exhibit a more persistent stochastic\ndynamics, and two other currencies (DASH and XEM) whose behavior is closer to a\nrandom walk. Consequently, similar financial assets, using blockchain\ntechnology, are differentiated by market participants.\n"
    },
    {
        "paper_id": 1808.02173,
        "authors": "Chol-Kyu Pak, Mun-Chol Kim, Chang-Ho Rim",
        "title": "Adapted $\\theta$-Scheme and Its Error Estimates for Backward Stochastic\n  Differential Equations",
        "comments": "18 pages, 3 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper we propose a new kind of high order numerical scheme for\nbackward stochastic differential equations(BSDEs). Unlike the traditional\n$\\theta$-scheme, we reduce truncation errors by taking $\\theta$ carefully for\nevery subinterval according to the characteristics of integrands. We give error\nestimates of this nonlinear scheme and verify the order of scheme through a\ntypical numerical experiment.\n"
    },
    {
        "paper_id": 1808.02341,
        "authors": "Denis Belomestny, John Schoenmakers, Vladimir Spokoiny and Bakhyt\n  Zharkynbay",
        "title": "Optimal stopping via reinforced regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we propose a new approach towards solving numerically optimal\nstopping problems via reinforced regression based Monte Carlo algorithms. The\nmain idea of the method is to reinforce standard linear regression algorithms\nin each backward induction step by adding new basis functions based on\npreviously estimated continuation values. The proposed methodology is\nillustrated by a numerical example from mathematical finance.\n"
    },
    {
        "paper_id": 1808.02365,
        "authors": "Slobodan Milovanovi\\'c",
        "title": "Pricing Financial Derivatives using Radial Basis Function generated\n  Finite Differences with Polyharmonic Splines on Smoothly Varying Node Layouts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the benefits of using polyharmonic splines and node\nlayouts with smoothly varying density for developing robust and efficient\nradial basis function generated finite difference (RBF-FD) methods for pricing\nof financial derivatives. We present a significantly improved RBF-FD scheme and\nsuccessfully apply it to two types of multidimensional partial differential\nequations in finance: a two-asset European call basket option under the\nBlack--Scholes--Merton model, and a European call option under the Heston\nmodel. We also show that the performance of the improved method is equally high\nwhen it comes to pricing American options. By studying convergence,\ncomputational performance, and conditioning of the discrete systems, we show\nthe superiority of the introduced approaches over previously used versions of\nthe RBF-FD method in financial applications.\n"
    },
    {
        "paper_id": 1808.02457,
        "authors": "Dietmar Pfeifer and Olena Ragulina",
        "title": "Generating VaR scenarios with product beta distributions",
        "comments": "10 pages, 25 figures, 5 tables",
        "journal-ref": "RISKS 2018, 6, 122",
        "doi": "10.3390/risks6040122",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a Monte Carlo simulation method to generate stress tests by VaR\nscenarios under Solvency II for dependent risks on the basis of observed data.\nThis is of particular interest for the construction of Internal Models and\nrequirements on evaluation processes formulated in the Commission Delegated\nRegulation. The approach is based on former work on partition-ofunity copulas,\nhowever with a direct scenario estimation of the joint density by product beta\ndistributions after a suitable transformation of the original data.\n"
    },
    {
        "paper_id": 1808.02478,
        "authors": "Chol-Kyu Pak, Mun-Chol Kim, O Hun",
        "title": "A generalized scheme for BSDEs based on derivative approximation and its\n  error estimates",
        "comments": "11 pages, 1 table. arXiv admin note: text overlap with\n  arXiv:1808.01564",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper we propose a generalized numerical scheme for backward\nstochastic differential equations(BSDEs). The scheme is based on approximation\nof derivatives via Lagrange interpolation. By changing the distribution of\nsample points used for interpolation, one can get various numerical schemes\nwith different stability and convergence order. We present a condition for the\ndistribution of sample points to guarantee the convergence of the scheme.\n"
    },
    {
        "paper_id": 1808.02505,
        "authors": "Phil Maguire, Karl Moffett, Rebecca Maguire",
        "title": "Combining Independent Smart Beta Strategies for Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Smart beta, also known as strategic beta or factor investing, is the idea of\nselecting an investment portfolio in a simple rule-based manner that\nsystematically captures market inefficiencies, thereby enhancing risk-adjusted\nreturns above capitalization-weighted benchmarks. We explore the idea of\napplying a smart strategy in reverse, yielding a \"bad beta\" portfolio which can\nbe shorted, thus allowing long and short positions on independent smart beta\nstrategies to generate beta neutral returns. In this article we detail the\nconstruction of a monthly reweighted portfolio involving two independent smart\nbeta strategies; the first component is a long-short beta-neutral strategy\nderived from running an adaptive boosting classifier on a suite of momentum\nindicators. The second component is a minimized volatility portfolio which\nexploits the observation that low-volatility stocks tend to yield higher\nrisk-adjusted returns than high-volatility stocks. Working off a market\nbenchmark Sharpe Ratio of 0.42, we find that the market neutral component\nachieves a ratio of 0.61, the low volatility approach achieves a ratio of 0.90,\nwhile the combined leveraged strategy achieves a ratio of 0.96. In six months\nof live trading, the combined strategy achieved a Sharpe Ratio of 1.35. These\nresults reinforce the effectiveness of smart beta strategies, and demonstrate\nthat combining multiple strategies simultaneously can yield better performance\nthan that achieved by any single component in isolation.\n"
    },
    {
        "paper_id": 1808.02791,
        "authors": "Anurag Sodhi",
        "title": "American Put Option pricing using Least squares Monte Carlo method under\n  Bakshi, Cao and Chen Model Framework (1997) and comparison to alternative\n  regression techniques in Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores alternative regression techniques in pricing American put\noptions and compares to the least-squares method (LSM) in Monte Carlo\nimplemented by Longstaff-Schwartz, 2001 which uses least squares to estimate\nthe conditional expected payoff to the option holder from continuation. The\npricing is done under general model framework of Bakshi, Cao and Chen 1997\nwhich incorporates, stochastic volatility, stochastic interest rate and jumps.\nAlternative regression techniques used are Artificial Neural Network (ANN) and\nGradient Boosted Machine (GBM) Trees. Model calibration is done on American put\noptions on SPY using these three techniques and results are compared on out of\nsample data.\n"
    },
    {
        "paper_id": 1808.02826,
        "authors": "Kyle Gatesman and James Unwin",
        "title": "Lattice Studies of Gerrymandering Strategies",
        "comments": "32 Pages, 15 Figures",
        "journal-ref": "Polit. Anal. 29 (2021) 167-192",
        "doi": "10.1017/pan.2020.22",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose three novel gerrymandering algorithms which incorporate the\nspatial distribution of voters with the aim of constructing gerrymandered,\nequal-population, connected districts. Moreover, we develop lattice models of\nvoter distributions, based on analogies to electrostatic potentials, in order\nto compare different gerrymandering strategies. Due to the probabilistic\npopulation fluctuations inherent to our voter models, Monte Carlo methods can\nbe applied to the districts constructed via our gerrymandering algorithms.\nThrough Monte Carlo studies we quantify the effectiveness of each of our\ngerrymandering algorithms and we also argue that gerrymandering strategies\nwhich do not include spatial data lead to (legally prohibited) highly\ndisconnected districts. Of the three algorithms we propose, two are based on\ndifferent strategies for packing opposition voters, and the third is a new\napproach to algorithmic gerrymandering based on genetic algorithms, which\nautomatically guarantees that all districts are connected. Furthermore, we use\nour lattice voter model to examine the effectiveness of isoperimetric quotient\ntests and our results provide further quantitative support for implementing\ncompactness tests in real-world political redistricting.\n"
    },
    {
        "paper_id": 1808.0291,
        "authors": "Ray Fair",
        "title": "Information Content of DSGE Forecasts",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the question whether information is contained in\nforecasts from DSGE models beyond that contained in lagged values, which are\nextensively used in the models. Four sets of forecasts are examined. The\nresults are encouraging for DSGE forecasts of real GDP. The results suggest\nthat there is information in the DSGE forecasts not contained in forecasts\nbased only on lagged values and that there is no information in the\nlagged-value forecasts not contained in the DSGE forecasts. The opposite is\ntrue for forecasts of the GDP deflator.\n  Keywords: DSGE forecasts, Lagged values\n  JEL Classification Codes: E10, E17, C53\n"
    },
    {
        "paper_id": 1808.02953,
        "authors": "Arnab Chakrabarti, Rituparna Sen",
        "title": "Some Statistical Problems with High Dimensional Financial data",
        "comments": "22 pages, 5 figures",
        "journal-ref": "Forthcoming in New Perspectives and Challenges in Econophysics,\n  New Economics Windows Series, Springer (2018)",
        "doi": "10.1007/978-3-030-11364-3_11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For high dimensional data, some of the standard statistical techniques do not\nwork well. So modification or further development of statistical methods are\nnecessary. In this paper, we explore these modifications. We start with the\nimportant problem of estimating high dimensional covariance matrix. Then we\nexplore some of the important statistical techniques such as high dimensional\nregression, principal component analysis, multiple testing problems and\nclassification. We describe some of the fast algorithms that can be readily\napplied in practice.\n"
    },
    {
        "paper_id": 1808.0307,
        "authors": "Yongli Li, Zhi-Ping Fan, and Wei Zhang",
        "title": "Network-based Referral Mechanism in a Crowdfunding-based Marketing\n  Pattern",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crowdfunding is gradually becoming a modern marketing pattern. By noting that\nthe success of crowdfunding depends on network externalities, our research aims\nto utilize them to provide an applicable referral mechanism in a\ncrowdfunding-based marketing pattern. In the context of network externalities,\nmeasuring the value of leading customers is chosen as the key to coping with\nthe research problem by considering that leading customers take a critical\nstance in forming a referral network. Accordingly, two sequential-move game\nmodels (i.e., basic model and extended model) were established to measure the\nvalue of leading customers, and a skill of matrix transformation was adopted to\nsolve the model by transforming a complicated multi-sequence game into a simple\nsimultaneous-move game. Based on the defined value of leading customers, a\nnetwork-based referral mechanism was proposed by exploring exactly how many\nawards are allocated along the customer sequence to encourage the leading\ncustomers' actions of successful recommendation and by demonstrating two\ngeneral rules of awarding the referrals in our model setting. Moreover, the\nproposed solution approach helps deepen an understanding of the effect of the\nleading position, which is meaningful for designing more numerous referral\napproaches.\n"
    },
    {
        "paper_id": 1808.03186,
        "authors": "Ayelet Amiran, Fabrice Baudoin, Skylyn Brock, Berend Coster, Ryan\n  Craver, Ugonna Ezeaka, Phanuel Mariano, Mary Wishart",
        "title": "The financial value of knowing the distribution of stock prices in\n  discrete market models",
        "comments": "Undergraduate summer research funded by REU NSF grant DMS 1659643",
        "journal-ref": "Involve 12 (2019) 883-899",
        "doi": "10.2140/involve.2019.12.883",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An explicit formula is derived for the value of weak information in a\ndiscrete time model that works for a wide range of utility functions including\nthe logarithmic and power utility. We assume a complete market with a finite\nnumber of assets and a finite number of possible outcomes. Explicit\ncalculations are performed for a binomial model with two assets. The case of\ntrinomial models is also discussed.\n"
    },
    {
        "paper_id": 1808.03297,
        "authors": "Eric Benhamou",
        "title": "Trend without hiccups: a Kalman filter approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Have you ever felt miserable because of a sudden whipsaw in the price that\ntriggered an unfortunate trade? In an attempt to remove this noise, technical\nanalysts have used various types of moving averages (simple, exponential,\nadaptive one or using Nyquist criterion). These tools may have performed\ndecently but we show in this paper that this can be improved dramatically\nthanks to the optimal filtering theory of Kalman filters (KF). We explain the\nbasic concepts of KF and its optimum criterion. We provide a pseudo code for\nthis new technical indicator that demystifies its complexity. We show that this\nnew smoothing device can be used to better forecast price moves as lag is\nreduced. We provide 4 Kalman filter models and their performance on the SP500\nmini-future contract. Results are quite illustrative of the efficiency of KF\nmodels with better net performance achieved by the KF model combining smoothing\nand extremum position.\n"
    },
    {
        "paper_id": 1808.03328,
        "authors": "Hampus Engsner, Kristoffer Lindensj\\\"o and Filip Lindskog",
        "title": "The value of a liability cash flow in discrete time subject to capital\n  requirements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to define the market-consistent multi-period value\nof an insurance liability cash flow in discrete time subject to repeated\ncapital requirements, and explore its properties. In line with current\nregulatory frameworks, the approach presented is based on a hypothetical\ntransfer of the original liability and a replicating portfolio to an empty\ncorporate entity whose owner must comply with repeated one-period capital\nrequirements but has the option to terminate the ownership at any time. The\nvalue of the liability is defined as the no-arbitrage price of the cash flow to\nthe policyholders, optimally stopped from the owner's perspective, taking\ncapital requirements into account. The value is computed as the solution to a\nsequence of coupled optimal stopping problems or, equivalently, as the solution\nto a backward recursion.\n"
    },
    {
        "paper_id": 1808.03404,
        "authors": "Ali Hosseiny, Mohammadreza Absalan, Mohammad Sherafati, Mauro\n  Gallegati",
        "title": "Hysteresis of economic networks in an XY model",
        "comments": "To be appeared in Physica A: Statistical Mechanics and its\n  Applications",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.08.064",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many-body systems can have multiple equilibria. Though the energy of\nequilibria might be the same, still systems may resist to switch from an\nunfavored equilibrium to a favored one. In this paper we investigate occurrence\nof such phenomenon in economic networks. In times of crisis when governments\nintend to stimulate economy, a relevant question is on the proper size of\nstimulus bill. To address the answer, we emphasize the role of hysteresis in\neconomic networks. In times of crises, firms and corporations cut their\nproductions; now since their level of activity is correlated, metastable\nfeatures in the network become prominent. This means that economic networks\nresist against the recovery actions. To measure the size of resistance in the\nnetwork against recovery, we deploy the XY model. Though theoretically the XY\nmodel has no hysteresis, when it comes to the kinetic behavior in the\ndeterministic regimes, we observe a dynamic hysteresis. We find that to\novercome the hysteresis of the network, a minimum size of stimulation is needed\nfor success. Our simulations show that as long as the networks are\nWatts-Strogatz, such minimum is independent of the characteristics of the\nnetworks.\n"
    },
    {
        "paper_id": 1808.03463,
        "authors": "Julian H\\\"olzermann",
        "title": "The Hull-White Model under Volatility Uncertainty",
        "comments": "rewrote/restructured the paper while main results remain unchanged;\n  left out the part about drift uncertainty; included a section about yield\n  curve fitting and a multifactor extension",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Hull-White model for the term structure of interest rates in the\npresence of volatility uncertainty. The uncertainty about the volatility is\nrepresented by a set of beliefs, which naturally leads to a sublinear\nexpectation and a G-Brownian motion. The main question in this setting is how\nto find an arbitrage-free term structure. This question is crucial, since we\ncan show that the classical approach, martingale modeling, does not work in the\npresence of volatility uncertainty. Therefore, we need to adjust the model in\norder to find an arbitrage-free term structure. The resulting term structure is\naffine with respect to the short rate and the adjustment factor. Although the\nadjustment changes the structure of the model, it is still consistent with the\ntraditional Hull-White model after fitting the yield curve. In addition, we\nextend the model and the results to a multifactor version, driven by multiple\nrisk factors.\n"
    },
    {
        "paper_id": 1808.03481,
        "authors": "Jian Sun",
        "title": "Concave Shape of the Yield Curve and No Arbitrage",
        "comments": "16pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In fixed income sector, the yield curve is probably the most observed\nindicator by the market for trading and fifinancing purposes. A yield curve\nplots interest rates across different contract maturities from short end to as\nlong as 30 years. For each currency, the corresponding curve shows the relation\nbetween the level of the interest rates (or cost of borrowing) and the time to\nmaturity. For example, the U.S. dollar interest rates paid on U.S. Treasury\nsecurities for various maturities are plotted as the US treasury curve. For the\nsame currency, if the swap market is used, we could also plot the swap rates\nacross the tenors which would be called the swap curve.Even the yield curve can\nbe at, upward or downward (inverted), however, yield curve is generally\nconcave. There is a lack of explanation of the concavity of the yield curve\nshape from economics theory. We offer in this article an explanation of the\nconcavity shape of the yield curve from trading perspectives.\n"
    },
    {
        "paper_id": 1808.03482,
        "authors": "Jaehyung Lee and Minhyung Cho",
        "title": "Exeum: A Decentralized Financial Platform for Price-Stable\n  Cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price stability has often been cited as a key reason that cryptocurrencies\nhave not gained widespread adoption as a medium of exchange and continue to\nprove incapable of powering the economy of decentralized applications (DApps)\nefficiently. Exeum proposes a novel method to provide price stable digital\ntokens whose values are pegged to real world assets, serving as a bridge\nbetween the real world and the decentralized economy.\n  Pegged tokens issued by Exeum - for example, USDE refers to a stable token\nissued by the system whose value is pegged to USD - are backed by virtual\nassets in a virtual asset exchange where users can deposit the base token of\nthe system and take long or short positions. Guaranteeing the stability of the\npegged tokens boils down to the problem of maintaining the peg of the virtual\nassets to real world assets, and the main mechanism used by Exeum is\ncontrolling the swap rate of assets. If the swap rate is fully controlled by\nthe system, arbitrageurs can be incentivized enough to restore a broken peg;\nExeum distributes statistical arbitrage trading software to decentralize this\ntype of market making activity. The last major component of the system is a\ncentral bank equivalent that determines the long term interest rate of the base\ntoken, pays interest on the deposit by inflating the supply if necessary, and\nremoves the need for stability fees on pegged tokens, improving their\nusability.\n  To the best of our knowledge, Exeum is the first to propose a truly\ndecentralized method for developing a stablecoin that enables 1:1 value\nconversion between the base token and pegged assets, completely removing the\nmismatch between supply and demand. In this paper, we will also discuss its\napplications, such as improving staking based DApp token models, price stable\ngas fees, pegging to an index of DApp tokens, and performing cross-chain asset\ntransfer of legacy crypto assets.\n"
    },
    {
        "paper_id": 1808.03548,
        "authors": "Antoine Jacquier and Fangwei Shi",
        "title": "Small-time moderate deviations for the randomised Heston model",
        "comments": "8 pages",
        "journal-ref": "J. Appl. Probab. 57 (2020) 19-28",
        "doi": "10.1017/jpr.2019.73",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend previous large deviations results for the randomised Heston model\nto the case of moderate deviations. The proofs involve the G\\\"artner-Ellis\ntheorem and sharp large deviations tools.\n"
    },
    {
        "paper_id": 1808.03607,
        "authors": "Igor Halperin and Matthew Dixon",
        "title": "\"Quantum Equilibrium-Disequilibrium\": Asset Price Dynamics, Symmetry\n  Breaking, and Defaults as Dissipative Instantons",
        "comments": "Improved presentation, typos fixed. 51 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple non-equilibrium model of a financial market as an open\nsystem with a possible exchange of money with an outside world and market\nfrictions (trade impacts) incorporated into asset price dynamics via a feedback\nmechanism. Using a linear market impact model, this produces a non-linear\ntwo-parametric extension of the classical Geometric Brownian Motion (GBM)\nmodel, that we call the \"Quantum Equilibrium-Disequilibrium\" (QED) model. The\nQED model gives rise to non-linear mean-reverting dynamics, broken scale\ninvariance, and corporate defaults. In the simplest one-stock (1D) formulation,\nour parsimonious model has only one degree of freedom, yet calibrates to both\nequity returns and credit default swap spreads. Defaults and market crashes are\nassociated with dissipative tunneling events, and correspond to instanton\n(saddle-point) solutions of the model. When market frictions and\ninflows/outflows of money are neglected altogether, \"classical\" GBM\nscale-invariant dynamics with an exponential asset growth and without defaults\nare formally recovered from the QED dynamics. However, we argue that this is\nonly a formal mathematical limit, and in reality the GBM limit is non-analytic\ndue to non-linear effects that produce both defaults and divergence of\nperturbation theory in a small market friction parameter.\n"
    },
    {
        "paper_id": 1808.0361,
        "authors": "Elisa Al\\`os, David Garc\\'ia-Lorite and Aitor Muguruza",
        "title": "On smile properties of volatility derivatives and exotic products:\n  understanding the VIX skew",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a method to study the implied volatility for exotic options and\nvolatility derivatives with European payoffs such as VIX options. Our approach,\nbased on Malliavin calculus techniques, allows us to describe the properties of\nthe at-the-money implied volatility (ATMI) in terms of the Malliavin\nderivatives of the underlying process. More precisely, we study the short-time\nbehaviour of the ATMI level and skew. As an application, we describe the\nshort-term behavior of the ATMI of VIX and realized variance options in terms\nof the Hurst parameter of the model, and most importantly we describe the class\nof volatility processes that generate a positive skew for the VIX implied\nvolatility. In addition, we find that our ATMI asymptotic formulae perform very\nwell even for large maturities. Several numerical examples are provided to\nsupport our theoretical results.\n"
    },
    {
        "paper_id": 1808.03668,
        "authors": "Zihao Zhang, Stefan Zohren, Stephen Roberts",
        "title": "DeepLOB: Deep Convolutional Neural Networks for Limit Order Books",
        "comments": "12 pages, 9 figures",
        "journal-ref": "IEEE Transactions on Signal Processing, Vol. 67, No. 11, June 1,\n  2019",
        "doi": "10.1109/TSP.2019.2907260",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a large-scale deep learning model to predict price movements from\nlimit order book (LOB) data of cash equities. The architecture utilises\nconvolutional filters to capture the spatial structure of the limit order books\nas well as LSTM modules to capture longer time dependencies. The proposed\nnetwork outperforms all existing state-of-the-art algorithms on the benchmark\nLOB dataset [1]. In a more realistic setting, we test our model by using one\nyear market quotes from the London Stock Exchange and the model delivers a\nremarkably stable out-of-sample prediction accuracy for a variety of\ninstruments. Importantly, our model translates well to instruments which were\nnot part of the training set, indicating the model's ability to extract\nuniversal features. In order to better understand these features and to go\nbeyond a \"black box\" model, we perform a sensitivity analysis to understand the\nrationale behind the model predictions and reveal the components of LOBs that\nare most relevant. The ability to extract robust features which translate well\nto other instruments is an important property of our model which has many other\napplications.\n"
    },
    {
        "paper_id": 1808.03804,
        "authors": "Sandra Schneemann, Hendrik Scholten, Christian Deutscher",
        "title": "The Impact of Age on Nationality Bias: Evidence from Ski Jumping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This empirical research explores the impact of age on nationality bias. World\nCup competition data suggest that judges of professional ski jumping\ncompetitions prefer jumpers of their own nationality and exhibit this\npreference by rewarding them with better marks. Furthermore, the current study\nreveals that this nationality bias is diminished among younger judges, in\naccordance with the reported lower levels of national discrimination among\nyounger generations. Globalisation and its effect in reducing class-based\nthinking may explain this reduced bias in judgment of others.\n"
    },
    {
        "paper_id": 1808.0415,
        "authors": "Sina Aghaei, Amirreza Safari Langroudi, Masoud Fekri",
        "title": "A Predictive Model for Oil Market under Uncertainty: Data-Driven System\n  Dynamics Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In recent years, there have been a lot of sharp changes in the oil price.\nThese rapid changes cause the traditional models to fail in predicting the\nprice behavior. The main reason for the failure of the traditional models is\nthat they consider the actual value of parameters instead of their\nexpectational ones. In this paper, we propose a system dynamics model that\nincorporates expectational variables in determining the oil price. In our\nmodel, the oil price is determined by the expected demand and supply vs. their\nactual values. Our core model is based upon regression analysis on several\nhistoric time series and adjusted by adding many casual loops in the oil\nmarket. The proposed model in simulated in different scenarios that have\nhappened in the past and our results comply with the trends of the oil price in\neach of the scenarios.\n"
    },
    {
        "paper_id": 1808.04231,
        "authors": "Richard Pincak, Kabin Kanjamapornkul",
        "title": "GARCH(1,1) model of the financial market with the Minkowski metric",
        "comments": "18 pages",
        "journal-ref": "Zeitschrift f\\\"ur Naturforschung A 73 (2018) 669",
        "doi": "10.1515/zna-2018-0199",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solved a stylized fact on a long memory process of volatility cluster\nphenomena by using Minkowski metric for GARCH(1,1) under assumption that price\nand time can not be separated. We provide a Yang-Mills equation in financial\nmarket and anomaly on superspace of time series data as a consequence of the\nproof from the general relativity theory. We used an original idea in Minkowski\nspacetime embedded in Kolmogorov space in time series data with behavior of\ntraders.The result of this work is equivalent to the dark volatility or the\nhidden risk fear field induced by the interaction of the behavior of the trader\nin the financial market panic when the market crashed.\n"
    },
    {
        "paper_id": 1808.04233,
        "authors": "Eric Benhamou",
        "title": "Connecting Sharpe ratio and Student t-statistic, and beyond",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sharpe ratio is widely used in asset management to compare and benchmark\nfunds and asset managers. It computes the ratio of the excess return over the\nstrategy standard deviation. However, the elements to compute the Sharpe ratio,\nnamely, the expected returns and the volatilities are unknown numbers and need\nto be estimated statistically. This means that the Sharpe ratio used by funds\nis subject to be error prone because of statistical estimation error. Lo\n(2002), Mertens (2002) derive explicit expressions for the statistical\ndistribution of the Sharpe ratio using standard asymptotic theory under several\nsets of assumptions (independent normally distributed - and identically\ndistributed returns). In this paper, we provide the exact distribution of the\nSharpe ratio for independent normally distributed return. In this case, the\nSharpe ratio statistic is up to a rescaling factor a non centered Student\ndistribution whose characteristics have been widely studied by statisticians.\nThe asymptotic behavior of our distribution provide the result of Lo (2002). We\nalso illustrate the fact that the empirical Sharpe ratio is asymptotically\noptimal in the sense that it achieves the Cramer Rao bound. We then study the\nempirical SR under AR(1) assumptions and investigate the effect of compounding\nperiod on the Sharpe (computing the annual Sharpe with monthly data for\ninstance). We finally provide general formula in this case of\nheteroscedasticity and autocorrelation.\n"
    },
    {
        "paper_id": 1808.04265,
        "authors": "Baojun Bian, Harry Zheng",
        "title": "Turnpike Property and Convergence Rate for an Investment and Consumption\n  Model",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the turnpike property for optimal investment and consumption\nproblems. We find there exists a threshold value that determines the turnpike\nproperty for investment policy. The threshold value only depends on the Sharpe\nratio, the riskless interest rate and the discount rate. We show that if\nutilities behave asymptotically like power utilities and satisfy some simple\nrelations with the threshold value, then the turnpike property for investment\nholds. There is in general no turnpike property for consumption policy. We also\nprovide the rate of convergence and illustrate the main results with examples\nof power and non-HARA utilities and numerical tests.\n"
    },
    {
        "paper_id": 1808.04604,
        "authors": "Rodwell Kufakunesu, Calisto Guambe and Lesedi Mabitsela",
        "title": "Risk-based optimal portfolio of an insurer with regime switching and\n  noisy memory",
        "comments": "21",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a risk-based optimal investment problem of an\ninsurer in a regime-switching jump diffusion model with noisy memory. Using the\nmodel uncertainty modeling, we formulate the investment problem as a zero-sum,\nstochastic differential delay game between the insurer and the market, with a\nconvex risk measure of the terminal surplus and the Brownian delay surplus over\na period $[T-\\varrho,T]$. Then, by the BSDE approach, the game problem is\nsolved. Finally, we derive analytical solutions of the game problem, for a\nparticular case of a quadratic penalty function and a numerical example is\nconsidered.\n"
    },
    {
        "paper_id": 1808.04608,
        "authors": "Rodwell Kufakunesu and Calisto Guambe",
        "title": "On the optimal investment-consumption and life insurance selection\n  problem with an external stochastic factor",
        "comments": "20",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a stochastic optimal control problem with stochastic\nvolatility. We prove the sufficient and necessary maximum principle for the\nproposed problem. Then we apply the results to solve an investment, consumption\nand life insurance problem with stochastic volatility, that is, we consider a\nwage earner investing in one risk-free asset and one risky asset described by a\njump-diffusion process and has to decide concerning consumption and life\ninsurance purchase. We assume that the life insurance for the wage earner is\nbought from a market composed of $M>1$ life insurance companies offering\npairwise distinct life insurance contracts. The goal is to maximize the\nexpected utilities derived from the consumption, the legacy in the case of a\npremature death and the investor's terminal wealth.\n"
    },
    {
        "paper_id": 1808.04611,
        "authors": "Lesedi Mabitsela, Calisto Guambe and Rodwell Kufakunesu",
        "title": "A note on representation of BSDE-based dynamic risk measures and dynamic\n  capital allocations",
        "comments": "17",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide a representation theorem for dynamic capital\nallocation under It{\\^o}-L{\\'e}vy model. We consider the representation of\ndynamic risk measures defined under Backward Stochastic Differential Equations\n(BSDE) with generators that grow quadratic-exponentially in the control\nvariables. Dynamic capital allocation is derived from the differentiability of\nBSDEs with jumps. The results are illustrated by deriving a capital allocation\nrepresentation for dynamic entropic risk measure and static coherent risk\nmeasure.\n"
    },
    {
        "paper_id": 1808.04613,
        "authors": "Rodwell Kufakunesu and Calisto Guambe",
        "title": "Optimal investment-consumption and life insurance with capital\n  constraints",
        "comments": "22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to solve an optimal investment, consumption and life\ninsurance problem when the investor is restricted to capital guarantee. We\nconsider an incomplete market described by a jump-diffusion model with\nstochastic volatility. Using the martingale approach, we prove the existence of\nthe optimal strategy and the optimal martingale measure and we obtain the\nexplicit solutions for the power utility functions.\n"
    },
    {
        "paper_id": 1808.0471,
        "authors": "Samuel Asante Gyamerah, Philip Ngare, and Dennis Ikpe",
        "title": "Regime-Switching Temperature Dynamics Model for Weather Derivatives",
        "comments": "15 pages, 25 figures, International Journal of Stochastic Analysis,\n  2018",
        "journal-ref": null,
        "doi": "10.1155/2018/8534131",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Weather is a key production factor in agricultural crop production and at the\nsame time the most significant and least controllable source of peril in\nagriculture. These effects of weather on agricultural crop production have\ntriggered a widespread support for weather derivatives as a means of mitigating\nthe risk associated with climate change on agriculture. However, these products\nare faced with basis risk as a result of poor design and modelling of the\nunderlying weather variable (temperature). In order to circumvent these\nproblems, a novel time-varying mean-reversion L\\'evy regime-switching model is\nused to model the dynamics of the deseasonalized temperature dynamics. Using\nplots and test statistics, it is observed that the residuals of the\ndeseasonalized temperature data are not normally distributed. To model the\nnon-normality in the residuals, we propose using the hyperbolic distribution to\ncapture the semi-heavy tails and skewness in the empirical distributions of the\nresiduals for the shifted regime. The proposed regime-switching model has a\nmean-reverting heteroskedastic process in the base regime and a L\\'evy process\nin the shifted regime. By using the Expectation-Maximization algorithm, the\nparameters of the proposed model are estimated. The proposed model is flexible\nas it modelled the deseasonalized temperature data accurately.\n"
    },
    {
        "paper_id": 1808.04725,
        "authors": "Christian Bayer, Martin Redmann, John Schoenmakers",
        "title": "Dynamic programming for optimal stopping via pseudo-regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce new variants of classical regression-based algorithms for\noptimal stopping problems based on computation of regression coefficients by\nMonte Carlo approximation of the corresponding $L^2$ inner products instead of\nthe least-squares error functional. Coupled with new proposals for simulation\nof the underlying samples, we call the approach \"pseudo regression\". A detailed\nconvergence analysis is provided and it is shown that the approach\nasymptotically leads to less computational cost for a pre-specified error\ntolerance, hence to lower complexity. The method is justified by numerical\nexamples.\n"
    },
    {
        "paper_id": 1808.04908,
        "authors": "Maxim Bichuch, Agostino Capponi, Stephan Sturm",
        "title": "Robust XVA",
        "comments": "45 pages, 6 figures; forthcoming in Math. Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an arbitrage-free framework for robust valuation adjustments. An\ninvestor trades a credit default swap portfolio with a risky counterparty, and\nhedges credit risk by taking a position in defaultable bonds. The investor does\nnot know the return rate of her counterparty's bond, but is confident that it\nlies within an uncertainty interval. We derive both upper and lower bounds for\nthe XVA process of the portfolio, and show that these bounds may be recovered\nas solutions of nonlinear ordinary differential equations. The presence of\ncollateralization and closeout payoffs leads to important differences with\nrespect to classical credit risk valuation. The value of the super-replicating\nportfolio cannot be directly obtained by plugging one of the extremes of the\nuncertainty interval in the valuation equation, but rather depends on the\nrelation between the XVA replicating portfolio and the close-out value\nthroughout the life of the transaction. Our comparative statics analysis\nindicates that credit contagion has a nonlinear effect on the replication\nstrategies and on the XVA.\n"
    },
    {
        "paper_id": 1808.05037,
        "authors": "Oleg Malafeyev, Shulga Andrey",
        "title": "Game-theoretic dynamic investment model with incomplete information:\n  futures contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past few years, the futures market has been successfully developing\nin the North-West region. Futures markets are one of the most effective and\nliquid-visible trading mechanisms. A large number of buyers are forced to\ncompete with each other and raise their prices. A large number of sellers make\nthem reduce prices. Thus, the gap between the prices of offers of buyers and\nsellers is reduced due to high competition, and this is a good criterion for\nthe liquidity of the market. This high degree of liquidity contributed to the\nfact that futures trading took such an important role in commerce and finance.\nA multi-step, non-cooperative n persons game is formalized and studied\n"
    },
    {
        "paper_id": 1808.05142,
        "authors": "D\\'ora Gr\\'eta Petr\\'oczy, Mark Francis Rogers, L\\'aszl\\'o \\'A.\n  K\\'oczy",
        "title": "Brexit: The Belated Threat",
        "comments": null,
        "journal-ref": "Games. 13(1):18. 2022",
        "doi": "10.3390/g13010018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Debates on an EU-leaving referendum arose in several member states after\nBrexit. We want to highlight how the exit of an additional country affects the\npower distribution in the Council of the European Union. We inspect the power\nindices of the member states both with and without the country which might\nleave the union. Our results show a pattern connected to a change in the\nthreshold of the number of member states required for a decision. An exit that\nmodifies this threshold benefits the countries with high population, while an\nexit that does not cause such a change benefits the small member states.\nAccording to our calculations, the threat of Brexit would have worked\ndifferently before the entry of Croatia.\n"
    },
    {
        "paper_id": 1808.05169,
        "authors": "Sebastian Herrmann, Johannes Muhle-Karbe, Dapeng Shang, Chen Yang",
        "title": "Inventory Management for High-Frequency Trading with Imperfect\n  Competition",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Nash equilibria for inventory-averse high-frequency traders (HFTs),\nwho trade to exploit information about future price changes. For discrete\ntrading rounds, the HFTs' optimal trading strategies and their equilibrium\nprice impact are described by a system of nonlinear equations; explicit\nsolutions obtain around the continuous-time limit. Unlike in the risk-neutral\ncase, the optimal inventories become mean-reverting and vanish as the number of\ntrading rounds becomes large. In contrast, the HFTs' risk-adjusted profits and\nthe equilibrium price impact converge to their risk-neutral counterparts.\nCompared to a social-planner solution for cooperative HFTs, Nash competition\nleads to excess trading, so that marginal transaction taxes in fact decrease\nmarket liquidity.\n"
    },
    {
        "paper_id": 1808.05289,
        "authors": "Liyuan Jiang, Shuang Zhou, Keren Li, Fangfang Wang and Jie Yang",
        "title": "A New Nonparametric Estimate of the Risk-Neutral Density with\n  Applications to Variance Swaps",
        "comments": "19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new nonparametric approach for estimating the risk-neutral\ndensity of asset prices and reformulate its estimation into a\ndouble-constrained optimization problem. We evaluate our approach using the\nS\\&P 500 market option prices from 1996 to 2015. A comprehensive\ncross-validation study shows that our approach outperforms the existing\nnonparametric quartic B-spline and cubic spline methods, as well as the\nparametric method based on the Normal Inverse Gaussian distribution. As an\napplication, we use the proposed density estimator to price long-term variance\nswaps, and the model-implied prices match reasonably well with those of the\nvariance future downloaded from the CBOE website.\n"
    },
    {
        "paper_id": 1808.05295,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "SINH-acceleration: efficient evaluation of probability distributions,\n  option pricing, and Monte-Carlo simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Characteristic functions of several popular classes of distributions and\nprocesses admit analytic continuation into unions of strips and open coni\naround $\\mathbb{R}\\subset \\mathbb{C}$. The Fourier transform techniques reduces\ncalculation of probability distributions and option prices to evaluation of\nintegrals whose integrands are analytic in domains enjoying these properties.\nIn the paper, we suggest to use changes of variables of the form\n$\\xi=\\sqrt{-1}\\omega_1+b\\sinh (\\sqrt{-1}\\omega+y)$ and the simplified trapezoid\nrule to evaluate the integrals accurately and fast. We formulate the general\nscheme, and apply the scheme for calculation probability distributions and\npricing European options in L\\'evy models, the Heston model, the CIR model, and\na L\\'evy model with the CIR-subordinator. We outline applications to fast and\naccurate calibration procedures and Monte Carlo simulations in L\\'evy models,\nregime switching L\\'evy models that can account for stochastic drift,\nvolatility and skewness, and the Heston model. For calculation of quantiles in\nthe tails using the Newton or bisection method, it suffices to precalculate\nseveral hundred of values of the characteristic exponent at points of an\nappropriate grid ({\\em conformal principal components}) and use these values in\nformulas for cpdf and pdf.\n"
    },
    {
        "paper_id": 1808.05311,
        "authors": "Alexander Lipton, Vadim Kaushansky, and Christoph Reisinger",
        "title": "Semi-analytical solution of a McKean-Vlasov equation with feedback\n  through hitting a boundary",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the non-linear diffusion equation associated with a\nparticle system where the common drift depends on the rate of absorption of\nparticles at a boundary. We provide an interpretation as a structural credit\nrisk model with default contagion in a large interconnected banking system.\nUsing the method of heat potentials, we derive a coupled system of Volterra\nintegral equations for the transition density and for the loss through\nabsorption. An approximation by expansion is given for a small interaction\nparameter. We also present a numerical solution algorithm and conduct\ncomputational tests.\n"
    },
    {
        "paper_id": 1808.05527,
        "authors": "Michael Polson and Vadim Sokolov",
        "title": "Deep Learning for Energy Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep Learning is applied to energy markets to predict extreme loads observed\nin energy grids. Forecasting energy loads and prices is challenging due to\nsharp peaks and troughs that arise due to supply and demand fluctuations from\nintraday system constraints. We propose deep spatio-temporal models and extreme\nvalue theory (EVT) to capture theses effects and in particular the tail\nbehavior of load spikes. Deep LSTM architectures with ReLU and $\\tanh$\nactivation functions can model trends and temporal dependencies while EVT\ncaptures highly volatile load spikes above a pre-specified threshold. To\nillustrate our methodology, we use hourly price and demand data from 4719 nodes\nof the PJM interconnection, and we construct a deep predictor. We show that\nDL-EVT outperforms traditional Fourier time series methods, both in-and\nout-of-sample, by capturing the observed nonlinearities in prices. Finally, we\nconclude with directions for future research.\n"
    },
    {
        "paper_id": 1808.0589,
        "authors": "Slobodan Milovanovi\\'c and Lina von Sydow",
        "title": "A High Order Method for Pricing of Financial Derivatives using Radial\n  Basis Function generated Finite Differences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the numerical pricing of financial derivatives\nusing Radial Basis Function generated Finite Differences in space. Such\ndiscretization methods have the advantage of not requiring Cartesian grids.\nInstead, the nodes can be placed with higher density in areas where there is a\nneed for higher accuracy. Still, the discretization matrix is fairly sparse. As\na model problem, we consider the pricing of European options in 2D. Since such\noptions have a discontinuity in the first derivative of the payoff function\nwhich prohibits high order convergence, we smooth this function using an\nestablished technique for Cartesian grids. Numerical experiments show that we\nacquire a fourth order scheme in space, both for the uniform and the nonuniform\nnode layouts that we use. The high order method with the nonuniform node layout\nachieves very high accuracy with relatively few nodes. This renders the\npotential for solving pricing problems in higher spatial dimensions since the\ncomputational memory and time demand become much smaller with this method\ncompared to standard techniques.\n"
    },
    {
        "paper_id": 1808.05893,
        "authors": "Marcel Ausloos, Francesca Bartolacci, Nicola G. Castellano, and Roy\n  Cerqueti",
        "title": "Exploring how innovation strategies at time of crisis influence\n  performance: a cluster analysis perspective",
        "comments": "22 pages, 4 Tables & 1 Figure=Table, 45 references; as prepared for\n  Technology Analysis & Strategic Management",
        "journal-ref": "Technology Analysis & Strategic Management 30(4), 484-497 (2018)",
        "doi": "10.1080/09537325.2017.1337889",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the connection between innovation activities of companies\n-- implemented before crisis -- and their performance -- measured at time of\ncrisis. The companies listed in the STAR Market Segment of the Italian Stock\nExchange are analyzed. Innovation is measured through the level of investments\nin total tangible and intangible fixed assets in 2006-2007, while performance\nis captured through growth -- expressed by variations of sales, total assets\nand employees -- profitability -- through ROI or ROS -- and productivity --\nthrough asset turnover or sales per employee in the period 2008-2010. The\nvariables of interest are analyzed and compared through statistical techniques\nand by adopting cluster analysis. In particular, a Voronoi tessellation is also\nimplemented in a varying centroids framework. In accord with a large part of\nthe literature, we find that the behavior of the performance of the companies\nis not univocal when they innovate.\n"
    },
    {
        "paper_id": 1808.06337,
        "authors": "Calisto Guambe, Rodwell Kufakunesu, Gusti Van Zyl and Conrad Beyers",
        "title": "Optimal asset allocation for a DC plan with partial information under\n  inflation and mortality risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an asset allocation stochastic problem with restriction for a\ndefined-contribution pension plan during the accumulation phase. We consider a\nfinancial market with stochastic interest rate, composed of a risk-free asset,\na real zero coupon bond price, the inflation-linked bond and the risky asset. A\nplan member aims to maximize the expected power utility derived from the\nterminal wealth. In order to protect the rights of a member who dies before\nretirement, we introduce a clause which allows to withdraw his premiums and the\ndifference is distributed among the survival members. Besides the mortality\nrisk, the fund manager takes into account the salary and the inflation risks.\nWe then obtain closed form solutions for the asset allocation problem using a\nsufficient maximum principle approach for the problem with partial information.\nFinally, we give a numerical example.\n"
    },
    {
        "paper_id": 1808.0643,
        "authors": "Jan Obloj, Johannes Wiesel",
        "title": "A unified Framework for Robust Modelling of Financial Markets in\n  discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We unify and establish equivalence between the pathwise and the quasi-sure\napproaches to robust modelling of financial markets in discrete time. In\nparticular, we prove a Fundamental Theorem of Asset Pricing and a Superhedging\nTheorem, which encompass the formulations of [Bouchard, B., & Nutz, M. (2015).\nArbitrage and duality in nondominated discrete-time models. The Annals of\nApplied Probability, 25(2), 823-859] and [Burzoni, M., Frittelli, M., Hou, Z.,\nMaggis, M., & Obloj, J. (2019). Pointwise arbitrage pricing theory in discrete\ntime. Mathematics of Operations Research]. In bringing the two streams of\nliterature together, we also examine and relate their many different notions of\narbitrage. We also clarify the relation between robust and classical\n$\\mathbb{P}$-specific results. Furthermore, we prove when a superhedging\nproperty w.r.t. the set of martingale measures supported on a set of paths\n$\\Omega$ may be extended to a pathwise superhedging on $\\Omega$ without\nchanging the superhedging price.\n"
    },
    {
        "paper_id": 1808.06718,
        "authors": "Edward Frees (for the Actuarial Community)",
        "title": "Loss Data Analytics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Loss Data Analytics is an interactive, online, freely available text. The\nidea behind the name Loss Data Analytics is to integrate classical loss data\nmodels from applied probability with modern analytic tools. In particular, we\nseek to recognize that big data (including social media and usage based\ninsurance) are here and high speed computation is readily available.\n  The online version contains many interactive objects (quizzes, computer\ndemonstrations, interactive graphs, video, and the like) to promote deeper\nlearning. A subset of the book is available for offline reading in pdf and EPUB\nformats. The online text will be available in multiple languages to promote\naccess to a worldwide audience.\n"
    },
    {
        "paper_id": 1808.07107,
        "authors": "Ben Hambly, Jasdeep Kalsi and James Newbury",
        "title": "Limit order books, diffusion approximations and reflected SPDEs: from\n  microscopic to macroscopic models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by a zero-intelligence approach, the aim of this paper is to\nconnect the microscopic (discrete price and volume), mesoscopic (discrete price\nand continuous volume) and macroscopic (continuous price and volume) frameworks\nfor the modelling of limit order books, with a view to providing a natural\nprobabilistic description of their behaviour in a high to ultra high-frequency\nsetting. Starting with a microscopic framework, we first examine the limiting\nbehaviour of the order book process when order arrival and cancellation rates\nare sent to infinity and when volumes are considered to be of infinitesimal\nsize. We then consider the transition between this mesoscopic model and a\nmacroscopic model for the limit order book, obtained by letting the tick size\ntend to zero. The macroscopic limit can then be described using reflected SPDEs\nwhich typically arise in stochastic interface models. We then use financial\ndata to discuss a possible calibration procedure for the model and illustrate\nnumerically how it can reproduce observed behaviour of prices. This could then\nbe used as a market simulator for short-term price prediction or for testing\noptimal execution strategies.\n"
    },
    {
        "paper_id": 1808.07339,
        "authors": "Ruodu Wang and Johanna F. Ziegel",
        "title": "Scenario-based Risk Evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk measures such as Expected Shortfall (ES) and Value-at-Risk (VaR) have\nbeen prominent in banking regulation and financial risk management. Motivated\nby practical considerations in the assessment and management of risks,\nincluding tractability, scenario relevance and robustness, we consider\ntheoretical properties of scenario-based risk evaluation. We propose several\nnovel scenario-based risk measures, including various versions of Max-ES and\nMax-VaR, and study their properties. We establish axiomatic characterizations\nof scenario-based risk measures that are comonotonic-additive or coherent and\nan ES-based representation result is obtained. These results provide a\ntheoretical foundation for the recent Basel III & IV market risk calculation\nformulas. We illustrate the theory with financial data examples.\n"
    },
    {
        "paper_id": 1808.07646,
        "authors": "Toma\\v{z} Ko\\v{s}ir, Matja\\v{z} Omladi\\v{c}",
        "title": "Reflected maxmin copulas and modelling quadrant subindependence",
        "comments": "30 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copula models have become popular in different applications, including\nmodeling shocks, in view of their ability to describe better the dependence\nconcepts in stochastic systems. The class of maxmin copulas was recently\nintroduced by Omladi\\v{c} and Ru\\v{z}i\\'{c}. It extends the well known classes\nof Marshall-Olkin and Marshall copulas by allowing the external shocks to have\ndifferent effects on the two components of the system. By a reflection (flip)\nin one of the variables we introduce a new class of bivariate copulas called\nreflected maxmin (RMM) copulas. We explore their properties and show that\nsymmetric RMM copulas relate to general RMM copulas similarly as do semilinear\ncopulas relate to Marshall copulas. We transfer that relation also to maxmin\ncopulas. We also characterize possible diagonal functions of symmetric RMM\ncopulas.\n"
    },
    {
        "paper_id": 1808.07737,
        "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, and Matja\\v{z} Omladi\\v{c}",
        "title": "Asymmetric linkages: maxmin vs. reflected maxmin copulas",
        "comments": "31 pages, 8 figures",
        "journal-ref": "Fuzzy sets and systems, vol. 393 (2020)",
        "doi": "10.1016/j.fss.2019.07.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce some new copulas emerging from shock models. It\nwas shown earlier that reflected maxmin copulas (RMM for short) are not just\nsome specific singular copulas; they contain many important absolutely\ncontinuous copulas including the negative quadrant dependent part of the\nEyraud-Farlie-Gumbel-Morgenstern class. The main goal of this paper is to\ndevelop the RMM copulas with dependent endogenous shocks and give evidence that\nRMM copulas may exhibit some characteristics better than the original maxmin\ncopulas (MM for short): (1) An important evidence for that is the iteration\nprocedure of the RMM transformation which we prove to be always convergent and\nwe give many properties of it that are useful in applications. (2) Using this\nresult we find also the limit of the iteration procedure of the MM\ntransformation thus answering a question proposed earlier by Durante,\nOmladi\\v{c}, Ora\\v{z}em, and Ru\\v{z}i\\'{c}. (3) We give the multivariate\ndependent RMM copula that compares to the MM version given by Durante,\nOmladi\\v{c}, Ora\\v{z}em, and Ru\\v{z}i\\'{c}. In all our copulas the\nidiosyncratic and systemic shocks are combined via asymmetric linking functions\nas opposed to Marshall copulas where symmetric linking functions are used.\n"
    },
    {
        "paper_id": 1808.07854,
        "authors": "Fabi\\'an Riquelme, Pablo Gonz\\'alez-Cantergiani, Gabriel Godoy",
        "title": "Voting power of political parties in the Senate of Chile during the\n  whole binomial system period: 1990-2017",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The binomial system is an electoral system unique in the world. It was used\nto elect the senators and deputies of Chile during 27 years, from the return of\ndemocracy in 1990 until 2017. In this paper we study the real voting power of\nthe different political parties in the Senate of Chile during the whole\nbinomial period. We not only consider the different legislative periods, but\nalso any party changes between one period and the next. The real voting power\nis measured by considering power indices from cooperative game theory, which\nare based on the capability of the political parties to form winning\ncoalitions. With this approach, we can do an analysis that goes beyond the\nsimple count of parliamentary seats.\n"
    },
    {
        "paper_id": 1808.07909,
        "authors": "Matheus R. Grasselli, Alexander Lipton",
        "title": "On the Normality of Negative Interest Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that a negative interest rate policy (NIRP) can be an effect tool\nfor macroeconomic stabilization. We first discuss how implementing negative\nrates on reserves held at a central bank does not pose any theoretical\ndifficulty, with a reduction in rates operating in exactly the same way when\nrates are positive or negative, and show that this is compatible with an\nendogenous money point of view. We then propose a simplified stock-flow\nconsistent macroeconomic model where rates are allowed to become arbitrarily\nnegative and present simulation evidence for their stabilizing effects. In\npractice, the existence of physical cash imposes a lower bound for interest\nrates, which in our view is the main reason for the lack of effectiveness of\nnegative interest rates in the countries that adopted them as part of their\nmonetary policy. We conclude by discussing alternative ways to overcome this\nlower bound , in particular the use of central bank digital currencies.\n"
    },
    {
        "paper_id": 1808.07949,
        "authors": "Jorge Faleiro, Edward Tsang",
        "title": "Black Magic Investigation Made Simple: Monte Carlo Simulations and\n  Historical Back Testing of Momentum Cross-Over Strategies Using FRACTI\n  Patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  To promote economic stability, finance should be studied as a hard science,\nwhere scientific methods apply. When a trading strategy is proposed, the\nunderlying model should be transparent and defined robustly to allow other\nresearchers to understand and examine it thoroughly. Like any hard sciences,\nresults must be repeatable to allow researchers to collaborate, and build upon\neach other's results. Large-scale collaboration, when applying the steps of\nscientific investigation, is an efficient way to leverage \"crowd science\" to\naccelerate research in finance. In this paper, we demonstrate how a real world\nproblem in economics, an old problem still subject to a lot of debate, can be\nsolved by the application of a crowd-powered, collaborative scientific\ncomputational framework, fully supporting the process of investigation dictated\nby the modern scientific method. This paper provides a real end-to-end example\nof investigation to illustrate the use of the framework. We intentionally\nselected an example that is self-contained, complete, simple, accessible, and\nof constant debate in both academia and the industry: the performance of a\ntrading strategy used commonly in technical analysis. Claims of efficiency in\ntechnical analysis, referred derisively by some sources as \"Black Magic\", are\nof widespread use in mainstream media and usually met with a lot of\ncontroversy. In this paper we show that different researchers assess this\nstrategy differently, and the subsequent debate is due more to the lack of\nmethod than purpose. Most results reported are not repeatable by other\nresearchers. This is not satisfactory if we intend to approach finance as a\nhard science. To counterweight the status quo, we demonstrate what one could do\nby using collaborative and investigative features of contributions and\nleveraging the power of crowds.\n"
    },
    {
        "paper_id": 1808.07959,
        "authors": "Jorge Faleiro, Edward Tsang",
        "title": "Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual\n  Framework for Large-Scale Collaboration and Transparent Investigation in\n  Financial Markets",
        "comments": "14th Simulation and Analytics Seminar, Bank of Finland, Helsinki,\n  2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Modern investigation in economics and in other sciences requires the ability\nto store, share, and replicate results and methods of experiments that are\noften multidisciplinary and yield a massive amount of data. Given the\nincreasing complexity and growing interaction across diverse bodies of\nknowledge it is becoming imperative to define a platform to properly support\ncollaborative research and track origin, accuracy and use of data. This paper\nstarts by defining a set of methods leveraging scientific principles and\nadvocating the importance of those methods in multidisciplinary, computer\nintensive fields like computational finance. The next part of this paper\ndefines a class of systems called scientific support systems, vis-a-vis usages\nin other research fields such as bioinformatics, physics and engineering. We\noutline a basic set of fundamental concepts, and list our goals and motivation\nfor leveraging such systems to enable large-scale investigation, \"crowd powered\nscience\", in economics. The core of this paper provides an outline of FRACTI in\nfive steps. First we present definitions related to scientific support systems\nintrinsic to finance and describe common characteristics of financial use\ncases. The second step concentrates on what can be exchanged through the\ndefinition of shareable entities called contributions. The third step is the\ndescription of a classification system for building blocks of the conceptual\nframework, called facets. The fourth step introduces the meta-model that will\nenable provenance tracking and representation of data fragments and simulation.\nFinally we describe intended cases of use to highlight main strengths of\nFRACTI: application of the scientific method for investigation in computational\nfinance, large-scale collaboration and simulation.\n"
    },
    {
        "paper_id": 1808.08054,
        "authors": "Kenjiro Oya",
        "title": "The Co-Terminal Swap Market Model with Bergomi Stochastic Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we apply the forward variance modeling approach by L.Bergomi\nto the co-terminal swap market model. We build an interest rate model for which\nall the market price changes of hedging instruments, interest rate swaps and\nEuropean swaptions, are interpreted as the state variable variations, and no\ndiffusion parameter calibration procedure is required. The model provides quite\nsimple profit and loss (PnL) formula, with which we can easily understand where\na material PnL trend comes from when it appears, and consider how we should\nmodify the model parameters. The model has high flexibility to control the\nmodel dynamics because parameter calibration is unnecessary and the model\nparameters can be used solely for the purpose of the model dynamics control.\nWith the model, the position management of the exotic interest rate products,\ne.g. Bermudan swaptions, can be carried out in a more sophisticated and\nsystematic manner. A numerical experiment is performed to show the\neffectiveness of the approach for a Canary swaption, which is a special form of\na Bermudan swaption.\n"
    },
    {
        "paper_id": 1808.08221,
        "authors": "Ignacio Ruiz, Mariano Zeron",
        "title": "Dynamic Initial Margin via Chebyshev Tensors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present two methods, based on Chebyshev tensors, to compute dynamic\nsensitivities of financial instruments within a Monte Carlo simulation. These\nmethods are implemented and run in a Monte Carlo engine to compute Dynamic\nInitial Margin as defined by ISDA (SIMM). We show that the levels of accuracy,\nspeed and implementation efforts obtained, compared to the benchmark (DIM\nobtained calling pricing functions such as are found in risk engines), are\nbetter than those obtained by alternative methods presented in the literature,\nsuch as regressions (\\cite{Zhu Chan}) and Deep Neural Nets (\\cite{DNNs IM}).\n"
    },
    {
        "paper_id": 1808.08249,
        "authors": "Orazio Angelini, Tiziana Di Matteo",
        "title": "Complexity of products: the effect of data regularisation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e20110814",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Among several developments, the field of Economic Complexity (EC) has notably\nseen the introduction of two new techniques. One is the Bootstrapped Selective\nPredictability Scheme (SPSb), which can provide quantitative forecasts of the\nGross Domestic Product of countries. The other, Hidden Markov Model (HMM)\nregularisation, denoises the datasets typically employed in the literature. We\ncontribute to EC along three different directions. First, we prove the\nconvergence of the SPSb algorithm to a well-known statistical learning\ntechnique known as Nadaraya-Watson Kernel regression. The latter has\nsignificantly lower time complexity, produces deterministic results, and it is\ninterchangeable with SPSb for the purpose of making predictions. Second, we\nstudy the effects of HMM regularization on the Product Complexity and logPRODY\nmetrics, for which a model of time evolution has been recently proposed. We\nfind confirmation for the original interpretation of the logPRODY model as\ndescribing the change in the global market structure of products with new\ninsights allowing a new interpretation of the Complexity measure, for which we\npropose a modification. Third, we explore new effects of regularisation on the\ndata. We find that it reduces noise, and observe for the first time that it\nincreases nestedness in the export network adjacency matrix.\n"
    },
    {
        "paper_id": 1808.08563,
        "authors": "Xingwei Hu",
        "title": "A Dichotomous Analysis of Unemployment Welfare",
        "comments": "47 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an economy which could not accommodate the full employment of its labor\nforce, it employs some labor but does not employ others. The bipartition of the\nlabor force is random, and we characterize it by an axiom of equal employment\nopportunity. We value each employed individual by his or her marginal\ncontribution to the production function; we also value each unemployed\nindividual by the potential marginal contribution the person would make if the\nmarket hired the individual. We then use the aggregate individual value to\ndistribute the net production to the unemployment welfare and the employment\nbenefits. Using real-time balanced-budget rule as a constraint and policy\nstability as an objective, we derive a scientific formula which describes a\nfair, debt-free, and asymptotic risk-free tax rate for any given unemployment\nrate and national spending level. The tax rate minimizes the asymptotic mean,\nvariance, semi-variance, and mean absolute deviation of the underlying\nposterior unemployment rate. The allocation rule stimulates employment and\nboosts productivity. Under some symmetry assumptions, we even find that an\nunemployed person should enjoy equivalent employment benefits, and the tax rate\ngoes with this welfare equality. The tool employed is the cooperative game\ntheory in which we assume many players. The players are randomly bipartitioned,\nand the payoff varies with the partition. One could apply the fair distribution\nrule and valuation approach to other profit-sharing or cost-sharing situations\nwith these characteristics. This framework is open to alternative\nidentification strategies and other forms of equal opportunity axiom.\n"
    },
    {
        "paper_id": 1808.08585,
        "authors": "Jiaqi Liang, Linjing Li, Daniel Zeng",
        "title": "Evolutionary dynamics of cryptocurrency transaction networks: An\n  empirical study",
        "comments": null,
        "journal-ref": "PLoS ONE 13(8): e0202202 (2018)",
        "doi": "10.1371/journal.pone.0202202",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrency is a well-developed blockchain technology application that is\ncurrently a heated topic throughout the world. The public availability of\ntransaction histories offers an opportunity to analyze and compare different\ncryptocurrencies. In this paper, we present a dynamic network analysis of three\nrepresentative blockchain-based cryptocurrencies: Bitcoin, Ethereum, and\nNamecoin. By analyzing the accumulated network growth, we find that, unlike\nmost other networks, these cryptocurrency networks do not always densify over\ntime, and they are changing all the time with relatively low node and edge\nrepetition ratios. Therefore, we then construct separate networks on a monthly\nbasis, trace the changes of typical network characteristics (including degree\ndistribution, degree assortativity, clustering coefficient, and the largest\nconnected component) over time, and compare the three. We find that the degree\ndistribution of these monthly transaction networks cannot be well fitted by the\nfamous power-law distribution, at the same time, different currency still has\ndifferent network properties, e.g., both Bitcoin and Ethereum networks are\nheavy-tailed with disassortative mixing, however, only the former can be\ntreated as a small world. These network properties reflect the evolutionary\ncharacteristics and competitive power of these three cryptocurrencies and\nprovide a foundation for future research.\n"
    },
    {
        "paper_id": 1808.08717,
        "authors": "Ashwin K Seshadri",
        "title": "Economics of carbon-dioxide abatement under an exogenous constraint on\n  cumulative emissions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fossil-fuel induced contribution to further warming over the 21st century\nwill be determined largely by integrated CO2 emissions over time rather than\nthe precise timing of the emissions, with a relation of near-proportionality\nbetween global warming and cumulative CO2 emissions. This paper examines\noptimal abatement pathways under an exogenous constraint on cumulative\nemissions. Least cost abatement pathways have carbon tax rising at the\nrisk-free interest rate, but if endogenous learning or climate damage costs are\nincluded in the analysis, the carbon tax grows more slowly. The inclusion of\ndamage costs in the optimization leads to a higher initial carbon tax, whereas\nthe effect of learning depends on whether it appears as an additive or\nmultiplicative contribution to the marginal cost curve. Multiplicative models\nare common in the literature and lead to delayed abatement and a smaller\ninitial tax. The required initial carbon tax increases with the cumulative\nabatement goal and is higher for lower interest rates. Delaying the start of\nabatement is costly owing to the increasing marginal abatement cost. Lower\ninterest rates lead to higher relative costs of delaying abatement because\nthese induce higher abatement rates early on. The fraction of business-as-usual\nemissions (BAU) avoided in optimal pathways increases for low interest rates\nand rapid growth of the abatement cost curve, which allows a lower threshold\nglobal warming goal to become attainable without overshoot in temperature. Each\nyear of delay in starting abatement raises this threshold by an increasing\namount, because the abatement rate increases exponentially with time.\n"
    },
    {
        "paper_id": 1808.09279,
        "authors": "Bikas K. Chakrabarti",
        "title": "Econophysics as conceived by Meghnad Saha",
        "comments": "9 pages, 1 figure (to be published in Spl. issue on Prof. M. N. Saha\n  in Science & Culture)",
        "journal-ref": "Science & Culture, Vol. 84, Nov.-Dec. 2018, pp. 365-369",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We trace the initiative by Professor Meghnad Saha to develop a (statistical)\nphysics model of market economy and his search for the mechanism to constrain\nthe entropy maximized width of the income distribution in a society such that\nthe spread of inequality can be minimized.\n"
    },
    {
        "paper_id": 1808.09378,
        "authors": "John Armstrong, Claudio Bellani, Damiano Brigo, Thomas Cass",
        "title": "Option pricing models without probability: a rough paths approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the pricing and hedging of financial options without the use of\nprobability using rough paths. By encoding the volatility of assets in an\nenhancement of the price trajectory, we give a pathwise presentation of the\nreplication of European options. The continuity properties of rough-paths allow\nus to generalise the so-called fundamental theorem of derivative trading,\nshowing that a small misspecification of the model will yield only a small\nexcess profit or loss of the replication strategy. Our hedging strategy is an\nenhanced version of classical delta hedging where we use volatility swaps to\nhedge the second order terms arising in rough-path integrals, resulting in\nimproved robustness.\n"
    },
    {
        "paper_id": 1808.09382,
        "authors": "Josselin Garnier and Knut Solna",
        "title": "Emergence of Turbulent Epochs in Oil Prices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2019.03.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oil price data have a complicated multi-scale structure that may vary with\ntime. We use time-frequency analysis to identify the main features of these\nvariations and, in particular, the regime shifts. The analysis is based on a\nwavelet-based decomposition and analysis of the associated scale spectrum. The\njoint estimation of the local Hurst exponent and volatility is the key to\ndetect and identify regime shifting and switching of the oil price. The\nframework involves in particular modeling in terms of a process of\n`multi-fractional' type so that both the roughness and the volatility of the\nprice process may vary with time. Special epochs then emerge as a result of\nthese degrees of freedom, moreover, as a result of the special type of spectral\nestimator used. These special epochs are discussed and related to historical\nevents. Some of them are not detected by standard analysis based on maximum\nlikelihood estimation. The paper presents a novel algorithm for robust\ndetection of such special epochs and multi-fractional behavior in financial or\nother types of data. In the financial context insight about such behavior of\nthe asset price is important to evaluate financial contracts involving the\nasset.\n"
    },
    {
        "paper_id": 1808.09666,
        "authors": "Carol Alexander, Emese Lazar, Silvia Stanescu",
        "title": "Analytic Moments for GARCH Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a GJR-GARCH specification with a generic innovation distribution we\nderive analytic expressions for the first four conditional moments of the\nforward and aggregated returns and variances. Moment for the most commonly used\nGARCH models are stated as special cases. We also the limits of these moments\nas the time horizon increases, establishing regularity conditions for the\nmoments of aggregated returns to converge to normal moments. Our empirical\nstudy yields excellent approximate predictive distributions from these analytic\nmoments, thus precluding the need for time-consuming simulations.\n"
    },
    {
        "paper_id": 1808.09677,
        "authors": "Lorenzo Dall'Amico, Antoine Fosset, Jean-Philippe Bouchaud, Michael\n  Benzaquen",
        "title": "How does latent liquidity get revealed in the limit order book?",
        "comments": "18 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aaf10e",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Latent order book models have allowed for significant progress in our\nunderstanding of price formation in financial markets. In particular they are\nable to reproduce a number of stylized facts, such as the square-root impact\nlaw. An important question that is raised -- if one is to bring such models\ncloser to real market data -- is that of the connection between the latent\n(unobservable) order book and the real (observable) order book. Here we suggest\na simple, consistent mechanism for the revelation of latent liquidity that\nallows for quantitative estimation of the latent order book from real market\ndata. We successfully confront our results to real order book data for over a\nhundred assets and discuss market stability. One of our key theoretical results\nis the existence of a market instability threshold, where the conversion of\nlatent order becomes too slow, inducing liquidity crises. Finally we compute\nthe price impact of a metaorder in different parameter regimes.\n"
    },
    {
        "paper_id": 1808.09685,
        "authors": "Emanuele Nastasi, Andrea Pallavicini, Giulio Sartorelli",
        "title": "Smile Modelling in Commodity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a stochastic-local volatility model for derivative contracts on\ncommodity futures able to describe forward-curve and smile dynamics with a fast\ncalibration to liquid market quotes. A parsimonious parametrization is\nintroduced to deal with the limited number of options quoted in the market.\nCleared commodity markets for futures and options are analyzed to include in\nthe pricing framework specific trading clauses and margining procedures.\nNumerical examples for calibration and pricing are provided for different\ncommodity products.\n"
    },
    {
        "paper_id": 1808.09686,
        "authors": "Samuel N. Cohen, Timo Henckel, Gordon D. Menzies, Johannes\n  Muhle-Karbe, Daniel J. Zizzo",
        "title": "Switching Cost Models as Hypothesis Tests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We relate models based on costs of switching beliefs (e.g. due to\ninattention) to hypothesis tests. Specifically, for an inference problem with a\npenalty for mistakes and for switching the inferred value, a band of inaction\nis optimal. We show this band is equivalent to a confidence interval, and\ntherefore to a two-sided hypothesis test.\n"
    },
    {
        "paper_id": 1808.09698,
        "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, and Matja\\v{z} Omladi\\v{c}",
        "title": "Non-exchangeability of copulas arising from shock models",
        "comments": "The latest version (V.4) contains a correction in Theorem 3.1 and\n  Remark 3.3 compared to the printed version in the journal and to the previous\n  version on arXiv. Functions $P_{\\lambda}$ in the earlier version of Theorem\n  3.1 are not copulas as claimed. We wish to thank Professor Piotr Jaworski for\n  pointing out the fact. (31 pages, 14 figures)",
        "journal-ref": "Journal of Computational and Applied Mathematics, Vol. 358 (2019),\n  61-83",
        "doi": "10.1016/j.cam.2019.02.031",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When choosing the right copula for our data a key point is to distinguish the\nfamily that describes it at the best. In this respect, a better choice of the\ncopulas could be obtained through the information about the (non)symmetry of\nthe data. Exchangeability as a probability concept (first next to independence)\nhas been studied since 1930's, copulas have been studied since 1950's, and even\nthe most important class of copulas from the point of view of applications,\ni.e. the ones arising from shock models s.a. Marshall's copulas, have been\nstudied since 1960's. However, the point of non-exchangeability of copulas was\nbrought up only in 2006 and has been intensively studied ever since. One of the\nmain contributions of this paper is the maximal asymmetry function for a family\nof copulas. We compute this function for the major families of shock-based\ncopulas, i.e. Marshall, maxmin and reflected maxmin (RMM for short) copulas and\nalso for some other important families. We compute the sharp bound of asymmetry\nmeasure $\\mu_\\infty$, the most important of the asymmetry measures, for the\nfamily of Marshall copulas and the family of maxmin copulas, which both equal\nto $\\frac{4}{27}\\ (\\approx 0.148)$. One should compare this bound to the one\nfor the class of PQD copulas to which they belong, which is $3-2\\sqrt{2}\\\n\\approx 0.172)$, and to the general bound for all copulas that is $\\frac13$.\nFurthermore, we give the sharp bound of the same asymmetry measure for RMM\ncopulas which is $3-2\\sqrt{2}$, compared to the same bound for NQD copulas,\nwhere they belong, which is $\\sqrt{5}-2\\ (\\approx 0.236)$. One of our main\nresults is also the statistical interpretation of shocks in a given model at\nwhich the maximal asymmetry measure bound is attained. These interpretations\nfor the three families studied are illustrated by examples that should be\nhelpful to practitioners when choosing the model for their data.\n"
    },
    {
        "paper_id": 1808.09807,
        "authors": "Peter Bank and Yan Dolinsky",
        "title": "Continuous-time Duality for Super-replication with Transient Price\n  Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a super-replication duality in a continuous-time financial model\nwhere an investor's trades adversely affect bid- and ask-prices for a risky\nasset and where market resilience drives the resulting spread back towards zero\nat an exponential rate. Similar to the literature on models with a constant\nspread, our dual description of super-replication prices involves the\nconstruction of suitable absolutely continuous measures with martingales close\nto the unaffected reference price. A novel feature in our duality is a\nliquidity weighted $L^2$-norm that enters as a measurement of this closeness\nand that accounts for strategy dependent spreads. As applications, we establish\noptimality of buy-and-hold strategies for the super-replication of call options\nand we prove a verification theorem for utility maximizing investment\nstrategies.\n"
    },
    {
        "paper_id": 1808.09887,
        "authors": "Andres Gonzalez Lira and Ahmed Mushfiq Mobarak",
        "title": "Enforcing Regulation Under Illicit Adaptation",
        "comments": "63 pages; Keywords: Enforcement, Regulation, Law and Economics,\n  Fisheries; JEL Classification Codes: K42, O1, L51",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Attempts to curb illegal activity by enforcing regulations gets complicated\nwhen agents react to the new regulatory regime in unanticipated ways to\ncircumvent enforcement. We present a research strategy that uncovers such\nreactions, and permits program evaluation net of such adaptive behaviors. Our\ninterventions were designed to reduce over-fishing of the critically endangered\nPacific hake by either (a) monitoring and penalizing vendors that sell illegal\nfish or (b) discouraging consumers from purchasing using an information\ncampaign. Vendors attempt to circumvent the ban through hidden sales and other\nmeans, which we track using mystery shoppers. Instituting random monitoring\nvisits are much more effective in reducing true hake availability by limiting\nsuch cheating, compared to visits that occur on a predictable schedule.\nMonitoring at higher frequency (designed to limit temporal displacement of\nillegal sales) backfires, because targeted agents learn faster, and cheat more\neffectively. Sophisticated policy design is therefore crucial for determining\nthe sustained, longer-term effects of enforcement. Data collected from\nfishermen, vendors, and consumers allow us to document the upstream,\ndownstream, spillover, and equilibrium effects of enforcement on the entire\nsupply chain. The consumer information campaign generates two-thirds of the\ngains compared to random monitoring, but is simpler for the government to\nimplement and almost as cost-effective.\n"
    },
    {
        "paper_id": 1808.0994,
        "authors": "Zhipeng Liang, Hao Chen, Junhao Zhu, Kangkang Jiang, Yanran Li",
        "title": "Adversarial Deep Reinforcement Learning in Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we implement three state-of-art continuous reinforcement\nlearning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy\nOptimization (PPO) and Policy Gradient (PG)in portfolio management. All of them\nare widely-used in game playing and robot control. What's more, PPO has\nappealing theoretical propeties which is hopefully potential in portfolio\nmanagement. We present the performances of them under different settings,\nincluding different learning rates, objective functions, feature combinations,\nin order to provide insights for parameters tuning, features selection and data\npreparation. We also conduct intensive experiments in China Stock market and\nshow that PG is more desirable in financial market than DDPG and PPO, although\nboth of them are more advanced. What's more, we propose a so called Adversarial\nTraining method and show that it can greatly improve the training efficiency\nand significantly promote average daily return and sharpe ratio in back test.\nBased on this new modification, our experiments results show that our agent\nbased on Policy Gradient can outperform UCRP.\n"
    },
    {
        "paper_id": 1808.1009,
        "authors": "Abhijit Chakraborty, Yuichi Kichikawa, Takashi Iino, Hiroshi Iyetomi,\n  Hiroyasu Inoue, Yoshi Fujiwara, Hideaki Aoyama",
        "title": "Hierarchical communities in the walnut structure of the Japanese\n  production network",
        "comments": "38 pages, 17 figures",
        "journal-ref": "PLoS ONE 13(8): e0202739 (2018)",
        "doi": "10.1371/journal.pone.0202739",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the structure of the Japanese production network, which\nincludes one million firms and five million supplier-customer links. This study\nfinds that this network forms a tightly-knit structure with a core giant\nstrongly connected component (GSCC) surrounded by IN and OUT components\nconstituting two half-shells of the GSCC, which we call a\\textit{walnut}\nstructure because of its shape. The hierarchical structure of the communities\nis studied by the Infomap method, and most of the irreducible communities are\nfound to be at the second level. The composition of some of the major\ncommunities, including overexpressions regarding their industrial or regional\nnature, and the connections that exist between the communities are studied in\ndetail. The findings obtained here cause us to question the validity and\naccuracy of using the conventional input-output analysis, which is expected to\nbe useful when firms in the same sectors are highly connected to each other.\n"
    },
    {
        "paper_id": 1808.10355,
        "authors": "Julia Eisenberg and Yuliya Mishura",
        "title": "An Exponential Cox-Ingersoll-Ross Process as Discounting Factor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an economic agent (a household or an insurance company) modelling\nits surplus process by a deterministic process or by a Brownian motion with\ndrift. The goal is to maximise the expected discounted spendings/dividend\npayments, given that the discounting factor is given by an exponential CIR\nprocess. In the deterministic case, we are able to find explicit expressions\nfor the optimal strategy and the value function. For the Brownian motion case,\nwe offer a method allowing to show that for a small volatility the optimal\nstrategy is a constant-barrier strategy.\n"
    },
    {
        "paper_id": 1808.10428,
        "authors": "Angelica Sbardella, Emanuele Pugliese, Andrea Zaccaria, and Pasquale\n  Scaramozzino",
        "title": "The role of complex analysis in modeling economic growth",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e20110883",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Development and growth are complex and tumultuous processes. Modern economic\ngrowth theories identify some key determinants of economic growth. However, the\nrelative importance of the determinants remains unknown, and additional\nvariables may help clarify the directions and dimensions of the interactions.\nThe novel stream of literature on economic complexity goes beyond aggregate\nmeasures of productive inputs, and considers instead a more granular and\nstructural view of the productive possibilities of countries, i.e. their\ncapabilities. Different endowments of capabilities are crucial ingredients in\nexplaining differences in economic performances. In this paper we employ\neconomic fitness, a measure of productive capabilities obtained through complex\nnetwork techniques. Focusing on the combined roles of fitness and some more\ntraditional drivers of growth, we build a bridge between economic growth\ntheories and the economic complexity literature. Our findings, in agreement\nwith other recent empirical studies, show that fitness plays a crucial role in\nfostering economic growth and, when it is included in the analysis, can be\neither complementary to traditional drivers of growth or can completely\novershadow them.\n"
    },
    {
        "paper_id": 1808.10651,
        "authors": "Jaap H. Abbring and {\\O}ystein Daljord",
        "title": "Identifying the Discount Factor in Dynamic Discrete Choice Models",
        "comments": "39 pages",
        "journal-ref": "Quantitative Economics 11(2) 471-501 (May 2020)",
        "doi": "10.3982/QE1352",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical research often cites observed choice responses to variation that\nshifts expected discounted future utilities, but not current utilities, as an\nintuitive source of information on time preferences. We study the\nidentification of dynamic discrete choice models under such economically\nmotivated exclusion restrictions on primitive utilities. We show that each\nexclusion restriction leads to an easily interpretable moment condition with\nthe discount factor as the only unknown parameter. The identified set of\ndiscount factors that solves this condition is finite, but not necessarily a\nsingleton. Consequently, in contrast to common intuition, an exclusion\nrestriction does not in general give point identification. Finally, we show\nthat exclusion restrictions have nontrivial empirical content: The implied\nmoment conditions impose restrictions on choices that are absent from the\nunconstrained model.\n"
    },
    {
        "paper_id": 1809.00082,
        "authors": "Anastasis Kratsios and Cody Hyndman",
        "title": "NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation",
        "comments": "28 pages: main body, 24 pages: appendix, 8 Figures, 11 Tables",
        "journal-ref": "Journal of Machine Learning Research (JMLR), Volume: 22; 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data.\n"
    },
    {
        "paper_id": 1809.00128,
        "authors": "Weike Zhang, Jiang Du, Xiaoli Tian",
        "title": "Finding a promising venture capital project with todim under\n  probabilistic hesitant fuzzy circumstance",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Considering the risk aversion for gains and the risk seeking for losses of\nventure capitalists, the TODIM has been chosen as the decision-making method.\nMoreover, group decision is an available way to avoid the limited ability and\nknowledge etc. of venture capitalists.Simultaneously, venture capitalists may\nbe hesitant among several assessed values with different probabilities to\nexpress their real perceptionbecause of the uncertain decision-making\nenvironment. However, the probabilistic hesitant fuzzy information can solve\nsuch problems effectively. Therefore, the TODIM has been extended to\nprobabilistic hesitant fuzzy circumstance for the sake of settling the\ndecision-making problem of venture capitalists in this paper. Moreover, due to\nthe uncertain investment environment, the criteria weights are considered as\nprobabilistic hesitant fuzzyinformation as well. Then, a case study has been\nused to verify the feasibility and validity of the proposed TODIM.Also, the\nTODIM with hesitant fuzzy information has been carried out to analysis the same\ncase.From the comparative analysis, the superiority of the proposed TODIM in\nthis paper has already appeared.\n"
    },
    {
        "paper_id": 1809.00149,
        "authors": "Tigran Atoyan",
        "title": "Model-free trading and hedging with continuous price paths",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide a model-independent extension of the paradigm of\ndynamic hedging of derivative claims. We relate model-independent replication\nstrategies to local martingales having a closed form which we can characterise\nvia solutions of coupled PDEs. We provide a general framework and then apply it\nto a market with no traded claims, a market with an underlying asset and a\nconvex claim and a market with an underlying asset and a set of co-maturing\ncall options. The results encompass known examples of model-independent\nidentities and provide a methodology for deriving new identities.\n"
    },
    {
        "paper_id": 1809.00306,
        "authors": "Xi Zhang and Yixuan Li and Senzhang Wang and Binxing Fang and Philip\n  S. Yu",
        "title": "Enhancing Stock Market Prediction with Extended Coupled Hidden Markov\n  Model over Multi-Sourced Data",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional stock market prediction methods commonly only utilize the\nhistorical trading data, ignoring the fact that stock market fluctuations can\nbe impacted by various other information sources such as stock related events.\nAlthough some recent works propose event-driven prediction approaches by\nconsidering the event data, how to leverage the joint impacts of multiple data\nsources still remains an open research problem. In this work, we study how to\nexplore multiple data sources to improve the performance of the stock\nprediction. We introduce an Extended Coupled Hidden Markov Model incorporating\nthe news events with the historical trading data. To address the data sparsity\nissue of news events for each single stock, we further study the fluctuation\ncorrelations between the stocks and incorporate the correlations into the model\nto facilitate the prediction task. Evaluations on China A-share market data in\n2016 show the superior performance of our model against previous methods.\n"
    },
    {
        "paper_id": 1809.00695,
        "authors": "Marian Gidea, Daniel Goldsmith, Yuri Katz, Pablo Roldan, Yonah Shmalo",
        "title": "Topological recognition of critical transitions in time series of\n  cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the time series of four major cryptocurrencies (Bitcoin, Ethereum,\nLitecoin, and Ripple) before the digital market crash at the end of 2017 -\nbeginning 2018. We introduce a methodology that combines topological data\nanalysis with a machine learning technique -- $k$-means clustering -- in order\nto automatically recognize the emerging chaotic regime in a complex system\napproaching a critical transition. We first test our methodology on the complex\nsystem dynamics of a Lorenz-type attractor, and then we apply it to the four\nmajor cryptocurrencies. We find early warning signals for critical transitions\nin the cryptocurrency markets, even though the relevant time series exhibit a\nhighly erratic behavior.\n"
    },
    {
        "paper_id": 1809.00741,
        "authors": "Eitan Sapiro-Gheiler",
        "title": "\"Read My Lips\": Using Automatic Text Analysis to Classify Politicians by\n  Party and Ideology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing digitization of political speech has opened the door to\nstudying a new dimension of political behavior using text analysis. This work\ninvestigates the value of word-level statistical data from the US Congressional\nRecord--which contains the full text of all speeches made in the US\nCongress--for studying the ideological positions and behavior of senators.\nApplying machine learning techniques, we use this data to automatically\nclassify senators according to party, obtaining accuracy in the 70-95% range\ndepending on the specific method used. We also show that using text to predict\nDW-NOMINATE scores, a common proxy for ideology, does not improve upon these\nalready-successful results. This classification deteriorates when applied to\ntext from sessions of Congress that are four or more years removed from the\ntraining set, pointing to a need on the part of voters to dynamically update\nthe heuristics they use to evaluate party based on political speech. Text-based\npredictions are less accurate than those based on voting behavior, supporting\nthe theory that roll-call votes represent greater commitment on the part of\npoliticians and are thus a more accurate reflection of their ideological\npreferences. However, the overall success of the machine learning approaches\nstudied here demonstrates that political speeches are highly predictive of\npartisan affiliation. In addition to these findings, this work also introduces\nthe computational tools and methods relevant to the use of political speech\ndata.\n"
    },
    {
        "paper_id": 1809.00817,
        "authors": "Carol Alexander and Xi Chen",
        "title": "Model Risk in Real Option Valuation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general decision tree framework to value an option to\ninvest/divest in a project, focusing on the model risk inherent in the\nassumptions made by standard real option valuation methods. We examine how real\noption values depend on the dynamics of project value and investment costs, the\nfrequency of exercise opportunities, the size of the project relative to\ninitial wealth, the investor's risk tolerance (and how it changes with wealth)\nand several other choices about model structure. For instance, contrary to\nstylized facts from previous literature, real option values can actually\ndecrease with the volatility of the underlying project value and increase with\ninvestment costs.\n"
    },
    {
        "paper_id": 1809.0082,
        "authors": "Jun-ichi Maskawa, Koji Kuroda and Joshin Murai",
        "title": "Multiplicative random cascades with additional stochastic process in\n  financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multiplicative random cascade model naturally reproduces the intermittency or\nmultifractality, which is frequently shown among hierarchical complex systems\nsuch as turbulence and financial markets. As described herein, we investigate\nthe validity of a multiplicative hierarchical random cascade model through an\nempirical study using financial data. Although the intermittency and\nmultifractality of the time series are verified, random multiplicative factors\nlinking successive hierarchical layers show strongly negative correlation. We\nextend the multiplicative model to incorporate an additional stochastic term.\nResults show that the proposed model is consistent with all the empirical\nresults presented here.\n"
    },
    {
        "paper_id": 1809.00885,
        "authors": "Hirdesh K. Pharasi, Kiran Sharma, Rakesh Chatterjee, Anirban\n  Chakraborti, Francois Leyvraz and Thomas H. Seligman",
        "title": "Identifying long-term precursors of financial market crashes using\n  correlation patterns",
        "comments": "30 pages, 10 figures, including supplementary figures and information",
        "journal-ref": null,
        "doi": "10.1088/1367-2630/aae7e0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study of the critical dynamics in complex systems is always interesting\nyet challenging. Here, we choose financial market as an example of a complex\nsystem, and do a comparative analyses of two stock markets - the S&P 500 (USA)\nand Nikkei 225 (JPN). Our analyses are based on the evolution of\ncrosscorrelation structure patterns of short time-epochs for a 32-year period\n(1985-2016). We identify \"market states\" as clusters of similar correlation\nstructures, which occur more frequently than by pure chance (randomness). The\ndynamical transitions between the correlation structures reflect the evolution\nof the market states. Power mapping method from the random matrix theory is\nused to suppress the noise on correlation patterns, and an adaptation of the\nintra-cluster distance method is used to obtain the \"optimum\" number of market\nstates. We find that the USA is characterized by four market states and JPN by\nfive. We further analyze the co-occurrence of paired market states; the\nprobability of remaining in the same state is much higher than the transition\nto a different state. The transitions to other states mainly occur among the\nimmediately adjacent states, with a few rare intermittent transitions to the\nremote states. The state adjacent to the critical state (market crash) may\nserve as an indicator or a \"precursor\" for the critical state and this novel\nmethod of identifying the long-term precursors may be very helpful for\nconstructing the early warning system in financial markets, as well as in other\ncomplex systems.\n"
    },
    {
        "paper_id": 1809.00964,
        "authors": "Dorje C. Brody and David M. Meier",
        "title": "Mathematical models for fake news",
        "comments": "Version to appear as Chapter 18 in Financial Informatics: An\n  Information-Based Approach to Asset Pricing. D. C. Brody, L. P. Hughston & A.\n  Macrina (editors). Singapore: World Scientific Publishing Company (2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past decade it has become evident that intentional disinformation in\nthe political context -- so-called fake news -- is a danger to democracy.\nHowever, until now there has been no clear understanding of how to define fake\nnews, much less how to model it. This paper addresses both of these issues. A\ndefinition of fake news is given, and two approaches for the modelling of fake\nnews and its impact in elections and referendums are introduced. The first\napproach, based on the idea of a representative voter, is shown to be suitable\nfor obtaining a qualitative understanding of phenomena associated with fake\nnews at a macroscopic level. The second approach, based on the idea of an\nelection microstructure, describes the collective behaviour of the electorate\nby modelling the preferences of individual voters. It is shown through a\nsimulation study that the mere knowledge that fake news may be in circulation\ngoes a long way towards mitigating the impact of fake news.\n"
    },
    {
        "paper_id": 1809.0099,
        "authors": "Michael Preischl and Stefan Thonhauser",
        "title": "Optimal Reinsurance for Gerber-Shiu Functions in the Cramer-Lundberg\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complementing existing results on minimal ruin probabilities, we minimize\nexpected discounted penalty functions (or Gerber-Shiu functions) in a\nCramer-Lundberg model by choosing optimal reinsurance. Reinsurance strategies\nare modelled as time dependant control functions, which leads to a setting from\nthe theory of optimal stochastic control and ultimately to the problem's\nHamilton-Jacobi-Bellman equation. We show existence and uniqueness of the\nsolution found by this method and provide numerical examples involving light\nand heavy tailed claims and also give a remark on the asymptotics.\n"
    },
    {
        "paper_id": 1809.01332,
        "authors": "Michael S. Harr\\'e",
        "title": "Multi-agent Economics and the Emergence of Critical Markets",
        "comments": "24 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dual crises of the sub-prime mortgage crisis and the global financial\ncrisis has prompted a call for explanations of non-equilibrium market dynamics.\nRecently a promising approach has been the use of agent based models (ABMs) to\nsimulate aggregate market dynamics. A key aspect of these models is the\nendogenous emergence of critical transitions between equilibria, i.e. market\ncollapses, caused by multiple equilibria and changing market parameters.\nSeveral research themes have developed microeconomic based models that include\nmultiple equilibria: social decision theory (Brock and Durlauf), quantal\nresponse models (McKelvey and Palfrey), and strategic complementarities\n(Goldstein). A gap that needs to be filled in the literature is a unified\nanalysis of the relationship between these models and how aggregate criticality\nemerges from the individual agent level. This article reviews the agent-based\nfoundations of markets starting with the individual agent perspective of\nMcFadden and the aggregate perspective of catastrophe theory emphasising\nconnections between the different approaches. It is shown that changes in the\nuncertainty agents have in the value of their interactions with one another,\neven if these changes are one-sided, plays a central role in systemic market\nrisks such as market instability and the twin crises effect. These interactions\ncan endogenously cause crises that are an emergent phenomena of markets.\n"
    },
    {
        "paper_id": 1809.01342,
        "authors": "Giovanni Paolinelli, Gianni Arioli",
        "title": "A model for stocks dynamics based on a non-Gaussian path integral",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.11.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model for the dynamics of stock prices based on a non\nquadratic path integral. The model is a generalization of Ilinski's path\nintegral model, more precisely we choose a different action, which can be tuned\nto different time scales. The result is a model with a very small number of\nparameters that provides very good fits of some stock prices and indices\nfluctuations.\n"
    },
    {
        "paper_id": 1809.01464,
        "authors": "Huyen Pham (LPSM (UMR\\_8001), ENSAE), Xiaoli Wei (LPSM (UMR\\_8001)),\n  Chao Zhou (NUS)",
        "title": "Portfolio diversification and model uncertainty: a robust dynamic\n  mean-variance approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on a dynamic multi-asset mean-variance portfolio selection\nproblem under model uncertainty. We develop a continuous time framework for\ntaking into account ambiguity aversion about both expected return rates and\ncorrelation matrix of the assets, and for studying the join effects on\nportfolio diversification. The dynamic setting allows us to consider time\nvarying ambiguity sets, which include the cases where the drift and correlation\nare estimated on a rolling window of historical data or when the investor takes\ninto account learning on the ambiguity. In this context, we prove a general\nseparation principle for the associated robust control problem, which allows us\nto reduce the determination of the optimal dynamic strategy to the parametric\ncomputation of the minimal risk premium function. Our results provide a\njustification for under-diversification, as documented in empirical studies and\nin the static models [16], [34]. Furthermore, we explicitly quantify the degree\nof under-diversification in termsof correlation bounds and Sharpe ratios\nproximities, and emphasize the different features induced by drift and\ncorrelation ambiguity. In particular, we show that an investor with a poor\nconfidence in the expected return estimation does not hold any risky asset, and\non the other hand, trades only one risky asset when the level of ambiguity on\ncorrelation matrix is large. We also provide a complete picture of the\ndiversification for the optimal robust portfolio in the three-asset case JEL\nClassification: G11, C61 MSC Classification: 91G10, 91G80, 60H30\n"
    },
    {
        "paper_id": 1809.01487,
        "authors": "Yngve Dahle, Martin Steinert, Anh Nguyen Duc, Roman Chizhevskiy",
        "title": "Resource and Competence (Internal) View vs. Environment and Market\n  (External) View when defining a Business",
        "comments": null,
        "journal-ref": "2018 IEEE International Conference on Engineering, Technology and\n  Innovation (ICE/ITMC)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Startups is a popular phenomenon that has a significant impact on global\neconomy growth, innovation and society development. However, there is still\ninsufficient understanding about startups, particularly, how to start a new\nbusiness in the relation to consequent performance. Toward this knowledge, we\nhave performed an empirical study regarding the differences between a Resource\nand Competence View (Internal) vs Environment and Market View (External) when\ndefining a Business. 701 entrepreneurs have reflected on their startups on nine\nclasses of Resources (values, vision, personal objectives, employees and\npartners, buildings and rental contracts, cash and credit, patents, IPR's and\nbrands, products and services and finally revenues and grants) and three\nelements of the Business Mission (\"KeyContribution\", \"KeyMarket\" and\n\"Distinction\"). It seems to be a tendency to favour the Internal View over the\nExternal View. This tendency is clearer in Stable Economies (Europe) than in\nEmerging Economies (South Africa). There seems to be a co-variation between the\ntendency to favour the Internal View and the tendency to focus on adding\nResources. Finally, we found that an order-based analysis seems to explain the\ndifferences between the two views better than a number-based method.\n"
    },
    {
        "paper_id": 1809.01489,
        "authors": "T. R. Santos",
        "title": "A Bayesian GED-Gamma stochastic volatility model for return data: a\n  marginal likelihood approach",
        "comments": "26 pages, 5 figures and 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several studies explore inferences based on stochastic volatility (SV)\nmodels, taking into account the stylized facts of return data. The common\nproblem is that the latent parameters of many volatility models are\nhigh-dimensional and analytically intractable, which means inferences require\napproximations using, for example, the Markov Chain Monte Carlo or Laplace\nmethods. Some SV models are expressed as a linear Gaussian state-space model\nthat leads to a marginal likelihood, reducing the dimensionality of the\nproblem. Others are not linearized, and the latent parameters are integrated\nout. However, these present a quite restrictive evolution equation. Thus, we\npropose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is\na product of the generalized Student's t-distributions in which the latent\nstates are related across time through a stationary Gaussian evolution\nequation. Then, an approximation is made for the prior distribution of\nlog-precision/volatility, without the need for model linearization. This also\nallows for the computation of the marginal likelihood function, where the\nhigh-dimensional latent states are integrated out and easily sampled in blocks\nusing a smoothing procedure. In addition, extensions of our GED-Gamma model are\neasily made to incorporate skew heavy-tailed distributions. We use the Bayesian\nestimator for the inference of static parameters, and perform a simulation\nstudy on several properties of the estimator. Our results show that the\nproposed model can be reasonably estimated. Furthermore, we provide case\nstudies of a Brazilian asset and the pound/dollar exchange rate to show the\nperformance of our approach in terms of fit and prediction.\n  Keywords: SV model, New sequential and smoothing procedures, Generalized\nStudent's t-distribution, Non-Gaussian errors, Heavy tails, Skewness\n"
    },
    {
        "paper_id": 1809.01501,
        "authors": "Arthur T. Rego and Thiago R. dos Santos",
        "title": "Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler",
        "comments": "27 pages, 12 figures, 5 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose a model for estimating volatility from financial\ntime series, extending the non-Gaussian family of space-state models with exact\nmarginal likelihood proposed by Gamerman, Santos and Franco (2013). On the\nliterature there are models focused on estimating financial assets risk,\nhowever, most of them rely on MCMC methods based on Metropolis algorithms,\nsince full conditional posterior distributions are not known. We present an\nalternative model capable of estimating the volatility, in an automatic way,\nsince all full conditional posterior distributions are known, and it is\npossible to obtain an exact sample of parameters via Gibbs Sampler. The\nincorporation of jumps in returns allows the model to capture speculative\nmovements of the data, so that their influence does not propagate to\nvolatility. We evaluate the performance of the algorithm using synthetic and\nreal data time series.\n  Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,\nDynamic linear models.\n"
    },
    {
        "paper_id": 1809.01506,
        "authors": "Prakhar Ganesh, Puneet Rakheja",
        "title": "VLSTM: Very Long Short-Term Memory Networks for High-Frequency Trading",
        "comments": "4 pages + 1 page references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial trading is at the forefront of time-series analysis, and has grown\nhand-in-hand with it. The advent of electronic trading has allowed complex\nmachine learning solutions to enter the field of financial trading. Financial\nmarkets have both long term and short term signals and thus a good predictive\nmodel in financial trading should be able to incorporate them together. One of\nthe most sought after forms of electronic trading is high-frequency trading\n(HFT), typically known for microsecond sensitive changes, which results in a\ntremendous amount of data. LSTMs are one of the most capable variants of the\nRNN family that can handle long-term dependencies, but even they are not\nequipped to handle such long sequences of the order of thousands of data points\nlike in HFT. We propose very-long short term memory networks, or VLSTMs, to\ndeal with such extreme length sequences. We explore the importance of VLSTMs in\nthe context of HFT. We compare our model on publicly available dataset and got\na 3.14\\% increase in F1-score over the existing state-of-the-art time-series\nforecasting models. We also show that our model has great parallelization\npotential, which is essential for practical purposes when trading on such\nmarkets.\n"
    },
    {
        "paper_id": 1809.01972,
        "authors": "Ulrich Horst and Xiaonyu Xia",
        "title": "Continuous viscosity solutions to linear-quadratic stochastic control\n  problems with singular terminal state constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper establishes the existence of a unique nonnegative continuous\nviscosity solution to the HJB equation associated with a Markovian\nlinear-quadratic control problems with singular terminal state constraint and\npossibly unbounded cost coefficients. The existence result is based on a novel\ncomparison principle for semi-continuous viscosity sub- and supersolutions for\nPDEs with singular terminal value. Continuity of the viscosity solution is\nenough to carry out the verification argument.\n"
    },
    {
        "paper_id": 1809.01983,
        "authors": "Julia Eisenberg and Paul Kr\\\"uhner",
        "title": "Suboptimal Control of Dividends under Exponential Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an insurance company modelling its surplus process by a Brownian\nmotion with drift. Our target is to maximise the expected exponential utility\nof discounted dividend payments, given that the dividend rates are bounded by\nsome constant. The utility function destroys the linearity and the time\nhomogeneity of the considered problem. The value function depends not only on\nthe surplus, but also on time. Numerical considerations suggest that the\noptimal strategy, if it exists, is of a barrier type with a non-linear barrier.\nIn the related article by granditz et al., it has been observed that standard\nnumerical methods break down in certain parameter cases and no close form\nsolution has been found.\n  For these reasons, we offer a new method allowing to estimate the distance of\nan arbitrary smooth enough function to the value function. Applying this\nmethod, we investigate the goodness of the most obvious suboptimal strategies -\npayout on the maximal rate, and constant barrier strategies - by measuring the\ndistance of its performance function to the value function.\n"
    },
    {
        "paper_id": 1809.01987,
        "authors": "Brian P. Hanley",
        "title": "The Impact of LIBOR Linked Borrowing to Cover Venture Bank Investment\n  Loans Creates a New Systemic Risk",
        "comments": "7 pages, 4 figures. This is part of the work for Venture Bank\n  design/modeling. arXiv admin note: substantial text overlap with\n  arXiv:1707.08078; EDCS terminology, miscellaneous clarifications",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A scenario in which regulators take the drastic step of requiring coverage of\nall venture bank investment loans using interbank borrowed funds is considered.\nIn this scenario, a minimal amount of default insurance is used, such that Tier\n1 and 2 capital requirements are still met. To do this, the default insurance\npercentage on all investment loans is cut to 3.88%, although the minimum is\n2.88%. Results: For a portfolio of 1.31X (ten year total conventional return)\nor better, at interest rates of 2% or better, the venture bank survives and can\nhave excellent returns. For a portfolio of 1.5X (ten year total conventional\nreturn) the bank can have extraordinary returns below 1.5% interest and survive\nup to 3%. interest. However, if returns fall, or interest rates rise, then\nventure banks go underwater quite rapidly. Conclusion: Using LIBOR funds limits\nprofitability, and damages stability of the bank, with no visible benefit to\nany party, thus creating a new systemic risk to the banking system.\n"
    },
    {
        "paper_id": 1809.01989,
        "authors": "Yu Zheng and Timothy M. Hospedales and Yongxin Yang",
        "title": "Diversity and Sparsity: A New Perspective on Index Tracking",
        "comments": "Accepted to ICASSP 2020. 5 pages. This is a conference version of the\n  work, for the full version, please refer to arXiv:1809.01989v1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of partial index tracking, replicating a benchmark\nindex using a small number of assets. Accurate tracking with a sparse portfolio\nis extensively studied as a classic finance problem. However in practice, a\ntracking portfolio must also be diverse in order to minimise risk -- a\nrequirement which has only been dealt with by ad-hoc methods before. We\nintroduce the first index tracking method that explicitly optimises both\ndiversity and sparsity in a single joint framework. Diversity is realised by a\nregulariser based on pairwise similarity of assets, and we demonstrate that\nlearning similarity from data can outperform some existing heuristics. Finally,\nwe show that the way we model diversity leads to an easy solution for sparsity,\nallowing both constraints to be optimised easily and efficiently. we run\nout-of-sample backtesting for a long interval of 15 years (2003 -- 2018), and\nthe results demonstrate the superiority of the proposed algorithm.\n"
    },
    {
        "paper_id": 1809.02098,
        "authors": "Omar El Euch, Jim Gatheral, Rado\\v{s} Radoi\\v{c}i\\'c, Mathieu\n  Rosenbaum",
        "title": "The Zumbach effect under rough Heston",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous literature has identified an effect, dubbed the Zumbach effect, that\nis nonzero empirically but conjectured to be zero in any conventional\nstochastic volatility model. Essentially this effect corresponds to the\nproperty that past squared returns forecast future volatilities better than\npast volatilities forecast future squared returns. We provide explicit\ncomputations of the Zumbach effect under rough Heston and show that they are\nconsistent with empirical estimates. In agreement with previous conjectures\nhowever, the Zumbach effect is found to be negligible in the classical Heston\nmodel.\n"
    },
    {
        "paper_id": 1809.02233,
        "authors": "Ryan Ferguson and Andrew Green",
        "title": "Deeply Learning Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses deep learning to value derivatives. The approach is broadly\napplicable, and we use a call option on a basket of stocks as an example. We\nshow that the deep learning model is accurate and very fast, capable of\nproducing valuations a million times faster than traditional models. We develop\na methodology to randomly generate appropriate training data and explore the\nimpact of several parameters including layer width and depth, training data\nquality and quantity on model speed and accuracy.\n"
    },
    {
        "paper_id": 1809.02245,
        "authors": "Peter Carr and Zhibai Zhang",
        "title": "Generalizing Geometric Brownian Motion",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To convert standard Brownian motion $Z$ into a positive process, Geometric\nBrownian motion (GBM) $e^{\\beta Z_t}, \\beta >0$ is widely used. We generalize\nthis positive process by introducing an asymmetry parameter $ \\alpha \\geq 0$\nwhich describes the instantaneous volatility whenever the process reaches a new\nlow. For our new process, $\\beta$ is the instantaneous volatility as prices\nbecome arbitrarily high. Our generalization preserves the positivity, constant\nproportional drift, and tractability of GBM, while expressing the instantaneous\nvolatility as a randomly weighted $L^2$ mean of $\\alpha$ and $\\beta$. The\nrunning minimum and relative drawup of this process are also analytically\ntractable. Letting $\\alpha = \\beta$, our positive process reduces to Geometric\nBrownian motion. By adding a jump to default to the new process, we introduce a\nnon-negative martingale with the same tractabilities. Assuming a security's\ndynamics are driven by these processes in risk neutral measure, we price\nseveral derivatives including vanilla, barrier and lookback options.\n"
    },
    {
        "paper_id": 1809.02362,
        "authors": "Philipp Grohs, Fabian Hornung, Arnulf Jentzen, Philippe von\n  Wurstemberger",
        "title": "A proof that artificial neural networks overcome the curse of\n  dimensionality in the numerical approximation of Black-Scholes partial\n  differential equations",
        "comments": "To appear in Mem. Amer. Math. Soc.; 126 pages",
        "journal-ref": "Mem. Amer. Math. Soc.284(2023), no.1410, v+93 pp",
        "doi": "10.1090/memo/1410",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial neural networks (ANNs) have very successfully been used in\nnumerical simulations for a series of computational problems ranging from image\nclassification/image recognition, speech recognition, time series analysis,\ngame intelligence, and computational advertising to numerical approximations of\npartial differential equations (PDEs). Such numerical simulations suggest that\nANNs have the capacity to very efficiently approximate high-dimensional\nfunctions and, especially, indicate that ANNs seem to admit the fundamental\npower to overcome the curse of dimensionality when approximating the\nhigh-dimensional functions appearing in the above named computational problems.\nThere are a series of rigorous mathematical approximation results for ANNs in\nthe scientific literature. Some of them prove convergence without convergence\nrates and some even rigorously establish convergence rates but there are only a\nfew special cases where mathematical results can rigorously explain the\nempirical success of ANNs when approximating high-dimensional functions. The\nkey contribution of this article is to disclose that ANNs can efficiently\napproximate high-dimensional functions in the case of numerical approximations\nof Black-Scholes PDEs. More precisely, this work reveals that the number of\nrequired parameters of an ANN to approximate the solution of the Black-Scholes\nPDE grows at most polynomially in both the reciprocal of the prescribed\napproximation accuracy $\\varepsilon > 0$ and the PDE dimension $d \\in\n\\mathbb{N}$. We thereby prove, for the first time, that ANNs do indeed overcome\nthe curse of dimensionality in the numerical approximation of Black-Scholes\nPDEs.\n"
    },
    {
        "paper_id": 1809.02433,
        "authors": "Rainer Schlosser and Martin Boissier",
        "title": "Dealing with the Dimensionality Curse in Dynamic Pricing Competition:\n  Using Frequent Repricing to Compensate Imperfect Market Anticipations",
        "comments": null,
        "journal-ref": "Computers & Operations Research Volume 100, December 2018, Pages\n  26-42",
        "doi": "10.1016/j.cor.2018.07.011",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Most sales applications are characterized by competition and limited demand\ninformation. For successful pricing strategies, frequent price adjustments as\nwell as anticipation of market dynamics are crucial. Both effects are\nchallenging as competitive markets are complex and computations of optimized\npricing adjustments can be time-consuming. We analyze stochastic dynamic\npricing models under oligopoly competition for the sale of perishable goods. To\ncircumvent the curse of dimensionality, we propose a heuristic approach to\nefficiently compute price adjustments. To demonstrate our strategy's\napplicability even if the number of competitors is large and their strategies\nare unknown, we consider different competitive settings in which competitors\nfrequently and strategically adjust their prices. For all settings, we verify\nthat our heuristic strategy yields promising results. We compare the\nperformance of our heuristic against upper bounds, which are obtained by\noptimal strategies that take advantage of perfect price anticipations. We find\nthat price adjustment frequencies can have a larger impact on expected profits\nthan price anticipations. Finally, our approach has been applied on Amazon for\nthe sale of used books. We have used a seller's historical market data to\ncalibrate our model. Sales results show that our data-driven strategy\noutperforms the rule-based strategy of an experienced seller by a profit\nincrease of more than 20%.\n"
    },
    {
        "paper_id": 1809.02465,
        "authors": "Atsuhiro Satoh and Yasuhito Tanaka",
        "title": "Nash equilibrium of partially asymmetric three-players zero-sum game\n  with two strategic variables",
        "comments": "13 pages. arXiv admin note: substantial text overlap with\n  arXiv:1809.01130; text overlap with arXiv:1806.07203",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a partially asymmetric three-players zero-sum game with two\nstrategic variables. Two players (A and B) have the same payoff functions, and\nPlayer C does not. Two strategic variables are $t_i$'s and $s_i$'s for $i=A, B,\nC$. Mainly we will show the following results.\n  1. The equilibrium when all players choose $t_i$'s is equivalent to the\nequilibrium when Players A and B choose $t_i$'s and Player C chooses $s_C$ as\ntheir strategic variables. 2. The equilibrium when all players choose $s_i$'s\nis equivalent to the equilibrium when Players A and B choose $s_i$'s and Player\nC chooses $t_C$ as their strategic variables.\n  The equilibrium when all players choose $t_i$'s and the equilibrium when all\nplayers choose $s_i$'s are not equivalent although they are equivalent in a\nsymmetric game in which all players have the same payoff functions.\n"
    },
    {
        "paper_id": 1809.02466,
        "authors": "Atsuhiro Satoh and Yasuhito Tanaka",
        "title": "Sion's mini-max theorem and Nash equilibrium in a five-players game with\n  two groups which is zero-sum and symmetric in each group",
        "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:1809.01488",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the relation between Sion's minimax theorem for a continuous\nfunction and a Nash equilibrium in a five-players game with two groups which is\nzero-sum and symmetric in each group. We will show the following results.\n  1. The existence of Nash equilibrium which is symmetric in each group implies\nSion's minimax theorem for a pair of playes in each group. 2. Sion's minimax\ntheorem for a pair of playes in each group imply the existence of a Nash\nequilibrium which is symmetric in each group.\n  Thus, they are equivalent. An example of such a game is a relative profit\nmaximization game in each group under oligopoly with two groups such that firms\nin each group have the same cost functions and maximize their relative profits\nin each group, and the demand functions are symmetric for the firms in each\ngroup.\n"
    },
    {
        "paper_id": 1809.02674,
        "authors": "Jaros{\\l}aw Klamut, Ryszard Kutner, Tomasz Gubiec, Zbigniew R. Struzik",
        "title": "The new face of multifractality: Multi-branchedness and the phase\n  transitions in time series of mean inter-event times",
        "comments": "24 pages, 13 figures",
        "journal-ref": "Phys. Rev. E 101, 063303 (2020)",
        "doi": "10.1103/PhysRevE.101.063303",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical time series of inter-event or waiting times are investigated using\na modified Multifractal Detrended Fluctuation Analysis operating on\nfluctuations of mean detrended dynamics. The core of the extended multifractal\nanalysis is the non-monotonic behavior of the generalized Hurst exponent $h(q)$\n-- the fundamental exponent in the study of multifractals. The consequence of\nthis behavior is the non-monotonic behavior of the coarse H\\\"older exponent\n$\\alpha (q)$ leading to multi-branchedness of the spectrum of dimensions. The\nLegendre-Fenchel transform is used instead of the routinely used canonical\nLegendre (single-branched) contact transform. Thermodynamic consequences of the\nmulti-branched multifractality are revealed. These are directly expressed in\nthe language of phase transitions between thermally stable, metastable, and\nunstable phases. These phase transitions are of the first and second orders\naccording to Mandelbrot's modified Ehrenfest classification. The discovery of\nmulti-branchedness is tantamount in significance to extending multifractal\nanalysis.\n"
    },
    {
        "paper_id": 1809.02769,
        "authors": "Sheikh Rabiul Islam",
        "title": "Worldcoin: A Hypothetical Cryptocurrency for the People and its\n  Government",
        "comments": "Under dual review in GSU FinTech Conference and The Review of\n  Financial Studies Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The world of cryptocurrency is not transparent enough though it was\nestablished for innate transparent tracking of capital flows. The most\ncontributing factor is the violation of securities laws and scam in Initial\nCoin Offering (ICO) which is used to raise capital through crowdfunding. There\nis a lack of proper regularization and appreciation from governments around the\nworld which is a serious problem for the integrity of cryptocurrency market. We\npresent a hypothetical case study of a new cryptocurrency to establish the\ntransparency and equal right for every citizen to be part of a global system\nthrough the collaboration between people and government. The possible outcome\nis a model of a regulated and trusted cryptocurrency infrastructure that can be\nfurther tailored to different sectors with a different scheme.\n"
    },
    {
        "paper_id": 1809.02772,
        "authors": "Aleksejus Kononovicius, Julius Ruseckas",
        "title": "Order book model with herd behavior exhibiting long-range memory",
        "comments": "27 pages, 20 figures, 3 tables",
        "journal-ref": "Physica A 525: 171-191 (2019)",
        "doi": "10.1016/j.physa.2019.03.059",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose an order book model with herd behavior. The proposed\nmodel is built upon two distinct approaches: a recent empirical study of the\ndetailed order book records by Kanazawa et al. [Phys. Rev. Lett. 120, 138301]\nand financial herd behavior model. Combining these approaches allows us to\npropose a model that replicates the long-range memory of absolute returns and\ntrading activity. We compare the statistical properties of the model against\nthe empirical statistical properties of the Bitcoin exchange rates and New York\nstock exchange tickers. We also show that the fracture in the spectral density\nof the high-frequency absolute return time series might be related to the\nmechanism of convergence towards the equilibrium price.\n"
    },
    {
        "paper_id": 1809.03072,
        "authors": "George Milunovich",
        "title": "Cryptocurrencies, Mainstream Asset Classes and Risk Factors - A Study of\n  Connectedness",
        "comments": "20 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate connectedness within and across two major groups or assets: i)\nfive popular cryptocurrencies, and ii) six major asset classes plus two\ncommonly employed risk factors. Granger-causality tests uncover six direct\nchannels of causality from the elements of the mainstream assets/risk factors\ngroup to digital assets. On the other hand there are two statistically\nsignificant causal links going in the other direction. In order to provide some\nperspective on the magnitude of the uncovered linkages we supplement the\nanalysis by estimating networks from forecast error variance decompositions.\nThe estimated connectedness within the groups is relatively large, whereas the\nlinkages across the two groups are small in comparison. Namely, less than 2.2\npercent of future uncertainty of any cryptocurrency is sourced from all\nnon-crypto assets combined, while the joint contribution of all digital assets\nto non-crypto uncertainty does not exceed 1.5 percent.\n"
    },
    {
        "paper_id": 1809.03222,
        "authors": "Matteo Bruno, Fabio Saracco, Tiziano Squartini, Marco Due\\~nas",
        "title": "Colombian export capabilities: building the firms-products network",
        "comments": null,
        "journal-ref": "Entropy, 20 (10), 785 (2018)",
        "doi": "10.3390/e20100785",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyse the bipartite Colombian firms-products network,\nthroughout a period of five years, from 2010 to 2014. Our analysis depicts a\nstrongly modular system, with several groups of firms specializing in the\nexport of specific categories of products. These clusters have been detected by\nrunning the bipartite variant of the traditional modularity maximization,\nrevealing a bi-modular structure. Interestingly, this finding is refined by\napplying a recently-proposed algorithm for projecting bipartite networks on the\nlayer of interest and, then, running the Louvain algorithm on the resulting\nmonopartite representations. Important structural differences emerge upon\ncomparing the Colombian firms-products network with the World Trade Web, in\nparticular, the bipartite representation of the latter is not characterized by\na similar block-structure, as the modularity maximization fails in revealing\n(bipartite) nodes clusters. This points out that economic systems behave\ndifferently at different scales: while countries tend to diversify their\nproduction --potentially exporting a large number of different products-- firms\nspecialize in exporting (substantially very limited) baskets of basically\nhomogeneous products.\n"
    },
    {
        "paper_id": 1809.03338,
        "authors": "Juan Ospina",
        "title": "Pricing the Aunt Michaela Option with a Modified Black-Scholes Equation\n  with a Maturity Condition of Gamma Type",
        "comments": "9 pages. arXiv admin note: substantial text overlap with\n  arXiv:1508.03841",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Maple, we compute a new exact series solution of a modified\nBlack-Scholes equation, recently proposed, for the case of the Aunt Michaela\noption with a maturity condition of gamma type. We show that the modified\nBlack-Scholes equation with the Aunt Michaela option is exactly solvable in\nterms of associated Laguerre polynomials or equivalently, in terms of Whittaker\nM functions. Finally, we make some numerical experiments with the analytical\nsolutions\n"
    },
    {
        "paper_id": 1809.03425,
        "authors": "Yu-Sin Chang",
        "title": "Systemic Risk and the Dependence Structures",
        "comments": "41 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose a dynamic model of dependence structure between financial\ninstitutions within a financial system and we construct measures for dependence\nand financial instability. Employing Markov structures of joint credit\nmigrations, our model allows for contagious simultaneous jumps in credit\nratings and provides flexibility in modeling dependence structures. Another key\naspect is that the proposed measures consider the interdependence and reflect\nthe changing economic landscape as financial institutions evolve over time. In\nthe final part, we give several examples, where we study various dependence\nstructures and investigate their systemic instability measures. In particular,\nwe show that subject to the same pool of Markov chains, the simulated Markov\nstructures with distinct dependence structures generate different sequences of\nsystemic instability.\n"
    },
    {
        "paper_id": 1809.03442,
        "authors": "Xingguang Chen",
        "title": "The Ladder Theory of Behavioral Decision Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study individual decision-making behavioral on generic view. Using a\nformal mathematical model, we investigate the action mechanism of decision\nbehavioral under subjective perception changing of task attributes. Our model\nis built on work in two kinds classical behavioral decision making theory:\n\"prospect theory (PT)\" and \"image theory (IT)\". We consider subjective\nattributes preference of decision maker under the whole decision process.\nStrategies collection and selection mechanism are induced according the\ndescription of multi-attributes decision making. A novel behavioral\ndecision-making framework named \"ladder theory (LT)\" is proposed. By real four\ncases comparing, the results shows that the LT have better explanation and\nprediction ability then PT and IT under some decision situations. Furthermore,\nwe use our model to shed light on that the LT theory can cover PT and IT\nideally. It is the enrichment and development for classical behavioral decision\ntheory and, it has positive theoretical value and instructive significance for\nexplaining plenty of real decision-making phenomena. It may facilitate our\nunderstanding of how individual decision-making performed actually.\n"
    },
    {
        "paper_id": 1809.03459,
        "authors": "Xin Guo and Wenpin Tang and Renyuan Xu",
        "title": "A class of stochastic games and moving free boundary problems",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose and analyze a class of $N$-player stochastic games\nthat include finite fuel stochastic games as a special case. We first derive\nsufficient conditions for the Nash equilibrium (NE) in the form of a\nverification theorem. The associated Quasi-Variational-Inequalities include an\nessential game component regarding the interactions among players, which may be\ninterpreted as the analytical representation of the conditional optimality for\nNEs. The derivation of NEs involves solving first a multi-dimensional free\nboundary problem and then a Skorokhod problem. We call it a \"moving free\nboundary\" to highlight the difference between standard control problems and\nstochastic games. Finally, we present an intriguing connection between these NE\nstrategies and controlled rank-dependent stochastic differential equations.\n"
    },
    {
        "paper_id": 1809.03584,
        "authors": "Matias D. Cattaneo and Richard K. Crump and Max H. Farrell and Ernst\n  Schaumburg",
        "title": "Characteristic-Sorted Portfolios: Estimation and Inference",
        "comments": null,
        "journal-ref": "Review of Economics and Statistics, 102(3), 531--551, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio sorting is ubiquitous in the empirical finance literature, where it\nhas been widely used to identify pricing anomalies. Despite its popularity,\nlittle attention has been paid to the statistical properties of the procedure.\nWe develop a general framework for portfolio sorting by casting it as a\nnonparametric estimator. We present valid asymptotic inference methods and a\nvalid mean square error expansion of the estimator leading to an optimal choice\nfor the number of portfolios. In practical settings, the optimal choice may be\nmuch larger than the standard choices of 5 or 10. To illustrate the relevance\nof our results, we revisit the size and momentum anomalies.\n"
    },
    {
        "paper_id": 1809.03641,
        "authors": "Yu Feng and Erik Schl\\\"ogl",
        "title": "Model Risk Measurement under Wasserstein Distance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a new approach to model risk measurement based on the\nWasserstein distance between two probability measures. It formulates the\ntheoretical motivation resulting from the interpretation of fictitious\nadversary of robust risk management. The proposed approach accounts for\nequivalent and non-equivalent probability measures and incorporates the\neconomic reality of the fictitious adversary. It provides practically feasible\nresults that overcome the restriction of considering only models implying\nprobability measures equivalent to the reference model. The Wasserstein\napproach suits for various types of model risk problems, ranging from the\nsingle-asset hedging risk problem to the multi-asset allocation problem. The\nrobust capital market line, accounting for the correlation risk, is not\nachievable with other non-parametric approaches.\n"
    },
    {
        "paper_id": 1809.03769,
        "authors": "Adrian Banner, Robert Fernholz, Vassilios Papathanakos, Johannes Ruf,\n  David Schofield",
        "title": "Diversification, Volatility, and Surprising Alpha",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been widely observed that capitalization-weighted indexes can be\nbeaten by surprisingly simple, systematic investment strategies. Indeed, in the\nU.S. stock market, equal-weighted portfolios, random-weighted portfolios, and\nother naive, non- optimized portfolios tend to outperform a\ncapitalization-weighted index over the long term. This outperformance is\ngenerally attributed to beneficial factor exposures. Here, we provide a deeper,\nmore general explanation of this phenomenon by decomposing portfolio\nlog-returns into an average growth and an excess growth component. Using a\nrank-based empirical study we argue that the excess growth component plays the\nmajor role in explaining the outperformance of naive portfolios. In particular,\nindividual stock growth rates are not as critical as is traditionally assumed.\n"
    },
    {
        "paper_id": 1809.03885,
        "authors": "Umut \\c{C}et{\\i}n",
        "title": "Mathematics of Market Microstructure under Asymmetric Information",
        "comments": "arXiv admin note: text overlap with arXiv:1407.2420, arXiv:1202.2980,\n  arXiv:1607.00035 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  These are the lecture notes for the summer course given for 2018 Mathematical\nFinance Summer School at Shandong Unversity. It contains a brief introduction\nto the Kyle model and the related topics in filtering, enlargement of\nfiltrations and Markov bridges.\n"
    },
    {
        "paper_id": 1809.03941,
        "authors": "Emanuele Fabbiani, Andrea Marziali, Giuseppe De Nicolao",
        "title": "Fast calibration of two-factor models for energy option pricing",
        "comments": "Accepted for publication in Applied Stochastic Models in Business and\n  Industry",
        "journal-ref": "Applied Stochastic Models in Business and Industry, 2021",
        "doi": "10.1002/asmb.2604",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy companies need efficient procedures to perform market calibration of\nstochastic models for commodities. If the Black framework is chosen for option\npricing, the bottleneck of the market calibration is the computation of the\nvariance of the asset. Energy commodities are commonly represented by\nmulti-factor linear models, whose variance obeys a matrix Lyapunov differential\nequation. In this paper, analytical and numerical methods to derive the\nvariance are discussed: the Lyapunov approach is shown to be more\nstraightforward than ad-hoc derivations found in the literature and can be\nreadily extended to higher-dimensional models. A case study is presented, where\nthe variance of a two-factor mean-reverting model is embedded into the Black\nformulae and the model parameters are calibrated against listed options. The\nanalytical and numerical method are compared, showing that the former makes the\ncalibration 14 times faster. A Python implementation of the proposed methods is\navailable as open-source software on GitHub.\n"
    },
    {
        "paper_id": 1809.03977,
        "authors": "D\\'ora Gr\\'eta Petr\\'oczy",
        "title": "An alternative quality of life ranking on the basis of remittances",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.seps.2021.101042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Remittances provide an essential connection between people working abroad and\ntheir home countries. This paper considers these transfers as a measure of\npreferences revealed by the workers, underlying a ranking of countries around\nthe world. In particular, we use the World Bank bilateral remittances data of\ninternational salaries and interpersonal transfers between 2010 and 2015 to\ncompare European countries. The suggested least squares method has favourable\naxiomatic properties. Our ranking reveals a crucial aspect of quality of life\nand may become an alternative to various composite indices.\n"
    },
    {
        "paper_id": 1809.04035,
        "authors": "Jaehyuk Choi, Chenru Liu, Byoung Ki Seo",
        "title": "Hyperbolic normal stochastic volatility model",
        "comments": "26 pages, 4 figures, 5 tables",
        "journal-ref": "Journal of Futures Markets, 39(2):186-204, 2019",
        "doi": "10.1002/fut.21967",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For option pricing models and heavy-tailed distributions, this study proposes\na continuous-time stochastic volatility model based on an arithmetic Brownian\nmotion: a one-parameter extension of the normal stochastic alpha-beta-rho\n(SABR) model. Using two generalized Bougerol's identities in the literature,\nthe study shows that our model has a closed-form Monte-Carlo simulation scheme\nand that the transition probability for one special case follows Johnson's\n$S_U$ distribution---a popular heavy-tailed distribution originally proposed\nwithout stochastic process. It is argued that the $S_U$ distribution serves as\nan analytically superior alternative to the normal SABR model because the two\ndistributions are empirically similar.\n"
    },
    {
        "paper_id": 1809.04401,
        "authors": "Guanxing Fu, Ulrich Horst",
        "title": "Mean-Field Leader-Follower Games with Terminal State Constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze linear McKean-Vlasov forward-backward SDEs arising in\nleader-follower games with mean-field type control and terminal state\nconstraints on the state process. We establish an existence and uniqueness of\nsolutions result for such systems in time-weighted spaces as well as a\n{convergence} result of the solutions with respect to certain perturbations of\nthe drivers of both the forward and the backward component. The general results\nare used to solve a novel single-player model of portfolio liquidation under\nmarket impact with expectations feedback as well as a novel Stackelberg game of\noptimal portfolio liquidation with asymmetrically informed players.\n"
    },
    {
        "paper_id": 1809.04775,
        "authors": "Yusuke Uchiyama and Takanori Kadoya",
        "title": "Superstatistics with cut-off tails for financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.04.166",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series have been investigated to follow fat-tailed\ndistributions. Further, an empirical probability distribution sometimes shows\ncut-off shapes on its tails. To describe this stylized fact, we incorporate the\ncut-off effect in superstatistics. Then we confirm that the presented\nstochastic model is capable of describing the statistical properties of real\nfinancial time series. In addition, we present an option pricing formula with\nrespect to superstatistics.\n"
    },
    {
        "paper_id": 1809.04925,
        "authors": "Jeonggyu Huh",
        "title": "Measuring Systematic Risk with Neural Network Factor Model",
        "comments": "16 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we measure systematic risk with a new nonparametric factor\nmodel, the neural network factor model. The suitable factors for systematic\nrisk can be naturally found by inserting daily returns on a wide range of\nassets into the bottleneck network. The network-based model does not stick to a\nprobabilistic structure unlike parametric factor models, and it does not need\nfeature engineering because it selects notable features by itself. In addition,\nwe compare performance between our model and the existing models using 20-year\ndata of S&P 100 components. Although the new model can not outperform the best\nones among the parametric factor models due to limitations of the variational\ninference, the estimation method used for this study, it is still noteworthy in\nthat it achieves the performance as best the comparable models could without\nany prior knowledge.\n"
    },
    {
        "paper_id": 1809.05243,
        "authors": "Veeraruna Kavitha, Indrajit Saha, Sandeep Juneja",
        "title": "Random Fixed Points, Limits and Systemic risk",
        "comments": "20 pages and 5 figures. In the current version, the Theorem 1 proof\n  is corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider vector fixed point (FP) equations in large dimensional spaces\ninvolving random variables, and study their realization-wise solutions. We have\nan underlying directed random graph, that defines the connections between\nvarious components of the FP equations. Existence of an edge between nodes i, j\nimplies the i th FP equation depends on the j th component. We consider a\nspecial case where any component of the FP equation depends upon an appropriate\naggregate of that of the random neighbor components. We obtain finite\ndimensional limit FP equations (in a much smaller dimensional space), whose\nsolutions approximate the solution of the random FP equations for almost all\nrealizations, in the asymptotic limit (number of components increase). Our\ntechniques are different from the traditional mean-field methods, which deal\nwith stochastic FP equations in the space of distributions to describe the\nstationary distributions of the systems. In contrast our focus is on\nrealization-wise FP solutions. We apply the results to study systemic risk in a\nlarge financial heterogeneous network with many small institutions and one big\ninstitution, and demonstrate some interesting phenomenon.\n"
    },
    {
        "paper_id": 1809.05328,
        "authors": "Ludovic Gouden\\`ege (FR3487), Andrea Molent, Antonino Zanette\n  (MATHRISK)",
        "title": "Computing Credit Valuation Adjustment solving coupled PIDEs in the Bates\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit value adjustment (CVA) is the charge applied by financial institutions\nto the counterparty to cover the risk of losses on a counterpart default event.\nIn this paper we estimate such a premium under the Bates stochastic model\n(Bates [4]), which considers an underlying affected by both stochastic\nvolatility and random jumps. We propose an efficient method which improves the\nfinite-difference Monte Carlo (FDMC) approach introduced by de Graaf et al.\n[11]. In particular, the method we propose consists in replacing the Monte\nCarlo step of the FDMC approach with a finite difference step and the whole\nmethod relies on the efficient solution of two coupled partial\nintegro-differential equations (PIDE) which is done by employing the Hybrid\nTree-Finite Difference method developed by Briani et al. [6, 7, 8]. Moreover,\nthe direct application of the hybrid techniques in the original FDMC approach\nis also considered for comparison purposes. Several numerical tests prove the\neffectiveness and the reliability of the proposed approach when both European\nand American options are considered.\n"
    },
    {
        "paper_id": 1809.05643,
        "authors": "Yuki Kinoshita and Yumiharu Nakano",
        "title": "Kernel-based collocation methods for Heath-Jarrow-Morton models with\n  Musiela parametrization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose kernel-based collocation methods for numerical solutions to\nHeath-Jarrow-Morton models with Musiela parametrization. The methods can be\nseen as the Euler-Maruyama approximation of some finite dimensional stochastic\ndifferential equations, and allow us to compute the derivative prices by the\nusual Monte Carlo methods. We derive a bound on the rate of convergence under\nsome decay condition on the inverse of the interpolation matrix and some\nregularity conditions on the volatility functionals.\n"
    },
    {
        "paper_id": 1809.05901,
        "authors": "Hunt Allcott, Matthew Gentzkow, Chuan Yu",
        "title": "Trends in the Diffusion of Misinformation on Social Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We measure trends in the diffusion of misinformation on Facebook and Twitter\nbetween January 2015 and July 2018. We focus on stories from 570 sites that\nhave been identified as producers of false stories. Interactions with these\nsites on both Facebook and Twitter rose steadily through the end of 2016.\nInteractions then fell sharply on Facebook while they continued to rise on\nTwitter, with the ratio of Facebook engagements to Twitter shares falling by\napproximately 60 percent. We see no similar pattern for other news, business,\nor culture sites, where interactions have been relatively stable over time and\nhave followed similar trends on the two platforms both before and after the\nelection.\n"
    },
    {
        "paper_id": 1809.05947,
        "authors": "Kim Weston and Gordan Zitkovic",
        "title": "An incomplete equilibrium with a stochastic annuity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the global existence of an incomplete, continuous-time finite-agent\nRadner equilibrium in which exponential agents optimize their expected utility\nover both running consumption and terminal wealth. The market consists of a\ntraded annuity, and, along with unspanned income, the market is incomplete. Set\nin a Brownian framework, the income is driven by a multidimensional diffusion,\nand, in particular, includes mean-reverting dynamics.\n  The equilibrium is characterized by a system of fully coupled quadratic\nbackward stochastic differential equations, a solution to which is proved to\nexist under Markovian assumptions.\n"
    },
    {
        "paper_id": 1809.05961,
        "authors": "Bahman Angoshtari, Tim Leung",
        "title": "Optimal Dynamic Basis Trading",
        "comments": "27 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of dynamically trading a futures contract and its\nunderlying asset under a stochastic basis model. The basis evolution is modeled\nby a stopped scaled Brownian bridge to account for non-convergence of the basis\nat maturity. The optimal trading strategies are determined from a utility\nmaximization problem under hyperbolic absolute risk aversion (HARA) risk\npreferences. By analyzing the associated Hamilton-Jacobi-Bellman equation, we\nderive the exact conditions under which the equation admits a solution and\nsolve the utility maximization explicitly. A series of numerical examples are\nprovided to illustrate the optimal strategies and examine the effects of model\nparameters.\n"
    },
    {
        "paper_id": 1809.06027,
        "authors": "Dave Cliff",
        "title": "BSE: A Minimal Simulation of a Limit-Order-Book Stock Exchange",
        "comments": "10 pages, 6 figures. To appear in Proceedings of 30th European\n  Modelling and Simulation Symposium (EMSS-2018), Budapest, Hungary, September\n  17-19, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes the design, implementation, and successful use of the\nBristol Stock Exchange (BSE), a novel minimal simulation of a centralised\nfinancial market, based on a Limit Order Book (LOB) such as is common in major\nstock exchanges. Construction of BSE was motivated by the fact that most of the\nworld's major financial markets have automated, with trading activity that\npreviously was the responsibility of human traders now being performed by\nhigh-speed autonomous automated trading systems. Research aimed at\nunderstanding the dynamics of this new style of financial market is hampered by\nthe fact that no operational real-world exchange is ever likely to allow\nexperimental probing of that market while it is open and running live, forcing\nresearchers to work primarily from time-series of past trading data. Similarly,\nuniversity-level education of the engineers who can create next-generation\nautomated trading systems requires that they have hands-on learning experience\nin a sufficiently realistic teaching environment. BSE as described here\naddresses both those needs: it has been successfully used for teaching and\nresearch in a leading UK university since 2012, and the BSE program code is\nfreely available as open-source on GitHuB.\n"
    },
    {
        "paper_id": 1809.06077,
        "authors": "Sourish Das",
        "title": "Modeling Nelson-Siegel Yield Curve using Bayesian Approach",
        "comments": "Book chapter; 14 pages, 5 tables, 7 figures",
        "journal-ref": "New Perspectives and Challenges in Econophysics and Sociophysics,\n  (2018) Edited by. F. Abergel, B.K. Chakrabarti, A. Chakraborti, N. Deo and K.\n  Sharma (Springer New Economic Windows)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Yield curve modeling is an essential problem in finance. In this work, we\nexplore the use of Bayesian statistical methods in conjunction with\nNelson-Siegel model. We present the hierarchical Bayesian model for the\nparameters of the Nelson-Siegel yield function. We implement the MAP estimates\nvia BFGS algorithm in rstan. The Bayesian analysis relies on the Monte Carlo\nsimulation method. We perform the Hamiltonian Monte Carlo (HMC), using the\nrstan package. As a by-product of the HMC, we can simulate the Monte Carlo\nprice of a Bond, and it helps us to identify if the bond is over-valued or\nunder-valued. We demonstrate the process with an experiment and US Treasury's\nyield curve data. One of the interesting observation of the experiment is that\nthere is a strong negative correlation between the price and long-term effect\nof yield. However, the relationship between the short-term interest rate effect\nand the value of the bond is weakly positive. This is because posterior\nanalysis shows that the short-term effect and the long-term effect are\nnegatively correlated.\n"
    },
    {
        "paper_id": 1809.06153,
        "authors": "Zorana Grbac, David Krief, Peter Tankov",
        "title": "Long-time trajectorial large deviations for affine stochastic volatility\n  models and application to variance reduction for option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work extends the variance reduction method for the pricing of possibly\npath-dependent derivatives, which was developed in (Genin and Tankov, 2016) for\nexponential L\\'evy models, to affine stochastic volatility models\n(Keller-Ressel, 2011). We begin by proving a pathwise large deviations\nprinciple for affine stochastic volatility models. We then apply a\ntime-dependent Esscher transform to the affine process and use Varadhan's\nlemma, in the fashion of (Guasoni and Robertson, 2008) and (Robertson, 2010),\nto approximate the problem of finding the Esscher measure that minimises the\nvariance of the Monte-Carlo estimator. We test the method on the Heston model\nwith and without jumps to demonstrate the numerical efficiency of the method.\n"
    },
    {
        "paper_id": 1809.06421,
        "authors": "Vitalik Buterin, Zoe Hitzig, E. Glen Weyl",
        "title": "A Flexible Design for Funding Public Goods",
        "comments": null,
        "journal-ref": "2019 Management Science 65(11): 5171-5187",
        "doi": "10.1287/mnsc.2019.3337",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a design for philanthropic or publicly-funded seeding to allow\n(near) optimal provision of a decentralized, self-organizing ecosystem of\npublic goods. The concept extends ideas from Quadratic Voting to a funding\nmechanism for endogenous community formation. Individuals make public goods\ncontributions to projects of value to them. The amount received by the project\nis (proportional to) the square of the sum of the square roots of contributions\nreceived. Under the \"standard model\" this yields first best public goods\nprovision. Variations can limit the cost, help protect against collusion and\naid coordination. We discuss applications to campaign finance, open source\nsoftware ecosystems, news media finance and urban public projects. More\nbroadly, we offer a resolution to the classic liberal-communitarian debate in\npolitical philosophy by providing neutral and non-authoritarian rules that\nnonetheless support collective organization.\n"
    },
    {
        "paper_id": 1809.06471,
        "authors": "Jorge Faleiro",
        "title": "A Language for Large-Scale Collaboration in Economics: A Streamlined\n  Computational Representation of Financial Models",
        "comments": "20 pages, 12 equations, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper introduces Sigma, a domain-specific computational representation\nfor collaboration in large-scale for the field of economics. A computational\nrepresentation is not a programming language or a software platform. A\ncomputational representation is a domain-specific representation system based\non three specific elements: facets, contributions, and constraints of data.\nFacets are definable aspects that make up a subject or an object. Contributions\nare shareable and formal evidence, carrying specific properties, and produced\nas a result of a crowd-based scientific investigation. Constraints of data are\nrestrictions defining domain-specific rules of association between entities and\nrelationships. A computational representation serves as a layer of abstraction\nthat is required in order to define domain-specific concepts in computers, in a\nway these concepts can be shared in a crowd for the purposes of a controlled\nscientific investigation in large-scale by crowds. Facets, contributions, and\nconstraints of data are defined for any domain of knowledge by the application\nof a generic set of inputs, procedural steps, and products called a\nrepresentational process. The application of this generic process to our domain\nof knowledge, the field of economics, produces Sigma. Sigma is described in\nthis paper in terms of its three elements: facets (streaming, reactives,\ndistribution, and simulation), contributions (financial models, processors, and\nendpoints), and constraints of data (configuration, execution, and simulation\nmeta-model). Each element of the generic representational process and the Sigma\ncomputational representation is described and formalized in details.\n"
    },
    {
        "paper_id": 1809.06592,
        "authors": "Daniela Escobar and Georg Pflug",
        "title": "The distortion principle for insurance pricing: properties,\n  identification and robustness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distortion (Denneberg 1990) is a well known premium calculation principle for\ninsurance contracts. In this paper, we study sensitivity properties of\ndistortion functionals w.r.t. the assumptions for risk aversion as well as\nrobustness w.r.t. ambiguity of the loss distribution. Ambiguity is measured by\nthe Wasserstein distance. We study variances of distances for probability\nmodels and identify some worst case distributions. In addition to the direct\nproblem we also investigate the inverse problem, that is how to identify the\ndistortion density on the basis of observations of insurance premia.\n"
    },
    {
        "paper_id": 1809.06643,
        "authors": "Mesias Alfeus, Martino Grasselli and Erik Schl\\\"ogl",
        "title": "A Consistent Stochastic Model of the Term Structure of Interest Rates\n  for Multiple Tenors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Explicitly taking into account the risk incurred when borrowing at a shorter\ntenor versus lending at a longer tenor (\"roll-over risk\"), we construct a\nstochastic model framework for the term structure of interest rates in which a\nfrequency basis (i.e. a spread applied to one leg of a swap to exchange one\nfloating interest rate for another of a different tenor in the same currency)\narises endogenously. This rollover risk consists of two components, a credit\nrisk component due to the possibility of being downgraded and thus facing a\nhigher credit spread when attempting to roll over short-term borrowing, and a\ncomponent reflecting the (systemic) possibility of being unable to roll over\nshort-term borrowing at the reference rate (e.g., LIBOR) due to an absence of\nliquidity in the market. The modelling framework is of \"reduced form\" in the\nsense that (similar to the credit risk literature) the source of credit risk is\nnot modelled (nor is the source of liquidity risk). However, the framework has\nmore structure than the literature seeking to simply model a different term\nstructure of interest rates for each tenor frequency, since relationships\nbetween rates for all tenor frequencies are established based on the modelled\nroll-over risk. We proceed to consider a specific case within this framework,\nwhere the dynamics of interest rate and roll-over risk are driven by a\nmultifactor Cox/Ingersoll/Ross-type process, show how such model can be\ncalibrated to market data, and used for relative pricing of interest rate\nderivatives, including bespoke tenor frequencies not liquidly traded in the\nmarket.\n"
    },
    {
        "paper_id": 1809.06728,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Rafa{\\l} Kowalski, Pawe{\\l} O\\'swi\\c{e}cimka,\n  Rafa{\\l} Rak, Robert G\\c{e}barowski",
        "title": "Dynamical variety of shapes in financial multifractality",
        "comments": "26 pages, 10 figures",
        "journal-ref": "Complexity, vol. 2018, Article ID 7015721 (2018)",
        "doi": "10.1155/2018/7015721",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of multifractality offers a powerful formal tool to filter out\nmultitude of the most relevant characteristics of complex time series. The\nrelated studies thus far presented in the scientific literature typically limit\nthemselves to evaluation of whether or not a time series is multifractal and\nwidth of the resulting singularity spectrum is considered a measure of the\ndegree of complexity involved. However, the character of the complexity of time\nseries generated by the natural processes usually appears much more intricate\nthan such a bare statement can reflect. As an example, based on the long-term\nrecords of S&P500 and NASDAQ - the two world leading stock market indices - the\npresent study shows that they indeed develop the multifractal features, but\nthese features evolve through a variety of shapes, most often strongly\nasymmetric, whose changes typically are correlated with the historically most\nsignificant events experienced by the world economy. Relating at the same time\nthe index multifractal singularity spectra to those of the component stocks\nthat form this index reflects the varying degree of correlations involved among\nthe stocks.\n"
    },
    {
        "paper_id": 1809.06736,
        "authors": "Jean-Philippe Aguilar",
        "title": "On expansions for the Black-Scholes prices and hedge parameters",
        "comments": "v4 (Some typos corrected + adding the DOI). To appear in Journal of\n  Mathematical Analysis and Applications. arXiv admin note: text overlap with\n  arXiv:1710.01141",
        "journal-ref": null,
        "doi": "10.1016/j.jmaa.2019.06.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive new formulas for the price of the European call and put options in\nthe Black-Scholes model, under the form of uniformly convergent series\ngeneralizing previously known approximations. We also provide precise\nboundaries for the convergence speed and apply the results to the calculation\nof hedge parameters (Greeks).\n"
    },
    {
        "paper_id": 1809.0704,
        "authors": "Jessica Martin (INSA Toulouse, IMT), Anthony R\\'eveillac (INSA\n  Toulouse, IMT)",
        "title": "Analysis of the Risk-Sharing Principal-Agent problem through the\n  Reverse-H{\\\"o}lder inequality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide an alternative framework to tackle the first-best\nPrincipal-Agent problem under CARA utilities. This framework leads to both a\nproof of existence and uniqueness of the solution to the Risk-Sharing problem\nunder very general assumptions on the underlying contract space. Our analysis\nrelies on an optimal decomposition of the expected utility of the Principal in\nterms of the reservation utility of the Agent and works both in a discrete time\nand continuous time setting. As a by-product this approach provides a novel way\nof characterizing the optimal contract in the CARA setting, which is as an\nalternative to the widely used Lagrangian method, and some analysis of the\noptimum.\n"
    },
    {
        "paper_id": 1809.071,
        "authors": "Hirdesh K. Pharasi, Kiran Sharma, Anirban Chakraborti and Thomas H.\n  Seligman",
        "title": "Complex market dynamics in the light of random matrix theory",
        "comments": "22 pages, 12 figures. To appear in the Proceedings of Econophys-2017\n  and APEC-2017, New Delhi",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a brief overview of random matrix theory (RMT) with the objectives\nof highlighting the computational results and applications in financial markets\nas complex systems. An oft-encountered problem in computational finance is the\nchoice of an appropriate epoch over which the empirical cross-correlation\nreturn matrix is computed. A long epoch would smoothen the fluctuations in the\nreturn time series and suffers from non-stationarity, whereas a short epoch\nresults in noisy fluctuations in the return time series and the correlation\nmatrices turn out to be highly singular. An effective method to tackle this\nissue is the use of the power mapping, where a non-linear distortion is applied\nto a short epoch correlation matrix. The value of distortion parameter controls\nthe noise-suppression. The distortion also removes the degeneracy of zero\neigenvalues. Depending on the correlation structures, interesting properties of\nthe eigenvalue spectra are found. We simulate different correlated Wishart\nmatrices to compare the results with empirical return matrices computed using\nthe S&P 500 (USA) market data for the period 1985-2016. We also briefly review\ntwo recent applications of RMT in financial stock markets: (i) Identification\nof \"market states\" and long-term precursor to a critical state; (ii)\nCharacterization of catastrophic instabilities (market crashes).\n"
    },
    {
        "paper_id": 1809.07195,
        "authors": "Jorge Faleiro",
        "title": "Enabling Scientific Crowds: The Theory of Enablers for Crowd-Based\n  Scientific Investigation",
        "comments": "6 pages. arXiv admin note: text overlap with arXiv:1809.02671",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Evidence shows that in a significant number of cases the current methods of\nresearch do not allow for reproducible and falsifiable procedures of scientific\ninvestigation. As a consequence, the majority of critical decisions at all\nlevels, from personal investment choices to overreaching global policies, rely\non some variation of try-and-error and are mostly non-scientific by definition.\nWe lack transparency for procedures and evidence, proper explanation of market\nevents, predictability on effects, or identification of causes. There is no\nclear demarcation of what is inherently scientific, and as a consequence, the\nline between fake and genuine is blurred. This paper presents highlights of the\nTheory of Enablers for Crowd-Based Scientific Investigation, or Theory of\nEnablers for short. The Theory of Enablers assumes the use of a next-generation\ninvestigative approach leveraging forces of human diversity, micro-specialized\ncrowds, and proper computer-assisted control methods associated with\naccessibility, reproducibility, communication, and collaboration. This paper\ndefines the set of very specific cognitive and non-cognitive enablers for\ncrowd-based scientific investigation: methods of proof, large-scale\ncollaboration, and a domain-specific computational representation. These\nenablers allow the application of procedures of structured scientific\ninvestigation powered by crowds, a collective brain in which neurons are human\ncollaborators\n"
    },
    {
        "paper_id": 1809.07203,
        "authors": "Junyan Liu, Sandeep Kumar, and Daniel P. Palomar",
        "title": "Parameter Estimation of Heavy-Tailed AR Model with Missing Data via\n  Stochastic EM",
        "comments": "This is a companion document to a paper that is accepted to IEEE\n  Transaction on Signal Processing 2019, complemented with the supplementary\n  material",
        "journal-ref": null,
        "doi": "10.1109/TSP.2019.2899816",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The autoregressive (AR) model is a widely used model to understand time\nseries data. Traditionally, the innovation noise of the AR is modeled as\nGaussian. However, many time series applications, for example, financial time\nseries data, are non-Gaussian, therefore, the AR model with more general\nheavy-tailed innovations is preferred. Another issue that frequently occurs in\ntime series is missing values, due to system data record failure or unexpected\ndata loss. Although there are numerous works about Gaussian AR time series with\nmissing values, as far as we know, there does not exist any work addressing the\nissue of missing data for the heavy-tailed AR model. In this paper, we consider\nthis issue for the first time, and propose an efficient framework for parameter\nestimation from incomplete heavy-tailed time series based on a stochastic\napproximation expectation maximization (SAEM) coupled with a Markov Chain Monte\nCarlo (MCMC) procedure. The proposed algorithm is computationally cheap and\neasy to implement. The convergence of the proposed algorithm to a stationary\npoint of the observed data likelihood is rigorously proved. Extensive\nsimulations and real datasets analyses demonstrate the efficacy of the proposed\nframework.\n"
    },
    {
        "paper_id": 1809.073,
        "authors": "Christian Bayer, Ra\\'ul Tempone, S\\\"oren Wolfers",
        "title": "Pricing American Options by Exercise Rate Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel method for the numerical pricing of American options based\non Monte Carlo simulation and the optimization of exercise strategies. Previous\nsolutions to this problem either explicitly or implicitly determine so-called\noptimal exercise regions, which consist of points in time and space at which a\ngiven option is exercised. In contrast, our method determines the exercise\nrates of randomized exercise strategies. We show that the supremum of the\ncorresponding stochastic optimization problem provides the correct option\nprice. By integrating analytically over the random exercise decision, we obtain\nan objective function that is differentiable with respect to perturbations of\nthe exercise rate even for finitely many sample paths. The global optimum of\nthis function can be approached gradually when starting from a constant\nexercise rate.\n  Numerical experiments on vanilla put options in the multivariate\nBlack-Scholes model and a preliminary theoretical analysis underline the\nefficiency of our method, both with respect to the number of\ntime-discretization steps and the required number of degrees of freedom in the\nparametrization of the exercise rates. Finally, we demonstrate the flexibility\nof our method through numerical experiments on max call options in the\nclassical Black-Scholes model, and vanilla put options in both the Heston model\nand the non-Markovian rough Bergomi model.\n"
    },
    {
        "paper_id": 1809.07407,
        "authors": "Luiz G. A. Alves, Giuseppe Mangioni, Francisco A. Rodrigues, Pietro\n  Panzarasa, and Yamir Moreno",
        "title": "Unfolding the complexity of the global value chain: Strengths and\n  entropy in the single-layer, multiplex, and multi-layer international trade\n  networks",
        "comments": "Accepted for publication in Entropy. 12 pages and 5 figures",
        "journal-ref": "Entropy 2018, 20(12), 909",
        "doi": "10.3390/e20120909",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The worldwide trade network has been widely studied through different data\nsets and network representations with a view to better understanding\ninteractions among countries and products. Here we investigate international\ntrade through the lenses of the single-layer, multiplex, and multi-layer\nnetworks. We discuss differences among the three network frameworks in terms of\ntheir relative advantages in capturing salient topological features of trade.\nWe draw on the World Input-Output Database to build the three networks. We then\nuncover sources of heterogeneity in the way strength is allocated among\ncountries and transactions by computing the strength distribution and entropy\nin each network. Additionally, we trace how entropy evolved, and show how the\nobserved peaks can be associated with the onset of the global economic\ndownturn. Findings suggest how more complex representations of trade, such as\nthe multi-layer network, enable us to disambiguate the distinct roles of intra-\nand cross-industry transactions in driving the evolution of entropy at a more\naggregate level. We discuss our results and the implications of our comparative\nanalysis of networks for research on international trade and other empirical\ndomains across the natural and social sciences.\n"
    },
    {
        "paper_id": 1809.07516,
        "authors": "Erhan Bayraktar, Matteo Burzoni",
        "title": "On the quasi-sure superhedging duality with frictions",
        "comments": "Final version. To appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the superhedging duality for a discrete-time financial market with\nproportional transaction costs under model uncertainty. Frictions are modeled\nthrough solvency cones as in the original model of [Kabanov, Y., Hedging and\nliquidation under transaction costs in currency markets. Fin. Stoch.,\n3(2):237-248, 1999] adapted to the quasi-sure setup of [Bouchard, B. and Nutz,\nM., Arbitrage and duality in nondominated discrete-time models. Ann. Appl.\nProbab., 25(2):823-859, 2015]. Our approach allows to remove the restrictive\nassumption of No Arbitrage of the Second Kind considered in [Bouchard, B.,\nDeng, S. and Tan, X., Super-replication with proportional transaction cost\nunder model uncertainty, Math. Fin., 29(3):837-860, 2019] and to show the\nduality under the more natural condition of No Strict Arbitrage. In addition,\nwe extend the results to models with portfolio constraints.\n"
    },
    {
        "paper_id": 1809.07545,
        "authors": "Sylvain Carr\\'e (EPFL), Pierre Collin-Dufresne (EPFL), Franck Gabriel",
        "title": "Insider Trading with Penalties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a one-period Kyle (1985) framework where the insider can be\nsubject to a penalty if she trades. We establish existence and uniqueness of\nequilibrium for virtually any penalty function when noise is uniform. In\nequilibrium, the demand of the insider and the price functions are in general\nnon-linear and remain analytically tractable because the expected price\nfunction is linear. We use this result to investigate the trade off between\nprice efficiency and 'fairness': we consider a regulator that wants to minimise\npost-trade standard deviation for a given level of uninformed traders' losses.\nThe minimisation is over the function space of penalties; for each possible\npenalty, our existence and uniqueness theorem allows to define unambiguously\nthe post-trade standard deviation and the uninformed traders' losses that\nprevail in equilibrium.Optimal penalties are characterized in closed-form. They\nmust increase quickly with the magnitude of the insider's order for small\norders and become flat for large orders: in cases where the fundamental\nrealizes at very high or very low values, the insider finds it optimal to trade\ndespite the high penalty. Although such trades-if they occur-are costly for\nliquidity traders, they signal extreme events and therefore incorporate a lot\nof information into prices. We generalize this result in two directions by\nimposing a budget constraint on the regulator and considering the cases of\neither non-pecuniary or pecuniary penalties. In the first case, we establish\nthat optimal penalties are a subset of the previously optimal penalties: the\npatterns of equilibrium trade volumes and prices is unchanged. In the second\ncase, we also fully characterize the constrained efficient points and penalties\nand show that new patterns emerge in the demand schedules of the insider trader\nand the associated price functions.\n"
    },
    {
        "paper_id": 1809.07727,
        "authors": "Peter Carr, Andrey Itkin",
        "title": "Geometric Local Variance Gamma model",
        "comments": "36 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1802.09611",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes another extension of the Local Variance Gamma model\noriginally proposed by P. Carr in 2008, and then further elaborated on by Carr\nand Nadtochiy, 2017 (CN2017), and Carr and Itkin, 2018 (CI2018). As compared\nwith the latest version of the model developed in CI2018 and called the ELVG\n(the Expanded Local Variance Gamma model), here we provide two innovations.\nFirst, in all previous papers the model was constructed based on a Gamma\ntime-changed {\\it arithmetic} Brownian motion: with no drift in CI2017, and\nwith drift in CI2018, and the local variance to be a function of the spot level\nonly. In contrast, here we develop a {\\it geometric} version of this model with\ndrift. Second, in CN2017 the model was calibrated to option smiles assuming the\nlocal variance is a piecewise constant function of strike, while in CI2018 the\nlocal variance is a piecewise linear} function of strike. In this paper we\nconsider 3 piecewise linear models: the local variance as a function of strike,\nthe local variance as function of log-strike, and the local volatility as a\nfunction of strike (so, the local variance is a piecewise quadratic function of\nstrike). We show that for all these new constructions it is still possible to\nderive an ordinary differential equation for the option price, which plays a\nrole of Dupire's equation for the standard local volatility model, and,\nmoreover, it can be solved in closed form. Finally, similar to CI2018, we show\nthat given multiple smiles the whole local variance/volatility surface can be\nrecovered which does not require solving any optimization problem. Instead, it\ncan be done term-by-term by solving a system of non-linear algebraic equations\nfor each maturity which is fast.\n"
    },
    {
        "paper_id": 1809.07856,
        "authors": "Nino Antulov-Fantulin and Dijana Tolic and Matija Piskorec and Zhang\n  Ce and Irena Vodenska",
        "title": "Inferring short-term volatility indicators from Bitcoin blockchain",
        "comments": null,
        "journal-ref": "7th International Conference on Complex Networks and their\n  Applications 2018",
        "doi": "10.1007/978-3-030-05414-4_41",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the possibility of inferring early warning indicators\n(EWIs) for periods of extreme bitcoin price volatility using features obtained\nfrom Bitcoin daily transaction graphs. We infer the low-dimensional\nrepresentations of transaction graphs in the time period from 2012 to 2017\nusing Bitcoin blockchain, and demonstrate how these representations can be used\nto predict extreme price volatility events. Our EWI, which is obtained with a\nnon-negative decomposition, contains more predictive information than those\nobtained with singular value decomposition or scalar value of the total Bitcoin\ntransaction volume.\n"
    },
    {
        "paper_id": 1809.0806,
        "authors": "Maxime Morariu-Patrichi and Mikko S. Pakkanen",
        "title": "State-dependent Hawkes processes and their application to limit order\n  book modelling",
        "comments": "33 pages, 13 figures, v3: added a comparison with other related\n  models plus some other minor amendments, to appear in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study statistical aspects of state-dependent Hawkes processes, which are\nan extension of Hawkes processes where a self- and cross-exciting counting\nprocess and a state process are fully coupled, interacting with each other. The\nexcitation kernel of the counting process depends on the state process that,\nreciprocally, switches state when there is an event in the counting process. We\nfirst establish the existence and uniqueness of state-dependent Hawkes\nprocesses and explain how they can be simulated. Then we develop maximum\nlikelihood estimation methodology for parametric specifications of the process.\nWe apply state-dependent Hawkes processes to high-frequency limit order book\ndata, allowing us to build a novel model that captures the feedback loop\nbetween the order flow and the shape of the limit order book. We estimate two\nspecifications of the model, using the bid-ask spread and the queue imbalance\nas state variables, and find that excitation effects in the order flow are\nstrongly state-dependent. Additionally, we find that the endogeneity of the\norder flow, measured by the magnitude of excitation, is also state-dependent,\nbeing more pronounced in disequilibrium states of the limit order book.\n"
    },
    {
        "paper_id": 1809.08139,
        "authors": "Sahar Albosaily and Serguei Pergamenshchikov",
        "title": "Optimal investment and consumption for Ornstein-Uhlenbeck spread\n  financial markets with logarithmic utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a spread financial market defined by the multidimensional\nOrnstein--Uhlenbeck (OU) process. We study the optimal consumption/investment\nproblem for logarithmic utility functions in the base of stochastic dynamical\nprogramming method. We show a special Verification Theorem for this case. We\nfind the solution to the Hamilton--Jacobi--Bellman (HJB) equation in explicit\nform and as a consequence we construct the optimal financial strategies.\nMoreover, we study the constructed strategy by numerical simulations.\n"
    },
    {
        "paper_id": 1809.08146,
        "authors": "L. S. Di Mauro, A. Pluchino, A. E. Biondo",
        "title": "A Game of Tax Evasion: evidences from an agent-based model",
        "comments": "19 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a simple agent-based model of an economic system,\npopulated by agents playing different games according to their different view\nabout social cohesion and tax payment. After a first set of simulations,\ncorrectly replicating results of existing literature, a wider analysis is\npresented in order to study the effects of a dynamic-adaptation rule, in which\ncitizens may possibly decide to modify their individual tax compliance\naccording to individual criteria, such as, the strength of their ethical\ncommitment, the satisfaction gained by consumption of the public good and the\nperceived opinion of neighbors. Results show the presence of thresholds levels\nin the composition of society - between taxpayers and evaders - which explain\nthe extent of damages deriving from tax evasion.\n"
    },
    {
        "paper_id": 1809.082,
        "authors": "Oleg Yu. Vorobyev",
        "title": "Eventological H-theorem",
        "comments": "8 pages",
        "journal-ref": "Proc. of the VII All-Russian FAM Conf. on Financial and Actuarial\n  Mathematics and Related Fields. Krasnoyarsk, KSU (Oleg Vorobyev ed.), part 1,\n  2008, 51-58",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the eventological $H$-theorem that complements the Boltzmann\nH-theorem from statistical mechanics and serves as a mathematical excuse\n(mathematically no less convincing than the Boltzmann H-theorem for the second\nlaw of thermodynamics) for what can be called \"the second law of eventology\",\nwhich justifies the application of Gibbs and \"anti-Gibbs\" distributions of sets\nof events minimizing relative entropy, as statistical models of the behavior of\na rational subject, striving for an equilibrium eventological choice between\nperception and activity in various spheres of her/his co-being.\n"
    },
    {
        "paper_id": 1809.08262,
        "authors": "Jin Ma, Ting-Kam Leonard Wong, Jianfeng Zhang",
        "title": "Time-consistent conditional expectation under probability distortion",
        "comments": "38 pages, 4 figures. To appear in Mathematics of Operations Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new notion of conditional nonlinear expectation under\nprobability distortion. Such a distorted nonlinear expectation is not\nsub-additive in general, so it is beyond the scope of Peng's framework of\nnonlinear expectations. A more fundamental problem when extending the distorted\nexpectation to a dynamic setting is time-inconsistency, that is, the usual\n\"tower property\" fails. By localizing the probability distortion and\nrestricting to a smaller class of random variables, we introduce a so-called\ndistorted probability and construct a conditional expectation in such a way\nthat it coincides with the original nonlinear expectation at time zero, but has\na time-consistent dynamics in the sense that the tower property remains valid.\nFurthermore, we show that in the continuous time model this conditional\nexpectation corresponds to a parabolic differential equation whose coefficient\ninvolves the law of the underlying diffusion. This work is the first step\ntowards a new understanding of nonlinear expectations under probability\ndistortion, and will potentially be a helpful tool for solving\ntime-inconsistent stochastic optimization problems.\n"
    },
    {
        "paper_id": 1809.08293,
        "authors": "Alberto Banterle",
        "title": "The \"power\" dimension in a process of exchange",
        "comments": "26 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The field of study of this paper is the analysis of the exchange between two\nsubjects. Circumscribed to the micro dimension, it is however expanded with\nrespect to standard economic theory by introducing both the dimension of power\nand the motivation to exchange. The basic reference is made by the reflections\nof those economists, preeminently John Kenneth Galbraith, who criticize the\nremoval from the neoclassical economy of the \"power\" dimension. We have also\nreferred to the criticism that Galbraith, among others, makes to the assumption\nof neoclassical economists that the \"motivation\" in exchanges is solely linked\nto the reward, to the money obtained in the exchange. We have got around the\nproblem of having a large number of types of power and also a large number of\nforms of motivation by directly taking into account the effects on the welfare\nof each subject, regardless of the means with which they are achieved: that is,\nreferring to everything that happens in the negotiation process to the\npotential or real variations of the welfare function induced in each subject\ndue to the exercise of the specific form of power, on a case by case basis, and\nof the intensity of the motivation to perform the exchange. In the construction\nof a mathematical model we paid great attention to its usability in field\ntesting.\n"
    },
    {
        "paper_id": 1809.0839,
        "authors": "Junfeng Jiang, Jiahao Li",
        "title": "Constructing Financial Sentimental Factors in Chinese Market Using\n  Natural Language Processing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we design an integrated algorithm to evaluate the sentiment of\nChinese market. Firstly, with the help of the web browser automation, we crawl\na lot of news and comments from several influential financial websites\nautomatically. Secondly, we use techniques of Natural Language Processing(NLP)\nunder Chinese context, including tokenization, Word2vec word embedding and\nsemantic database WordNet, to compute Senti-scores of these news and comments,\nand then construct the sentimental factor. Here, we build a finance-specific\nsentimental lexicon so that the sentimental factor can reflect the sentiment of\nfinancial market but not the general sentiments as happiness, sadness, etc.\nThirdly, we also implement an adjustment of the standard sentimental factor.\nOur experimental performance shows that there is a significant correlation\nbetween our standard sentimental factor and the Chinese market, and the\nadjusted factor is even more informative, having a stronger correlation with\nthe Chinese market. Therefore, our sentimental factors can be important\nreferences when making investment decisions. Especially during the Chinese\nmarket crash in 2015, the Pearson correlation coefficient of adjusted\nsentimental factor with SSE is 0.5844, which suggests that our model can\nprovide a solid guidance, especially in the special period when the market is\ninfluenced greatly by public sentiment.\n"
    },
    {
        "paper_id": 1809.08403,
        "authors": "Josselin Garnier, Knut Solna",
        "title": "Chaos and Order in the Bitcoin Market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.04.164",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The bitcoin price has surged in recent years and it has also exhibited phases\nof rapid decay. In this paper we address the question to what extent this novel\ncryptocurrency market can be viewed as a classic or semi-efficient market.\nNovel and robust tools for estimation of multi-fractal properties are used to\nshow that the bitcoin price exhibits a very interesting multi-scale correlation\nstructure. This structure can be described by a power-law behavior of the\nvariances of the returns as functions of time increments and it can be\ncharacterized by two parameters, the volatility and the Hurst exponent. These\npower-law parameters, however, vary in time. A new notion of generalized Hurst\nexponent is introduced which allows us to check if the multi-fractal character\nof the underlying signal is well captured. It is moreover shown how the\nmonitoring of the power-law parameters can be used to identify regime shifts\nfor the bitcoin price. A novel technique for identifying the regimes switches\nbased on a goodness of fit of the local power-law parameters is presented. It\nautomatically detects dates associated with some known events in the bitcoin\nmarket place. A very surprising result is moreover that, despite the wild ride\nof the bitcoin price in recent years and its multi-fractal and non-stationary\ncharacter, this price has both local power-law behaviors and a very orderly\ncorrelation structure when it is observed on its entire period of existence.\n"
    },
    {
        "paper_id": 1809.08416,
        "authors": "Henrik O. Rasmussen and Paul Wilmott",
        "title": "Tail probabilities for short-term returns on stocks",
        "comments": "Minor fixes",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.18816.28165",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the tail probabilities of stock returns for a general class of\nstochastic volatility models. In these models, the stochastic differential\nequation for volatility is autonomous, time-homogeneous and dependent on only a\nfinite number of dimensional parameters. Three bounds on the high-volatility\nlimits of the drift and diffusion coefficients of volatility ensure that\nvolatility is mean-reverting, has long memory and is as volatile as the stock\nprice. Dimensional analysis then provides leading-order approximations to the\ndrift and diffusion coefficients of volatility for the high-volatility limit.\nThereby, using the Kolmogorov forward equation for the transition probability\nof volatility, we find that the tail probability for short-term returns falls\noff like an inverse cubic. Our analysis then provides a possible explanation\nfor the inverse cubic fall off that Gopikrishnan et al. (1998) report for\nreturns over 5-120 minutes intervals. We find, moreover, that the tail\nprobability scales like the length of the interval, over which the return is\nmeasured, to the power 3/2. There do not seem to be any empirical results in\nthe literature with which to compare this last prediction.\n"
    },
    {
        "paper_id": 1809.085,
        "authors": "Patrick Barranger, Rohit Nair, Rob Mulla, Shane Conner",
        "title": "Eliciting the Endowment Effect under Assigned Ownership",
        "comments": "14 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we present evidence that endowment effect can be elicited\nmerely by assigned ownership. Using Google Customer Survey, we administered a\nsurvey were participants (n=495) were randomly split into 4 groups. Each group\nwas assigned ownership of either legroom or their ability to recline on an\nairline. Using this experiment setup we were able to generate endowment effect,\na 15-20x (at p<0.05) increase between participant's willingness to pay (WTP)\nand their willingness to accept (WTA).\n"
    },
    {
        "paper_id": 1809.08635,
        "authors": "Alan L. Lewis",
        "title": "Exact Solutions for a GBM-type Stochastic Volatility Model having a\n  Stationary Distribution",
        "comments": "42 pages, 5 figures, one typo (eqn (66)) corrected",
        "journal-ref": "Wilmott mag. 101 (2019) 20-41",
        "doi": "10.1002/wilm.10761",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find various exact solutions for a new stochastic volatility (SV) model:\nthe transition probability density, European-style option values, and (when it\nexists) the martingale defect. This may represent the first example of an SV\nmodel combining exact solutions, GBM-type volatility noise, and a stationary\nvolatility density.\n"
    },
    {
        "paper_id": 1809.08681,
        "authors": "Yuri Biondi and Stefano Olla",
        "title": "Financial accumulation implies ever-increasing wealth inequality",
        "comments": null,
        "journal-ref": "Journal of Economic Interaction and Coordination (JEIC), vol.\n  15(4), October 2020, pages 943-951",
        "doi": "10.1007/s11403-020-00281-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wealth inequality is an important matter for economic theory and policy.\nOngoing debates have been discussing recent rise in wealth inequality in\nconnection with recent development of active financial markets around the\nworld. Existing literature on wealth distribution connects the origins of\nwealth inequality with a variety of drivers. Our approach develops a minimalist\nmodelling strategy that combines three featuring mechanisms: active financial\nmarkets; individual wealth accumulation; and compound interest structure. We\nprovide mathematical proof that accumulated financial investment returns\ninvolve ever-increasing wealth concentration and inequality across individual\ninvestors through time. This cumulative effect through space and time depends\non the financial accumulation process and holds also under efficient financial\nmarkets, which generate some fair investment game that individual investors do\nrepeatedly play through time.\n"
    },
    {
        "paper_id": 1809.08718,
        "authors": "Ancil Crayton",
        "title": "Central Bank Communication and the Yield Curve: A Semi-Automatic\n  Approach using Non-Negative Matrix Factorization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Communication is now a standard tool in the central bank's monetary policy\ntoolkit. Theoretically, communication provides the central bank an opportunity\nto guide public expectations, and it has been shown empirically that central\nbank communication can lead to financial market fluctuations. However, there\nhas been little research into which dimensions or topics of information are\nmost important in causing these fluctuations. We develop a semi-automatic\nmethodology that summarizes the FOMC statements into its main themes,\nautomatically selects the best model based on coherency, and assesses whether\nthere is a significant impact of these themes on the shape of the U.S Treasury\nyield curve using topic modeling methods from the machine learning literature.\nOur findings suggest that the FOMC statements can be decomposed into three\ntopics: (i) information related to the economic conditions and the mandates,\n(ii) information related to monetary policy tools and intermediate targets, and\n(iii) information related to financial markets and the financial crisis. We\nfind that statements are most influential during the financial crisis and the\neffects are mostly present in the curvature of the yield curve through\ninformation related to the financial theme.\n"
    },
    {
        "paper_id": 1809.0896,
        "authors": "Nikolai Dokuchaev",
        "title": "On a gap between rational annuitization price for producer and price for\n  customer",
        "comments": null,
        "journal-ref": "Journal of Revenue and Pricing Management 2019 V. 18, Issue 2, pp.\n  147-154",
        "doi": "10.1057/s41272-018-00163-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies pricing of insurance products focusing on the pricing of\nannuities under uncertainty. This pricing problem is crucial for financial\ndecision making and was studied intensively, however, many open questions still\nremain. In particular, there is a so-called \"annuity puzzle\" related to certain\ninconsistency of existing financial theory with the empirical observations for\nthe annuities market. The paper suggests a pricing method based on the risk\nminimization such that both producer and customer seek to minimize the mean\nsquare hedging error accepted as a measure of risk. This leads to two different\nversions of the pricing problem: the selection of the annuity price given the\nrate of regular payments, and the selection of the rate of payments given the\nannuity price. It appears that solutions of these two problems are different.\nThis can contribute to explanation for the \"annuity puzzle\".\n"
    },
    {
        "paper_id": 1809.09069,
        "authors": "Javier de Frutos, Victor Gaton",
        "title": "An extension of Heston's SV model to Stochastic Interest Rates",
        "comments": null,
        "journal-ref": "Journal of Computational and Applied Mathematics Volume 354, July\n  2019, Pages 174-182",
        "doi": "10.1016/j.cam.2018.09.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 'A Closed-Form Solution for Options with Stochastic Volatility with\nApplications to Bond and Currency Options', Heston proposes a Stochastic\nVolatility (SV) model with constant interest rate and derives a semi-explicit\nvaluation formula. Heston also describes, in general terms, how the model could\nbe extended to incorporate Stochastic Interest Rates (SIR). This paper is\ndevoted to the construction of an extension of Heston's SV model with a\nparticular stochastic bond model which, just increasing in one the number of\nparameters, allows to incorporate SIR and to derive a semi-explicit formula for\noption pricing.\n"
    },
    {
        "paper_id": 1809.09243,
        "authors": "Yu-Jui Huang and Zhou Zhou",
        "title": "Strong and Weak Equilibria for Time-Inconsistent Stochastic Control in\n  Continuous Time",
        "comments": null,
        "journal-ref": "Mathematics of Operations Research, Vol. 46 (2021), Issue 2, pp\n  428-451",
        "doi": "10.1287/moor.2020.1066",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new definition of continuous-time equilibrium controls is introduced. As\nopposed to the standard definition, which involves a derivative-type operation,\nthe new definition parallels how a discrete-time equilibrium is defined, and\nallows for unambiguous economic interpretation. The terms \"strong equilibria\"\nand \"weak equilibria\" are coined for controls under the new and the standard\ndefinitions, respectively. When the state process is a time-homogeneous\ncontinuous-time Markov chain, a careful asymptotic analysis gives complete\ncharacterizations of weak and strong equilibria. Thanks to Kakutani-Fan's\nfixed-point theorem, general existence of weak and strong equilibria is also\nestablished, under additional compactness assumption. Our theoretic results are\napplied to a two-state model under non-exponential discounting. In particular,\nwe demonstrate explicitly that there can be incentive to deviate from a weak\nequilibrium, which justifies the need for strong equilibria. Our analysis also\nprovides new results for the existence and characterization of discrete-time\nequilibria under infinite horizon.\n"
    },
    {
        "paper_id": 1809.09268,
        "authors": "Paul Embrechts, Alexander Schied, Ruodu Wang",
        "title": "Robustness in the Optimization of Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study issues of robustness in the context of Quantitative Risk Management\nand Optimization. We develop a general methodology for determining whether a\ngiven risk measurement related optimization problem is robust, which we call\n\"robustness against optimization\". The new notion is studied for various\nclasses of risk measures and expected utility and loss functions. Motivated by\npractical issues from financial regulation, special attention is given to the\ntwo most widely used risk measures in the industry, Value-at-Risk (VaR) and\nExpected Shortfall (ES). We establish that for a class of general optimization\nproblems, VaR leads to non-robust optimizers whereas convex risk measures\ngenerally lead to robust ones. Our results offer extra insight on the ongoing\ndiscussion about the comparative advantages of VaR and ES in banking and\ninsurance regulation. Our notion of robustness is conceptually different from\nthe field of robust optimization, to which some interesting links are derived.\n"
    },
    {
        "paper_id": 1809.09273,
        "authors": "Misha Perepelitsa and Ilya Timofeyev",
        "title": "Asynchronous stochastic price pump",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.10.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for equity trading in a population of agents where each\nagent acts to achieve his or her target stock-to-bond ratio, and, as a feedback\nmechanism, follows a market adaptive strategy. In this model only a fraction of\nagents participates in buying and selling stock during a trading period, while\nthe rest of the group accepts the newly set price. Using numerical simulations\nwe show that the stochastic process settles on a stationary regime for the\nreturns. The mean return can be greater or less than the return on the bond and\nit is determined by the parameters of the adaptive mechanism. When the number\nof interacting agents is fixed, the distribution of the returns follows the\nlog-normal density. In this case, we give an analytic formula for the mean rate\nof return in terms of the rate of change of agents' risk levels and confirm the\nformula by numerical simulations. However, when the number of interacting\nagents per period is random, the distribution of returns can significantly\ndeviate from the log-normal, especially as the variance of the distribution for\nthe number of interacting agents increases.\n"
    },
    {
        "paper_id": 1809.09441,
        "authors": "Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu, Tat-Seng\n  Chua",
        "title": "Temporal Relational Ranking for Stock Prediction",
        "comments": "Transactions on Information Systems (TOIS)",
        "journal-ref": "ACM Trans. Inf. Syst. 37, 2, Article 27 (March 2019), 30 pages",
        "doi": "10.1145/3309547",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock prediction aims to predict the future trends of a stock in order to\nhelp investors to make good investment decisions. Traditional solutions for\nstock prediction are based on time-series models. With the recent success of\ndeep neural networks in modeling sequential data, deep learning has become a\npromising choice for stock prediction. However, most existing deep learning\nsolutions are not optimized towards the target of investment, i.e., selecting\nthe best stock with the highest expected revenue. Specifically, they typically\nformulate stock prediction as a classification (to predict stock trend) or a\nregression problem (to predict stock price). More importantly, they largely\ntreat the stocks as independent of each other. The valuable signal in the rich\nrelations between stocks (or companies), such as two stocks are in the same\nsector and two companies have a supplier-customer relation, is not considered.\nIn this work, we contribute a new deep learning solution, named Relational\nStock Ranking (RSR), for stock prediction. Our RSR method advances existing\nsolutions in two major aspects: 1) tailoring the deep learning models for stock\nranking, and 2) capturing the stock relations in a time-sensitive manner. The\nkey novelty of our work is the proposal of a new component in neural network\nmodeling, named Temporal Graph Convolution, which jointly models the temporal\nevolution and relation network of stocks. To validate our method, we perform\nback-testing on the historical data of two stock markets, NYSE and NASDAQ.\nExtensive experiments demonstrate the superiority of our RSR method. It\noutperforms state-of-the-art stock prediction solutions achieving an average\nreturn ratio of 98% and 71% on NYSE and NASDAQ, respectively.\n"
    },
    {
        "paper_id": 1809.09466,
        "authors": "Imanol Perez Arribas",
        "title": "Derivatives pricing using signature payoffs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce signature payoffs, a family of path-dependent derivatives that\nare given in terms of the signature of the price path of the underlying asset.\nWe show that these derivatives are dense in the space of continuous payoffs, a\nresult that is exploited to quickly price arbitrary continuous payoffs. This\napproach to pricing derivatives is then tested with European options, American\noptions, Asian options, lookback options and variance swaps. As we show,\nsignature payoffs can be used to price these derivatives with very high\naccuracy.\n"
    },
    {
        "paper_id": 1809.09588,
        "authors": "David Criens",
        "title": "No Arbitrage in Continuous Financial Markets",
        "comments": "The article has been fully revised. To appear in \"Mathematics and\n  Financial Economics\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive integral tests for the existence and absence of arbitrage in a\nfinancial market with one risky asset which is either modeled as stochastic\nexponential of an Ito process or a positive diffusion with Markov switching. In\nparticular, we derive conditions for the existence of the minimal martingale\nmeasure. We also show that for Markov switching models the minimal martingale\nmeasure preserves the independence of the noise and we study how the minimal\nmartingale measure can be modified to change the structure of the switching\nmechanism. Our main mathematical tools are new criteria for the martingale and\nstrict local martingale property of certain stochastic exponentials.\n"
    },
    {
        "paper_id": 1809.09601,
        "authors": "Misha Perepelitsa",
        "title": "A model of adaptive, market behavior generating positive returns,\n  volatility and system risk",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a simple model for speculative trading based on adaptive behavior\nof economic agents.The adaptive behavior is expressed through a feedback\nmechanism for changing agents' stock-to-bond ratios, depending on the past\nperformance of their portfolios.The stock price is set according to the\ndemand-supply for the asset derived from the agents' target risk levels. Using\nthe methodology of agent-based modeling we show that agents, acting\nendogenously and adaptively, create a persistent price bubble. The price\ndynamics generated by the trading process does not reveal any singularities,\nhowever the process is accompanied by growing aggregated risk that indicates\nincreasing likelihood of a crash.\n"
    },
    {
        "paper_id": 1809.09724,
        "authors": "Fernando A Morales and Cristian C Chica and Carlos A Osorio and Daniel\n  Cabarcas J",
        "title": "A big data based method for pass rates optimization in mathematics\n  university lower division courses",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper an algorithm designed for large databases is introduced for the\nenhancement of pass rates in mathematical university lower division courses\nwith several sections. Using integer programming techniques, the algorithm\nfinds the optimal pairing of students and lecturers in order to maximize the\nsuccess chances of the students' body. The students-lecturer success\nprobability is computed according to their corresponding profiles stored in the\ndata bases.\n"
    },
    {
        "paper_id": 1809.09734,
        "authors": "Hassan Shavandi, Mehrdad Pirnia, J. David Fuller",
        "title": "Extended opportunity cost model to find near equilibrium electricity\n  prices under non-convexities",
        "comments": null,
        "journal-ref": "Applied Energy, April 2019, Pages 251-264, Volume 240",
        "doi": "10.1016/j.apenergy.2019.02.059",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper finds near equilibrium prices for electricity markets with\nnonconvexities due to binary variables, in order to reduce the market\nparticipants' opportunity costs, such as generators' unrecovered costs. The\nopportunity cost is defined as the difference between the profit when the\ninstructions of the market operator are followed and when the market\nparticipants can freely make their own decisions based on the market prices. We\nuse the minimum complementarity approximation to the minimum total opportunity\ncost (MTOC) model, from previous research, with tests on a much more realistic\nunit commitment (UC) model than in previous research, including features such\nas reserve requirements, ramping constraints, and minimum up and down times.\nThe developed model incorporates flexible price responsive demand, as in\nprevious research, but since not all demand is price responsive, we consider\nthe more realistic case that total demand is a mixture of fixed and flexible.\nAnother improvement over previous MTOC research is computational: whereas the\nprevious research had nonconvex terms among the objective function's continuous\nvariables, we convert the objective to an equivalent form that contains only\nlinear and convex quadratic terms in the continuous variables. We compare the\nunit commitment model with the standard social welfare optimization version of\nUC, in a series of sensitivity analyses, varying flexible demand to represent\nvarying degrees of future penetration of electric vehicles and smart\nappliances, different ratios of generation availability, and different values\nof transmission line capacities to consider possible congestion. The minimum\ntotal opportunity cost and social welfare solutions are mostly very close in\ndifferent scenarios, except in some extreme cases.\n"
    },
    {
        "paper_id": 1809.09889,
        "authors": "Marius Pfeuffer, Goncalo dos Reis, Greig smith",
        "title": "Capturing Model Risk and Rating Momentum in the Estimation of\n  Probabilities of Default and Credit Rating Migrations",
        "comments": "22 pages, 5 Figures, 4 Tables. To Appear in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present two methodologies on the estimation of rating transition\nprobabilities within Markov and non-Markov frameworks. We first estimate a\ncontinuous-time Markov chain using discrete (missing) data and derive a simpler\nexpression for the Fisher information matrix, reducing the computational time\nneeded for the Wald confidence interval by a factor of a half. We provide an\nefficient procedure for transferring such uncertainties from the generator\nmatrix of the Markov chain to the corresponding rating migration probabilities\nand, crucially, default probabilities.\n  For our second contribution, we assume access to the full (continuous) data\nset and propose a tractable and parsimonious self-exciting marked point\nprocesses model able to capture the non-Markovian effect of rating momentum.\nCompared to the Markov model, the non-Markov model yields higher probabilities\nof default in the investment grades, but also lower default probabilities in\nsome speculative grades. Both findings agree with empirical observations and\nhave clear practical implications.\n  We illustrate all methods using data from Moody's proprietary corporate\ncredit ratings data set. Implementations are available in the R package ctmcd.\n"
    },
    {
        "paper_id": 1809.10015,
        "authors": "Felix-Benedikt Liebrich and Gregor Svindland",
        "title": "Risk sharing for capital requirements with multidimensional security\n  markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the risk sharing problem for capital requirements induced by\ncapital adequacy tests and security markets. The agents involved in the sharing\nprocedure may be heterogeneous in that they apply varying capital adequacy\ntests and have access to different security markets. We discuss conditions\nunder which there exists a representative agent. Thereafter, we study two\nframeworks of capital adequacy more closely, polyhedral constraints and\ndistribution based constraints. We prove existence of optimal risk allocations\nand equilibria within these frameworks and elaborate on their robustness.\n"
    },
    {
        "paper_id": 1809.10123,
        "authors": "Ioannis Karatzas, Donghan Kim",
        "title": "Trading Strategies Generated Pathwise by Functions of Market Weights",
        "comments": "45 pages, 3 figures",
        "journal-ref": "Finance and Stochastics, 2020",
        "doi": "10.1007/s00780-019-00414-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Almost twenty years ago, E.R. Fernholz introduced portfolio generating\nfunctions which can be used to construct a variety of portfolios, solely in the\nterms of the individual companies' market weights. I. Karatzas and J. Ruf\nrecently developed another methodology for the functional construction of\nportfolios, which leads to very simple conditions for strong relative arbitrage\nwith respect to the market. In this paper, both of these notions of functional\nportfolio generation are generalized in a pathwise, probability-free setting;\nportfolio generating functions are substituted by path-dependent functionals,\nwhich involve the current market weights, as well as additional\nbounded-variation functions of past and present market weights. This\ngeneralization leads to a wider class of functionally-generated portfolios than\nwas heretofore possible, and yields improved conditions for outperforming the\nmarket portfolio over suitable time-horizons.\n"
    },
    {
        "paper_id": 1809.10193,
        "authors": "Mikhail Zhitlukhin",
        "title": "Monotone Sharpe ratios and related measures of investment performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new measure of performance of investment strategies, the\nmonotone Sharpe ratio. We study its properties, establish a connection with\ncoherent risk measures, and obtain an efficient representation for using in\napplications.\n"
    },
    {
        "paper_id": 1809.10256,
        "authors": "Jimin Lin, Matthew Lorig",
        "title": "On Carr and Lee's correlation immunization strategy",
        "comments": "23 pages, 14 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In their seminal work Carr and Lee (2008) show how to robustly price and\nreplicate a variety of claims written on the quadratic variation of a risky\nasset under the assumption that the asset's volatility process is independent\nof the Brownian motion that drives the asset's price. Additionally, they\npropose a correlation immunization strategy that minimizes the pricing and\nhedging error that results when the correlation between the risky asset's price\nand volatility is nonzero. In this paper, we show that the correlation\nimmunization strategy is the only strategy among the class of strategies\ndiscussed in Carr and Lee (2008) that results in real-valued hedging portfolios\nwhen the correlation between the asset's price and volatility is nonzero.\nAdditionally, we perform a number of Monte Carlo experiments to test the\neffectiveness of Carr and Lee's immunization strategy. Our results indicate\nthat the correlation immunization method is an effective means of reducing\npricing and hedging errors that result from nonzero correlation.\n"
    },
    {
        "paper_id": 1809.10554,
        "authors": "Nermin Elif Kurt, H. Bahadir Sahin, K\\\"ur\\c{s}ad Derinkuyu",
        "title": "An Adaptive Tabu Search Algorithm for Market Clearing Problem in Turkish\n  Day-Ahead Market",
        "comments": "6 pages, 2 figures, 2018 15th International Conference on the\n  European Energy Market (EEM)",
        "journal-ref": null,
        "doi": "10.1109/EEM.2018.8469926",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we focus on the market clearing problem of Turkish day-ahead\nelectricity market. We propose a mathematical model by extending the variety of\nbid types for different price regions. The commercial solvers may not find any\nfeasible solution for the proposed problem in some instances within the given\ntime limits. Hence, we design an adaptive tabu search (ATS) algorithm to solve\nthe problem. ATS discretizes continuous search space arising from the flow\nvariables. Our method has adaptive radius and it achieves backtracking by a\ncommercial solver. Then, we compare the performance of ATS with a heuristic\ndecomposition method from the literature by using synthetic data sets. We\nevaluate the performances of the algorithms with respect to their solution\ntimes and surplus differences. ATS performs better in most of the sets.\n"
    },
    {
        "paper_id": 1809.10566,
        "authors": "Isaac M. Sonin and Mark Whitmeyer",
        "title": "Some Nontrivial Properties of a Formula for Compound Interest",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the classical model of compound interest with a constant\nper-period payment and interest rate. We examine the outstanding balance\nfunction as well as the periodic payment function and show that the outstanding\nbalance function is not generally concave in the interest rate, but instead may\nbe initially convex on its domain and then concave.\n"
    },
    {
        "paper_id": 1809.10716,
        "authors": "Nicole B\\\"auerle, Sascha Desmettre",
        "title": "Portfolio Optimization in Fractional and Rough Heston Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a fractional version of the Heston volatility model which is\ninspired by [16]. Within this model we treat portfolio optimization problems\nfor power utility functions. Using a suitable representation of the fractional\npart, followed by a reasonable approximation we show that it is possible to\ncast the problem into the classical stochastic control framework. This approach\nis generic for fractional processes. We derive explicit solutions and obtain as\na by-product the Laplace transform of the integrated volatility. In order to\nget rid of some undesirable features we introduce a new model for the rough\npath scenario which is based on the Marchaud fractional derivative. We provide\na numerical study to underline our results.\n"
    },
    {
        "paper_id": 1809.10781,
        "authors": "Andres Gomez-Lievano",
        "title": "Methods and Concepts in Economic Complexity",
        "comments": "43 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Knowhow in societies accumulates as it gets transmitted from group to group,\nand from generation to generation. However, we lack of a unified quantitative\nformalism that takes into account the structured process for how this\naccumulation occurs, and this has precluded the development of a unified view\nof human development in the past and in the present. Here, we summarize a\nparadigm to understand and model this process. The paradigm goes under the\ngeneral name of the Theory of Economic Complexity (TEC). Based on it, we\npresent a combination of analytical, numerical and empirical results that\nillustrate how to characterize the process of development, providing measurable\nquantities that can be used to predict future developments. The emphasis is the\nquantification of the collective knowhow an economy has accumulated, and what\nare the directions in which it is likely to expand. As a case study we consider\ndata on trade, which provides consistent data on the technological\ndiversification of 200 countries across more than 50 years. The paradigm\nrepresented by TEC should be relevant for anthropologists, sociologists, and\neconomists interested in the role of collective knowhow as the main determinant\nof the success and welfare of a society.\n"
    },
    {
        "paper_id": 1809.10955,
        "authors": "Sascha Desmettre",
        "title": "Change of Measure in the Heston Model given a violated Feller Condition",
        "comments": "I detected some wrong statements in this article so I withdraw it",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When dealing with Heston's stochastic volatility model, the change of measure\nfrom the subjective measure P to the objective measure Q is usually\ninvestigated under the assumption that the Feller condition is satisfied. This\npaper closes this gap in the literature by deriving sufficient conditions for\nthe existence of an equivalent (local) martingale measure in the Heston model\nwhen the Feller condition is violated. We also supplement the existing\nliterature by the case of a finite lifetime of the Laplace transform of the\nintegrated volatility process. Moreover, we deduce conditions for the stock\nprice process in the Heston model being a true martingale, regardless if the\nFeller condition is satisfied or not.\n"
    },
    {
        "paper_id": 1809.1101,
        "authors": "Marcellino Gaudenzi and Michel Vellekoop",
        "title": "Exact Solutions for Optimal Investment Strategies and Indifference\n  Prices under Non-Differentiable Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an algorithm to calculate the exact solution for utility\noptimization problems on finite state spaces under a class of\nnon-differentiable preferences. We prove that optimal strategies must lie on a\ndiscrete grid in the plane, and this allows us to reduce the dimension of the\nproblem and define a very efficient method to obtain those strategies. We also\nshow how fast approximations for the value function can be obtained with an a\npriori specified error bound and we use these to replicate results for\ninvestment problems with a known closed-form solution. These results show the\nefficiency of our approach, which can then be used to obtain numerical\nsolutions for problems for which no explicit formulas are known.\n"
    },
    {
        "paper_id": 1809.11052,
        "authors": "Mark Levene, Aleksejus Kononovicius",
        "title": "Empirical Survival Jensen-Shannon Divergence as a Goodness-of-Fit\n  Measure for Maximum Likelihood Estimation and Curve Fitting",
        "comments": "20 pages, 1 figure, 13 tables",
        "journal-ref": "Communications in Statistics - Simulation and Computation 50:\n  3751-3767 (2021)",
        "doi": "10.1080/03610918.2019.1630435",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The coefficient of determination, known as $R^2$, is commonly used as a\ngoodness-of-fit criterion for fitting linear models. $R^2$ is somewhat\ncontroversial when fitting nonlinear models, although it may be generalised on\na case-by-case basis to deal with specific models such as the logistic model.\nAssume we are fitting a parametric distribution to a data set using, say, the\nmaximum likelihood estimation method. A general approach to measure the\ngoodness-of-fit of the fitted parameters, which is advocated herein, is to use\na nonparametric measure for comparison between the empirical distribution,\ncomprising the raw data, and the fitted model. In particular, for this purpose\nwe put forward the Survival Jensen-Shannon divergence ($SJS$) and its empirical\ncounterpart (${\\cal E}SJS$) as a metric which is bounded, and is a natural\ngeneralisation of the Jensen-Shannon divergence. We demonstrate, via a\nstraightforward procedure making use of the ${\\cal E}SJS$, that it can be used\nas part of maximum likelihood estimation or curve fitting as a measure of\ngoodness-of-fit, including the construction of a confidence interval for the\nfitted parametric distribution. Furthermore, we show the validity of the\nproposed method with simulated data, and three empirical data sets.\n"
    },
    {
        "paper_id": 1810.00155,
        "authors": "Tho V. Le and Junyi Zhang and Makoto Chikaraishi and Akimasa Fujiwara",
        "title": "Influence of introducing high speed railways on intercity travel\n  behavior in Vietnam",
        "comments": "13 pages, 2 figures, 6 tables. Proceedings of The 45th Conference of\n  Infrastructure Planning and Management, Japan Society of Civil Engineers",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is one of hottest topics in Vietnam whether to construct a High Speed Rail\n(HSR) system or not in near future. To analyze the impacts of introducing the\nHSR on the intercity travel behavior, this research develops an integrated\nintercity demand forecasting model to represent trip generation and frequency,\ndestination choice and travel mode choice behavior. For this purpose, a\ncomprehensive questionnaire survey with both Revealed Preference (RP)\ninformation (an inter-city trip diary) and Stated Preference (SP) information\nwas conducted in Hanoi in 2011. In the SP part, not only HSR, but also Low Cost\nCarrier is included in the choice set, together with other existing inter-city\ntravel modes. To make full use of the advantages of each type of data and to\novercome their disadvantages, RP and SP data are combined to describe the\ndestination choice and mode choice behavior, while trip generation and\nfrequency are represented by using the RP data. The model estimation results\nshow the inter-relationship between trip generation and frequency, destination\nchoice and travel mode choice, and confirm that those components should not\ndealt with separately.\n"
    },
    {
        "paper_id": 1810.00516,
        "authors": "Brian P Hanley",
        "title": "A New Form of Banking -- Concept and Mathematical Model of Venture\n  Banking",
        "comments": "32 pages, 24 figures, 2 tables, 27 equations (This version includes\n  clarifications of banking loan/deposit operation citing Bank of England)\n  Added preamble section, fixed a figure numbering problem",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This theoretical model contains concept, equations, and graphical results for\nventure banking. A system of 27 equations describes the behavior of the\nventure-bank and underwriter system allowing phase-space type graphs that show\nwhere profits and losses occur. These results confirm and expand those obtained\nfrom the original spreadsheet based model. An example investment in a castle at\na loss is provided to clarify concept. This model requires that all investments\nare in enterprises that create new utility value. The assessed utility value\ncreated is the new money out of which the venture bank and underwriter are\npaid. The model presented chooses parameters that ensure that the venture-bank\nexperiences losses before the underwriter does. Parameters are: DIN Premium,\n0.05; Clawback lien fraction, 0.77; Clawback bonds and equity futures discount,\n1.5 x (USA 12 month LIBOR); Range of clawback bonds sold, 0 to 100%; Range of\nequity futures sold 0 to 70%.\n"
    },
    {
        "paper_id": 1810.00985,
        "authors": "Tho V. Le and Satish V. Ukkusuri",
        "title": "Selectivity correction in discrete-continuous models for the willingness\n  to work as crowd-shippers and travel time tolerance",
        "comments": "16 pages, two figures, four tables. 97th Annual Meeting\n  Transportation Research Board",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this study is to understand the different behavioral\nconsiderations that govern the choice of people to engage in a crowd-shipping\nmarket. Using novel data collected by the researchers in the US, we develop\ndiscrete-continuous models. A binary logit model has been used to estimate\ncrowd-shippers' willingness to work, and an ordinary least-square regression\nmodel has been employed to calculate crowd-shippers' maximum tolerance for\nshipping and delivery times. A selectivity-bias term has been included in the\nmodel to correct for the conditional relationships of the crowd-shipper's\nwillingness to work and their maximum travel time tolerance. The results show\nsocio-demographic characteristics (e.g. age, gender, race, income, and\neducation level), transporting freight experience, and number of social media\nusages significant influence the decision to participate in the crowd-shipping\nmarket. In addition, crowd-shippers pay expectations were found to be\nreasonable and concurrent with the literature on value-of-time. Findings from\nthis research are helpful for crowd-shipping companies to identify and attract\npotential shippers. In addition, an understanding of crowd-shippers - their\nbehaviors, perceptions, demographics, pay expectations, and in which contexts\nthey are willing to divert from their route - are valuable to the development\nof business strategies such as matching criteria and compensation schemes for\ndriver-partners.\n"
    },
    {
        "paper_id": 1810.01116,
        "authors": "Jaehyuk Choi, Yeda Du, Qingshuo Song",
        "title": "Inverse Gaussian quadrature and finite normal-mixture approximation of\n  the generalized hyperbolic distribution",
        "comments": null,
        "journal-ref": "Journal of Computational and Applied Mathematics, 388:113302, 2021",
        "doi": "10.1016/j.cam.2020.113302",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, a numerical quadrature for the generalized inverse Gaussian\ndistribution is derived from the Gauss-Hermite quadrature by exploiting its\nrelationship with the normal distribution. The proposed quadrature is not\nGaussian, but it exactly integrates the polynomials of both positive and\nnegative orders. Using the quadrature, the generalized hyperbolic distribution\nis efficiently approximated as a finite normal variance-mean mixture.\nTherefore, the expectations under the distribution, such as cumulative\ndistribution function and European option price, are accurately computed as\nweighted sums of those under normal distributions. The generalized hyperbolic\nrandom variates are also sampled in a straightforward manner. The accuracy of\nthe methods is illustrated with numerical examples.\n"
    },
    {
        "paper_id": 1810.01165,
        "authors": "Tao Li, Xudong Liu, Shihan Su",
        "title": "Semi-supervised Text Regression with Conditional Generative Adversarial\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/BigData.2018.8622140",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Enormous online textual information provides intriguing opportunities for\nunderstandings of social and economic semantics. In this paper, we propose a\nnovel text regression model based on a conditional generative adversarial\nnetwork (GAN), with an attempt to associate textual data and social outcomes in\na semi-supervised manner. Besides promising potential of predicting\ncapabilities, our superiorities are twofold: (i) the model works with\nunbalanced datasets of limited labelled data, which align with real-world\nscenarios; and (ii) predictions are obtained by an end-to-end framework,\nwithout explicitly selecting high-level representations. Finally we point out\nrelated datasets for experiments and future research directions.\n"
    },
    {
        "paper_id": 1810.01278,
        "authors": "Kei Nakagawa, Takumi Uchida, and Tomohisa Aoshima",
        "title": "Deep Factor Model",
        "comments": "MIDAS 2018 : MIDAS @ECML-PKDD 2018 - 3rd Workshop on MIning DAta for\n  financial applicationS. arXiv admin note: text overlap with arXiv:1712.08268,\n  arXiv:1801.01777, arXiv:1602.06561 by other authors",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-13463-1_3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to represent a return model and risk model in a unified manner\nwith deep learning, which is a representative model that can express a\nnonlinear relationship. Although deep learning performs quite well, it has\nsignificant disadvantages such as a lack of transparency and limitations to the\ninterpretability of the prediction. This is prone to practical problems in\nterms of accountability. Thus, we construct a multifactor model by using\ninterpretable deep learning. We implement deep learning as a return model to\npredict stock returns with various factors. Then, we present the application of\nlayer-wise relevance propagation (LRP) to decompose attributes of the predicted\nreturn as a risk model. By applying LRP to an individual stock or a portfolio\nbasis, we can determine which factor contributes to prediction. We call this\nmodel a deep factor model. We then perform an empirical analysis on the\nJapanese stock market and show that our deep factor model has better predictive\ncapability than the traditional linear model or other machine learning methods.\nIn addition , we illustrate which factor contributes to prediction.\n"
    },
    {
        "paper_id": 1810.0131,
        "authors": "Oleg Yu Vorobyev",
        "title": "The logic of uncertainty as a logic of experience and chance and the\n  co~event-based Bayes' theorem",
        "comments": null,
        "journal-ref": "Proc. of the XVI Intern. FAMEMS Conf. on Financial and Actuarial\n  Mathematics and Eventology of Multivariate Statistics and the II Workshop on\n  Hilbert's Sixth Problem; Krasnoyarsk, SFU (Oleg Vorobyev ed.), 92-110, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The logic of uncertainty is not the logic of experience and as well as it is\nnot the logic of chance. It is the logic of experience and chance. Experience\nand chance are two inseparable poles. These are two dual reflections of one\nessence, which is called co~event. The theory of experience and chance is the\ntheory of co~events. To study the co~events, it is not enough to study the\nexperience and to study the chance. For this, it is necessary to study the\nexperience and chance as a single entire, a co~event. In other words, it is\nnecessary to study their interaction within a co~event. The new co~event\naxiomatics and the theory of co~events following from it were created precisely\nfor these purposes. In this work, I am going to demonstrate the effectiveness\nof the new theory of co~events in a studying the logic of uncertainty. I will\ndo this by the example of a co~event splitting of the logic of the Bayesian\nscheme, which has a long history of fierce debates between Bayesianists and\nfrequentists. I hope the logic of the theory of experience and chance will make\nits modest contribution to the application of these old dual debaters.\n"
    },
    {
        "paper_id": 1810.01372,
        "authors": "Tathagata Banerjee and Zachary Feinstein",
        "title": "Pricing of debt and equity in a financial network with comonotonic\n  endowments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present formulas for the valuation of debt and equity of\nfirms in a financial network under comonotonic endowments. We demonstrate that\nthe comonotonic setting provides a lower bound and Jensen's inequality provides\nan upper bound to the price of debt under Eisenberg-Noe financial networks with\nbankruptcy costs. Such financial networks encode the interconnection of firms\nthrough debt claims. The proposed pricing formulas consider the realized,\nendogenous, recovery rate on debt claims. Special consideration is given to the\nCAPM setting in which firms invest in correlated portfolios so as to provide\nanalytical stress testing formulas. We endogenously construct the comonotonic\nendowment setting from a equity maximizing standpoint with capital transfers.\nWe conclude by, numerically, comparing the network valuation problem with two\nsingle firm baseline heuristics which can, respectively, approximate the price\nof debt and equity.\n"
    },
    {
        "paper_id": 1810.01736,
        "authors": "Ravi Kashyap",
        "title": "Auction Theory Adaptations for Real Life Applications",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1603.00987",
        "journal-ref": "Research in Economics, December 2018, Volume 72, Issue 4, pp.\n  452-481",
        "doi": "10.1016/j.rie.2018.09.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop extensions to auction theory results that are useful in real life\nscenarios.\n  1. Since valuations are generally positive we first develop approximations\nusing the log-normal distribution. This would be useful for many finance\nrelated auction settings since asset prices are usually non-negative.\n  2. We formulate a positive symmetric discrete distribution, which is likely\nto be followed by the total number of auction participants, and incorporate\nthis into auction theory results.\n  3. We develop extensions when the valuations of the bidders are\ninterdependent and incorporate all the results developed into a final combined\nrealistic setting.\n  4. Our methods can be a practical tool for bidders and auction sellers to\nmaximize their profits. The models developed here could be potentially useful\nfor inventory estimation and for wholesale procurement of financial instruments\nand also non-financial commodities.\n  All the propositions are new results and they refer to existing results which\nare stated as Lemmas.\n"
    },
    {
        "paper_id": 1810.01971,
        "authors": "Noah Haber, Till B\\\"arnighausen, Jacob Bor, Jessica Cohen, Frank\n  Tanser, Deenan Pillay, G\\\"unther Fink",
        "title": "Disability for HIV and Disincentives for Health: The Impact of South\n  Africa's Disability Grant on HIV/AIDS Recovery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  South Africa's disability grants program is tied to its HIV/AIDS recovery\nprogram, such that individuals who are ill enough may qualify. Qualification is\nhistorically tied to a CD4 count of 200 cells/mm3, which improve when a person\nadheres to antiretroviral therapy. This creates a potential unintended\nconsequence where poor individuals, faced with potential loss of their income,\nmay choose to limit their recovery through non-adherence. To test for\nmanipulation caused by grant rules, we identify differences in disability grant\nrecipients and non-recipients' rate of CD4 recovery around the qualification\nthreshold, implemented as a fixed-effects difference-in-difference around the\nthreshold. We use data from the Africa Health Research Institute Demographic\nand Health Surveillance System (AHRI DSS) in rural KwaZulu-Natal, South Africa,\nutilizing DG status and laboratory CD4 count records for 8,497 individuals to\ntest whether there are any systematic differences in CD4 recover rates among\neligible patients. We find that disability grant threshold rules caused\nrecipients to have a relatively slower CD4 recovery rate of about 20-30\ncells/mm3/year, or a 20% reduction in the speed of recovery around the\nthreshold.\n"
    },
    {
        "paper_id": 1810.02071,
        "authors": "Jeechul Woo, Chenru Liu, Jaehyuk Choi",
        "title": "Leave-one-out least squares Monte Carlo algorithm for pricing Bermudan\n  options",
        "comments": null,
        "journal-ref": "Journal of Futures Markets (2024)",
        "doi": "10.1002/fut.22515",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The least squares Monte Carlo (LSM) algorithm proposed by Longstaff and\nSchwartz (2001) is widely used for pricing Bermudan options. The LSM estimator\ncontains undesirable look-ahead bias, and the conventional technique of\navoiding it requires additional simulation paths. We present the leave-one-out\nLSM (LOOLSM) algorithm to eliminate look-ahead bias without doubling\nsimulations. We also show that look-ahead bias is asymptotically proportional\nto the regressors-to-paths ratio. Our findings are demonstrated with several\noption examples in which the LSM algorithm overvalues the options. The LOOLSM\nmethod can be extended to other regression-based algorithms that improve the\nLSM method.\n"
    },
    {
        "paper_id": 1810.02109,
        "authors": "Sebastian Wehrle and Johannes Schmidt",
        "title": "District heating systems under high CO2 emission prices: the role of the\n  pass-through from emission cost to electricity prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Low CO2 prices have prompted discussion about political measures aimed at\nincreasing the cost of carbon dioxide emissions. These costs affect, inter\nalia, integrated district heating system operators (DHSO), often owned by\nmunicipalities with some political influence, that use a variety of (CO2 emis-\nsion intense) heat generation technologies. We examine whether DHSOs have an\nincentive to support measures that increase CO2 emission prices in the short\nterm. Therefore, we (i) develop a simplified analytical framework to analyse\noptimal decisions of a district heating operator, and (ii) investigate the\nmarket-wide effects of increasing emission prices, in particular the pass-\nthrough from emission costs to electricity prices. Using a numerical model of\nthe common Austrian and German power system, we estimate a pass-through from\nCO2 emission prices to power prices between 0.69 and 0.53 as of 2017, depending\non the absolute emission price level. We find the CO2 emission cost\npass-through to be sufficiently high so that low-emission district heating\nsystems operating at least moderately efficient generation units benefit from\nrising CO2 emission prices in the short term.\n"
    },
    {
        "paper_id": 1810.02125,
        "authors": "Adriano Soares Koshiyama, Nick Firoozye and Philip Treleaven",
        "title": "A Machine Learning-based Recommendation System for Swaptions Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Derivative traders are usually required to scan through hundreds, even\nthousands of possible trades on a daily basis. Up to now, not a single solution\nis available to aid in their job. Hence, this work aims to develop a trading\nrecommendation system, and apply this system to the so-called Mid-Curve\nCalendar Spread (MCCS), an exotic swaption-based derivatives package. In\nsummary, our trading recommendation system follows this pipeline: (i) on a\ncertain trade date, we compute metrics and sensitivities related to an MCCS;\n(ii) these metrics are feed in a model that can predict its expected return for\na given holding period; and after repeating (i) and (ii) for all trades we\n(iii) rank the trades using some dominance criteria. To suggest that such\napproach is feasible, we used a list of 35 different types of MCCS; a total of\n11 predictive models; and 4 benchmark models. Our results suggest that in\ngeneral linear regression with lasso regularisation compared favourably to\nother approaches from a predictive and interpretability perspective.\n"
    },
    {
        "paper_id": 1810.0239,
        "authors": "Alexander Lipton and Vadim Kaushansky",
        "title": "On the First Hitting Time Density of an Ornstein-Uhlenbeck Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the classical problem of the first passage hitting\ndensity of an Ornstein--Uhlenbeck process. We give two complementary (forward\nand backward) formulations of this problem and provide semi-analytical\nsolutions for both. The corresponding problems are comparable in complexity. By\nusing the method of heat potentials, we show how to reduce these problems to\nlinear Volterra integral equations of the second kind. For small values of $t$,\nwe solve these equations analytically by using Abel equation approximation; for\nlarger $t$ we solve them numerically. We also provide a comparison with other\nknown methods for finding the hitting density of interest, and argue that our\nmethod has considerable advantages and provides additional valuable insights.\n"
    },
    {
        "paper_id": 1810.02444,
        "authors": "Alex Garivaltis",
        "title": "Super-Replication of the Best Pairs Trade in Hindsight",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": "Cogent Economics & Finance (2019), 7: 1568657",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper derives a robust on-line equity trading algorithm that achieves\nthe greatest possible percentage of the final wealth of the best pairs\nrebalancing rule in hindsight. A pairs rebalancing rule chooses some pair of\nstocks in the market and then perpetually executes rebalancing trades so as to\nmaintain a target fraction of wealth in each of the two. After each discrete\nmarket fluctuation, a pairs rebalancing rule will sell a precise amount of the\noutperforming stock and put the proceeds into the underperforming stock. Under\ntypical conditions, in hindsight one can find pairs rebalancing rules that\nwould have spectacularly beaten the market. Our trading strategy, which extends\nOrdentlich and Cover's (1998) \"max-min universal portfolio,\" guarantees to\nachieve an acceptable percentage of the hindsight-optimized wealth, a\npercentage which tends to zero at a slow (polynomial) rate. This means that on\na long enough investment horizon, the trader can enforce a compound-annual\ngrowth rate that is arbitrarily close to that of the best pairs rebalancing\nrule in hindsight. The strategy will \"beat the market asymptotically\" if there\nturns out to exist a pairs rebalancing rule that grows capital at a higher\nasymptotic rate than the market index. The advantages of our algorithm over the\nOrdentlich and Cover (1998) strategy are twofold. First, their strategy is\nimpossible to compute in practice. Second, in considering the more modest\nbenchmark (instead of the best all-stock rebalancing rule in hindsight), we\nreduce the \"cost of universality\" and achieve a higher learning rate.\n"
    },
    {
        "paper_id": 1810.02447,
        "authors": "Alex Garivaltis",
        "title": "Multilinear Superhedging of Lookback Options",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a pathbreaking paper, Cover and Ordentlich (1998) solved a max-min\nportfolio game between a trader (who picks an entire trading algorithm,\n$\\theta(\\cdot)$) and \"nature,\" who picks the matrix $X$ of gross-returns of all\nstocks in all periods. Their (zero-sum) game has the payoff kernel\n$W_\\theta(X)/D(X)$, where $W_\\theta(X)$ is the trader's final wealth and $D(X)$\nis the final wealth that would have accrued to a $\\$1$ deposit into the best\nconstant-rebalanced portfolio (or fixed-fraction betting scheme) determined in\nhindsight. The resulting \"universal portfolio\" compounds its money at the same\nasymptotic rate as the best rebalancing rule in hindsight, thereby beating the\nmarket asymptotically under extremely general conditions. Smitten with this\n(1998) result, the present paper solves the most general tractable version of\nCover and Ordentlich's (1998) max-min game. This obtains for performance\nbenchmarks (read: derivatives) that are separately convex and homogeneous in\neach period's gross-return vector. For completely arbitrary (even\nnon-measurable) performance benchmarks, we show how the axiom of choice can be\nused to \"find\" an exact maximin strategy for the trader.\n"
    },
    {
        "paper_id": 1810.02485,
        "authors": "Alex Garivaltis",
        "title": "Exact Replication of the Best Rebalancing Rule in Hindsight",
        "comments": "37 pages, 10 figures",
        "journal-ref": "The Journal of Derivatives 26(4), pp.35-53 (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper prices and replicates the financial derivative whose payoff at $T$\nis the wealth that would have accrued to a $\\$1$ deposit into the best\ncontinuously-rebalanced portfolio (or fixed-fraction betting scheme) determined\nin hindsight. For the single-stock Black-Scholes market, Ordentlich and Cover\n(1998) only priced this derivative at time-0, giving\n$C_0=1+\\sigma\\sqrt{T/(2\\pi)}$. Of course, the general time-$t$ price is not\nequal to $1+\\sigma\\sqrt{(T-t)/(2\\pi)}$. I complete the Ordentlich-Cover (1998)\nanalysis by deriving the price at any time $t$. By contrast, I also study the\nmore natural case of the best levered rebalancing rule in hindsight. This\nyields $C(S,t)=\\sqrt{T/t}\\cdot\\,\\exp\\{rt+\\sigma^2b(S,t)^2\\cdot t/2\\}$, where\n$b(S,t)$ is the best rebalancing rule in hindsight over the observed history\n$[0,t]$. I show that the replicating strategy amounts to betting the fraction\n$b(S,t)$ of wealth on the stock over the interval $[t,t+dt].$ This fact holds\nfor the general market with $n$ correlated stocks in geometric Brownian motion:\nwe get $C(S,t)=(T/t)^{n/2}\\exp(rt+b'\\Sigma b\\cdot t/2)$, where $\\Sigma$ is the\ncovariance of instantaneous returns per unit time. This result matches the\n$\\mathcal{O}(T^{n/2})$ \"cost of universality\" derived by Cover in his\n\"universal portfolio theory\" (1986, 1991, 1996, 1998), which super-replicates\nthe same derivative in discrete-time. The replicating strategy compounds its\nmoney at the same asymptotic rate as the best levered rebalancing rule in\nhindsight, thereby beating the market asymptotically. Naturally enough, we find\nthat the American-style version of Cover's Derivative is never exercised early\nin equilibrium.\n"
    },
    {
        "paper_id": 1810.02529,
        "authors": "Lionel Yelibi, Tim Gebbie",
        "title": "Fast Super-Paramagnetic Clustering",
        "comments": "25 pages, 41 Figures and code at\n  https://github.com/tehraio/potts-model-clustering",
        "journal-ref": "Physica A, Volume 551, 1 August 2020, 124049",
        "doi": "10.1016/j.physa.2019.124049",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We map stock market interactions to spin models to recover their hierarchical\nstructure using a simulated annealing based Super-Paramagnetic Clustering (SPC)\nalgorithm. This is directly compared to a modified implementation of a maximum\nlikelihood approach we call Fast Super-Paramagnetic Clustering (f-SPC). The\nmethods are first applied standard toy test-case problems, and then to a\ndata-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249\ndays. The signal to noise ratio of stock market correlation matrices is briefly\nconsidered. Our result recover approximately clusters representative of\nstandard economic sectors and mixed ones whose dynamics shine light on the\nadaptive nature of financial markets and raise concerns relating to the\neffectiveness of industry based static financial market classification in the\nworld of real-time data analytics. A key result is that we show that f-SPC\nmaximum likelihood solutions converge to ones found within the\nSuper-Paramagnetic Phase where the entropy is maximum, and those solutions are\nqualitatively better for high dimensionality data-sets.\n"
    },
    {
        "paper_id": 1810.02613,
        "authors": "Kiril Dimitrov",
        "title": "Exploring the nuances in the relationship \"culture-strategy\" for the\n  business world",
        "comments": null,
        "journal-ref": "Vanguard scientific instruments in management journal (VSIM),\n  Volume 2(13) 2016, 37 pages, ISSN 1314-0582, University of National And World\n  Economy",
        "doi": "10.5281/zenodo.1434904",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The current article explores interesting, significant and recently identified\nnuances in the relationship \"culture-strategy\". The shared views of leading\nscholars at the University of National and World Economy in relation with the\nessence, direction, structure, role and hierarchy of \"culture-strategy\"\nrelation are defined as a starting point of the analysis. The research emphasis\nis directed on recent developments in interpreting the observed realizations of\nthe aforementioned link among the community of international scholars and\nconsultants, publishing in selected electronic scientific databases. In this\nway a contemporary notion of the nature of \"culture-strategy\" relationship for\nthe entities from the world of business is outlined.\n"
    },
    {
        "paper_id": 1810.02615,
        "authors": "Kiril Dimitrov",
        "title": "Talent management - an etymological study",
        "comments": null,
        "journal-ref": "Vanguard scientific instruments in management journal (VSIM),\n  Volume 2(11) 2015, 39 pages, ISSN 1314-0582, University of National And World\n  Economy",
        "doi": "10.5281/zenodo.1434892",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The current article unveils and analyzes important shades of meaning for the\nwidely discussed term talent management. It not only grounds the outlined\nperspectives in incremental formulation and elaboration of this construct, but\nalso is oriented to exploring the underlying reasons for the social actors,\nproposing new nuances. Thus, a mind map and a fish-bone diagram are constructed\nto depict effectively and efficiently the current state of development for\ntalent management and make easier the realizations of future research\nendeavours in this field.\n"
    },
    {
        "paper_id": 1810.02617,
        "authors": "Kiril Dimitrov, Marin Geshkov",
        "title": "Dominating Attributes Of Professed Firm Culture Of Holding Companies -\n  Members Of The Bulgarian Industrial Capital Association",
        "comments": null,
        "journal-ref": "Economic Alternatives Journal, Vol.12, Issue 3, 2018, pp. 384-418,\n  available at: https://www.unwe.bg/uploads/Alternatives/7_EA_3_2018_en.pdf,\n  ISSN (print): 1312-7462, ISSN (online): 2367-9409",
        "doi": "10.5281/zenodo.1442380",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article aims to outline the diversity of cultural phenomena that occur\nat organizational level, emphasizing the place and role of the key attributes\nof professed firm culture for the survival and successful development of big\nbusiness organizations. The holding companies, members of the Bulgarian\nIndustrial Capital Association, are chosen as a survey object as the mightiest\ndriving engines of the local economy. That is why their emergence and\ndevelopment in the transition period is monitored and analyzed. Based on an\nempirical study of relevant website content, important implications about\ndominating attributes of professed firm culture on them are found and several\nuseful recommendations to their senior management are made.\n"
    },
    {
        "paper_id": 1810.02621,
        "authors": "Kiril Dimitrov",
        "title": "Geert Hofstede et al's set of national cultural dimensions - popularity\n  and criticisms",
        "comments": "available at: http://www.unwe.bg/uploads/Alternatives/3_Dimitrov.pdf",
        "journal-ref": "Economic Alternatives journal, 2d issue, 2014, pp30-60, ISSN\n  (print): 1312-7462, ISSN (online): 2367-9409",
        "doi": "10.5281/zenodo.1434882",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article outlines different stages in development of the national culture\nmodel, created by Geert Hofstede and his affiliates. This paper reveals and\nsynthesizes the contemporary review of the application spheres of this\nframework. Numerous applications of the dimensions set are used as a source of\nidentifying significant critiques, concerning different aspects in model's\noperation. These critiques are classified and their underlying reasons are also\noutlined by means of a fishbone diagram.\n"
    },
    {
        "paper_id": 1810.02622,
        "authors": "Kiril Dimitrov",
        "title": "Contemporary facets of business successes among leading companies,\n  operating in Bulgaria",
        "comments": null,
        "journal-ref": "Vanguard scientific instruments in management journal (VSIM),\n  Volume 2(7)/2013, pp189-214, ISSN 1314-0582, University of National And World\n  Economy",
        "doi": "10.5281/zenodo.1434878",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The current article unveils and analyzes some important factors, influencing\ndiversity in strategic decision-making approaches in local companies.\nResearcher's attention is oriented to survey important characteristics of the\nstrategic moves, undertaken by leading companies in Bulgaria.\n"
    },
    {
        "paper_id": 1810.02815,
        "authors": "Ding Xiang, Ermin Wei",
        "title": "A General Sensitivity Analysis Approach for Demand Response\n  Optimizations",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that demand response can improve the system efficiency as\nwell as lower consumers' (prosumers') electricity bills. However, it is not\nclear how we can either qualitatively identify the prosumer with the most\nimpact potential or quantitatively estimate each prosumer's contribution to the\ntotal social welfare improvement when additional resource capacity/flexibility\nis introduced to the system with demand response, such as allowing net-selling\nbehavior. In this work, we build upon existing literature on the electricity\nmarket, which consists of price-taking prosumers each with various appliances,\nan electric utility company and a social welfare optimizing distribution system\noperator, to design a general sensitivity analysis approach (GSAA) that can\nestimate the potential of each consumer's contribution to the social welfare\nwhen given more resource capacity. GSAA is based on existence of an efficient\ncompetitive equilibrium, which we establish in the paper. When prosumers'\nutility functions are quadratic, GSAA can give closed forms characterization on\nsocial welfare improvement based on duality analysis. Furthermore, we extend\nGSAA to a general convex settings, i.e., utility functions with strong\nconvexity and Lipschitz continuous gradient. Even without knowing the specific\nforms the utility functions, we can derive upper and lower bounds of the social\nwelfare improvement potential of each prosumer, when extra resource is\nintroduced. For both settings, several applications and numerical examples are\nprovided: including extending AC comfort zone, ability of EV to discharge and\nnet selling. The estimation results show that GSAA can be used to decide how to\nallocate potentially limited market resources in the most impactful way.\n"
    },
    {
        "paper_id": 1810.02952,
        "authors": "Qi-lin Cao, Hua-yun Xiang, You-jia Mao, Ben-zhang Yang",
        "title": "Social capital at venture capital firms and their financial performance:\n  Evidence from China",
        "comments": "28 pages, 3 figures, and 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the extent to which social capital drives performance in\nthe Chinese venture capital market and explores the trend toward VC syndication\nin China. First, we propose a hybrid model based on syndicated social networks\nand the latent-variable model, which describes the social capital at venture\ncapital firms and builds relationships between social capital and performance\nat VC firms. Then, we build three hypotheses about the relationships and test\nthe hypotheses using our proposed model. Some numerical simulations are given\nto support the test results. Finally, we show that the correlations between\nsocial capital and financial performance at venture capital firms are weak in\nChina and find that China's venture capital firms lack mature social capital\nlinks.\n"
    },
    {
        "paper_id": 1810.03348,
        "authors": "Andrea Bastianin, Paolo Castelnovo, Massimo Florio",
        "title": "Evaluating regulatory reform of network industries: a survey of\n  empirical models based on categorical proxies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jup.2018.09.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Proxies for regulatory reforms based on categorical variables are\nincreasingly used in empirical evaluation models. We surveyed 63 studies that\nrely on such indices to analyze the effects of entry liberalization,\nprivatization, unbundling, and independent regulation of the electricity,\nnatural gas, and telecommunications sectors. We highlight methodological issues\nrelated to the use of these proxies. Next, taking stock of the literature, we\nprovide practical advice for the design of the empirical strategy and discuss\nthe selection of control and instrumental variables to attenuate endogeneity\nproblems undermining identification of the effects of regulatory reforms.\n"
    },
    {
        "paper_id": 1810.03399,
        "authors": "Christian Bayer, Benjamin Stemper",
        "title": "Deep calibration of rough stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sparked by Al\\`os, Le\\'on, and Vives (2007); Fukasawa (2011, 2017); Gatheral,\nJaisson, and Rosenbaum (2018), so-called rough stochastic volatility models\nsuch as the rough Bergomi model by Bayer, Friz, and Gatheral (2016) constitute\nthe latest evolution in option price modeling. Unlike standard bivariate\ndiffusion models such as Heston (1993), these non-Markovian models with\nfractional volatility drivers allow to parsimoniously recover key stylized\nfacts of market implied volatility surfaces such as the exploding power-law\nbehaviour of the at-the-money volatility skew as time to maturity goes to zero.\nStandard model calibration routines rely on the repetitive evaluation of the\nmap from model parameters to Black-Scholes implied volatility, rendering\ncalibration of many (rough) stochastic volatility models prohibitively\nexpensive since there the map can often only be approximated by costly Monte\nCarlo (MC) simulations (Bennedsen, Lunde, & Pakkanen, 2017; McCrickerd &\nPakkanen, 2018; Bayer et al., 2016; Horvath, Jacquier, & Muguruza, 2017). As a\nremedy, we propose to combine a standard Levenberg-Marquardt calibration\nroutine with neural network regression, replacing expensive MC simulations with\ncheap forward runs of a neural network trained to approximate the implied\nvolatility map. Numerical experiments confirm the high accuracy and speed of\nour approach.\n"
    },
    {
        "paper_id": 1810.03466,
        "authors": "Kaveh Bastani, Elham Asgari, Hamed Namavari",
        "title": "Wide and Deep Learning for Peer-to-Peer Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper proposes a two-stage scoring approach to help lenders decide their\nfund allocations in the peer-to-peer (P2P) lending market. The existing scoring\napproaches focus on only either probability of default (PD) prediction, known\nas credit scoring, or profitability prediction, known as profit scoring, to\nidentify the best loans for investment. Credit scoring fails to deliver the\nmain need of lenders on how much profit they may obtain through their\ninvestment. On the other hand, profit scoring can satisfy that need by\npredicting the investment profitability. However, profit scoring completely\nignores the class imbalance problem where most of the past loans are\nnon-default. Consequently, ignorance of the class imbalance problem\nsignificantly affects the accuracy of profitability prediction. Our proposed\ntwo-stage scoring approach is an integration of credit scoring and profit\nscoring to address the above challenges. More specifically, stage 1 is designed\nas credit scoring to identify non-default loans while the imbalanced nature of\nloan status is considered in PD prediction. The loans identified as non-default\nare then moved to stage 2 for prediction of profitability, measured by internal\nrate of return. Wide and deep learning is used to build the predictive models\nin both stages to achieve both memorization and generalization. Extensive\nnumerical studies are conducted based on real-world data to verify the\neffectiveness of the proposed approach. The numerical studies indicate our\ntwo-stage scoring approach outperforms the existing credit scoring and profit\nscoring approaches.\n"
    },
    {
        "paper_id": 1810.03494,
        "authors": "Martin Mihelich and Yan Shu",
        "title": "k-price auctions and Combination auctions",
        "comments": "12 pages. All comments are welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an exact analytical solution of the Nash equilibrium for $k$-\nprice auctions. We also introduce a new type of auction and demonstrate that it\nhas fair solutions other than the second price auctions, therefore paving the\nway for replacing second price auctions.\n"
    },
    {
        "paper_id": 1810.03501,
        "authors": "Alex S.L. Tse",
        "title": "Dividend Policy and Capital Structure of a Defaultable Firm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Default risk significantly affects the corporate policies of a firm. We\ndevelop a model in which a limited liability entity subject to Poisson default\nshock jointly sets its dividend policy and capital structure to maximize the\nexpected lifetime utility from consumption of risk averse equity investors. We\ngive a complete characterization of the solution to the singular stochastic\ncontrol problem. The optimal policy involves paying dividends to keep the ratio\nof firm's equity value to investors' wealth below a critical threshold.\nDividend payout acts as a precautionary channel to transfer wealth from the\nfirm to investors for mitigation of losses in the event of default. Higher the\ndefault risk, more aggressively the firm leverages and pays dividends.\n"
    },
    {
        "paper_id": 1810.03546,
        "authors": "John Armstrong",
        "title": "Classifying Financial Markets up to Isomorphism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two markets should be considered isomorphic if they are financially\nindistinguishable. We define a notion of isomorphism for financial markets in\nboth discrete and continuous time. We then seek to identify the distinct\nisomorphism classes, that is to classify markets.\n  We classify complete one-period markets. We define an invariant of continuous\ntime complete markets which we call the absolute market price of risk. This\ninvariant plays a role analogous to the curvature in Riemannian geometry. We\nclassify markets when the absolute market price of risk is deterministic.\n  We show that, in general, markets with non-trivial automorphism groups admit\nmutual fund theorems. We prove a number of such theorems.\n"
    },
    {
        "paper_id": 1810.03605,
        "authors": "Kiril Dimitrov",
        "title": "Critical review of models, containing cultural levels beyond the\n  organizational one",
        "comments": null,
        "journal-ref": "Economic alternatives journal, Issue 1, 2012, ISSN (print):\n  1312-7462, ISSN (online): 2367-9409, pp.98-125, available at:\n  http://www.unwe.bg/uploads/Alternatives/BROI_1_ALTERNATIVI_ENGLISH_2012-Kiril.pdf",
        "doi": "10.5281/zenodo.1434856",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The current article traces back the scientific interest to cultural levels\nacross the organization at the University of National and World Economy, and\nespecially in the series of Economic Alternatives - an official scientific\nmagazine, issued by this Institution. Further, a wider and critical review of\ninternational achievements in this field is performed, revealing diverse\nanalysis perspectives with respect to cultural levels. Also, a useful model of\nexploring and teaching the cultural levels beyond the organization is proposed.\n  Keywords: globalization, national culture, organization culture, cultural\nlevels, cultural economics. JEL: M14, Z10.\n"
    },
    {
        "paper_id": 1810.04087,
        "authors": "L\\'aszl\\'o Csat\\'o and Csaba T\\'oth",
        "title": "University rankings from the revealed preferences of the applicants",
        "comments": "38 pages, 4 figures, 8 tables",
        "journal-ref": "European Journal of Operational Research, 286(1): 309-320, 2020",
        "doi": "10.1016/j.ejor.2020.03.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A methodology is presented to rank universities on the basis of the lists of\nprogrammes the students applied for. We exploit a crucial feature of the\ncentralised assignment system to higher education in Hungary: a student is\nadmitted to the first programme where the score limit is achieved. This makes\nit possible to derive a partial preference order of each applicant. Our\napproach integrates the information from all students participating in the\nsystem, is free of multicollinearity among the indicators, and contains few ad\nhoc parameters. The procedure is implemented to rank faculties in the Hungarian\nhigher education between 2001 and 2016. We demonstrate that the ranking given\nby the least squares method has favourable theoretical properties, is robust\nwith respect to the aggregation of preferences, and performs well in practice.\nThe suggested ranking is worth considering as a reasonable alternative to the\nstandard composite indices.\n"
    },
    {
        "paper_id": 1810.0437,
        "authors": "Yusuke Uchiyama, Takanori Kadoya, Kei Nakagawa",
        "title": "Complex Valued Risk Diversification",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e21020119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk diversification is one of the dominant concerns for portfolio managers.\nVarious portfolio constructions have been proposed to minimize the risk of the\nportfolio under some constrains including expected returns. We propose a\nportfolio construction method that incorporates the complex valued principal\ncomponent analysis into the risk diversification portfolio construction. The\nproposed method is verified to outperform the conventional risk parity and risk\ndiversification portfolio constructions.\n"
    },
    {
        "paper_id": 1810.04383,
        "authors": "Philippe Bergault, David Evangelista, Olivier Gu\\'eant, Douglas Vieira",
        "title": "Closed-form approximations in multi-asset market making",
        "comments": "36 pages, 33 references, 13 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large proportion of market making models derive from the seminal model of\nAvellaneda and Stoikov. The numerical approximation of the value function and\nthe optimal quotes in these models remains a challenge when the number of\nassets is large. In this article, we propose closed-form approximations for the\nvalue functions of many multi-asset extensions of the Avellaneda-Stoikov model.\nThese approximations or proxies can be used (i) as heuristic evaluation\nfunctions, (ii) as initial value functions in reinforcement learning\nalgorithms, and/or (iii) directly to design quoting strategies through a greedy\napproach. Regarding the latter, our results lead to new and easily\ninterpretable closed-form approximations for the optimal quotes, both in the\nfinite-horizon case and in the asymptotic (ergodic) regime.\n"
    },
    {
        "paper_id": 1810.04623,
        "authors": "Michele Mininni, Giuseppe Orlando, Giovanni Taglialatela",
        "title": "Challenges in approximating the Black and Scholes call formula with\n  hyperbolic tangents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce the concept of standardized call function and we\nobtain a new approximating formula for the Black and Scholes call function\nthrough the hyperbolic tangent. This formula is useful for pricing and risk\nmanagement as well as for extracting the implied volatility from quoted\noptions. The latter is of particular importance since it indicates the risk of\nthe underlying and it is the main component of the option's price. Further we\nestimate numerically the approximating error of the suggested solution and, by\ncomparing our results in computing the implied volatility with the most common\nmethods available in literature we discuss the challenges of this approach.\n"
    },
    {
        "paper_id": 1810.04624,
        "authors": "J. Rosenblatt (Institut National de Sciences Appliqu\\'ees, Rennes,\n  France)",
        "title": "Symmetry, Entropy, Diversity and (why not?) Quantum Statistics in\n  Society",
        "comments": "13 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.3390/e21020144",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe society as a nonequilibrium probabilistic system: N individuals\noccupy W resource states in it and produce entropy S over definite time\nperiods. Resulting thermodynamics is however unusual because a second entropy,\nH, measures a typically social feature, inequality or diversity in the\ndistribution of available resources. A symmetry phase transition takes place at\nGini values 1/3, where realistic distributions become asymmetric. Four\nconstraints act on S: expectedly, N and W, and new ones, diversity and\ninteractions between individuals; the latter result from the two coordinates of\na single point in the data, the peak. The occupation number of a job is either\nzero or one, suggesting Fermi-Dirac statistics for employment. Contrariwise, an\nindefinite nujmber of individuals can occupy a state defined as a quantile of\nincome or of age, so Bose-Einstein statistics may be required.\nIndistinguishability rather than anonymity of individuals and resources is thus\nneeded. Interactions between individuals define define classes of equivalence\nthat happen to coincide with acceptable definitions of social classes or\nperiods in human life. The entropy S is non-extensive and obtainable from data.\nTheoretical laws are compared to data in four different cases of economical or\nphysiological diversity. Acceptable fits are found for all of them.\n"
    },
    {
        "paper_id": 1810.04725,
        "authors": "Richard Y. Chen",
        "title": "Inference for Volatility Functionals of Multivariate It\\^o\n  Semimartingales Observed with Jump and Noise",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the nonparametric inference for nonlinear volatility\nfunctionals of general multivariate It\\^o semimartingales, in high-frequency\nand noisy setting. Pre-averaging and truncation enable simultaneous handling of\nnoise and jumps. Second-order expansion reveals explicit biases and a pathway\nto bias correction. Estimators based on this framework achieve the optimal\nconvergence rate. A class of stable central limit theorems are attained with\nestimable asymptotic covariance matrices. This paper form a basis for infill\nasymptotic results of, for example, the realized Laplace transform, the\nrealized principal component analysis, the continuous-time linear regression,\nand the generalized method of integrated moments, hence helps to extend the\napplication scopes to more frequently sampled noisy data.\n"
    },
    {
        "paper_id": 1810.04759,
        "authors": "Steven Dahlke",
        "title": "Integrating electricity markets: Impacts of increasing trade on prices\n  and emissions in the western United States",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5278/ijsepm.3416",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents empirically-estimated average hourly relationships\nbetween regional electricity trade in the United States and prices, emissions,\nand generation from 2015 through 2018. Consistent with economic theory, the\nanalysis finds a negative relationship between electricity prices in California\nand regional trade, conditional on local demand. Each 1 gigawatt-hour increase\nin California electricity imports is associated with an average $0.15 per\nmegawatt-hour decrease in the California Independent System Operator's\nwholesale electricity price. There is a net-negative short term relationship\nbetween carbon dioxide emissions in California and electricity imports that is\npartially offset by positive emissions from exporting neighbors. Specifically,\neach 1 GWh increase in regional trade is associated with a net 70-ton average\ndecrease in CO2 emissions across the western U.S., conditional on demand\nlevels. The results provide evidence that electricity imports mostly displace\nnatural gas generation on the margin in the California electricity market. A\nsmall positive relationship is observed between short-run SO2 and NOx emissions\nin neighboring regions and California electricity imports. The magnitude of the\nSO2 and NOx results suggest an average increase of 0.1 MWh from neighboring\ncoal plants is associated with a 1 MWh increase in imports to California.\n"
    },
    {
        "paper_id": 1810.04819,
        "authors": "Yoshiaki Nakada",
        "title": "Deriving the factor endowment--commodity output relationship for\n  Thailand (1920-1927) using a three-factor two-good general equilibrium trade\n  model",
        "comments": "36 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Feeny (1982, pp. 26-28) referred to a three-factor two-good general\nequilibrium trade model, when he explained the relative importance of trade and\nfactor endowments in Thailand 1880-1940. For example, Feeny (1982) stated that\nthe growth in labor stock would be responsible for a substantial increase in\nrice output relative to textile output. Is Feeny's statement plausible? The\npurpose of this paper is to derive the Rybczynski sign patterns, which express\nthe factor endowment--commodity output relationship, for Thailand during the\nperiod 1920 to 1927 using the EWS (economy-wide substitution)-ratio vector. A\n'strong Rybczynski result' necessarily holds. I derived three Rybczynski sign\npatterns. However, a more detailed estimate allowed a reduction from three\ncandidates to two. I restrict the analysis to the period 1920-1927 because of\ndata availability. The results imply that Feeny's statement might not\nnecessarily hold. Hence, labor stock might not affect the share of exportable\nsector in national income positively. Moreover, the percentage of Chinese\nimmigration in the total population growth was not as large as expected. This\nstudy will be useful when simulating real wage in Thailand.\n"
    },
    {
        "paper_id": 1810.04868,
        "authors": "Eduardo Abi Jaber (CEREMADE)",
        "title": "Lifting the Heston model",
        "comments": "Quantitative Finance, Taylor & Francis (Routledge), In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How to reconcile the classical Heston model with its rough counterpart? We\nintroduce a lifted version of the Heston model with n multi-factors, sharing\nthe same Brownian motion but mean reverting at different speeds. Our model\nnests as extreme cases the classical Heston model (when n = 1), and the rough\nHeston model (when n goes to infinity). We show that the lifted model enjoys\nthe best of both worlds: Markovianity and satisfactory fits of implied\nvolatility smiles for short maturities with very few parameters. Further, our\napproach speeds up the calibration time and opens the door to time-efficient\nsimulation schemes.\n"
    },
    {
        "paper_id": 1810.04978,
        "authors": "Zachary Feinstein and Birgit Rudloff",
        "title": "Time consistency for scalar multivariate risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present results on dynamic multivariate scalar risk\nmeasures, which arise in markets with transaction costs and systemic risk. Dual\nrepresentations of such risk measures are presented. These are then used to\nobtain the main results of this paper on time consistency; namely, an\nequivalent recursive formulation of multivariate scalar risk measures to\nmultiportfolio time consistency. We are motivated to study time consistency of\nmultivariate scalar risk measures as the superhedging risk measure in markets\nwith transaction costs (with a single eligible asset) (Jouini and Kallal\n(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy\nthe usual scalar concept of time consistency. In fact, as demonstrated in\n(Feinstein and Rudloff (2021)), scalar risk measures with the same\nscalarization weight at all times would not be time consistent in general. The\ndeduced recursive relation for the scalarizations of multiportfolio time\nconsistent set-valued risk measures provided in this paper requires\nconsideration of the entire family of scalarizations. In this way we develop a\ndirect notion of a \"moving scalarization\" for scalar time consistency that\ncorroborates recent research on scalarizations of dynamic multi-objective\nproblems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2021)).\n"
    },
    {
        "paper_id": 1810.05094,
        "authors": "Marc Sabate Vidales and David Siska and Lukasz Szpruch",
        "title": "Unbiased deep solvers for linear parametric PDEs",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/1350486X.2022.2030773",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop several deep learning algorithms for approximating families of\nparametric PDE solutions. The proposed algorithms approximate solutions\ntogether with their gradients, which in the context of mathematical finance\nmeans that the derivative prices and hedging strategies are computed\nsimulatenously. Having approximated the gradient of the solution one can\ncombine it with a Monte-Carlo simulation to remove the bias in the deep network\napproximation of the PDE solution (derivative price). This is achieved by\nleveraging the Martingale Representation Theorem and combining the Monte Carlo\nsimulation with the neural network. The resulting algorithm is robust with\nrespect to quality of the neural network approximation and consequently can be\nused as a black-box in case only limited a priori information about the\nunderlying problem is available. We believe this is important as neural network\nbased algorithms often require fair amount of tuning to produce satisfactory\nresults. The methods are empirically shown to work for high-dimensional\nproblems (e.g. 100 dimensions). We provide diagnostics that shed light on\nappropriate network architectures.\n"
    },
    {
        "paper_id": 1810.05689,
        "authors": "Matheus R Grasselli and Alexander Lipton",
        "title": "The Broad Consequences of Narrow Banking",
        "comments": "21 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the macroeconomic consequences of narrow banking in the\ncontext of stock-flow consistent models. We begin with an extension of the\nGoodwin-Keen model incorporating time deposits, government bills, cash, and\ncentral bank reserves to the base model with loans and demand deposits and use\nit to describe a fractional reserve banking system. We then characterize narrow\nbanking by a full reserve requirement on demand deposits and describe the\nresulting separation between the payment system and lending functions of the\nresulting banking sector. By way of numerical examples, we explore the\nproperties of fractional and full reserve versions of the model and compare\ntheir asymptotic properties. We find that narrow banking does not lead to any\nloss in economic growth when the models converge to a finite equilibrium, while\nallowing for more direct monitoring and prevention of financial breakdowns in\nthe case of explosive asymptotic behaviour.\n"
    },
    {
        "paper_id": 1810.05884,
        "authors": "Olivier Gu\\'eant, Jiang Pu",
        "title": "Mid-price estimation for European corporate bonds: a particle filtering\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In most illiquid markets, there is no obvious proxy for the market price of\nan asset. The European corporate bond market is an archetypal example of such\nan illiquid market where mid-prices can only be estimated with a statistical\nmodel. In this OTC market, dealers / market makers only have access, indeed, to\npartial information about the market. In real time, they know the price\nassociated with their trades on the dealer-to-dealer (D2D) and dealer-to-client\n(D2C) markets, they know the result of the requests for quotes (RFQ) they\nanswered, and they have access to composite prices (e.g., Bloomberg CBBT). This\npaper presents a Bayesian method for estimating the mid-price of corporate\nbonds by using the real-time information available to a dealer. This method\nrelies on recent ideas coming from the particle filtering / sequential\nMonte-Carlo literature.\n"
    },
    {
        "paper_id": 1810.06101,
        "authors": "Philippe Casgrain, Sebastian Jaimungal",
        "title": "Mean-Field Games with Differing Beliefs for Algorithmic Trading",
        "comments": "36 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Even when confronted with the same data, agents often disagree on a model of\nthe real-world. Here, we address the question of how interacting heterogenous\nagents, who disagree on what model the real-world follows, optimize their\ntrading actions. The market has latent factors that drive prices, and agents\naccount for the permanent impact they have on prices. This leads to a large\nstochastic game, where each agents' performance criteria are computed under a\ndifferent probability measure. We analyse the mean-field game (MFG) limit of\nthe stochastic game and show that the Nash equilibrium is given by the solution\nto a non-standard vector-valued forward-backward stochastic differential\nequation. Under some mild assumptions, we construct the solution in terms of\nexpectations of the filtered states. Furthermore, we prove the MFG strategy\nforms an $\\epsilon$-Nash equilibrium for the finite player game. Lastly, we\npresent a least-squares Monte Carlo based algorithm for computing the\nequilibria and show through simulations that increasing disagreement may\nincrease price volatility and trading activity.\n"
    },
    {
        "paper_id": 1810.06335,
        "authors": "Rodwell Kufakunesu and Farai Mhlanga",
        "title": "On the sensitivity analysis of energy quanto options",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years there has been an advent of quanto options in energy markets.\nThe structure of the payoff is rather a different type from other markets since\nit is written as a product of an underlying energy index and a measure of\ntemperature. In the HJM framework, by adopting the futures energy dynamics, we\nuse the Malliavin calculus to derive the delta and the cross-gamma expectation\nformulas. This work can be viewed as an extension of the work done, for example\nby Benth et al. [1].\n"
    },
    {
        "paper_id": 1810.06366,
        "authors": "Takashi Shinzato",
        "title": "Replica Analysis for Maximization of Net Present Value",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we use replica analysis to determine the investment strategy\nthat can maximize the net present value for portfolios containing multiple\ndevelopment projects. Replica analysis was developed in statistical mechanical\ninformatics and econophysics to evaluate disordered systems, and here we use it\nto formulate the maximization of the net present value as an optimization\nproblem under budget and investment concentration constraints. Furthermore, we\nconfirm that a common approach from operations research underestimates the true\nmaximal net present value as the maximal expected net present value by\ncomparing our results with the maximal expected net present value as derived in\noperations research. Moreover, it is shown that the conventional method for\nestimating the net present value does not consider variance in the cash flow.\n"
    },
    {
        "paper_id": 1810.06696,
        "authors": "Zvezdin Besarabov, Todor Kolev",
        "title": "Predicting digital asset market based on blockchain activity data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain technology shows significant results and huge potential for\nserving as an interweaving fabric that goes through every industry and market,\nallowing decentralized and secure value exchange, thus connecting our\ncivilization like never before. The standard approach for asset value\npredictions is based on market analysis with an LSTM neural network. Blockchain\ntechnologies, however, give us access to vast amounts of public data, such as\nthe executed transactions and the account balance distribution. We explore\nwhether analyzing this data with modern Deep Leaning techniques results in\nhigher accuracies than the standard approach. During a series of experiments on\nthe Ethereum blockchain, we achieved $4$ times error reduction with blockchain\ndata than an LSTM approach with trade volume data. By utilizing blockchain\naccount distribution histograms, spatial dataset modeling, and a Convolutional\narchitecture, the error was reduced further by 26\\%. The proposed methodologies\nare implemented in an open source cryptocurrency prediction framework, allowing\nthem to be used in other analysis contexts.\n"
    },
    {
        "paper_id": 1810.06698,
        "authors": "Idris Adjerid, Rachael Purta, Aaron Striegel, George Loewenstein",
        "title": "Aggressive Economic Incentives and Physical Activity: The Role of Choice\n  and Technology Decision Aids",
        "comments": "Paper is undergoing a major revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aggressive incentive schemes that allow individuals to impose economic\npunishment on themselves if they fail to meet health goals present a promising\napproach for encouraging healthier behavior. However, the element of choice\ninherent in these schemes introduces concerns that only non-representative\nsectors of the population will select aggressive incentives, leaving value on\nthe table for those who don't opt in. In a field experiment conducted over a 29\nweek period on individuals wearing Fitbit activity trackers, we find modest and\nshort lived increases in physical activity for those provided the choice of\naggressive incentives. In contrast, we find significant and persistent\nincreases for those assigned (oftentimes against their stated preference) to\nthe same aggressive incentives. The modest benefits for those provided a choice\nseems to emerge because those who benefited most from the aggressive incentives\nwere the least likely to choose them, and it was those who did not need them\nwho opted in. These results are confirmed in a follow up lab experiment. We\nalso find that benefits to individuals assigned to aggressive incentives were\npronounced if they also updated their step target in the Fitbit mobile\napplication to match the new activity goal we provided them. Our findings have\nimportant implications for incentive based interventions to improve health\nbehavior. For firms and policy makers, our results suggest that one effective\nstrategy for encouraging sustained healthy behavior combines exposure to\naggressive incentive schemes to jolt individuals out of their comfort zones\nwith technology decision aids that help individuals sustain this behavior after\nincentives end.\n"
    },
    {
        "paper_id": 1810.06973,
        "authors": "Fabrizio Germano and Francesco Sobbrio",
        "title": "Opinion Dynamics via Search Engines (and other Algorithmic Gatekeepers)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ranking algorithms are the information gatekeepers of the Internet era. We\ndevelop a stylized model to study the effects of ranking algorithms on opinion\ndynamics. We consider a search engine that uses an algorithm based on\npopularity and on personalization. We find that popularity-based rankings\ngenerate an advantage of the fewer effect: fewer websites reporting a given\nsignal attract relatively more traffic overall. This highlights a novel,\nranking-driven channel that explains the diffusion of misinformation, as\nwebsites reporting incorrect information may attract an amplified amount of\ntraffic precisely because they are few. Furthermore, when individuals provide\nsufficiently positive feedback to the ranking algorithm, popularity-based\nrankings tend to aggregate information while personalization acts in the\nopposite direction.\n"
    },
    {
        "paper_id": 1810.07112,
        "authors": "Viktoras Kulionis",
        "title": "Constructing energy accounts for WIOD 2016 release",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most of today's products and services are made in global supply chains. As a\nresult, a consumption of goods and services in one country is associated with\nvarious environmental pressures all over the world due to international trade.\nAdvances in global multi-region input-output models have allowed researchers to\ndraw detailed, international supply-chain connections between production and\nconsumptions activities and associated environmental impacts. Due to a limited\ndata availability there is little evidence about the more recent trends in\nglobal energy footprint. In order to expand the analytical potential of the\nexisting WIOD 2016 dataset to a wider range of research themes, this paper\ndevelops energy accounts and presents the global energy footprint trends for\nthe period 2000-2014.\n"
    },
    {
        "paper_id": 1810.07178,
        "authors": "A\\\"ileen Lotz, Pierre Gosselin (IF), Marc Wambst (IRMA)",
        "title": "A Path Integral Approach to Business Cycle Models with Large Number of\n  Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an analytical treatment of economic systems with an\narbitrary number of agents that keeps track of the systems' interactions and\nagents' complexity. This formalism does not seek to aggregate agents. It rather\nreplaces the standard optimization approach by a probabilistic description of\nboth the entire system and agents'behaviors. This is done in two distinct\nsteps. A first step considers an interacting system involving an arbitrary\nnumber of agents, where each agent's utility function is subject to\nunpredictable shocks. In such a setting, individual optimization problems need\nnot be resolved. Each agent is described by a time-dependent probability\ndistribution centered around his utility optimum. The entire system of agents\nis thus defined by a composite probability depending on time, agents'\ninteractions and forward-looking behaviors. This dynamic system is described by\na path integral formalism in an abstract space-the space of the agents'\nactions-and is very similar to a statistical physics or quantum mechanics\nsystem. We show that this description, applied to the space of agents'actions,\nreduces to the usual optimization results in simple cases. Compared to a\nstandard optimization, such a description markedly eases the treatment of\nsystems with small number of agents. It becomes however useless for a large\nnumber of agents. In a second step therefore, we show that for a large number\nof agents, the previous description is equivalent to a more compact description\nin terms of field theory. This yields an analytical though approximate\ntreatment of the system. This field theory does not model the aggregation of a\nmicroeconomic system in the usual sense. It rather describes an environment of\na large number of interacting agents. From this description, various phases or\nequilibria may be retrieved, along with individual agents' behaviors and their\ninteractions with the environment. For illustrative purposes, this paper\nstudies a Business Cycle model with a large number of agents.\n"
    },
    {
        "paper_id": 1810.07457,
        "authors": "Volodymyr Perederiy",
        "title": "Vanna-Volga Method for Normal Volatilities",
        "comments": "Keywords: Vanna-Volga, Option Pricing, Normal Bachelier model, Option\n  Greeks, Delta, Vega, Vanna, Volga, Volatility Smiles, Volatility Frowns, SABR\n  model, Interest Rates",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Vanna-Volga is a popular method for the interpolation/extrapolation of\nvolatility smiles. The technique is widely used in the FX markets context, due\nto its ability to consistently construct the entire Lognormal smile using only\nthree Lognormal market quotes. However, the derivation of the Vanna-Volga\nmethod itself is free of distributional assumptions. With this is mind, it is\nsurprising there have been no attempts to apply the method to Normal\nvolatilities (the current standard for interest rate markets). We show how the\nmethod can be modified to build Normal volatility smiles. As it turns out, only\nminor modifications are required compared to the Lognormal case. Moreover, as\nthe inversion of Normal volatilities from option prices is easier in the Normal\ncase, the smile construction can occur at a machine-precision level using\nanalytical formulae, making the approximations via Taylor-series unnecessary.\nApart from being based on practical and intuitive hedging arguments, the\nVanna-Volga has further important advantages. In comparison to the Normal SABR\nmodel, the Vanna-Volga can easily fit both classical convex and atypical\nconcave smiles (frowns). Concave smile patterns are sometimes observed around\nATM strikes in the interest rate markets, particularly in the situations of\nanticipated jumps (with an unclear outcome) in interest rates. Besides,\nconcavity is often observed towards the lower/left end of the Normal volatility\nsmiles of interest rates. At least in these situations, the Vanna-Volga can be\nexpected to interpolate/extrapolate better than SABR.\n"
    },
    {
        "paper_id": 1810.07674,
        "authors": "Tiziano De Angelis, Erik Ekstr\\\"om and Kristoffer Glover",
        "title": "Dynkin games with incomplete and asymmetric information",
        "comments": "31 pages, 5 figures, small changes in the terminology from game\n  theory",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the value and the optimal strategies for a two-player zero-sum\noptimal stopping game with incomplete and asymmetric information. In our\nBayesian set-up, the drift of the underlying diffusion process is unknown to\none player (incomplete information feature), but known to the other one\n(asymmetric information feature). We formulate the problem and reduce it to a\nfully Markovian setup where the uninformed player optimises over stopping times\nand the informed one uses randomised stopping times in order to hide their\ninformational advantage. Then we provide a general verification result which\nallows us to find the value of the game and players' optimal strategies by\nsolving suitable quasi-variational inequalities with some non-standard\nconstraints. Finally, we study an example with linear payoffs, in which an\nexplicit solution of the corresponding quasi-variational inequalities can be\nobtained.\n"
    },
    {
        "paper_id": 1810.0769,
        "authors": "Roman Orus, Samuel Mugel, Enrique Lizaso",
        "title": "Forecasting financial crashes with quantum computing",
        "comments": "6 pages, 4 figures, revised version. To appear in PRA",
        "journal-ref": "Phys. Rev. A 99, 060301 (2019)",
        "doi": "10.1103/PhysRevA.99.060301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A key problem in financial mathematics is the forecasting of financial\ncrashes: if we perturb asset prices, will financial institutions fail on a\nmassive scale? This was recently shown to be a computationally intractable\n(NP-hard) problem. Financial crashes are inherently difficult to predict, even\nfor a regulator which has complete information about the financial system. In\nthis paper we show how this problem can be handled by quantum annealers. More\nspecifically, we map the equilibrium condition of a toy-model financial network\nto the ground-state problem of a spin-1/2 quantum Hamiltonian with 2-body\ninteractions, i.e., a quadratic unconstrained binary optimization (QUBO)\nproblem. The equilibrium market values of institutions after a sudden shock to\nthe network can then be calculated via adiabatic quantum computation and, more\ngenerically, by quantum annealers. Our procedure could be implemented on\nnear-term quantum processors, thus providing a potentially more efficient way\nto assess financial equilibrium and predict financial crashes.\n"
    },
    {
        "paper_id": 1810.07735,
        "authors": "M. Dashti Moghaddam and R. A. Serota",
        "title": "Implied and Realized Volatility: A Study of the Ratio Distribution",
        "comments": "9 pages, 10 figures, 9 tables",
        "journal-ref": "Applied Economics and Finance (2019) Vol. 6, No. 5, pp. 104-130",
        "doi": "10.11114/aef.v6i5.4416",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze correlations between squared volatility indices, VIX and VXO, and\nrealized variances -- the known one, for the current month, and the predicted\none, for the following month. We show that the ratio of the two is best fitted\nby a Beta Prime distribution, whose shape parameters depend strongly on which\nof the two months is used.\n"
    },
    {
        "paper_id": 1810.07774,
        "authors": "James McNerney, Charles Savoie, Francesco Caravelli, Vasco M.\n  Carvalho, J. Doyne Farmer",
        "title": "How production networks amplify economic growth",
        "comments": "45 pages, 12 figures",
        "journal-ref": "PNAS 119 (1) e2106031118 (2021)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological improvement is the most important cause of long-term economic\ngrowth. We study the effects of technology improvement in the setting of a\nproduction network, in which each producer buys input goods and converts them\nto other goods, selling the product to households or other producers. We show\nhow this network amplifies the effects of technological improvements as they\npropagate along chains of production. Longer production chains for an industry\nbias it towards faster price reduction, and longer production chains for a\ncountry bias it towards faster GDP growth. These predictions are in good\nagreement with data and improve with the passage of time, demonstrating a key\ninfluence of production chains in price change and output growth over the long\nterm.\n"
    },
    {
        "paper_id": 1810.07783,
        "authors": "Rex Yuxing Du, Mingyu Joo, Kenneth C. Wilbur",
        "title": "Advertising and Brand Attitudes: Evidence from 575 Brands over Five\n  Years",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Little is known about how different types of advertising affect brand\nattitudes. We investigate the relationships between three brand attitude\nvariables (perceived quality, perceived value and recent satisfaction) and\nthree types of advertising (national traditional, local traditional and\ndigital). The data represent ten million brand attitude surveys and $264\nbillion spent on ads by 575 regular advertisers over a five-year period,\napproximately 37% of all ad spend measured between 2008 and 2012. Inclusion of\nbrand/quarter fixed effects and industry/week fixed effects brings parameter\nestimates closer to expectations without major reductions in estimation\nprecision. The findings indicate that (i) national traditional ads increase\nperceived quality, perceived value, and recent satisfaction; (ii) local\ntraditional ads increase perceived quality and perceived value; (iii) digital\nads increase perceived value; and (iv) competitor ad effects are generally\nnegative.\n"
    },
    {
        "paper_id": 1810.0779,
        "authors": "Rahul Roy, Santhakumar Shijin",
        "title": "A six-factor asset pricing model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.bir.2018.02.001",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The present study introduce the human capital component to the Fama and\nFrench five-factor model proposing an equilibrium six-factor asset pricing\nmodel. The study employs an aggregate of four sets of portfolios mimicking size\nand industry with varying dimensions. The first set consists of three set of\nsix portfolios each sorted on size to B/M, size to investment, and size to\nmomentum. The second set comprises of five index portfolios, third, a four-set\nof twenty-five portfolios each sorted on size to B/M, size to investment, size\nto profitability, and size to momentum, and the final set constitute thirty\nindustry portfolios. To estimate the parameters of six-factor asset pricing\nmodel for the four sets of variant portfolios, we use OLS and Generalized\nmethod of moments based robust instrumental variables technique (IVGMM). The\nresults obtained from the relevance, endogeneity, overidentifying restrictions,\nand the Hausman's specification, tests indicate that the parameter estimates of\nthe six-factor model using IVGMM are robust and performs better than the OLS\napproach. The human capital component shares equally the predictive power\nalongside the factors in the framework in explaining the variations in return\non portfolios. Furthermore, we assess the t-ratio of the human capital\ncomponent of each IVGMM estimates of the six-factor asset pricing model for the\nfour sets of variant portfolios. The t-ratio of the human capital of the\neighty-three IVGMM estimates are more than 3.00 with reference to the standard\nproposed by Harvey et al. (2016). This indicates the empirical success of the\nsix-factor asset-pricing model in explaining the variation in asset returns.\n"
    },
    {
        "paper_id": 1810.07832,
        "authors": "Peter Bank and Yan Dolinsky",
        "title": "Scaling Limits for Super--replication with Transient Price Impact",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a scaling limit theorem for the super-replication cost of options in\na Cox--Ross--Rubinstein binomial model with transient price impact. The correct\nscaling turns out to keep the market depth parameter constant while resilience\nover fixed periods of time grows in inverse proportion with the duration\nbetween trading times. For vanilla options, the scaling limit is found to\ncoincide with the one obtained by PDE methods in [12] for models with purely\ntemporary price impact. These models are a special case of our framework and so\nour probabilistic scaling limit argument allows one to expand the scope of the\nscaling limit result to path-dependent options.\n"
    },
    {
        "paper_id": 1810.0833,
        "authors": "Inho Hong, Morgan R. Frank, Iyad Rahwan, Woo-Sung Jung, Hyejin Youn",
        "title": "A common trajectory recapitulated by urban economies",
        "comments": null,
        "journal-ref": "Science Advances 6, eaba4934 (2020)",
        "doi": "10.1126/sciadv.aba4934",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is there a general economic pathway recapitulated by individual cities over\nand over? Identifying such evolution structure, if any, would inform models for\nthe assessment, maintenance, and forecasting of urban sustainability and\neconomic success as a quantitative baseline. This premise seems to contradict\nthe existing body of empirical evidences for path-dependent growth shaping the\nunique history of individual cities. And yet, recent empirical evidences and\ntheoretical models have amounted to the universal patterns, mostly\nsize-dependent, thereby expressing many of urban quantities as a set of simple\nscaling laws. Here, we provide a mathematical framework to integrate repeated\ncross-sectional data, each of which freezes in time dimension, into a frame of\nreference for longitudinal evolution of individual cities in time. Using data\nof over 100 millions employment in thousand business categories between 1998\nand 2013, we decompose each city's evolution into a pre-factor and relative\nchanges to eliminate national and global effects. In this way, we show the\nlongitudinal dynamics of individual cities recapitulate the observed\ncross-sectional regularity. Larger cities are not only scaled-up versions of\ntheir smaller peers but also of their past. In addition, our model shows that\nboth specialization and diversification are attributed to the distribution of\nindustry's scaling exponents, resulting a critical population of 1.2 million at\nwhich a city makes an industrial transition into innovative economies.\n"
    },
    {
        "paper_id": 1810.08337,
        "authors": "Josselin Garnier, Knut Solna",
        "title": "Optimal hedging under fast-varying stochastic volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market with a rough or Markovian mean-reverting stochastic volatility\nthere is no perfect hedge. Here it is shown how various delta-type hedging\nstrategies perform and can be evaluated in such markets in the case of European\noptions. A precise characterization of the hedging cost, the replication cost\ncaused by the volatility fluctuations, is presented in an asymptotic regime of\nrapid mean reversion for the volatility fluctuations. The optimal dynamic asset\nbased hedging strategy in the considered regime is identified as the so-called\n`practitioners' delta hedging scheme. It is moreover shown that the\nperformances of the delta-type hedging schemes are essentially independent of\nthe regularity of the volatility paths in the considered regime and that the\nhedging costs are related to a vega risk martingale whose magnitude is\nproportional to a new market risk parameter. It is also shown via numerical\nsimulations that the proposed hedging schemes which derive from option price\napproximations in the regime of rapid mean reversion, are robust: the\n`practitioners' delta hedging scheme that is identified as being optimal by our\nasymptotic analysis when the mean reversion time is small seems to be optimal\nwith arbitrary mean reversion times.\n"
    },
    {
        "paper_id": 1810.08384,
        "authors": "Stefano Ciliberti and Stanislao Gualdi",
        "title": "Portfolio Construction Matters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The role of portfolio construction in the implementation of equity market\nneutral factors is often underestimated. Taking the classical momentum strategy\nas an example, we show that one can significantly improve the main strategy's\nfeatures by properly taking care of this key step. More precisely, an optimized\nportfolio construction algorithm allows one to significantly improve the Sharpe\nRatio, reduce sector exposures and volatility fluctuations, and mitigate the\nstrategy's skewness and tail correlation with the market. These results are\nsupported by long-term, world-wide simulations and will be shown to be\nuniversal. Our findings are quite general and hold true for a number of other\n\"equity factors\". Finally, we discuss the details of a more realistic set-up\nwhere we also deal with transaction costs.\n"
    },
    {
        "paper_id": 1810.08396,
        "authors": "Yong Jiang, Yi-Shuai Ren, Chao-Qun Ma, Jiang-Long Liu, Basil Sharp",
        "title": "Does the price of strategic commodities respond to U.S. Partisan\n  Conflict?",
        "comments": null,
        "journal-ref": "Resources Policy 66, 101617 (2020)",
        "doi": "10.1016/j.resourpol.2020.101617",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A noteworthy feature of U.S. politics in recent years is serious partisan\nconflict, which has led to intensifying polarization and exacerbating high\npolicy uncertainty. The US is a significant player in oil and gold markets. Oil\nand gold also form the basis of important strategic reserves in the US. We\ninvestigate whether U.S. partisan conflict affects the returns and price\nvolatility of oil and gold using a parametric test of Granger causality in\nquantiles. The empirical results suggest that U.S. partisan conflict has an\neffect on the returns of oil and gold, and the effects are concentrated at the\ntail of the conditional distribution of returns. More specifically, the\npartisan conflict mainly affects oil returns when the crude oil market is in a\nbearish state (lower quantiles). By contrast, partisan conflict matters for\ngold returns only when the gold market is in a bullish scenario (higher\nquantiles). In addition, for the volatility of oil and gold, the predictability\nof partisan conflict index virtually covers the entire distribution of\nvolatility.\n"
    },
    {
        "paper_id": 1810.08466,
        "authors": "Rafael Serrano and Camilo Castillo",
        "title": "ALM for insurers with multiple underwriting lines and portfolio\n  constraints: a Lagrangian duality approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous-time asset-allocation problem for an insurance firm\nthat backs up liabilities from multiple non-life business lines with\nunderwriting profits and investment income. The insurance risks are captured\nvia a multidimensional jump-diffusion process with a multivariate compound\nPoisson process with dependent components, which allows to model claims that\noccur in different lines simultaneously. Using Lagrangian convex duality\ntechniques, we provide a general verification-type result for\ninvestment-underwriting strategies that maximize expected utility from the\ndividend payout rate and final wealth over a finite-time horizon. We also study\nthe precautionary effect on earnings retention of risk aversion, prudence,\nportfolio constraints and multivariate insurance risk. We find an explicit\ncharacterization of optimal strategies under CRRA preferences. Numerical\nresults for two-dimensional examples with policy limits illustrate the impact\nof co-integration for ALM with multiple (dependent and independent) sources of\ninsurance risk.\n"
    },
    {
        "paper_id": 1810.08495,
        "authors": "Peter Bank and David Besslich",
        "title": "Modelling information flows by Meyer-$\\sigma$-fields in the singular\n  stochastic control problem of irreversible investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In stochastic control problems delicate issues arise when the controlled\nsystem can jump due to both exogenous shocks and endogenous controls. Here one\nhas to specify what the controller knows when about the exogenous shocks and\nhow and when she can act on this information. We propose to use\nMeyer-$\\sigma$-fields as a flexible tool to model information flow in such\nsituations. The possibilities of this approach are illustrated first in a very\nsimple linear stochastic control problem and then in a fairly general\nformulation for the singular stochastic control problem of irreversible\ninvestment with inventory risk. For the latter, we illustrate in a first case\nstudy how different signals on exogenous jumps lead to different optimal\ncontrols, interpolating between the predictable and the optional case in a\nsystematic manner.\n"
    },
    {
        "paper_id": 1810.08584,
        "authors": "Davide Venturelli and Alexei Kondratyev",
        "title": "Reverse Quantum Annealing Approach to Portfolio Optimization Problems",
        "comments": "19 pages, 6 figures (incl. appendix)",
        "journal-ref": "Quantum Mach. Intell. (2019)",
        "doi": "10.1007/s42484-019-00001-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a hybrid quantum-classical solution method to the\nmean-variance portfolio optimization problems. Starting from real financial\ndata statistics and following the principles of the Modern Portfolio Theory, we\ngenerate parametrized samples of portfolio optimization problems that can be\nrelated to quadratic binary optimization forms programmable in the analog\nD-Wave Quantum Annealer 2000Q. The instances are also solvable by an\nindustry-established Genetic Algorithm approach, which we use as a classical\nbenchmark. We investigate several options to run the quantum computation\noptimally, ultimately discovering that the best results in terms of expected\ntime-to-solution as a function of number of variables for the hardest instances\nset are obtained by seeding the quantum annealer with a solution candidate\nfound by a greedy local search and then performing a reverse annealing\nprotocol. The optimized reverse annealing protocol is found to be more than 100\ntimes faster than the corresponding forward quantum annealing on average.\n"
    },
    {
        "paper_id": 1810.08923,
        "authors": "Ehsan Hoseinzade, Saman Haratizadeh",
        "title": "CNNPred: CNN-based stock market prediction using several data sources",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Feature extraction from financial data is one of the most important problems\nin market prediction domain for which many approaches have been suggested.\nAmong other modern tools, convolutional neural networks (CNN) have recently\nbeen applied for automatic feature selection and market prediction. However, in\nexperiments reported so far, less attention has been paid to the correlation\namong different markets as a possible source of information for extracting\nfeatures. In this paper, we suggest a CNN-based framework with specially\ndesigned CNNs, that can be applied on a collection of data from a variety of\nsources, including different markets, in order to extract features for\npredicting the future of those markets. The suggested framework has been\napplied for predicting the next day's direction of movement for the indices of\nS&P 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of\ninitial features. The evaluations show a significant improvement in\nprediction's performance compared to the state of the art baseline algorithms.\n"
    },
    {
        "paper_id": 1810.09063,
        "authors": "Ren\\'e A\\\"id and Dylan Possama\\\"i and Nizar Touzi",
        "title": "Optimal electricity demand response contracting with responsiveness\n  incentives",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the success of demand response programs in retail electricity markets\nin reducing average consumption, the random responsiveness of consumers to\nprice event makes their efficiency questionable to achieve the flexibility\nneeded for electric systems with a large share of renewable energy. The\nvariance of consumers' responses depreciates the value of these mechanisms and\nmakes them weakly reliable. This paper aims at designing demand response\ncontracts which allow to act on both the average consumption and its variance.\nThe interaction between a risk--averse producer and a risk--averse consumer is\nmodelled through a Principal--Agent problem, thus accounting for the moral\nhazard underlying demand response contracts. We provide closed--form solution\nfor the optimal contract in the case of constant marginal costs of energy and\nvolatility for the producer and constant marginal value of energy for the\nconsumer. We show that the optimal contract has a rebate form where the initial\ncondition of the consumption serves as a baseline. Further, the consumer cannot\nmanipulate the baseline at his own advantage. The second--best price for energy\nand volatility are non--constant and non--increasing in time. The price for\nenergy is lower (resp. higher) than the marginal cost of energy during\npeak--load (resp. off--peak) periods. We illustrate the potential benefit\nissued from the implementation of an incentive mechanism on the responsiveness\nof the consumer by calibrating our model with publicly available data. We\npredict a significant increase of responsiveness under our optimal contract and\na significant increase of the producer satisfaction.\n"
    },
    {
        "paper_id": 1810.09112,
        "authors": "Yu Feng, Ralph Rudd, Christopher Baker, Qaphela Mashalaba, Melusi\n  Mavuso, Erik Schl\\\"ogl",
        "title": "Quantifying the Model Risk Inherent in the Calibration and Recalibration\n  of Option Pricing Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on two particular aspects of model risk: the inability of a chosen\nmodel to fit observed market prices at a given point in time (calibration\nerror) and the model risk due to recalibration of model parameters (in\ncontradiction to the model assumptions). In this context, we follow the\napproach of Glasserman and Xu (2014) and use relative entropy as a pre-metric\nin order to quantify these two sources of model risk in a common framework, and\nconsider the trade-offs between them when choosing a model and the frequency\nwith which to recalibrate to the market. We illustrate this approach applied to\nthe models of Black and Scholes (1973) and Heston (1993), using option data for\nApple (AAPL) and Google (GOOG). We find that recalibrating a model more\nfrequently simply shifts model risk from one type to another, without any\nsubstantial reduction of aggregate model risk. Furthermore, moving to a more\ncomplicated stochastic model is seen to be counterproductive if one requires a\nhigh degree of robustness, for example as quantified by a 99 percent quantile\nof aggregate model risk.\n"
    },
    {
        "paper_id": 1810.09179,
        "authors": "Eoghan O'Neill, Melvyn Weeks",
        "title": "Causal Tree Estimation of Heterogeneous Household Response to\n  Time-Of-Use Electricity Pricing Schemes",
        "comments": "34 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the household-specific effects of the introduction of Time-of-Use\n(TOU) electricity pricing schemes. Using a causal forest (Athey and Imbens,\n2016; Wager and Athey, 2018; Athey et al., 2019), we consider the association\nbetween past consumption and survey variables, and the effect of TOU pricing on\nhousehold electricity demand. We describe the heterogeneity in household\nvariables across quartiles of estimated demand response and utilise variable\nimportance measures.\n  Household-specific estimates produced by a causal forest exhibit reasonable\nassociations with covariates. For example, households that are younger, more\neducated, and that consume more electricity, are predicted to respond more to a\nnew pricing scheme. In addition, variable importance measures suggest that some\naspects of past consumption information may be more useful than survey\ninformation in producing these estimates.\n"
    },
    {
        "paper_id": 1810.09366,
        "authors": "N.S. Gonchar",
        "title": "Description of Incomplete Financial Markets for the Discrete Time\n  Evolution of Risk Assets",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, the martingales and super-martingales relative to a regular set\nof measures are systematically studied. The notion of local regular\nsuper-martingale relative to a set of equivalent measures is introduced and the\nnecessary and sufficient conditions of the local regularity of it in the\ndiscrete case are founded. The regular set of measures play fundamental role\nfor the description of incomplete markets. In the partial case, the description\nof the regular set of measures is presented. The notion of completeness of the\nregular set of measures have the important significance for the simplification\nof the proof of the optional decomposition for super-martingales. Using this\nnotion, the important inequalities for some random values are obtained. These\ninequalities give the simple proof of the optional decomposition of the\nmajorized super-martingales. The description of all local regular\nsuper-martingales relative to the regular set of measures is presented. It is\nproved that every majorized super-martingale relative to the complete set of\nmeasures is a local regular one. In the case, as evolution of a risk asset is\ngiven by the discrete geometric Brownian motion, the financial market is\nincomplete and a new formula for the fair price of super-hedge is founded.\n"
    },
    {
        "paper_id": 1810.09386,
        "authors": "Tolulope Fadina, Frederik Herzberg",
        "title": "Hyperfinite Construction of $G$-expectation",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The hyperfinite $G$-expectation is a nonstandard discrete analogue of\n$G$-expectation (in the sense of Robinsonian nonstandard analysis). A lifting\nof a continuous-time $G$-expectation operator is defined as a hyperfinite\n$G$-expectation which is infinitely close, in the sense of nonstandard\ntopology, to the continuous-time $G$-expectation. We develop the basic theory\nfor hyperfinite $G$-expectations and prove an existence theorem for liftings of\n(continuous-time) $G$-expectation. For the proof of the lifting theorem, we use\na new discretization theorem for the $G$-expectation (also established in this\npaper, based on the work of Dolinsky et al. [Weak approximation of\n$G$-expectations, Stoch. Proc. Appl. 122(2), (2012), pp.664--675]).\n"
    },
    {
        "paper_id": 1810.09397,
        "authors": "Jingtang Ma, Jie Xing, Harry Zheng",
        "title": "Global Closed-form Approximation of Free Boundary for Optimal Investment\n  Stopping Problems",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a utility maximization problem with both optimal\ncontrol and optimal stopping in a finite time horizon. The value function can\nbe characterized by a variational equation that involves a free boundary\nproblem of a fully nonlinear partial differential equation. Using the dual\ncontrol method, we derive the asymptotic properties of the dual value function\nand the associated dual free boundary for a class of utility functions,\nincluding power and non-HARA utilities. We construct a global closed-form\napproximation to the dual free boundary, which greatly reduces the\ncomputational cost. Using the duality relation, we find the approximate\nformulas for the optimal value function, trading strategy, and exercise\nboundary for the optimal investment stopping problem. Numerical examples show\nthe approximation is robust, accurate and fast.\n"
    },
    {
        "paper_id": 1810.09521,
        "authors": "Szabolcs Majoros and Andr\\'as Zempl\\'eni",
        "title": "Multivariate stable distributions and their applications for modelling\n  cryptocurrency-returns",
        "comments": "29 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the known methodology for fitting stable\ndistributions to the multivariate case and apply the suggested method to the\nmodelling of daily cryptocurrency-return data. The investigated time period is\ncut into 10 non-overlapping sections, thus the changes can also be observed. We\napply bootstrap tests for checking the models and compare our approach to the\nmore traditional extreme-value and copula models.\n"
    },
    {
        "paper_id": 1810.0967,
        "authors": "Markus Hess",
        "title": "Cliquet option pricing in a jump-diffusion L\\'{e}vy model",
        "comments": "Published at https://doi.org/10.15559/18-VMSTA107 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 3,\n  317-336",
        "doi": "10.15559/18-VMSTA107",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the pricing of cliquet options in a jump-diffusion model. The\nconsidered option is of monthly sum cap style while the underlying stock price\nmodel is driven by a drifted L\\'{e}vy process entailing a Brownian diffusion\ncomponent as well as compound Poisson jumps. We also derive representations for\nthe density and distribution function of the emerging L\\'{e}vy process. In this\nsetting, we infer semi-analytic expressions for the cliquet option price by two\ndifferent approaches. The first one involves the probability distribution\nfunction of the driving L\\'{e}vy process whereas the second draws upon Fourier\ntransform techniques. With view on sensitivity analysis and hedging purposes,\nwe eventually deduce representations for several Greeks while putting emphasis\non the Vega.\n"
    },
    {
        "paper_id": 1810.09803,
        "authors": "Bertrand Corn\\'elusse, Iacopo Savelli, Simone Paoletti, Antonio\n  Giannitrapani and Antonio Vicino",
        "title": "A Community Microgrid Architecture with an Internal Local Market",
        "comments": "16 pages, 15 figures",
        "journal-ref": null,
        "doi": "10.1016/j.apenergy.2019.03.109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work fits in the context of community microgrids, where members of a\ncommunity can exchange energy and services among themselves, without going\nthrough the usual channels of the public electricity grid. We introduce and\nanalyze a framework to operate a community microgrid, and to share the\nresulting revenues and costs among its members. A market-oriented pricing of\nenergy exchanges within the community is obtained by implementing an internal\nlocal market based on the marginal pricing scheme. The market aims at\nmaximizing the social welfare of the community, thanks to the more efficient\nallocation of resources, the reduction of the peak power to be paid, and the\nincreased amount of reserve, achieved at an aggregate level. A community\nmicrogrid operator, acting as a benevolent planner, redistributes revenues and\ncosts among the members, in such a way that the solution achieved by each\nmember within the community is not worse than the solution it would achieve by\nacting individually. In this way, each member is incentivized to participate in\nthe community on a voluntary basis. The overall framework is formulated in the\nform of a bilevel model, where the lower level problem clears the market, while\nthe upper level problem plays the role of the community microgrid operator.\nNumerical results obtained on a real test case implemented in Belgium show\naround 54% cost savings on a yearly scale for the community, as compared to the\ncase when its members act individually.\n"
    },
    {
        "paper_id": 1810.09825,
        "authors": "Gian Paolo Clemente, Rosanna Grassi, Asmerilda Hitaj",
        "title": "Asset allocation: new evidence through network approaches",
        "comments": "Submitted",
        "journal-ref": "Annals of Operations Research, 2019",
        "doi": "10.1007/s10479-019-03136-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main contribution of the paper is to employ the financial market network\nas a useful tool to improve the portfolio selection process, where nodes\nindicate securities and edges capture the dependence structure of the system.\nThree different methods are proposed in order to extract the dependence\nstructure between assets in a network context. Starting from this modified\nstructure, we formulate and then we solve the asset allocation problem. We find\nthat the portfolios obtained through a network-based approach are composed\nmainly of peripheral assets, which are poorly connected with the others. These\nportfolios, in the majority of cases, are characterized by an higher trade-off\nbetween performance and risk with respect to the traditional Global Minimum\nVariance (GMV) portfolio. Additionally, this methodology benefits of a\ngraphical visualization of the selected portfolio directly over the graphic\nlayout of the network, which helps in improving our understanding of the\noptimal strategy.\n"
    },
    {
        "paper_id": 1810.09869,
        "authors": "Roland Hodler, Paul Raschky, Anthony Strittmatter",
        "title": "Religion and Terrorism: Evidence from Ramadan Fasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study the effect of religion and intense religious experiences on\nterrorism by focusing on one of the five pillars of Islam: Ramadan fasting. For\nidentification, we exploit two facts: First, daily fasting from dawn to sunset\nduring Ramadan is considered mandatory for most Muslims. Second, the Islamic\ncalendar is not synchronized with the solar cycle. We find a robust negative\neffect of more intense Ramadan fasting on terrorist events within districts and\ncountry-years in predominantly Muslim countries. This effect seems to operate\npartly through decreases in public support for terrorism and the operational\ncapabilities of terrorist groups.\n"
    },
    {
        "paper_id": 1810.09876,
        "authors": "Ralph-Christopher Bayer, Roland Hodler, Paul Raschky, Anthony\n  Strittmatter",
        "title": "Expropriations, Property Confiscations and New Offshore Entities:\n  Evidence from the Panama Papers",
        "comments": null,
        "journal-ref": "Journal of Economic Behavior and Organization, 2020, vol. 171, pp.\n  132-152",
        "doi": "10.1016/j.jebo.2020.01.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the Panama Papers, we show that the beginning of media reporting on\nexpropriations and property confiscations in a country increases the\nprobability that offshore entities are incorporated by agents from the same\ncountry in the same month. This result is robust to the use of country-year\nfixed effects and the exclusion of tax havens. Further analysis shows that the\neffect is driven by countries with non-corrupt and effective governments, which\nsupports the notion that offshore entities are incorporated when reasonably\nwell-intended and well-functioning governments become more serious about\nfighting organized crime by confiscating proceeds of crime.\n"
    },
    {
        "paper_id": 1810.09882,
        "authors": "Claudio Fontana, Zorana Grbac, Sandrine G\\\"umbel, Thorsten Schmidt",
        "title": "Term structure modeling for multiple curves with stochastic\n  discontinuities",
        "comments": null,
        "journal-ref": "Finance and Stochastics, 24: 465-511, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general term structure framework taking stochastic\ndiscontinuities explicitly into account. Stochastic discontinuities are a key\nfeature in interest rate markets, as for example the jumps of the term\nstructures in correspondence to monetary policy meetings of the ECB show. We\nprovide a general analysis of multiple curve markets under minimal assumptions\nin an extended HJM framework and provide a fundamental theorem of asset pricing\nbased on NAFLVR. The approach with stochastic discontinuities permits to embed\nmarket models directly, unifying seemingly different modeling philosophies. We\nalso develop a tractable class of models, based on affine semimartingales,\ngoing beyond the requirement of stochastic continuity.\n"
    },
    {
        "paper_id": 1810.09936,
        "authors": "Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun, Tat-Seng\n  Chua",
        "title": "Enhancing Stock Movement Prediction with Adversarial Training",
        "comments": "IJCAI 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper contributes a new machine learning solution for stock movement\nprediction, which aims to predict whether the price of a stock will be up or\ndown in the near future. The key novelty is that we propose to employ\nadversarial training to improve the generalization of a neural network\nprediction model. The rationality of adversarial training here is that the\ninput features to stock prediction are typically based on stock price, which is\nessentially a stochastic variable and continuously changed with time by nature.\nAs such, normal training with static price-based features (e.g. the close\nprice) can easily overfit the data, being insufficient to obtain reliable\nmodels. To address this problem, we propose to add perturbations to simulate\nthe stochasticity of price variable, and train the model to work well under\nsmall yet intentional perturbations. Extensive experiments on two real-world\nstock data show that our method outperforms the state-of-the-art solution with\n3.11% relative improvements on average w.r.t. accuracy, validating the\nusefulness of adversarial training for stock prediction task.\n"
    },
    {
        "paper_id": 1810.09965,
        "authors": "Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho\n  Kanniainen, Moncef Gabbouj, Alexandros Iosifidis",
        "title": "Using Deep Learning for price prediction by exploiting stationary limit\n  order book features",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent surge in Deep Learning (DL) research of the past decade has\nsuccessfully provided solutions to many difficult problems. The field of\nquantitative analysis has been slowly adapting the new methods to its problems,\nbut due to problems such as the non-stationary nature of financial data,\nsignificant challenges must be overcome before DL is fully utilized. In this\nwork a new method to construct stationary features, that allows DL models to be\napplied effectively, is proposed. These features are thoroughly tested on the\ntask of predicting mid price movements of the Limit Order Book. Several DL\nmodels are evaluated, such as recurrent Long Short Term Memory (LSTM) networks\nand Convolutional Neural Networks (CNN). Finally a novel model that combines\nthe ability of CNNs to extract useful features and the ability of LSTMs' to\nanalyze time series, is proposed and evaluated. The combined model is able to\noutperform the individual LSTM and CNN models in the prediction horizons that\nare tested.\n"
    },
    {
        "paper_id": 1810.10374,
        "authors": "Made Tantrawan, Denny H. Leung",
        "title": "On closedness of law-invariant convex sets in rearrangement invariant\n  spaces",
        "comments": "10 pages",
        "journal-ref": "Archiv der Mathematik. Published online on 16 November 2019",
        "doi": "10.1007/s00013-019-01398-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents relations between several types of closedness of a\nlaw-invariant convex set in a rearrangement invariant space $\\mathcal{X}$. In\nparticular, we show that order closedness,\n$\\sigma(\\mathcal{X},\\mathcal{X}_n^\\sim)$-closedness and\n$\\sigma(\\mathcal{X},L^\\infty)$-closedness of a law-invariant convex set in\n$\\mathcal{X}$ are equivalent, where $\\mathcal{X}_n^\\sim$ is the order\ncontinuous dual of $\\mathcal{X}$. We also provide some application to proper\nquasiconvex law-invariant functionals with the Fatou property.\n"
    },
    {
        "paper_id": 1810.10465,
        "authors": "Shenhao Wang, Qingyi Wang, Nate Bailey, Jinhua Zhao",
        "title": "Deep Neural Networks for Choice Analysis: A Statistical Learning Theory\n  Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While researchers increasingly use deep neural networks (DNN) to analyze\nindividual choices, overfitting and interpretability issues remain as obstacles\nin theory and practice. By using statistical learning theory, this study\npresents a framework to examine the tradeoff between estimation and\napproximation errors, and between prediction and interpretation losses. It\noperationalizes the DNN interpretability in the choice analysis by formulating\nthe metrics of interpretation loss as the difference between true and estimated\nchoice probability functions. This study also uses the statistical learning\ntheory to upper bound the estimation error of both prediction and\ninterpretation losses in DNN, shedding light on why DNN does not have the\noverfitting issue. Three scenarios are then simulated to compare DNN to binary\nlogit model (BNL). We found that DNN outperforms BNL in terms of both\nprediction and interpretation for most of the scenarios, and larger sample size\nunleashes the predictive power of DNN but not BNL. DNN is also used to analyze\nthe choice of trip purposes and travel modes based on the National Household\nTravel Survey 2017 (NHTS2017) dataset. These experiments indicate that DNN can\nbe used for choice analysis beyond the current practice of demand forecasting\nbecause it has the inherent utility interpretation, the flexibility of\naccommodating various information formats, and the power of automatically\nlearning utility specification. DNN is both more predictive and interpretable\nthan BNL unless the modelers have complete knowledge about the choice task, and\nthe sample size is small. Overall, statistical learning theory can be a\nfoundation for future studies in the non-asymptotic data regime or using\nhigh-dimensional statistical models in choice analysis, and the experiments\nshow the feasibility and effectiveness of DNN for its wide applications to\npolicy and behavioral analysis.\n"
    },
    {
        "paper_id": 1810.10563,
        "authors": "Jize Zhang, Tim Leung, and Aleksandr Aravkin",
        "title": "A Relaxed Optimization Approach for Cardinality-Constrained Portfolio\n  Optimization",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A cardinality-constrained portfolio caps the number of stocks to be traded\nacross and within groups or sectors. These limitations arise from real-world\nscenarios faced by fund managers, who are constrained by transaction costs and\nclient preferences as they seek to maximize return and limit risk.\n  We develop a new approach to solve cardinality-constrained portfolio\noptimization problems, extending both Markowitz and conditional value at risk\n(CVaR) optimization models with cardinality constraints. We derive a continuous\nrelaxation method for the NP-hard objective, which allows for very efficient\nalgorithms with standard convergence guarantees for nonconvex problems. For\nsmaller cases, where brute force search is feasible to compute the globally\noptimal cardinality- constrained portfolio, the new approach finds the best\nportfolio for the cardinality-constrained Markowitz model and a very good local\nminimum for the cardinality-constrained CVaR model. For higher dimensions,\nwhere brute-force search is prohibitively expensive, we find feasible\nportfolios that are nearly as efficient as their non-cardinality constrained\ncounterparts.\n"
    },
    {
        "paper_id": 1810.1066,
        "authors": "Debasis Mitra, Abhinav Sridhar",
        "title": "The Case for Formation of ISP-Content Providers Consortiums by Nash\n  Bargaining for Internet Content Delivery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The formation of consortiums of a broadband access Internet Service Provider\n(ISP) and multiple Content Providers (CP) is considered for large-scale content\ncaching. The consortium members share costs from operations and investments in\nthe supporting infrastructure. Correspondingly, the model's cost function\nincludes marginal and fixed costs; the latter has been important in determining\nindustry structure. Also, if Net Neutrality regulations permit, additional\nnetwork capacity on the ISP's last mile may be contracted by the CPs. The\nnumber of subscribers is determined by a combination of users' price elasticity\nof demand and Quality of Experience. The profit generated by a coalition after\npricing and design optimization determines the game's characteristic function.\nCoalition formation is by a bargaining procedure due to Okada (1996) based on\nrandom proposers in a non-cooperative, multi-player game-theoretic framework. A\nnecessary and sufficient condition is obtained for the Grand Coalition to form,\nwhich bounds subsidies from large to small contributors. Caching is generally\nsupported even under Net Neutrality regulations. The Grand Coalition's profit\nmatches upper bounds. Numerical results illustrate the analytic results.\n"
    },
    {
        "paper_id": 1810.10726,
        "authors": "Vic Norton",
        "title": "How Not To Do Mean-Variance Analysis",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the 2014 market history of two high-returning biotechnology\nexchange-traded funds to illustrate how ex post mean-variance analysis should\nnot be done. Unfortunately, the way it should not be done is the way it\ngenerally is done -- to our knowledge.\n"
    },
    {
        "paper_id": 1810.108,
        "authors": "Stelios Arvanitis, Olivier Scaillet, Nikolas Topaloglou",
        "title": "Spanning Tests for Markowitz Stochastic Dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive properties of the cdf of random variables defined as saddle-type\npoints of real valued continuous stochastic processes. This facilitates the\nderivation of the first-order asymptotic properties of tests for stochastic\nspanning given some stochastic dominance relation. We define the concept of\nMarkowitz stochastic dominance spanning, and develop an analytical\nrepresentation of the spanning property. We construct a non-parametric test for\nspanning based on subsampling, and derive its asymptotic exactness and\nconsistency. The spanning methodology determines whether introducing new\nsecurities or relaxing investment constraints improves the investment\nopportunity set of investors driven by Markowitz stochastic dominance. In an\napplication to standard data sets of historical stock market returns, we reject\nmarket portfolio Markowitz efficiency as well as two-fund separation. Hence, we\nfind evidence that equity management through base assets can outperform the\nmarket, for investors with Markowitz type preferences.\n"
    },
    {
        "paper_id": 1810.10845,
        "authors": "Ymir M\\\"akinen, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis",
        "title": "Forecasting of Jump Arrivals in Stock Prices: New Attention-based\n  Network Architecture using Limit Order Book Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existing literature provides evidence that limit order book data can be\nused to predict short-term price movements in stock markets. This paper\nproposes a new neural network architecture for predicting return jump arrivals\nin equity markets with high-frequency limit order book data. This new\narchitecture, based on Convolutional Long Short-Term Memory with Attention, is\nintroduced to apply time series representation learning with memory and to\nfocus the prediction attention on the most important features to improve\nperformance. The data set consists of order book data on five liquid U.S.\nstocks. The use of the attention mechanism makes it possible to analyze the\nimportance of the inclusion limit order book data and other input variables. By\nusing this mechanism, we provide evidence that the use of limit order book data\nwas found to improve the performance of the proposed model in jump prediction,\neither clearly or marginally, depending on the underlying stock. This suggests\nthat path-dependence in limit order book markets is a stock specific feature.\nMoreover, we find that the proposed approach with an attention mechanism\noutperforms the multi-layer perceptron network as well as the convolutional\nneural network and Long Short-Term memory model.\n"
    },
    {
        "paper_id": 1810.1097,
        "authors": "Roland R. Ramsahai",
        "title": "Defining and estimating stochastic rate change in a dynamic general\n  insurance portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rate change calculations in the literature involve deterministic methods that\nmeasure the change in premium for a given policy. The definition of rate change\nas a statistical parameter is proposed to address the stochastic nature of the\npremium charged for a policy. It promotes the idea that rate change is a\nproperty of an asymptotic population to be estimated, not just a property to\nmeasure or monitor in the sample of observed policies that are written. Various\nmodels and techniques are given for estimating this stochastic rate change and\nquantifying the uncertainty in the estimates. The use of matched sampling is\nemphasized for rate change estimation, as it adjusts for changes in policy\ncharacteristics by directly searching for similar policies across policy years.\nThis avoids any of the assumptions and recipes that are required to re-rate\npolicies in years where they were not written, as is common with deterministic\nmethods. Such procedures can be subjective or implausible if the structure of\nrating algorithms change or there are complex and heterogeneous exposure bases\nand coverages. The methods discussed are applied to a motor premium database.\nThe application includes the use of a genetic algorithm with parallel\ncomputations to automatically optimize the matched sampling.\n"
    },
    {
        "paper_id": 1810.11039,
        "authors": "Jorge Ignacio Gonz\\'alez C\\'azares, Aleksandar Mijatovi\\'c, Ger\\'onimo\n  Uribe Bravo",
        "title": "Geometrically Convergent Simulation of the Extrema of L\\'{e}vy Processes",
        "comments": "Minor revision: reintroduction of the result on the scaling limits.\n  37 pages and 5 figures. Short presentation on: https://youtu.be/P3vHmJUCFbU",
        "journal-ref": "Mathematics of Operations Research 47(2) (2022) 1141-1168",
        "doi": "10.1287/moor.2021.1163",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel approximate simulation algorithm for the joint law of the\nposition, the running supremum and the time of the supremum of a general L\\'evy\nprocess at an arbitrary finite time. We identify the law of the error in simple\nterms. We prove that the error decays geometrically in $L^p$ (for any $p\\geq\n1$) as a function of the computational cost, in contrast with the polynomial\ndecay for the approximations available in the literature. We establish a\ncentral limit theorem and construct non-asymptotic and asymptotic confidence\nintervals for the corresponding Monte Carlo estimator. We prove that the\nmultilevel Monte Carlo estimator has optimal computational complexity (i.e. of\norder $\\epsilon^{-2}$ if the mean squared error is at most $\\epsilon^2$) for\nlocally Lipschitz and barrier-type functionals of the triplet and develop an\nunbiased version of the estimator. We illustrate the performance of the\nalgorithm with numerical examples.\n"
    },
    {
        "paper_id": 1810.11091,
        "authors": "Brian F. Tivnan, David Slater, James R. Thompson, Tobin A.\n  Bergen-Hill, Carl D. Burke, Shaun M. Brady, Matthew T. K. Koehler, Matthew T.\n  McMahon, Brendan F. Tivnan and Jason Veneman",
        "title": "Price Discovery and the Accuracy of Consolidated Data Feeds in the U.S.\n  Equity Markets",
        "comments": "18 pages, 20 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Both the scientific community and the popular press have paid much attention\nto the speed of the Securities Information Processor, the data feed\nconsolidating all trades and quotes across the US stock market. Rather than the\nspeed of the Securities Information Processor, or SIP, we focus here on its\naccuracy. Relying on Trade and Quote data, we provide various measures of SIP\nlatency relative to high-speed data feeds between exchanges, known as direct\nfeeds. We use first differences to highlight not only the divergence between\nthe direct feeds and the SIP, but also the fundamental inaccuracy of the SIP.\nWe find that as many as 60 percent or more of trades are reported out of\nsequence for stocks with high trade volume, therefore skewing simple measures\nsuch as returns. While not yet definitive, this analysis supports our\npreliminary conclusion that the underlying infrastructure of the SIP is\ncurrently unable to keep pace with the trading activity in today's stock\nmarket.\n"
    },
    {
        "paper_id": 1810.11299,
        "authors": "Bogdan Grechuk and Andrzej Palczewski and Jan Palczewski",
        "title": "On the solution uniqueness in portfolio optimization and risk analysis",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the issue of solution uniqueness for portfolio optimization\nproblem and its inverse for asset returns with a finite number of possible\nscenarios. The risk is assessed by deviation measures introduced by\n[Rockafellar et al., Mathematical Programming, Ser. B, 108 (2006), pp. 515-540]\ninstead of variance as in the Markowitz optimization problem. We prove that in\ngeneral one can expect uniqueness neither in forward nor in inverse problems.\nWe discuss consequences of that non-uniqueness for several problems in risk\nanalysis and portfolio optimization, including capital allocation, risk\nsharing, cooperative investment, and the Black-Litterman methodology. In all\ncases, the issue with non-uniqueness is closely related to the fact that\nsubgradient of a convex function is non-unique at the points of\nnon-differentiability. We suggest methodology to resolve this issue by\nidentifying a unique \"special\" subgradient satisfying some natural axioms. This\n\"special\" subgradient happens to be the Stainer point of the subdifferential\nset.\n"
    },
    {
        "paper_id": 1810.11449,
        "authors": "Li Hu, Anqi Li",
        "title": "The Politics of Attention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an equilibrium theory of attention and politics. In a spatial\nmodel of electoral competition where candidates have varying policy\npreferences, we examine what kinds of political behaviors capture voters'\nlimited attention and how this concern affects the overall political outcomes.\nFollowing the seminal works of Downs (1957) and Sims (1998), we assume that\nvoters are rationally inattentive and can process information about the\npolicies at a cost proportional to entropy reduction. The main finding is an\nequilibrium phenomenon called attention- and media-driven extremism, namely as\nwe increase the attention cost or garble the news technology, a truncated set\nof the equilibria captures voters' attention through enlarging the policy\ndifferentials between the varying types of the candidates. We supplement our\nanalysis with historical accounts, and discuss its relevance in the new era\nfeatured with greater media choices and distractions, as well as the rise of\npartisan media and fake news.\n"
    },
    {
        "paper_id": 1810.11454,
        "authors": "Julien Vaes and Raphael Hauser",
        "title": "Optimal Trade Execution with Uncertain Volume Target",
        "comments": null,
        "journal-ref": null,
        "doi": "10.21314/JCF.2022.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the seminal paper on optimal execution of portfolio transactions, Almgren\nand Chriss (2001) define the optimal trading strategy to liquidate a fixed\nvolume of a single security under price uncertainty. Yet there exist\nsituations, such as in the power market, in which the volume to be traded can\nonly be estimated and becomes more accurate when approaching a specified\ndelivery time. During the course of execution, a trader should then constantly\nadapt their trading strategy to meet their fluctuating volume target. In this\npaper, we develop a model that accounts for volume uncertainty and we show that\na risk-averse trader has benefit in delaying their trades. More precisely, we\nargue that the optimal strategy is a trade-off between early and late trades in\norder to balance risk associated with both price and volume. By incorporating a\nrisk term related to the volume to trade, the static optimal strategies\nsuggested by our model avoid the explosion in the algorithmic complexity\nusually associated with dynamic programming solutions, all the while yielding\ncompetitive performance.\n"
    },
    {
        "paper_id": 1810.11458,
        "authors": "Alvaro Gonzalez-Castellanos, David Pozo, Sergio Martinez, Luis Lopez,\n  Ingrid Oliveros",
        "title": "Economic Impact of Wind Generation Penetration in the Colombian\n  Electricity Market",
        "comments": "This paper is a preprint of a paper submitted to \"IET Generation,\n  Transmission & Distribution\". If accepted, the copy of record will be\n  available at the IET Digital Library",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The creation of the Renewable Energy Law (Law 1715 of 2014) promotes the\nintroduction of large-scale renewable energy generation in the Colombian\nelectricity market. The new legislation aims to diversify the country's\ngeneration matrix, mainly composed of hydro and fuel-based generation, with a\nshare of 66% and 34% respectively. Currently, three wind generation projects,\nwith an aggregated capacity of 500 MW, have been commissioned in the North of\nthe country. This study analyses the economic impact of the large-scale\nintroduction of wind generation on both, the market spot price and conventional\ngeneration plants operation. For this purpose, the study builds a unit\ncommitment model to mimic the current market legislation and the system's\ngeneration data. We show that the introduction of wind energy into the\nColombian electricity market would impact the generation share of large hydro\nand gas-fired power plants. The hydro generation has an important role in\nbalancing the generation for fluctuations on the wind resource. Meanwhile, the\ngas-fired plants would decrease their participation in the market,\nproportionally to the introduction of wind generation in the system, by as low\nas 20% of its current operation.\n"
    },
    {
        "paper_id": 1810.11471,
        "authors": "Anqi Li, Ming Yang",
        "title": "Optimal Incentive Contract with Endogenous Monitoring Technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent technology advances have enabled firms to flexibly process and analyze\nsophisticated employee performance data at a reduced and yet significant cost.\nWe develop a theory of optimal incentive contracting where the monitoring\ntechnology that governs the above procedure is part of the designer's strategic\nplanning. In otherwise standard principal-agent models with moral hazard, we\nallow the principal to partition agents' performance data into any finite\ncategories and to pay for the amount of information the output signal carries.\nThrough analysis of the trade-off between giving incentives to agents and\nsaving the monitoring cost, we obtain characterizations of optimal monitoring\ntechnologies such as information aggregation, strict MLRP, likelihood\nratio-convex performance classification, group evaluation in response to rising\nmonitoring costs, and assessing multiple task performances according to agents'\nendogenous tendencies to shirk. We examine the implications of these results\nfor workforce management and firms' internal organizations.\n"
    },
    {
        "paper_id": 1810.11475,
        "authors": "Anqi Li, Yiqing Xing",
        "title": "Intermediated Implementation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine problems of ``intermediated implementation,'' in which a single\nprincipal can only regulate limited aspects of the consumption bundles traded\nbetween intermediaries and agents with hidden characteristics. An example is\nsales, in which retailers offer menus of consumption bundles to customers with\nhidden tastes, whereas a manufacturer with a potentially different goal from\nretailers' is limited to regulating sold consumption goods but not retail\nprices by legal barriers. We study how the principal can implement through\nintermediaries any social choice rule that is incentive compatible and\nindividually rational for agents. We demonstrate the effectiveness of per-unit\nfee schedules and distribution regulations, which hinges on whether\nintermediaries have private or interdependent values. We give further\napplications to healthcare regulation and income redistribution.\n"
    },
    {
        "paper_id": 1810.11619,
        "authors": "Sona Kilianova, Daniel Sevcovic",
        "title": "Expected Utility Maximization and Conditional Value-at-Risk\n  Deviation-based Sharpe Ratio in Dynamic Stochastic Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the expected terminal utility maximization\napproach for a dynamic stochastic portfolio optimization problem. We solve it\nnumerically by solving an evolutionary Hamilton-Jacobi-Bellman equation which\nis transformed by means of the Riccati transformation. We examine the\ndependence of the results on the shape of a chosen utility function in regard\nto the associated risk aversion level. We define the\n  Conditional value-at-risk deviation ($CVaRD$) based Sharpe ratio for\nmeasuring risk-adjusted performance of a dynamic portfolio. We compute optimal\nstrategies for a portfolio investment problem motivated by the German DAX 30\nIndex and we evaluate and analyze the dependence of the $CVaRD$-based Sharpe\nratio on the utility function and the associated risk aversion level.\n"
    },
    {
        "paper_id": 1810.11849,
        "authors": "Nils Bertschinger, Julian Stobbe",
        "title": "Systemic Greeks: Measuring risk in financial networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the latest financial crisis, the idea of systemic risk has received\nconsiderable interest. In particular, contagion effects arising from\ncross-holdings between interconnected financial firms have been studied\nextensively. Drawing inspiration from the field of complex networks, these\nattempts are largely unaware of models and theories for credit risk of\nindividual firms. Here, we note that recent network valuation models extend the\nseminal structural risk model of Merton (1974). Furthermore, we formally\ncompute sensitivities to various risk factors -- commonly known as Greeks -- in\na network context. In particular, we propose the network $\\Delta$ as a\nquantitative measure of systemic risk and illustrate our findings on some\nnumerical examples.\n"
    },
    {
        "paper_id": 1810.12022,
        "authors": "Jozef Barunik and Mattia Bevilacqua and Radu Tunaru",
        "title": "Asymmetric Network Connectedness of Fears",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces forward-looking measures of the network connectedness\nof fears in the financial system, arising due to the good and bad beliefs of\nmarket participants about uncertainty that spreads unequally across a network\nof banks. We argue that this asymmetric network structure extracted from call\nand put traded option prices of the main U.S. banks contains valuable\ninformation for predicting macroeconomic conditions and economic uncertainty,\nand it can serve as a tool for forward-looking systemic risk monitoring.\n"
    },
    {
        "paper_id": 1810.12099,
        "authors": "Michelle B Graczyk, Silvio M D Queir\\'os",
        "title": "Intraday Seasonalities and Nonstationarity of Trading Volume in\n  Financial Markets: Individual and Cross-Sectional Features",
        "comments": "25 pages, 16 figures, 1 data",
        "journal-ref": "PLoS ONE 11(11): e0165057",
        "doi": "10.1371/journal.pone.0165057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the intraday behaviour of the statistical moments of the trading\nvolume of the blue chip equities that composed the Dow Jones Industrial Average\nindex between 2003 and 2014. By splitting that time interval into semesters, we\nprovide a quantitative account of the non-stationary nature of the intraday\nstatistical properties as well. Explicitly, we prove the well-known U-shape\nexhibited by the average trading volume-as well as the volatility of the price\nfluctuations-experienced a significant change from 2008 (the year of the\nsub-prime financial crisis) onwards. That has resulted in a faster relaxation\nafter the market opening and relates to a consistent decrease in the convexity\nof the average trading volume intraday profile. Simultaneously, the last part\nof the session has become steeper as well, a modification that is likely to\nhave been triggered by the new short-selling rules that were introduced in 2007\nby the Securities and Exchange Commission. The combination of both results\nreveals that the has been turning into a t. Additionally, the analysis of\nhigher-order cumulants namely the skewness and the kurtosis-shows that the\nmorning and the afternoon parts of the trading session are each clearly\nassociated with different statistical features and hence dynamical rules.\nConcretely, we claim that the large initial trading volume is due to wayward\nstocks whereas the large volume during the last part of the session hinges on a\ncohesive increase of the trading volume. That dissimilarity between the two\nparts of the trading session is stressed in periods of higher uproar in the\nmarket.\n"
    },
    {
        "paper_id": 1810.122,
        "authors": "Juho Kanniainen and Martin Magris",
        "title": "Option market (in)efficiency and implied volatility dynamics after\n  return jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In informationally efficient financial markets, option prices and this\nimplied volatility should immediately be adjusted to new information that\narrives along with a jump in underlying's return, whereas gradual changes in\nimplied volatility would indicate market inefficiency. Using minute-by-minute\ndata on S&P 500 index options, we provide evidence regarding delayed and\ngradual movements in implied volatility after the arrival of return jumps.\nThese movements are directed and persistent, especially in the case of negative\nreturn jumps. Our results are significant when the implied volatilities are\nextracted from at-the-money options and out-of-the-money puts, while the\nimplied volatility obtained from out-of-the-money calls converges to its new\nlevel immediately rather than gradually. Thus, our analysis reveals that the\nimplied volatility smile is adjusted to jumps in underlying's return\nasymmetrically. Finally, it would be possible to have statistical arbitrage in\nzero-transaction-cost option markets, but under actual option price spreads,\nour results do not imply abnormal option returns.\n"
    },
    {
        "paper_id": 1810.12762,
        "authors": "Tahir Choulli and Sina Yansori",
        "title": "Log-optimal portfolio and num\\'eraire portfolio for market models\n  stopped at a random time",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1803.10128",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on num\\'eraire portfolio and log-optimal portfolio\n(portfolio with finite expected utility that maximizes the expected logarithm\nutility from terminal wealth), when a market model $(S,\\mathbb F)$ -specified\nby its assets' price $S$ and its flow of information $\\mathbb F$- is stopped at\na random time $\\tau$. This setting covers the areas of credit risk and life\ninsurance, where $\\tau$ represents the default time and the death time\nrespectively. Thus, the progressive enlargement of $\\mathbb F$ with $\\tau$,\ndenoted by $\\mathbb G$, sounds tailor-fit for modelling the new flow of\ninformation that incorporates both $\\mathbb F$ and $\\tau$. For the resulting\nstopped model $(S^{\\tau},\\mathbb G)$, we study the two portfolios in different\nmanners, and describe their computations in terms of the $\\mathbb F$-observable\nparameters of the pair $(S, \\tau)$.\n"
    },
    {
        "paper_id": 1810.1284,
        "authors": "Ricardo T. Fernholz and Caleb Stroup",
        "title": "Asset Price Distributions and Efficient Markets",
        "comments": "45 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore a decomposition in which returns on a large class of portfolios\nrelative to the market depend on a smooth non-negative drift and changes in the\nasset price distribution. This decomposition is obtained using general\ncontinuous semimartingale price representations, and is thus consistent with\nvirtually any asset pricing model. Fluctuations in portfolio relative returns\ndepend on stochastic time-varying dispersion in asset prices. Thus, our\nframework uncovers an asset pricing factor whose existence emerges from an\naccounting identity universal across different economic and financial\nenvironments, a fact that has deep implications for market efficiency. In\nparticular, in a closed, dividend-free market in which asset price dispersion\nis relatively constant, a large class of portfolios must necessarily outperform\nthe market portfolio over time. We show that price dispersion in commodity\nfutures markets has increased only slightly, and confirm the existence of\nsubstantial excess returns that co-vary with changes in price dispersion as\npredicted by our theory.\n"
    },
    {
        "paper_id": 1810.12996,
        "authors": "Ore Koren and Laura Mann",
        "title": "Nighttime Light, Superlinear Growth, and Economic Inequalities at the\n  Country Level",
        "comments": "This paper was written as part of a project submitted to the Santa Fe\n  Institute (SFI) Summer 2018 It is 7 pages long and contains 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research has highlighted relationships between size and scaled growth across\na large variety of biological and social organisms, ranging from bacteria,\nthrough animals and plants, to cities an companies. Yet, heretofore,\nidentifying a similar relationship at the country level has proven challenging.\nOne reason is that, unlike the former, countries have predefined borders, which\nlimit their ability to grow \"organically.\" This paper addresses this issue by\nidentifying and validating an effective measure of organic growth at the\ncountry level: nighttime light emissions, which serve as a proxy of energy\nallocations where more productive activity takes place. This indicator is\ncompared to population size to illustrate that while nighttime light emissions\nare associated with superlinear growth, population size at the country level is\nassociated with sublinear growth. These relationships and their implications\nfor economic inequalities are then explored using high-resolution geospatial\ndatasets spanning the last three decades.\n"
    },
    {
        "paper_id": 1810.13248,
        "authors": "Bertram D\\\"uring, Alexander Pitkin",
        "title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility with contemporaneous jump models",
        "comments": "6 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1704.05308, arXiv:1710.05542",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the scheme developed in B. D\\\"uring, A. Pitkin, \"High-order compact\nfinite difference scheme for option pricing in stochastic volatility jump\nmodels\", 2019, to the so-called stochastic volatility with contemporaneous\njumps (SVCJ) model, derived by Duffie, Pan and Singleton. The performance of\nthe scheme is assessed through a number of numerical experiments, using\ncomparisons against a standard second-order central difference scheme. We\nobserve that the new high-order compact scheme achieves fourth order\nconvergence and discuss the effects on efficiency and computation time.\n"
    },
    {
        "paper_id": 1810.1325,
        "authors": "Roy Cerqueti, Gian Paolo Clemente, Rosanna Grassi",
        "title": "Systemic risk assessment through high order clustering coefficient",
        "comments": "Submitted",
        "journal-ref": "ANNALS OF OPERATIONS RESEARCH 2020",
        "doi": "10.1007/s10479-020-03525-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we propose a novel measure of systemic risk in the context of\nfinancial networks. To this aim, we provide a definition of systemic risk which\nis based on the structure, developed at different levels, of clustered\nneighbours around the nodes of the network. The proposed measure incorporates\nthe generalized concept of clustering coefficient of order $l$ of a node $i$\nintroduced in Cerqueti et al. (2018). Its properties are also explored in terms\nof systemic risk assessment. Empirical experiments on the time-varying global\nbanking network show the effectiveness of the presented systemic risk measure\nand provide insights on how systemic risk has changed over the last years, also\nin the light of the recent financial crisis and the subsequent more stringent\nregulation for globally systemically important banks.\n"
    },
    {
        "paper_id": 1811.00122,
        "authors": "Xiaowei Zhang and Peter W. Glynn",
        "title": "Affine Jump-Diffusions: Stochastic Stability and Limit Theorems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Affine jump-diffusions constitute a large class of continuous-time stochastic\nmodels that are particularly popular in finance and economics due to their\nanalytical tractability. Methods for parameter estimation for such processes\nrequire ergodicity in order establish consistency and asymptotic normality of\nthe associated estimators. In this paper, we develop stochastic stability\nconditions for affine jump-diffusions, thereby providing the needed\nlarge-sample theoretical support for estimating such processes. We establish\nergodicity for such models by imposing a `strong mean reversion' condition and\na mild condition on the distribution of the jumps, i.e. the finiteness of a\nlogarithmic moment. Exponential ergodicity holds if the jumps have a finite\nmoment of a positive order. In addition, we prove strong laws of large numbers\nand functional central limit theorems for additive functionals for this class\nof models.\n"
    },
    {
        "paper_id": 1811.00137,
        "authors": "K. Buchardt, C. Furrer, M. Steffensen",
        "title": "Forward transition rates",
        "comments": "Revision of manuscript. The manuscript now contains a section on\n  'Forward-thinking and actuarial practice'. Furthermore, we have corrected\n  typos and re-written certain sentences to improve readability and accuracy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The idea of forward rates stems from interest rate theory. It has natural\nconnotations to transition rates in multi-state models. The generalization from\nthe forward mortality rate in a survival model to multi-state models is\nnon-trivial and several definitions have been proposed. We establish a\ntheoretical framework for the discussion of forward rates. Furthermore, we\nprovide a novel definition with its own logic and merits and compare it with\nthe proposals in the literature. The definition turns the Kolmogorov forward\nequations inside out by interchanging the transition probabilities with the\ntransition intensities as the object to be calculated.\n"
    },
    {
        "paper_id": 1811.00267,
        "authors": "Peter K. Friz, Paul Gassiat, Paolo Pigato",
        "title": "Precise asymptotics: robust stochastic volatility models",
        "comments": null,
        "journal-ref": "[v2] published in Ann. Appl. Proba. (2021)",
        "doi": "10.1214/20-AAP1608",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new methodology to analyze large classes of (classical and\nrough) stochastic volatility models, with special regard to short-time and\nsmall noise formulae for option prices. Our main tool is the theory of\nregularity structures, which we use in the form of [Bayer et al; A regularity\nstructure for rough volatility, 2017]. In essence, we implement a Laplace\nmethod on the space of models (in the sense of Hairer), which generalizes\nclassical works of Azencott and Ben Arous on path space and then Aida,\nInahama--Kawabi on rough path space. When applied to rough volatility models,\ne.g. in the setting of [Forde-Zhang, Asymptotics for rough stochastic\nvolatility models, 2017], one obtains precise asymptotic for European options\nwhich refine known large deviation asymptotics.\n"
    },
    {
        "paper_id": 1811.00304,
        "authors": "Stephan Eckstein, Michael Kupper, Mathias Pohl",
        "title": "Robust risk aggregation with neural networks",
        "comments": "Revised version. Accepted for publication in \"Mathematical Finance\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider settings in which the distribution of a multivariate random\nvariable is partly ambiguous. We assume the ambiguity lies on the level of the\ndependence structure, and that the marginal distributions are known.\nFurthermore, a current best guess for the distribution, called reference\nmeasure, is available. We work with the set of distributions that are both\nclose to the given reference measure in a transportation distance (e.g. the\nWasserstein distance), and additionally have the correct marginal structure.\nThe goal is to find upper and lower bounds for integrals of interest with\nrespect to distributions in this set. The described problem appears naturally\nin the context of risk aggregation. When aggregating different risks, the\nmarginal distributions of these risks are known and the task is to quantify\ntheir joint effect on a given system. This is typically done by applying a\nmeaningful risk measure to the sum of the individual risks. For this purpose,\nthe stochastic interdependencies between the risks need to be specified. In\npractice the models of this dependence structure are however subject to\nrelatively high model ambiguity. The contribution of this paper is twofold:\nFirstly, we derive a dual representation of the considered problem and prove\nthat strong duality holds. Secondly, we propose a generally applicable and\ncomputationally feasible method, which relies on neural networks, in order to\nnumerically solve the derived dual problem. The latter method is tested on a\nnumber of toy examples, before it is finally applied to perform robust risk\naggregation in a real world instance.\n"
    },
    {
        "paper_id": 1811.00476,
        "authors": "J. Martin van Zyl",
        "title": "An Empirical Study of the Behaviour of the Sample Kurtosis in Samples\n  from Symmetric Stable Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kurtosis is seen as a measure of the discrepancy between the observed data\nand a Gaussian distribution and is defined when the 4th moment is finite. In\nthis work an empirical study is conducted to investigate the behaviour of the\nsample estimate of kurtosis with respect to sample size and the tail index when\napplied to heavy-tailed data where the 4th moment does not exist. The study\nwill focus on samples from the symmetric stable distributions. It was found\nthat the expected value of excess kurtosis divided by the sample size is finite\nfor any value of the tail index and the sample estimate of kurtosis increases\nas a linear function of sample size and tail index. It is very sensitive to\nchanges in the tail-index.\n"
    },
    {
        "paper_id": 1811.00875,
        "authors": "Sandro Sozzo",
        "title": "Quantum Structures in Human Decision-making: Towards Quantum Expected\n  Utility",
        "comments": "13 pages, 1 figure, standard LateX",
        "journal-ref": null,
        "doi": "10.1007/s10773-019-04022-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  {\\it Ellsberg thought experiments} and empirical confirmation of Ellsberg\npreferences pose serious challenges to {\\it subjective expected utility theory}\n(SEUT). We have recently elaborated a quantum-theoretic framework for human\ndecisions under uncertainty which satisfactorily copes with the Ellsberg\nparadox and other puzzles of SEUT. We apply here the quantum-theoretic\nframework to the {\\it Ellsberg two-urn example}, showing that the paradox can\nbe explained by assuming a state change of the conceptual entity that is the\nobject of the decision ({\\it decision-making}, or {\\it DM}, {\\it entity}) and\nrepresenting subjective probabilities by quantum probabilities. We also model\nthe empirical data we collected in a DM test on human participants within the\ntheoretic framework above. The obtained results are relevant, as they provide a\nline to model real life, e.g., financial and medical, decisions that show the\nsame empirical patterns as the two-urn experiment.\n"
    },
    {
        "paper_id": 1811.00952,
        "authors": "Marcus C. Christiansen",
        "title": "A martingale concept for non-monotone information in a jump process\n  framework",
        "comments": null,
        "journal-ref": "Finance and Stochastics 25, 563-596 (2021)",
        "doi": "10.1007/s00780-021-00456-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The information dynamics in finance and insurance applications is usually\nmodeled by a filtration. This paper looks at situations where information\nrestrictions apply such that the information dynamics may become non-monotone.\nA fundamental tool for calculating and managing risks in finance and insurance\nare martingale representations. We present a general theory that extends\nclassical martingale representations to non-monotone information generated by\nmarked point processes. The central idea is to focus only on those properties\nthat martingales and compensators show on infinitesimally short intervals.\nWhile classical martingale representations describe innovations only, our\nrepresentations have an additional symmetric counterpart that quantifies the\neffect of information loss. We exemplify the results with examples from life\ninsurance and credit risk.\n"
    },
    {
        "paper_id": 1811.01081,
        "authors": "Scott C. Merrill (1), Christopher J. Koliba (2), Susan M. Moegenburg\n  (1), Asim Zia (2), Jason Parker (3), Timothy Sellnow (4), Serge Wiltshire\n  (5), Gabriela Bucini (1), Caitlin Danehy (6), and Julia M. Smith (6) ((1)\n  Department of Plant and Soil Science, University of Vermont, (2) Department\n  of Community Development and Applied Economics, University of Vermont, (3)\n  School of Environment and Natural Resources, The Ohio State University at\n  Mansfield, (4) Nicholson School of Communication, University of Central\n  Florida, (5) Department of Food Systems. University of Vermont, (6)\n  Department of Animal and Veterinary Sciences, University of Vermont)",
        "title": "Decision-making in Livestock Biosecurity Practices amidst Environmental\n  and Social Uncertainty: Evidence from an Experimental Game",
        "comments": "26 pages, 6 figures",
        "journal-ref": "PLoS ONE. April 17, 2019. 14(4): e0214500",
        "doi": "10.1371/journal.pone.0214500",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Livestock industries are vulnerable to disease threats, which can cost\nbillions of dollars and have substantial negative social ramifications. Losses\nare mitigated through increased use of disease-related biosecurity practices,\nmaking increased biosecurity an industry goal. Currently, there is no\nindustry-wide standard for sharing information about disease incidence or\non-site biosecurity strategies, resulting in uncertainty regarding disease\nprevalence and biosecurity strategies employed by industry stakeholders. Using\nan experimental simulation game, we examined human participant's willingness to\ninvest in biosecurity when confronted with disease outbreak scenarios. We\nvaried the scenarios by changing the information provided about 1) disease\nincidence and 2) biosecurity strategy or response by production facilities to\nthe threat of disease. Here we show that willingness to invest in biosecurity\nincreases with increased information about disease incidence, but decreases\nwith increased information about biosecurity practices used by nearby\nfacilities. Thus, the type or context of the uncertainty confronting the\ndecision maker may be a major factor influencing behavior. Our findings suggest\nthat policies and practices that encourage greater sharing of disease incidence\ninformation should have the greatest benefit for protecting herd health.\n"
    },
    {
        "paper_id": 1811.0142,
        "authors": "Erhan Bayraktar, Yan Dolinsky, Jia Guo",
        "title": "Continuity of Utility Maximization under Weak Convergence",
        "comments": "Keywords: Incomplete Markets, Utility Maximization, Weak Convergence.\n  To appear in Mathematics and Financial Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we find tight sufficient conditions for the continuity of the\nvalue of the utility maximization problem from terminal wealth with respect to\nthe convergence in distribution of the underlying processes. We also establish\na weak convergence result for the terminal wealths of the optimal portfolios.\nFinally, we apply our results to the computation of the minimal expected\nshortfall (shortfall risk) in the Heston model by building an appropriate\nlattice approximation.\n"
    },
    {
        "paper_id": 1811.01624,
        "authors": "Andrea Flori, Fabrizio Lillo, Fabio Pammolli, Alessandro Spelta",
        "title": "Better to stay apart: asset commonality, bipartite network centrality,\n  and investment strategies",
        "comments": "38 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By exploiting a bipartite network representation of the relationships between\nmutual funds and portfolio holdings, we propose an indicator that we derive\nfrom the analysis of the network, labelled the Average Commonality Coefficient\n(ACC), which measures how frequently the assets in the fund portfolio are\npresent in the portfolios of the other funds of the market. This indicator\nreflects the investment behavior of funds' managers as a function of the\npopularity of the assets they held. We show that $ACC$ provides useful\ninformation to discriminate between funds investing in niche markets and those\ninvesting in more popular assets. More importantly, we find that $ACC$ is able\nto provide indication on the performance of the funds. In particular, we find\nthat funds investing in less popular assets generally outperform those\ninvesting in more popular financial instruments, even when correcting for\nstandard factors. Moreover, funds with a low $ACC$ have been less affected by\nthe 2007-08 global financial crisis, likely because less exposed to fire sales\nspillovers.\n"
    },
    {
        "paper_id": 1811.01664,
        "authors": "Dalal Al Ghanim, Ronnie Loeffen, Alex Watson",
        "title": "The equivalence of two tax processes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2019.10.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce two models of taxation, the latent and natural tax processes,\nwhich have both been used to represent loss-carry-forward taxation on the\ncapital of an insurance company. In the natural tax process, the tax rate is a\nfunction of the current level of capital, whereas in the latent tax process,\nthe tax rate is a function of the capital that would have resulted if no tax\nhad been paid. Whereas up to now these two types of tax processes have been\ntreated separately, we show that, in fact, they are essentially equivalent.\nThis allows a unified treatment, translating results from one model to the\nother. Significantly, we solve the question of existence and uniqueness for the\nnatural tax process, which is defined via an integral equation. Our results\nclarify the existing literature on processes with tax.\n"
    },
    {
        "paper_id": 1811.01916,
        "authors": "Tim Leung and Raphael Yan",
        "title": "A Stochastic Control Approach to Managed Futures Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a stochastic control approach to managed futures portfolios.\nBuilding on the Schwartz 97 stochastic convenience yield model for commodity\nprices, we formulate a utility maximization problem for dynamically trading a\nsingle-maturity futures or multiple futures contracts over a finite horizon. By\nanalyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the\ninvestor's utility maximization problem explicitly and derive the optimal\ndynamic trading strategies in closed form. We provide numerical examples and\nillustrate the optimal trading strategies using WTI crude oil futures data.\n"
    },
    {
        "paper_id": 1811.02028,
        "authors": "Vinicius Albani and Jorge Zubelli",
        "title": "A Splitting Strategy for the Calibration of Jump-Diffusion Models",
        "comments": "34 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a detailed analysis and implementation of a splitting strategy to\nidentify simultaneously the local-volatility surface and the jump-size\ndistribution from quoted European prices. The underlying model consists of a\njump-diffusion driven asset with time and price dependent volatility. Our\napproach uses a forward Dupire-type partial-integro-differential equations for\nthe option prices to produce a parameter-to-solution map. The ill-posed inverse\nproblem for such map is then solved by means of a Tikhonov-type convex\nregularization. The proofs of convergence and stability of the algorithm are\nprovided together with numerical examples that substantiate the robustness of\nthe method both for synthetic and real data.\n"
    },
    {
        "paper_id": 1811.02106,
        "authors": "Adam Ploszaj, Xiaoran Yan, Katy Borner",
        "title": "The impact of air transport availability on research collaboration: A\n  case study of four universities",
        "comments": "3 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the impact of air transport connectivity and\naccessibility on scientific collaboration.\n"
    },
    {
        "paper_id": 1811.02382,
        "authors": "Hayette Gatfaoui",
        "title": "Diversifying portfolios of U.S. stocks with crude oil and natural gas: A\n  regime-dependent optimization with several risk measures",
        "comments": "29 pages, 4 figures, 12 tables, Submitted on 23rd November 2016,\n  Second major revision of 6 August 2018 still under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy markets are strategic to governments and economic development. Several\ncommodities compete as substitutable energy sources and energy diversifiers.\nSuch competition reduces the energy vulnerability of countries as well as\nportfolios' risk exposure. Vulnerability results mainly from price trends and\nfluctuations, following supply and demand shocks. Such energy price uncertainty\nattracts many market participants in the energy commodity markets. First,\nenergy producers and consumers hedge adverse price changes with energy\nderivatives. Second, financial market participants use commodities and\ncommodity derivatives to diversify their conventional portfolios. For that\nreason, we consider the joint dependence between the United States (U.S.)\nnatural gas, crude oil and stock markets. We use Gatfaoui's (2015) time varying\nmultivariate copula analysis and related variance regimes. Such approach\nhandles structural changes in asset prices. In this light, we draw implications\nfor portfolio optimization, when investors diversify their stock portfolios\nwith natural gas and crude oil assets. We minimize the portfolio's variance,\nsemi-variance and tail risk, in the presence and the absence of constraints on\nthe portfolio's expected return and/or U.S. stock investment. The return\nconstraint reduces the performance of the optimal portfolio. Moreover, the\nregime-specific portfolio optimization helps implement an enhanced active\nmanagement strategy over the whole sample period. Under a return constraint,\nthe semi-variance optimal portfolio offers the best risk-return tradeoff,\nwhereas the tail-risk optimal portfolio offers the best tradeoff in the absence\nof a return constraint.\n"
    },
    {
        "paper_id": 1811.02497,
        "authors": "Carlos Alos-Ferrer, Ernst Fehr, Nick Netzer",
        "title": "Time will tell - Recovering Preferences when Choices are Noisy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ability to uncover preferences from choices is fundamental for both\npositive economics and welfare analysis. Overwhelming evidence shows that\nchoice is stochastic, which has given rise to random utility models as the\ndominant paradigm in applied microeconomics. However, as is well known, it is\nnot possible to infer the structure of preferences in the absence of\nassumptions on the structure of noise. This makes it impossible to empirically\ntest the structure of noise independently from the structure of preferences.\nHere, we show that the difficulty can be bypassed if data sets are enlarged to\ninclude response times. A simple condition on response time distributions (a\nweaker version of first order stochastic dominance) ensures that choices reveal\npreferences without assumptions on the structure of utility noise. Sharper\nresults are obtained if the analysis is restricted to specific classes of\nmodels. Under symmetric noise, response times allow to uncover preferences for\nchoice pairs outside the data set, and if noise is Fechnerian, even choice\nprobabilities can be forecast out of sample. We conclude by showing that\nstandard random utility models from economics and standard drift-diffusion\nmodels from psychology necessarily generate data sets fulfilling our sufficient\ncondition on response time distributions.\n"
    },
    {
        "paper_id": 1811.0253,
        "authors": "Delia Coculescu and Freddy Delbaen",
        "title": "Surplus sharing with coherent utility functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the theory of coherent measures to look at the problem of surplus\nsharing in an insurance business. The surplus share of an insured is calculated\nby the surplus premium in the contract. The theory of coherent risk measures\nand the resulting capital allocation gives a way to divide the surplus between\nthe insured and the capital providers, i.e. the shareholders.\n"
    },
    {
        "paper_id": 1811.0288,
        "authors": "Arthur le Calvez and Dave Cliff",
        "title": "Deep Learning can Replicate Adaptive Traders in a Limit-Order-Book\n  Financial Market",
        "comments": "8 pages, 4 figures. To be presented at IEEE Symposium on\n  Computational Intelligence in Financial Engineering (CIFEr), Bengaluru; Nov\n  18-21, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report successful results from using deep learning neural networks (DLNNs)\nto learn, purely by observation, the behavior of profitable traders in an\nelectronic market closely modelled on the limit-order-book (LOB) market\nmechanisms that are commonly found in the real-world global financial markets\nfor equities (stocks & shares), currencies, bonds, commodities, and\nderivatives. Successful real human traders, and advanced automated algorithmic\ntrading systems, learn from experience and adapt over time as market conditions\nchange; our DLNN learns to copy this adaptive trading behavior. A novel aspect\nof our work is that we do not involve the conventional approach of attempting\nto predict time-series of prices of tradeable securities. Instead, we collect\nlarge volumes of training data by observing only the quotes issued by a\nsuccessful sales-trader in the market, details of the orders that trader is\nexecuting, and the data available on the LOB (as would usually be provided by a\ncentralized exchange) over the period that the trader is active. In this paper\nwe demonstrate that suitably configured DLNNs can learn to replicate the\ntrading behavior of a successful adaptive automated trader, an algorithmic\nsystem previously demonstrated to outperform human traders. We also demonstrate\nthat DLNNs can learn to perform better (i.e., more profitably) than the trader\nthat provided the training data. We believe that this is the first ever\ndemonstration that DLNNs can successfully replicate a human-like, or\nsuper-human, adaptive trader operating in a realistic emulation of a real-world\nfinancial market. Our results can be considered as proof-of-concept that a DLNN\ncould, in principle, observe the actions of a human trader in a real financial\nmarket and over time learn to trade equally as well as that human trader, and\npossibly better.\n"
    },
    {
        "paper_id": 1811.02886,
        "authors": "Ellie Birbeck and Dave Cliff",
        "title": "Using Stock Prices as Ground Truth in Sentiment Analysis to Generate\n  Profitable Trading Signals",
        "comments": "8 pages, 6 figures. To be presented at IEEE Symposium on\n  Computational Intelligence in Financial Engineering (CIFEr), Bengaluru,\n  November 18-21, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing availability of \"big\" (large volume) social media data has\nmotivated a great deal of research in applying sentiment analysis to predict\nthe movement of prices within financial markets. Previous work in this field\ninvestigates how the true sentiment of text (i.e. positive or negative\nopinions) can be used for financial predictions, based on the assumption that\nsentiments expressed online are representative of the true market sentiment.\nHere we consider the converse idea, that using the stock price as the\nground-truth in the system may be a better indication of sentiment. Tweets are\nlabelled as Buy or Sell dependent on whether the stock price discussed rose or\nfell over the following hour, and from this, stock-specific dictionaries are\nbuilt for individual companies. A Bayesian classifier is used to generate stock\npredictions, which are input to an automated trading algorithm. Placing 468\ntrades over a 1 month period yields a return rate of 5.18%, which annualises to\napproximately 83% per annum. This approach performs significantly better than\nrandom chance and outperforms two baseline sentiment analysis methods tested.\n"
    },
    {
        "paper_id": 1811.03092,
        "authors": "Tanya Ara\\'ujo and Maximilian G\\\"obel",
        "title": "Reframing the S\\&P500 Network of Stocks along the \\nth{21} Century",
        "comments": "21 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.121062",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the beginning of the new millennium, stock markets went through every\nstate from long-time troughs, trade suspensions to all-time highs. The\nliterature on asset pricing hence assumes random processes to be underlying the\nmovement of stock returns. Observed procyclicality and time-varying correlation\nof stock returns tried to give the apparently random behavior some sort of\nstructure. However, common misperceptions about the co-movement of asset prices\nin the years preceding the \\emph{Great Recession} and the \\emph{Global\nCommodity Crisis}, is said to have even fueled the crisis' economic impact.\nHere we show how a varying macroeconomic environment influences stocks'\nclustering into communities. From a sample of 296 stocks of the S\\&P 500 index,\ndistinct periods in between 2004 and 2011 are used to develop networks of\nstocks. The Minimal Spanning Tree analysis of those time-varying networks of\nstocks demonstrates that the crises of 2007-2008 and 2010-2011 drove the market\nto clustered community structures in both periods, helping to restore the stock\nmarket's ceased order of the pre-crises era. However, a comparison of the\nemergent clusters with the \\textit{General Industry Classification Standard}\nconveys the impression that industry sectors do not play a major role in that\norder.\n"
    },
    {
        "paper_id": 1811.03146,
        "authors": "Marvin Aron Kennis",
        "title": "Multi-channel discourse as an indicator for Bitcoin price and volume\n  movements",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research aims to identify how Bitcoin-related news publications and\nonline discourse are expressed in Bitcoin exchange movements of price and\nvolume. Being inherently digital, all Bitcoin-related fundamental data (from\nexchanges, as well as transactional data directly from the blockchain) is\navailable online, something that is not true for traditional businesses or\ncurrencies traded on exchanges. This makes Bitcoin an interesting subject for\nsuch research, as it enables the mapping of sentiment to fundamental events\nthat might otherwise be inaccessible. Furthermore, Bitcoin discussion largely\ntakes place on online forums and chat channels. In stock trading, the value of\nsentiment data in trading decisions has been demonstrated numerous times [1]\n[2] [3], and this research aims to determine whether there is value in such\ndata for Bitcoin trading models. To achieve this, data over the year 2015 has\nbeen collected from Bitcointalk.org, (the biggest Bitcoin forum in post\nvolume), established news sources such as Bloomberg and the Wall Street\nJournal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and\nbitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we\nfind weak to moderate correlations between forum, news, and Reddit sentiment\nand movements in price and volume from 1 to 5 days after the sentiment was\nexpressed. A Granger causality test confirms the predictive causality of the\nsentiment on the daily percentage price and volume movements, and at the same\ntime underscores the predictive causality of market movements on sentiment\nexpressions in online communities\n"
    },
    {
        "paper_id": 1811.03711,
        "authors": "Qiang Zhang, Rui Luo, Yaodong Yang, Yuanyuan Liu",
        "title": "Benchmarking Deep Sequential Models on Volatility Predictions for\n  Financial Time Series",
        "comments": "NIPS 2018, Workshop on Challenges and Opportunities for AI in\n  Financial Services",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility is a quantity of measurement for the price movements of stocks or\noptions which indicates the uncertainty within financial markets. As an\nindicator of the level of risk or the degree of variation, volatility is\nimportant to analyse the financial market, and it is taken into consideration\nin various decision-making processes in financial activities. On the other\nhand, recent advancement in deep learning techniques has shown strong\ncapabilities in modelling sequential data, such as speech and natural language.\nIn this paper, we empirically study the applicability of the latest deep\nstructures with respect to the volatility modelling problem, through which we\naim to provide an empirical guidance for the theoretical analysis of the\nmarriage between deep learning techniques and financial applications in the\nfuture. We examine both the traditional approaches and the deep sequential\nmodels on the task of volatility prediction, including the most recent variants\nof convolutional and recurrent networks, such as the dilated architecture.\nAccordingly, experiments with real-world stock price datasets are performed on\na set of 1314 daily stock series for 2018 days of transaction. The evaluation\nand comparison are based on the negative log likelihood (NLL) of real-world\nstock price time series. The result shows that the dilated neural models,\nincluding dilated CNN and Dilated RNN, produce most accurate estimation and\nprediction, outperforming various widely-used deterministic models in the GARCH\nfamily and several recently proposed stochastic models. In addition, the high\nflexibility and rich expressive power are validated in this study.\n"
    },
    {
        "paper_id": 1811.03718,
        "authors": "Hadrien De March and Charles-Albert Lehalle",
        "title": "Optimal trading using signals",
        "comments": "21 pages, 5 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a mathematical framework to address the uncertainty\nemergingwhen the designer of a trading algorithm uses a threshold on a signal\nas a control. We rely ona theorem by Benveniste and Priouret to deduce our\nInventory Asymptotic Behaviour (IAB)Theorem giving the full distribution of the\ninventory at any point in time for a well formulatedtime continuous version of\nthe trading algorithm.Since this is the first time a paper proposes to address\nthe uncertainty linked to the use of athreshold on a signal for trading, we\ngive some structural elements about the kind of signals thatare using in\nexecution. Then we show how to control this uncertainty for a given cost\nfunction.There is no closed form solution to this control, hence we propose\nseveral approximation schemesand compare their performances.Moreover, we\nexplain how to apply the IAB Theorem to any trading algorithm drivenby a\ntrading speed. It is not needed to control the uncertainty due to the\nthresholding of asignal to exploit the IAB Theorem; it can be applied ex-post\nto any traditional trading algorithm.\n"
    },
    {
        "paper_id": 1811.03766,
        "authors": "Miko{\\l}aj Bi\\'nkowski and Charles-Albert Lehalle",
        "title": "Endogeneous Dynamics of Intraday Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the endogenous information contained in four\nliquidity variables at a five minutes time scale on equity markets around the\nworld: the traded volume, the bid-ask spread, the volatility and the volume at\nfirst limits of the orderbook. In the spirit of Granger causality, we measure\nthe level of information by the level of accuracy of linear autoregressive\nmodels. This empirical study is carried out on a dataset of more than 300\nstocks from four different markets (US, UK, Japan and Hong Kong) from a period\nof over five years. We discuss the obtained performances of autoregressive (AR)\nmodels on stationarized versions of the variables, focusing on explaining the\nobserved differences between stocks.\n  Since empirical studies are often conducted at this time scale, we believe it\nis of paramount importance to document endogenous dynamics in a simple\nframework with no addition of supplemental information. Our study can hence be\nused as a benchmark to identify exogenous effects. On the other hand, most\noptimal trading frameworks (like the celebrated Almgren and Chriss one), focus\non computing an optimal trading speed at a frequency close to the one we\nconsider. Such frameworks very often take i.i.d. assumptions on liquidity\nvariables; this paper document the auto-correlations emerging from real data,\nopening the door to new developments in optimal trading.\n"
    },
    {
        "paper_id": 1811.0382,
        "authors": "Andrea Bastianin, Matteo Manera",
        "title": "How does stock market volatility react to oil shocks?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1017/S1365100516000353",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of oil price shocks on the U.S. stock market volatility.\nWe jointly analyze three different structural oil market shocks (i.e.,\naggregate demand, oil supply, and oil-specific demand shocks) and stock market\nvolatility using a structural vector autoregressive model. Identification is\nachieved by assuming that the price of crude oil reacts to stock market\nvolatility only with delay. This implies that innovations to the price of crude\noil are not strictly exogenous, but predetermined with respect to the stock\nmarket. We show that volatility responds significantly to oil price shocks\ncaused by unexpected changes in aggregate and oil-specific demand, whereas the\nimpact of supply-side shocks is negligible.\n"
    },
    {
        "paper_id": 1811.03931,
        "authors": "Sebastian del Bano Rollin and Zsolt Bihari and Tomaso Aste",
        "title": "Risk-Neutral Pricing and Hedging of In-Play Football Bets",
        "comments": "21 pages, 4 Tables, 8 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A risk-neutral valuation framework is developed for pricing and hedging\nin-play football bets based on modelling scores by independent Poisson\nprocesses with constant intensities. The Fundamental Theorems of Asset Pricing\nare applied to this set-up which enables us to derive novel arbitrage-free\nvaluation formul\\ae\\ for contracts currently traded in the market. We also\ndescribe how to calibrate the model to the market and how trades can be\nreplicated and hedged.\n"
    },
    {
        "paper_id": 1811.04197,
        "authors": "Gholamreza Hajargasht and Prasada Rao",
        "title": "Multilateral Index Number Systems for International Price Comparisons:\n  Properties, Existence and Uniqueness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past five decades a number of multilateral index number systems have\nbeen proposed for spatial and cross-country price comparisons. These\nmultilateral indexes are usually expressed as solutions to systems of linear or\nnonlinear equations. In this paper, we provide general theorems that can be\nused to establish necessary and sufficient conditions for the existence and\nuniqueness of the Geary-Khamis, IDB, Neary and Rao indexes as well as potential\nnew systems including two generalized systems of index numbers. One of our main\nresults is that the necessary and sufficient conditions for existence and\nuniqueness of solutions can often be stated in terms of graph-theoretic\nconcepts and a verifiable condition based on observed quantities of\ncommodities.\n"
    },
    {
        "paper_id": 1811.04223,
        "authors": "Nadine M Walters, Conrad Beyers, Gusti van Zyl, Rolf van den Heever",
        "title": "A framework for simulating systemic risk and its application to the\n  South African banking sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a network-based framework for simulating systemic risk that\nconsiders shock propagation in banking systems. In particular, the framework\nallows the modeller to reflect a top-down framework where a shock to one bank\nin the system affects the solvency and liquidity position of other banks,\nthrough systemic market risks and consequential liquidity strains. We\nillustrate the framework with an application using South African bank balance\nsheet data. Spikes in simulated assessments of systemic risk agree closely with\nspikes in documented subjective assessments of this risk. This indicates that\nnetwork models can be useful for monitoring systemic risk levels. The model\nresults are sensitive to liquidity risk and market sentiment and therefore the\nrelated parameters are important considerations when using a network approach\nto systemic risk modelling.\n"
    },
    {
        "paper_id": 1811.04473,
        "authors": "Andreas Kaloudis, Dimitrios Tsolis",
        "title": "Capital Structure and Speed of Adjustment in U.S. Firms. A Comparative\n  Study in Microeconomic and Macroeconomic Conditions - A Quantille Regression\n  Approach",
        "comments": "18 Pages, 2 Figures, 6 Tables. arXiv admin note: substantial text\n  overlap with arXiv:1801.06651",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The major perspective of this paper is to provide more evidence regarding how\n\"quickly\", in different macroeconomic states, companies adjust their capital\nstructure to their leverage targets. This study extends the empirical research\non the topic of capital structure by focusing on a quantile regression method\nto investigate the behavior of firm-specific characteristics and macroeconomic\nfactors across all quantiles of distribution of leverage (book leverage and\nmarket leverage). Therefore, depending on a partial adjustment model, we find\nthat the adjustment speed fluctuated in different stages of book versus market\nleverage. Furthermore, while macroeconomic states change, we detect clear\ndifferentiations of the contribution and the effects of the firm-specific and\nthe macroeconomic variables between market leverage and book leverage debt\nratios. Consequently, we deduce that across different macroeconomic states the\nnature and maturity of borrowing influence the persistence and endurance of the\nrelation between determinants and borrowing.\n"
    },
    {
        "paper_id": 1811.04502,
        "authors": "Roger Koppl, Abigail Devereaux, Jim Herriot, Stuart Kauffman",
        "title": "A Simple Combinatorial Model of World Economic History",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a simple combinatorial model of technological change to explain the\nIndustrial Revolution. The Industrial Revolution was a sudden large improvement\nin technology, which resulted in significant increases in human wealth and life\nspans. In our model, technological change is combining or modifying earlier\ngoods to produce new goods. The underlying process, which has been the same for\nat least 200,000 years, was sure to produce a very long period of relatively\nslow change followed with probability one by a combinatorial explosion and\nsudden takeoff. Thus, in our model, after many millennia of relative quiescence\nin wealth and technology, a combinatorial explosion created the sudden takeoff\nof the Industrial Revolution.\n"
    },
    {
        "paper_id": 1811.04994,
        "authors": "Bruce Knuteson",
        "title": "How to Increase Global Wealth Inequality for Fun and Profit",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We point out a simple equities trading strategy that allows a sufficiently\nlarge, market-neutral, quantitative hedge fund to achieve outsized returns\nwhile simultaneously contributing significantly to increasing global wealth\ninequality. Overnight and intraday return distributions in major equity indices\nin the United States, Canada, France, Germany, and Japan suggest a few such\nfirms have been implementing this strategy successfully for more than\ntwenty-five years.\n"
    },
    {
        "paper_id": 1811.05206,
        "authors": "Alessandro Pluchino, Alessio. E. Biondo, Andrea Rapisarda",
        "title": "Exploring the role of talent and luck in getting success",
        "comments": "7 pages, 2 figures, talk presented at the Summer Solstice Conference\n  2018 on \"Discrete models of complex systems\", 25-27 June 2018, Gda\\'nsk,\n  Poland. To be published in Acta Physics Polonica B",
        "journal-ref": "Acta Physica Polonica B Proceedings Supplement 12 (2019) 17",
        "doi": "10.5506/APhysPolBSupp.12.17",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review recent numerical results on the role of talent and luck in getting\nsuccess by means of a schematic agent-based model. In general the role of luck\nis found to be very relevant in order to get success, while talent is necessary\nbut not sufficient. Funding strategies to improve the success of the most\ntalented people are also discussed.\n"
    },
    {
        "paper_id": 1811.0523,
        "authors": "Fr\\'ed\\'eric Bucci, Michael Benzaquen, Fabrizio Lillo, Jean-Philippe\n  Bouchaud",
        "title": "Crossover from linear to square-Root market impact",
        "comments": "5 pages, 2 figures",
        "journal-ref": "Phys. Rev. Lett. 122, 108302 (2019)",
        "doi": "10.1103/PhysRevLett.122.108302",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a large database of 8 million institutional trades executed in the U.S.\nequity market, we establish a clear crossover between a linear market impact\nregime and a square-root regime as a function of the volume of the order. Our\nempirical results are remarkably well explained by a recently proposed\ndynamical theory of liquidity that makes specific predictions about the scaling\nfunction describing this crossover. Allowing at least two characteristic time\nscales for the liquidity (`fast' and `slow') enables one to reach quantitative\nagreement with the data.\n"
    },
    {
        "paper_id": 1811.0527,
        "authors": "Rastin Matin, Casper Hansen, Christian Hansen and Pia M{\\o}lgaard",
        "title": "Predicting Distresses using Deep Learning of Text Segments in Annual\n  Reports",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corporate distress models typically only employ the numerical financial\nvariables in the firms' annual reports. We develop a model that employs the\nunstructured textual data in the reports as well, namely the auditors' reports\nand managements' statements. Our model consists of a convolutional recurrent\nneural network which, when concatenated with the numerical financial variables,\nlearns a descriptive representation of the text that is suited for corporate\ndistress prediction. We find that the unstructured data provides a\nstatistically significant enhancement of the distress prediction performance,\nin particular for large firms where accurate predictions are of the utmost\nimportance. Furthermore, we find that auditors' reports are more informative\nthan managements' statements and that a joint model including both managements'\nstatements and auditors' reports displays no enhancement relative to a model\nincluding only auditors' reports. Our model demonstrates a direct improvement\nover existing state-of-the-art models.\n"
    },
    {
        "paper_id": 1811.05421,
        "authors": "Yunhee Chang, Jinhee Kim, and Swarn Chatterjee",
        "title": "Health Care Expenditures, Financial Stability, and Participation in the\n  Supplemental Nutrition Assistance Program (SNAP)",
        "comments": "Forthcoming in the Journal of Policy Practice",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the association between household healthcare expenses and\nparticipation in the Supplemental Nutrition Assistance Program (SNAP) when\nmoderated by factors associated with financial stability of households. Using a\nlarge longitudinal panel encompassing eight years, this study finds that an\ninter-temporal increase in out-of-pocket medical expenses increased the\nlikelihood of household SNAP participation in the current period. Financially\nstable households with precautionary financial assets to cover at least 6\nmonths worth of household expenses were significantly less likely to\nparticipate in SNAP. The low income households who recently experienced an\nincrease in out of pocket medical expenses but had adequate precautionary\nsavings were less likely than similar households who did not have precautionary\nsavings to participate in SNAP. Implications for economists, policy makers, and\nhousehold finance professionals are discussed.\n"
    },
    {
        "paper_id": 1811.05424,
        "authors": "Samuel J. Ferguson",
        "title": "The Affordable Care Act and the IRS Iterative Fixed Point Procedure",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the quantities appearing in Internal Revenue Service (IRS) tax\nguidance for calculating the health insurance premium tax credit created by the\nPatient Protection and Affordable Care Act, also called Obamacare. We ask the\nquestion of whether there is a procedure, computable by hand, which can\ncalculate the appropriate premium tax credit for any household with\nself-employment income. We give an example showing that IRS tax guidance, which\nhas had self-employed taxpayers use an iterative fixed point procedure to\ncalculate their premium tax credits since 2014, can lead to a divergent\nsequence of iterates. As a consequence, IRS guidance does not calculate\nappropriate premium tax credits for tax returns in certain income intervals,\nadversely affecting eligible beneficiaries. A bisection procedure for\ncalculating premium tax credits is proposed. We prove that this procedure\ncalculates appropriate premium tax credits for a model of simple tax returns;\nand apparently, this procedure has already been used to prepare accepted tax\nreturns. We outline the problem of finding a procedure which calculates\nappropriate premium tax credits for models of general tax returns. While the\nbisection procedure will work with the tax code in its current configuration,\nit could fail, in states which have not expanded Medicaid, if a certain\ndeduction were to revert to an earlier form. Future policy objectives might\nalso lead to further problems.\n"
    },
    {
        "paper_id": 1811.05464,
        "authors": "Damian Jelito, Marcin Pitera",
        "title": "New fat-tail normality test based on conditional second moments with\n  applications to finance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s00362-020-01176-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce an efficient fat-tail measurement framework that\nis based on the conditional second moments. We construct a goodness-of-fit\nstatistic that has a direct interpretation and can be used to assess the impact\nof fat-tails on central data conditional dispersion. Next, we show how to use\nthis framework to construct a powerful normality test. In particular, we\ncompare our methodology to various popular normality tests, including the\nJarque--Bera test that is based on third and fourth moments, and show that in\nmany cases our framework outperforms all others, both on simulated and market\nstock data. Finally, we derive asymptotic distributions for conditional mean\nand variance estimators, and use this to show asymptotic normality of the\nproposed test statistic.\n"
    },
    {
        "paper_id": 1811.05524,
        "authors": "Seungki Min, Costis Maglaras, Ciamac C. Moallemi",
        "title": "Cross-Sectional Variation of Intraday Liquidity, Cross-Impact, and their\n  Effect on Portfolio Execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The composition of natural liquidity has been changing over time. An analysis\nof intraday volumes for the S&P500 constituent stocks illustrates that (i)\nvolume surprises, i.e., deviations from their respective forecasts, are\ncorrelated across stocks, and (ii) this correlation increases during the last\nfew hours of the trading session. These observations could be attributed, in\npart, to the prevalence of portfolio trading activity that is implicit in the\ngrowth of ETF, passive and systematic investment strategies; and, to the\nincreased trading intensity of such strategies towards the end of the trading\nsession, e.g., due to execution of mutual fund inflows/outflows that are\nbenchmarked to the closing price on each day. In this paper, we investigate the\nconsequences of such portfolio liquidity on price impact and portfolio\nexecution. We derive a linear cross-asset market impact from a stylized model\nthat explicitly captures the fact that a certain fraction of natural liquidity\nproviders only trade portfolios of stocks whenever they choose to execute. We\nfind that due to cross-impact and its intraday variation, it is optimal for a\nrisk-neutral, cost minimizing liquidator to execute a portfolio of orders in a\ncoupled manner, as opposed to a separable VWAP-like execution that is often\nassumed. The optimal schedule couples the execution of the various orders so as\nto be able to take advantage of increased portfolio liquidity towards the end\nof the day. A worst case analysis shows that the potential cost reduction from\nthis optimized execution schedule over the separable approach can be as high as\n6% for plausible model parameters. Finally, we discuss how to estimate\ncross-sectional price impact if one had a dataset of realized portfolio\ntransaction records that exploits the low-rank structure of its coefficient\nmatrix suggested by our analysis.\n"
    },
    {
        "paper_id": 1811.05741,
        "authors": "Christian P. Fries",
        "title": "Stochastic Algorithmic Differentiation of (Expectations of)\n  Discontinuous Functions (Indicator Functions)",
        "comments": "21 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper, we present a method for the accurate estimation of the\nderivative (aka.~sensitivity) of expectations of functions involving an\nindicator function by combining a stochastic algorithmic differentiation and a\nregression.\n  The method is an improvement of the approach presented in [Risk Magazine\nApril 2018].\n  The finite difference approximation of a partial derivative of a Monte-Carlo\nintegral of a discontinuous function is known to exhibit a high Monte-Carlo\nerror. The issue is evident since the Monte-Carlo approximation of a\ndiscontinuous function is just a finite sum of discontinuous functions and as\nsuch, not even differentiable.\n  The algorithmic differentiation of a discontinuous function is problematic. A\nnatural approach is to replace the discontinuity by continuous functions. This\nis equivalent to replacing a path-wise automatic differentiation by a (local)\nfinite difference approximation.\n  We present an improvement (in terms of variance reduction) by decoupling the\nintegration of the Dirac delta and the remaining conditional expectation and\nestimating the two parts by separate regressions. For the algorithmic\ndifferentiation, we derive an operator that can be injected seamlessly - with\nminimal code changes - into the algorithm resulting in the exact result.\n"
    },
    {
        "paper_id": 1811.05935,
        "authors": "Hina Binte Haq, Syed Taha Ali",
        "title": "Navigating the Cryptocurrency Landscape: An Islamic Perspective",
        "comments": "6TH INTERNATIONAL CONFERENCE ON ISLAM AND LIBERTY",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to\ngenerate headlines and intense debate. What started as an underground\nexperiment by a rag tag group of programmers armed with a Libertarian manifesto\nhas now resulted in a thriving $230 billion ecosystem, with constant on-going\ninnovation. Scholars and researchers alike are realizing that cryptocurrencies\nare far more than mere technical innovation; they represent a distinct and\nrevolutionary new economic paradigm tending towards decentralization.\nUnfortunately, this bold new universe is little explored from the perspective\nof Islamic economics and finance. Our work aims to address these deficiencies.\nOur paper makes the following distinct contributions We significantly expand\nthe discussion on whether cryptocurrencies qualify as \"money\" from an Islamic\nperspective and we argue that this debate necessitates rethinking certain\nfundamental definitions. We conclude that the cryptocurrency phenomenon, with\nits radical new capabilities, may hold considerable opportunity which merits\ndeeper investigation.\n"
    },
    {
        "paper_id": 1811.06173,
        "authors": "Huicheng Liu",
        "title": "Leveraging Financial News for Stock Trend Prediction with\n  Attention-Based Recurrent Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market prediction is one of the most attractive research topic since\nthe successful prediction on the market's future movement leads to significant\nprofit. Traditional short term stock market predictions are usually based on\nthe analysis of historical market data, such as stock prices, moving averages\nor daily returns. However, financial news also contains useful information on\npublic companies and the market. Existing methods in finance literature exploit\nsentiment signal features, which are limited by not considering factors such as\nevents and the news context. We address this issue by leveraging deep neural\nmodels to extract rich semantic features from news text. In particular, a\nBidirectional-LSTM are used to encode the news text and capture the context\ninformation, self attention mechanism are applied to distribute attention on\nmost relative words, news and days. In terms of predicting directional changes\nin both Standard & Poor's 500 index and individual companies stock price, we\nshow that this technique is competitive with other state of the art approaches,\ndemonstrating the effectiveness of recent NLP technology advances for\ncomputational finance.\n"
    },
    {
        "paper_id": 1811.06323,
        "authors": "Fabio Gaetano Santeramo and Emilia Lamonaca",
        "title": "The effects of non-tariff measures on agri-food trade: a review and\n  meta-analysis of empirical evidence",
        "comments": null,
        "journal-ref": "Journal of Agricultural Economics, 2020",
        "doi": "10.1111/1477-9552.12316",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing policy interests and the vivid academic debate on non-tariff\nmeasures (NTMs) has stimulated a growing literature on how NTMs affect agrifood\ntrade. The empirical literature provides contrasting and heterogeneous\nevidence, with some studies supporting the standards as catalysts view, and\nothers favouring the standards as barriers explanation. To the extent that NTMs\ncan influence trade, understanding the prevailing effect, and the motivations\nbehind one effect or the other, is a pressing issue. We review a large body of\nempirical evidence on the effect of NTMs on agri-food trade and conduct a\nmeta-analysis to disentangle potential determinants of heterogeneity in\nestimates. Our findings show the role played by the publication process and by\nstudy-specific assumptions. Some characteristics of the studies are correlated\nwith positive significant estimates, others covary with negative significant\nestimates. Overall, we found that the effects of NTMs vary across types of\nNTMs, proxy for NTMs, and levels of details of studies. Not negligible is the\ninfluence of methodological issues and publication process.\n"
    },
    {
        "paper_id": 1811.06361,
        "authors": "Matyas Barczy, Adam Dudas, Jozsef Gall",
        "title": "On approximations of Value at Risk and Expected Shortfall involving\n  kurtosis",
        "comments": "30 pages, 11 figures",
        "journal-ref": "Communications in Statistics - Simulation and Computation 52(3),\n  (2023), 770-794",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive new approximations for the Value at Risk and the Expected Shortfall\nat high levels of loss distributions with positive skewness and excess\nkurtosis, and we describe their precisions for notable ones such as for\nexponential, Pareto type I, lognormal and compound (Poisson) distributions. Our\napproximations are motivated by that kind of extensions of the so-called Normal\nPower Approximation, used for approximating the cumulative distribution\nfunction of a random variable, which incorporate not only the skewness but the\nkurtosis of the random variable in question as well. We show the performance of\nour approximations in numerical examples and we also give comparisons with some\nknown ones in the literature.\n"
    },
    {
        "paper_id": 1811.06606,
        "authors": "Daniel Muller",
        "title": "Economics of Human-AI Ecosystem: Value Bias and Lost Utility in\n  Multi-Dimensional Gaps",
        "comments": "8 pages, typos corrected, examples added to Table 1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, artificial intelligence (AI) decision-making and autonomous\nsystems became an integrated part of the economy, industry, and society. The\nevolving economy of the human-AI ecosystem raising concerns regarding the risks\nand values inherited in AI systems. This paper investigates the dynamics of\ncreation and exchange of values and points out gaps in perception of\ncost-value, knowledge, space and time dimensions. It shows aspects of value\nbias in human perception of achievements and costs that encoded in AI systems.\nIt also proposes rethinking hard goals definitions and cost-optimal\nproblem-solving principles in the lens of effectiveness and efficiency in the\ndevelopment of trusted machines. The paper suggests a value-driven with cost\nawareness strategy and principles for problem-solving and planning of effective\nresearch progress to address real-world problems that involve diverse forms of\nachievements, investments, and survival scenarios.\n"
    },
    {
        "paper_id": 1811.0665,
        "authors": "Erhan Bayraktar, Thomas Caye and Ibrahim Ekren",
        "title": "Asymptotics for Small Nonlinear Price Impact: a PDE Approach to the\n  Multidimensional Case",
        "comments": "to appear in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an asymptotic expansion of the value function of a\nmultidimensional utility maximization problem from consumption with small\nnon-linear price impact. In our model cross-impacts between assets are allowed.\nIn the limit for small price impact, we determine the asymptotic expansion of\nthe value function around its frictionless version. The leading order\ncorrection is characterized by a nonlinear second order PDE related to an\nergodic control problem and a linear parabolic PDE. We illustrate our result on\na multivariate geometric Brownian motion price model.\n"
    },
    {
        "paper_id": 1811.06766,
        "authors": "Georgios Sermpinis, Arman Hassanniakalager, Charalampos Stasinakis,\n  Ioannis Psaradellis",
        "title": "Technical Analysis and Discrete False Discovery Rate: Evidence from MSCI\n  Indices",
        "comments": "72 pages, 2 figues, 14 (main) and 13 (appendix) tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the performance of dynamic portfolios constructed using more\nthan 21,000 technical trading rules on 12 categorical and country-specific\nmarkets over the 2004-2015 study period, on rolling forward structures of\ndifferent lengths. We also introduce a discrete false discovery rate (DFRD+/-)\nmethod for controlling data snooping bias. Compared to the existing methods,\nDFRD+/- is adaptive and more powerful, and accommodates for discrete p-values.\nThe profitability, persistence and robustness of the technical rules are\nexamined. Technical analysis still has short-term value in advanced, emerging\nand frontier markets. Financial stress, the economic environment and market\ndevelopment seem to affect the performance of trading rules. A cross-validation\nexercise highlights the importance of frequent rebalancing and the variability\nof profitability in trading with technical analysis.\n"
    },
    {
        "paper_id": 1811.06772,
        "authors": "Josef Taalbi",
        "title": "Evolution and structure of technological systems - An innovation output\n  network",
        "comments": null,
        "journal-ref": "Research Policy 49, 8 (2020)",
        "doi": "10.1016/j.respol.2020.104010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the network of supply and use of significant innovations\nacross industries in Sweden, 1970-2013. It is found that 30% of innovation\npatterns can be predicted by network stimulus from backward and forward\nlinkages. The network is hierarchical, characterized by hubs that connect\ndiverse industries in closely knitted communities. To explain the network\nstructure, a preferential weight assignment process is proposed as an\nadaptation of the classical preferential attachment process to weighted\ndirected networks. The network structure is strongly predicted by this process\nwhere historical technological linkages and proximities matter, while human\ncapital flows and economic input-output flows have conflicting effects on link\nformation. The results are consistent with the idea that innovations emerge in\nclosely connected communities, but suggest that the transformation of\ntechnological systems are shaped by technological requirements, imbalances and\nopportunities that are not straightforwardly related to other proximities.\n"
    },
    {
        "paper_id": 1811.06893,
        "authors": "Carmine De Franco, Johann Nicolle (LPSM UMR 8001), Huy\\^en Pham (LPSM\n  UMR 8001, CREST)",
        "title": "Bayesian learning for the Markowitz portfolio selection problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Markowitz portfolio selection problem with unknown drift vector\nin the multidimensional framework. The prior belief on the uncertain expected\nrate of return is modeled by an arbitrary probability law, and a Bayesian\napproach from filtering theory is used to learn the posterior distribution\nabout the drift given the observed market data of the assets. The Bayesian\nMarkowitz problem is then embedded into an auxiliary standard control problem\nthat we characterize by a dynamic programming method and prove the existence\nand uniqueness of a smooth solution to the related semi-linear partial\ndifferential equation (PDE). The optimal Markowitz portfolio strategy is\nexplicitly computed in the case of a Gaussian prior distribution. Finally, we\nmeasure the quantitative impact of learning, updating the strategy from\nobserved data, compared to non-learning, using a constant drift in an uncertain\ncontext, and analyze the sensitivity of the value of information w.r.t. various\nrelevant parameters of our model.\n"
    },
    {
        "paper_id": 1811.07188,
        "authors": "Dhanya Jothimani, Ravi Shankar, Surendra S. Yadav",
        "title": "A Big data analytical framework for portfolio optimization",
        "comments": "Workshop on Internet and BigData Finance (WIBF 14)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the advent of Web 2.0, various types of data are being produced every\nday. This has led to the revolution of big data. Huge amount of structured and\nunstructured data are produced in financial markets. Processing these data\ncould help an investor to make an informed investment decision. In this paper,\na framework has been developed to incorporate both structured and unstructured\ndata for portfolio optimization. Portfolio optimization consists of three\nprocesses: Asset selection, Asset weighting and Asset management. This\nframework proposes to achieve the first two processes using a 5-stage\nmethodology. The stages include shortlisting stocks using Data Envelopment\nAnalysis (DEA), incorporation of the qualitative factors using text mining,\nstock clustering, stock ranking and optimizing the portfolio using heuristics.\nThis framework would help the investors to select appropriate assets to make\nportfolio, invest in them to minimize the risk and maximize the return and\nmonitor their performance.\n"
    },
    {
        "paper_id": 1811.0722,
        "authors": "George Bouzianis, Lane Hughston",
        "title": "Determination of the L\\'evy Exponent in Asset Pricing Models",
        "comments": "International Journal of Theoretical and Applied Finance, Vol. 22,\n  No. 1 (2019) 1950008:1-18",
        "journal-ref": null,
        "doi": "10.1142/S0219024919500080",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of determining the L\\'evy exponent in a L\\'evy model\nfor asset prices given the price data of derivatives. The model, formulated\nunder the real-world measure $\\mathbb P$, consists of a pricing kernel\n$\\{\\pi_t\\}_{t\\geq0}$ together with one or more non-dividend-paying risky assets\ndriven by the same L\\'evy process. If $\\{S_t\\}_{t\\geq0}$ denotes the price\nprocess of such an asset then $\\{\\pi_t S_t\\}_{t\\geq0}$ is a $\\mathbb\nP$-martingale. The L\\'evy process $\\{ \\xi_t \\}_{t\\geq0}$ is assumed to have\nexponential moments, implying the existence of a L\\'evy exponent $\\psi(\\alpha)\n= t^{-1}\\log \\mathbb E(\\rm e^{\\alpha \\xi_t})$ for $\\alpha$ in an interval $A\n\\subset \\mathbb R$ containing the origin as a proper subset. We show that if\nthe initial prices of power-payoff derivatives, for which the payoff is $H_T =\n(\\zeta_T)^q$ for some time $T>0$, are given for a range of values of $q$, where\n$\\{\\zeta_t\\}_{t\\geq0}$ is the so-called benchmark portfolio defined by $\\zeta_t\n= 1/\\pi_t$, then the L\\'evy exponent is determined up to an irrelevant linear\nterm. In such a setting, derivative prices embody complete information about\nprice jumps: in particular, the spectrum of the price jumps can be worked out\nfrom current market prices of derivatives. More generally, if $H_T = (S_T)^q$\nfor a general non-dividend-paying risky asset driven by a L\\'evy process, and\nif we know that the pricing kernel is driven by the same L\\'evy process, up to\na factor of proportionality, then from the current prices of power-payoff\nderivatives we can infer the structure of the L\\'evy exponent up to a\ntransformation $\\psi(\\alpha) \\rightarrow \\psi(\\alpha + \\mu) - \\psi(\\mu) + c\n\\alpha$, where $c$ and $\\mu$ are constants.\n"
    },
    {
        "paper_id": 1811.07237,
        "authors": "Marco A. S. Trindade, Sergio Floquet and Lourival M. S. Filho",
        "title": "Portfolio Theory, Information Theory and Tsallis Statistics",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We developed a strategic of optimal portfolio based on information theory and\nTsallis statistics. The growth rate of a stock market is defined by using\n$q$-deformed functions and we find that the wealth after n days with the\noptimal portfolio is given by a $q$-exponential function. In this context, the\nasymptotic optimality is investigated on causal portfolios, showing advantages\nof the optimal portfolio over an arbitrary choice of causal portfolios.\nFinally, we apply the formulation in a small number of stocks in brazilian\nstock market $[B]^{3}$ and analyzed the results.\n"
    },
    {
        "paper_id": 1811.07294,
        "authors": "Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "CVA and vulnerable options pricing by correlation expansions",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of computing the Credit Value Adjustment ({CVA}) of a\nEuropean option in presence of the Wrong Way Risk ({WWR}) in a default\nintensity setting. Namely we model the asset price evolution as solution to a\nlinear equation that might depend on different stochastic factors and we\nprovide an approximate evaluation of the option's price, by exploiting a\ncorrelation expansion approach, introduced in \\cite{AS}. We compare the\nnumerical performance of such a method with that recently proposed by Brigo et\nal. (\\cite{BR18}, \\cite{BRH18}) in the case of a call option driven by a GBM\ncorrelated with the CIR default intensity. We additionally report some\nnumerical evaluations obtained by other methods.\n"
    },
    {
        "paper_id": 1811.07499,
        "authors": "Jos\\'e E. Figueroa-L\\'opez, Cheng Li, and Jeffrey Nisen",
        "title": "Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion\n  Processes",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": "10.1007/s11203-020-09211-7.",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a new threshold-kernel jump-detection method for\njump-diffusion processes, which iteratively applies thresholding and kernel\nmethods in an approximately optimal way to achieve improved finite-sample\nperformance. We use the expected number of jump misclassifications as the\nobjective function to optimally select the threshold parameter of the jump\ndetection scheme. We prove that the objective function is quasi-convex and\nobtain a new second-order infill approximation of the optimal threshold in\nclosed form. The approximate optimal threshold depends not only on the spot\nvolatility, but also the jump intensity and the value of the jump density at\nthe origin. Estimation methods for these quantities are then developed, where\nthe spot volatility is estimated by a kernel estimator with thresholding and\nthe value of the jump density at the origin is estimated by a density kernel\nestimator applied to those increments deemed to contain jumps by the chosen\nthresholding criterion. Due to the interdependency between the model parameters\nand the approximate optimal estimators built to estimate them, a type of\niterative fixed-point algorithm is developed to implement them. Simulation\nstudies for a prototypical stochastic volatility model show that it is not only\nfeasible to implement the higher-order local optimal threshold scheme but also\nthat this is superior to those based only on the first order approximation\nand/or on average values of the parameters over the estimation time period.\n"
    },
    {
        "paper_id": 1811.07509,
        "authors": "Abdelkarem Berkaoui",
        "title": "On the degree of incompleteness of an incomplete financial market",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to find a way of measuring the degree of incompleteness of an\nincomplete financial market, the rank of the vector price process of the traded\nassets and the dimension of the associated acceptance set are introduced. We\nshow that they are equal and state a variety of consequences.\n"
    },
    {
        "paper_id": 1811.07522,
        "authors": "Xiao-Yang Liu, Zhuoran Xiong, Shan Zhong, Hongyang Yang, and Anwar\n  Walid",
        "title": "Practical Deep Reinforcement Learning Approach for Stock Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trading strategy plays a crucial role in investment companies. However,\nit is challenging to obtain optimal strategy in the complex and dynamic stock\nmarket. We explore the potential of deep reinforcement learning to optimize\nstock trading strategy and thus maximize investment return. 30 stocks are\nselected as our trading stocks and their daily prices are used as the training\nand trading market environment. We train a deep reinforcement learning agent\nand obtain an adaptive trading strategy. The agent's performance is evaluated\nand compared with Dow Jones Industrial Average and the traditional min-variance\nportfolio allocation strategy. The proposed deep reinforcement learning\napproach is shown to outperform the two baselines in terms of both the Sharpe\nratio and cumulative returns.\n"
    },
    {
        "paper_id": 1811.07792,
        "authors": "Javier Franco-Pedroso, Joaquin Gonzalez-Rodriguez, Maria Planas, Jorge\n  Cubero, Rafael Cobo, Fernando Pablos",
        "title": "The ETS challenges: a machine learning approach to the evaluation of\n  simulated financial time series for improving generation processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an evaluation framework that attempts to quantify the\n\"degree of realism\" of simulated financial time series, whatever the simulation\nmethod could be, with the aim of discover unknown characteristics that are not\nbeing properly reproduced by such methods in order to improve them. For that\npurpose, the evaluation framework is posed as a machine learning problem in\nwhich some given time series examples have to be classified as simulated or\nreal financial time series. The \"challenge\" is proposed as an open competition,\nsimilar to those published at the Kaggle platform, in which participants must\nsend their classification results along with a description of the features and\nthe classifiers used. The results of these \"challenges\" have revealed some\ninteresting properties of financial data, and have lead to substantial\nimprovements in our simulation methods under research, some of which will be\ndescribed in this work.\n"
    },
    {
        "paper_id": 1811.0786,
        "authors": "Zura Kakushadze",
        "title": "Cryptoasset Factor Models",
        "comments": "45 pages; 2 trivial typos corrected, no other changes; to appear in\n  Algorithmic Finance",
        "journal-ref": "Algorithmic Finance 7(3-4) (2018) 87-104",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose factor models for the cross-section of daily cryptoasset returns\nand provide source code for data downloads, computing risk factors and\nbacktesting them out-of-sample. In \"cryptoassets\" we include all\ncryptocurrencies and a host of various other digital assets (coins and tokens)\nfor which exchange market data is available. Based on our empirical analysis,\nwe identify the leading factor that appears to strongly contribute into daily\ncryptoasset returns. Our results suggest that cross-sectional statistical\narbitrage trading may be possible for cryptoassets subject to efficient\nexecutions and shorting.\n"
    },
    {
        "paper_id": 1811.08038,
        "authors": "Raymond Brummelhuis and Zhongmin Luo",
        "title": "Arbitrage Opportunities in CDS Term Structure: Theory and Implications\n  for OTC Derivatives",
        "comments": "37 pages, 8 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Absence-of-Arbitrage (AoA) is the basic assumption underpinning derivatives\npricing theory. As part of the OTC derivatives market, the CDS market not only\nprovides a vehicle for participants to hedge and speculate on the default risks\nof corporate and sovereign entities, it also reveals important market-implied\ndefault-risk information concerning the counterparties with which financial\ninstitutions trade, and for which these financial institutions have to\ncalculate various valuation adjustments (collectively referred to as XVA) as\npart of their pricing and risk management of OTC derivatives, to account for\ncounterparty default risks. In this study, we derive No-arbitrage conditions\nfor CDS term structures, first in a positive interest rate environment and then\nin an arbitrary one. Using an extensive CDS dataset which covers the 2007-09\nfinancial crisis, we present a catalogue of 2,416 pairs of anomalous CDS\ncontracts which violate the above conditions. Finally, we show in an example\nthat such anomalies in the CDS term structure can lead to persistent arbitrage\nprofits and to nonsensical default probabilities. The paper is a first\nsystematic study on CDS-term-structure arbitrage providing model-free AoA\nconditions supported by ample empirical evidence.\n"
    },
    {
        "paper_id": 1811.08076,
        "authors": "Hai-Chuan Xu and Wei-Xing Zhou",
        "title": "Modeling aggressive market order placements with Hawkes factor models",
        "comments": "9 pages, 7 figures",
        "journal-ref": "PLoS ONE 15 (1), e0226667 (2020)",
        "doi": "10.1371/journal.pone.0226667",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price changes are induced by aggressive market orders in stock market. We\nintroduce a bivariate marked Hawkes process to model aggressive market order\narrivals at the microstructural level. The order arrival intensity is marked by\nan exogenous part and two endogenous processes reflecting the self-excitation\nand cross-excitation respectively. We calibrate the model for an SSE stock. We\nfind that the exponential kernel with a smooth cut-off (i.e. the subtraction of\ntwo exponentials) produces much better calibration than the monotonous\nexponential kernel (i.e. the sum of two exponentials). The exogenous baseline\nintensity explains the $U$-shaped intraday pattern. Our empirical results show\nthat the endogenous submission clustering is mainly caused by self-excitation\nrather than cross-excitation.\n"
    },
    {
        "paper_id": 1811.08255,
        "authors": "Johannes Bock",
        "title": "An updated review of (sub-)optimal diversification models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.18539.80165",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past decade many researchers have proposed new optimal portfolio\nselection strategies to show that sophisticated diversification can outperform\nthe na\\\"ive 1/N strategy in out-of-sample benchmarks. Providing an updated\nreview of these models since DeMiguel et al. (2009b), I test sixteen strategies\nacross six empirical datasets to see if indeed progress has been made. However,\nI find that none of the recently suggested strategies consistently outperforms\nthe 1/N or minimum-variance approach in terms of Sharpe ratio,\ncertainty-equivalent return or turnover. This suggests that simple\ndiversification rules are not in fact inefficient, and gains promised by\noptimal portfolio choice remain unattainable out-of-sample due to large\nestimation errors in expected returns. Therefore, further research effort\nshould be devoted to both improving estimation of expected returns, and\npossibly exploring diversification rules that do not require the estimation of\nexpected returns directly, but also use other available information about the\nstock characteristics.\n"
    },
    {
        "paper_id": 1811.08308,
        "authors": "Andrei N. Soklakov",
        "title": "Economics of disagreement -- financial intuition for the R\\'enyi\n  divergence",
        "comments": "17 pages, 1 figure",
        "journal-ref": "Entropy 22 (8), 860 (2020)",
        "doi": "10.3390/e22080860",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Disagreement is an essential element of science and life in general. The\nlanguage of probabilities and statistics is often used to describe\ndisagreements quantitatively. In practice, however, we want much more than\nthat. We want disagreements to be resolved. This leaves us with a substantial\nknowledge gap which is often perceived as a lack of practical intuition\nregarding probabilistic and statistical concepts.\n  Take for instance the R\\'enyi divergence which is a well-known statistical\nquantity specifically designed as a measure of disagreement between\nprobabilistic models. Despite its widespread use in science and engineering,\nthe R\\'enyi divergence remains a highly abstract axiomatically-motivated\nmeasure. Certainly, it offers no practical insight as to how disagreements can\nbe resolved.\n  Here we propose to address disagreements using the methods of financial\neconomics. In particular, we show how a large class of disagreements can be\ntransformed into investment opportunities. The expected financial performance\nof such investments quantifies the amount of disagreement in a tangible way.\nThis provides intuition for statistical concepts such as the R\\'enyi divergence\nwhich becomes connected to the financial performance of optimized investments.\nInvestment optimization takes into account individual opinions as well as\nattitudes towards risk. The result is a market-like social mechanism by which\nfunds flow naturally to support a more accurate view. Such social mechanisms\ncan help us with difficult disagreements (e.g., financial arguments concerning\nthe future climate).\n  In terms of scientific validation, we used the findings of independent\nneurophysiological experiments as well as our own research on the equity\npremium.\n"
    },
    {
        "paper_id": 1811.08365,
        "authors": "Nektarios Aslanidis, Aurelio F. Bariviera, Oscar Martinez-Iba\\~nez",
        "title": "An analysis of cryptocurrencies conditional cross correlations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This letter explores the behavior of conditional correlations among main\ncryptocurrencies, stock and bond indices, and gold, using a generalized DCC\nclass model. From a portfolio management point of view, asset correlation is a\nkey metric in order to construct efficient portfolios. We find that: (i)\ncorrelations among cryptocurrencies are positive, albeit varying across time;\n(ii) correlations with Monero are more stable across time; (iii) correlations\nbetween cryptocurrencies and traditional financial assets are negligible.\n"
    },
    {
        "paper_id": 1811.08376,
        "authors": "Shuyao Wu, Shuangcheng Li",
        "title": "A possible alternative evaluation method for the non-use and nonmarket\n  values of ecosystem services",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Monetization of the non-use and nonmarket values of ecosystem services is\nimportant especially in the areas of environmental cost-benefit analysis,\nmanagement and environmental impact assessment. However, the reliability of\nvaluation estimations has been criticized due to the biases that associated\nwith methods like the popular contingent valuation method (CVM). In order to\nprovide alternative valuation results for comparison purpose, we proposed the\npossibility of using a method that incorporates fact-based costs and contingent\npreferences for evaluating non-use and nonmarket values, which we referred to\nas value allotment method (VAM). In this paper, we discussed the economic\nprinciples of VAM, introduced the performing procedure, analyzed assumptions\nand potential biases that associated with the method and compared VAM with CVM\nthrough a case study in Guangzhou, China. The case study showed that the VAM\ngave more conservative estimates than the CVM, which could be a merit since CVM\noften generates overestimated values. We believe that this method can be used\nat least as a referential alternative to CVM and might be particularly useful\nin assessing the non-use and nonmarket values of ecosystem services from\nhuman-invested ecosystems, such as restored ecosystems, man-made parks and\ncroplands.\n"
    },
    {
        "paper_id": 1811.08509,
        "authors": "Christian Wei{\\ss}, Zoran Nikoli\\'c",
        "title": "An Aspect of Optimal Regression Design for LSMC",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Practitioners sometimes suggest to use a combination of Sobol sequences and\northonormal polynomials when applying an LSMC algorithm for evaluation of\noption prices or in the context of risk capital calculation under the Solvency\nII regime. In this paper, we give a theoretical justification why good\nimplementations of an LSMC algorithm should indeed combine these two features\nin order to assure numerical stability. Moreover, an explicit bound for the\nnumber of outer scenarios necessary to guarantee a prescribed degree of\nnumerical stability is derived. We embed our observations into a coherent\npresentation of the theoretical background of LSMC in the insurance setting.\n"
    },
    {
        "paper_id": 1811.08604,
        "authors": "Christopher Kath, Florian Ziel",
        "title": "The value of forecasts: Quantifying the economic gains of accurate\n  quarter-hourly electricity price forecasts",
        "comments": null,
        "journal-ref": "Energy Economics, 76 (2018) 411-423",
        "doi": "10.1016/j.eneco.2018.10.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a multivariate elastic net regression forecast model for German\nquarter-hourly electricity spot markets. While the literature is diverse on\nday-ahead prediction approaches, both the intraday continuous and intraday\ncall-auction prices have not been studied intensively with a clear focus on\npredictive power. Besides electricity price forecasting, we check for the\nimpact of early day-ahead (DA) EXAA prices on intraday forecasts. Another\nnovelty of this paper is the complementary discussion of economic benefits. A\nprecise estimation is worthless if it cannot be utilized. We elaborate possible\ntrading decisions based upon our forecasting scheme and analyze their monetary\neffects. We find that even simple electricity trading strategies can lead to\nsubstantial economic impact if combined with a decent forecasting technique.\n"
    },
    {
        "paper_id": 1811.08706,
        "authors": "Cyril B\\'en\\'ezet, J\\'er\\'emie Bonnefoy, Jean-Fran\\c{c}ois\n  Chassagneux, Shuoqing Deng, Camilo Garcia Trillos, Lionel Len\\^otre",
        "title": "A sparse grid approach to balance sheet risk measurement",
        "comments": "27 pages, 7 figures. CEMRACS 2017",
        "journal-ref": "ESAIM: PROCEEDINGS AND SURVEYS, February 2019, Vol. 65, p. 236-265",
        "doi": "10.1051/proc/201965236",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we present a numerical method based on a sparse grid\napproximation to compute the loss distribution of the balance sheet of a\nfinancial or an insurance company. We first describe, in a stylised way, the\nassets and liabilities dynamics that are used for the numerical estimation of\nthe balance sheet distribution. For the pricing and hedging model, we chose a\nclassical Black & Scholes model with a stochastic interest rate following a\nHull & White model. The risk management model describing the evolution of the\nparameters of the pricing and hedging model is a Gaussian model. The new\nnumerical method is compared with the traditional nested simulation approach.\nWe review the convergence of both methods to estimate the risk indicators under\nconsideration. Finally, we provide numerical results showing that the sparse\ngrid approach is extremely competitive for models with moderate dimension.\n"
    },
    {
        "paper_id": 1811.08726,
        "authors": "Jian-Huang She, and Dan Grecu",
        "title": "Neural Network for CVA: Learning Future Values",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new challenge to quantitative finance after the recent financial crisis is\nthe study of credit valuation adjustment (CVA), which requires modeling of the\nfuture values of a portfolio. In this paper, following recent work in [Weinan\nE(2017), Han(2017)], we apply deep learning to attack this problem. The future\nvalues are parameterized by neural networks, and the parameters are then\ndetermined through optimization. Two concrete products are studied: Bermudan\nswaption and Mark-to-Market cross-currency swap. We obtain their expected\npositive/negative exposures, and further study the resulting functional form of\nfuture values. Such an approach represents a new framework for modeling XVA,\nand it also sheds new lights on other methods like American Monte Carlo.\n"
    },
    {
        "paper_id": 1811.08773,
        "authors": "Michael S. Harre",
        "title": "Entropy and Transfer Entropy: The Dow Jones and the build up to the 1997\n  Asian Crisis",
        "comments": "11 pages, 5 figures. econophysics conference",
        "journal-ref": "Proceedings of the International Conference on Social Modeling and\n  Simulation, plus Econophysics Colloquium, 2014, pages 15-25",
        "doi": "10.1007/978-3-319-20591-5_2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entropy measures in their various incarnations play an important role in the\nstudy of stochastic time series providing important insights into both the\ncorrelative and the causative structure of the stochastic relationships between\nthe individual components of a system. Recent applications of entropic\ntechniques and their linear progenitors such as Pearson correlations and\nGranger causality have have included both normal as well as critical periods in\na system's dynamical evolution. Here I measure the entropy, Pearson correlation\nand transfer entropy of the intra-day price changes of the Dow Jones Industrial\nAverage in the period immediately leading up to and including the Asian\nfinancial crisis and subsequent mini-crash of the DJIA on the 27th October\n1997. I use a novel variation of transfer entropy that dynamically adjusts to\nthe arrival rate of individual prices and does not require the binning of data\nto show that quite different relationships emerge from those given by the\nconventional Pearson correlations between equities. These preliminary results\nillustrate how this modified form of the TE compares to results using Pearson\ncorrelation.\n"
    },
    {
        "paper_id": 1811.08782,
        "authors": "Ali Al-Aradi, Adolfo Correia, Danilo Naiff, Gabriel Jardim, Yuri\n  Saporito",
        "title": "Solving Nonlinear and High-Dimensional Partial Differential Equations\n  via Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we apply the Deep Galerkin Method (DGM) described in Sirignano\nand Spiliopoulos (2018) to solve a number of partial differential equations\nthat arise in quantitative finance applications including option pricing,\noptimal execution, mean field games, etc. The main idea behind DGM is to\nrepresent the unknown function of interest using a deep neural network. A key\nfeature of this approach is the fact that, unlike other commonly used numerical\napproaches such as finite difference methods, it is mesh-free. As such, it does\nnot suffer (as much as other numerical methods) from the curse of\ndimensionality associated with highdimensional PDEs and PDE systems. The main\ngoals of this paper are to elucidate the features, capabilities and limitations\nof DGM by analyzing aspects of its implementation for a number of different\nPDEs and PDE systems. Additionally, we present: (1) a brief overview of PDEs in\nquantitative finance along with numerical methods for solving them; (2) a brief\noverview of deep learning and, in particular, the notion of neural networks;\n(3) a discussion of the theoretical foundations of DGM with a focus on the\njustification of why this method is expected to perform well.\n"
    },
    {
        "paper_id": 1811.08808,
        "authors": "Ben Hambly and Nikolaos Kolliopoulos",
        "title": "Fast mean-reversion asymptotics for large portfolios of stochastic\n  volatility models",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": "10.1007/s00780-020-00422-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an SPDE description of a large portfolio limit model where the\nunderlying asset prices evolve according to certain stochastic volatility\nmodels with default upon hitting a lower barrier. The asset prices and their\nvolatilities are correlated via systemic Brownian motions, and the resulting\nSPDE is defined on the positive half-space with Dirichlet boundary conditions.\nWe study the convergence of the loss from the system, a function of the total\nmass of a solution to this stochastic initial-boundary value problem under fast\nmean reversion of the volatility. We consider two cases. In the first case the\nvolatility converges to a limiting distribution and the convergence of the\nsystem is in the sense of weak convergence. On the other hand, when only the\nmean reversion of the volatility goes to infinity we see a stronger form of\nconvergence of the system to its limit. Our results show that in a fast\nmean-reverting volatility environment we can accurately estimate the\ndistribution of the loss from a large portfolio by using an approximate\nconstant volatility model which is easier to handle.\n"
    },
    {
        "paper_id": 1811.08949,
        "authors": "Ruoxi Lu, David A. Bessler and David J. Leatham",
        "title": "The transmission of liquidity shocks via China's segmented money market:\n  evidence from recent market events",
        "comments": "38 pages, 13 figures, 1 table, in press",
        "journal-ref": "Lu, R., et al. The transmission of liquidity shocks via China's\n  segmented money market: Evidence from recent market events. J. Int. Financ.\n  Markets Inst. Money (2018)",
        "doi": "10.1016/j.intfin.2018.07.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is the first study to explore the transmission paths for liquidity\nshocks in China's segmented money market. We examine how money market\ntransactions create such pathways between China's closely-guarded banking\nsector and the rest of its financial system, and empirically capture the\ntransmission of liquidity shocks through these pathways during two recent\nmarket events. We find strong indications that money market transactions allow\nliquidity shocks to circumvent certain regulatory restrictions and financial\nmarket segmentation in China. Our findings suggest that a widespread\nilliquidity contagion facilitated by money market transactions can happen in\nChina and new policy measures are needed to prevent such contagion.\n"
    },
    {
        "paper_id": 1811.09004,
        "authors": "Patrick Asuming, Hyuncheol Bryant Kim, and Armand Sim",
        "title": "Long-run Consequences of Health Insurance Promotion When Mandates are\n  Not Enforceable: Evidence from a Field Experiment in Ghana",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study long-run selection and treatment effects of a health insurance\nsubsidy in Ghana, where mandates are not enforceable. We randomly provide\ndifferent levels of subsidy (1/3, 2/3, and full), with follow-up surveys seven\nmonths and three years after the initial intervention. We find that a one-time\nsubsidy promotes and sustains insurance enrollment for all treatment groups,\nbut long-run health care service utilization increases only for the partial\nsubsidy groups. We find evidence that selection explains this pattern: those\nwho were enrolled due to the subsidy, especially the partial subsidy, are more\nill and have greater health care utilization.\n"
    },
    {
        "paper_id": 1811.09257,
        "authors": "Tat Lung Chan and Nicholas Hale",
        "title": "Hedging and Pricing European-type, Early-Exercise and Discrete Barrier\n  Options using Algorithm for the Convolution of Legendre Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper applies an algorithm for the convolution of compactly supported\nLegendre series (the CONLeg method) (cf. Hale and Townsend 2014a), to\npricing/hedging European-type, early-exercise and discrete-monitored barrier\noptions under a Levy process. The paper employs Chebfun (cf. Trefethen et al.\n2014) in computational finance and provides a quadrature-free approach by\napplying the Chebyshev series in financial modelling. A significant advantage\nof using the CONLeg method is to formulate option pricing and option Greek\ncurves rather than individual prices/values. Moreover, the CONLeg method can\nyield high accuracy in option pricing and hedging when the risk-free smooth\nprobability density function (PDF) is smooth/non-smooth. Finally, we show that\nour method can accurately price/hedge options deep in/out of the money and with\nvery long/short maturities. Compared with existing techniques, the CONLeg\nmethod performs either favourably or comparably in numerical experiments.\n"
    },
    {
        "paper_id": 1811.09309,
        "authors": "Mihnea S. Andrei and John S.J. Hsu",
        "title": "Bayesian Alternatives to the Black-Litterman Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Litterman model combines investors' personal views with historical\ndata and gives optimal portfolio weights. In this paper we will introduce the\noriginal Black-Litterman model (section 1), we will modify the model such that\nit fits in a Bayesian framework by considering the investors' personal views to\nbe a direct prior on the means of the returns and by adding a typical Inverse\nWishart prior on the covariance matrix of the returns (section 2). Lastly, we\nwill use Leonard and Hsu's (1992) idea of adding a prior on the logarithm of\nthe covariance matrix (section 3). Sensitivity simulations for the level of\nconfidence that the investor has in their own personal views were performed and\nperformance of the models was assessed on a test data set consisting of returns\nover the month of January 2018.\n"
    },
    {
        "paper_id": 1811.09312,
        "authors": "Vladim\\'ir Hol\\'y, Petra Tomanov\\'a",
        "title": "Estimation of Ornstein-Uhlenbeck Process Using Ultra-High-Frequency Data\n  with Application to Intraday Pairs Trading Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When stock prices are observed at high frequencies, more information can be\nutilized in estimation of parameters of the price process. However,\nhigh-frequency data are contaminated by the market microstructure noise which\ncauses significant bias in parameter estimation when not taken into account. We\npropose an estimator of the Ornstein-Uhlenbeck process based on the maximum\nlikelihood which is robust to the noise and utilizes irregularly spaced data.\nWe also show that the Ornstein-Uhlenbeck process contaminated by the\nindependent Gaussian white noise and observed at discrete equidistant times\nfollows an ARMA(1,1) process. To illustrate benefits of the proposed\nnoise-robust approach, we introduce a novel intraday pairs trading strategy\nbased on the mean-variance optimization. In an empirical study of 7 Big Oil\ncompanies, we show that the use of the proposed estimator of the\nOrnstein-Uhlenbeck process leads to an increase in profitability of the pairs\ntrading strategy.\n"
    },
    {
        "paper_id": 1811.09475,
        "authors": "Zhu Liu, Bo Zheng, Qiang Zhang",
        "title": "New dynamics of energy use and CO2 emissions in China",
        "comments": "Submitted to Nature, under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global achievement of climate change mitigation will heavy reply on how much\nof CO2 emission has and will be released by China. After rapid growth of\nemissions during last decades, China CO2 emissions declined since 2014 that\ndriven by decreased coal consumption, suggesting a possible peak of China coal\nconsumption and CO2 emissions. Here, by combining a updated methodology and\nunderlying data from different sources, we reported the soaring 5.5% (range:\n+2.5% to +8.5% for one sigma) increase of China CO2 emissions in 2018 compared\nto 2017, suggesting China CO2 is not yet to peak and leaving a big uncertain to\nwhether China emission will continue to rise in the future. Although our best\nestimate of total emission (9.9Gt CO2 in 2018) is lower than international\nagencies in the same year, the results show robust on a record-high energy\nconsumption and total CO2 emission in 2018. During 2014-2016, China energy\nintensity (energy consumption per unit of GDP) and total CO2 emissions has\ndecreased driven by energy and economic structure optimization. However, the\ndecrease in emissions is now offset by stimulates of heavy industry production\nunder economic downturn that driving coal consumption (+5% in 2018), as well as\nthe surging of natural gas consumption (+18% in 2018) due to the government led\ncoal-to-gas energy transition to reduce local air pollutions. Timing policy and\nactions are urgent needed to address on these new drivers to turn down the\ntotal emission growth trend.\n"
    },
    {
        "paper_id": 1811.09549,
        "authors": "Vangelis Bacoyannis, Vacslav Glukhov, Tom Jin, Jonathan Kochems, Doo\n  Re Song",
        "title": "Idiosyncrasies and challenges of data driven learning in electronic\n  trading",
        "comments": "Accepted for NIPS 2018 Workshop on Challenges and Opportunities for\n  AI in Financial Services: the Impact of Fairness, Explainability, Accuracy,\n  and Privacy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We outline the idiosyncrasies of neural information processing and machine\nlearning in quantitative finance. We also present some of the approaches we\ntake towards solving the fundamental challenges we face.\n"
    },
    {
        "paper_id": 1811.09615,
        "authors": "Mitja Stadje",
        "title": "Representation Results for Law Invariant Recursive Dynamic Deviation\n  Measures and Risk Sharing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyze a dynamic recursive extension of the (static) notion\nof a deviation measure and its properties. We study distribution invariant\ndeviation measures and show that the only dynamic deviation measure which is\nlaw invariant and recursive is the variance. We also solve the problem of\noptimal risk-sharing generalizing classical risk-sharing results for variance\nthrough a dynamic inf-convolution problem involving a transformation of the\noriginal dynamic deviation measures.\n"
    },
    {
        "paper_id": 1811.09622,
        "authors": "J. Cerda-Hern\\'andez and A. Sikov",
        "title": "Lee-Carter method for forecasting mortality for Peruvian Population",
        "comments": "16 pages, 6 figures",
        "journal-ref": "Selecciones Matematicas (2021)",
        "doi": "10.17268/sel.mat.2021.01.05",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we have modeled mortality rates of Peruvian female and male\npopulations during the period of 1950-2017 using the Lee-Carter (LC) model. The\nstochastic mortality model was introduced by Lee and Carter (1992) and has been\nused by many authors for fitting and forecasting the human mortality rates. The\nSingular Value Decomposition (SVD) approach is used for estimation of the\nparameters of the LC model. Utilizing the best fitted auto regressive\nintegrated moving average (ARIMA) model we forecast the values of the time\ndependent parameter of the LC model for the next thirty years. The forecasted\nvalues of life expectancy at different age group with $95\\%$ confidence\nintervals are also reported for the next thirty years. In this research we use\nthe data, obtained from the Peruvian National Institute of Statistics (INEI).\n"
    },
    {
        "paper_id": 1811.09921,
        "authors": "Huaxiong Huang, Moshe A. Milevsky, Thomas S. Salisbury",
        "title": "Retirement spending and biological age",
        "comments": null,
        "journal-ref": "J. Econom. Dynam. Control 84 (2017), 58-76",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve a lifecycle model in which the consumer's chronological age does not\nmove in lockstep with calendar time. Instead, biological age increases at a\nstochastic non-linear rate in time like a broken clock that might occasionally\nmove backwards. In other words, biological age could actually decline. Our\npaper is inspired by the growing body of medical literature that has identified\nbiomarkers which indicate how people age at different rates. This offers better\nestimates of expected remaining lifetime and future mortality rates. It isn't\nfarfetched to argue that in the not-too-distant future personal age will be\nmore closely associated with biological vs. calendar age. Thus, after\nintroducing our stochastic mortality model we derive optimal consumption rates\nin a classic Yaari (1965) framework adjusted to our proper clock time. In\naddition to the normative implications of having access to biological age, our\npositive objective is to partially explain the cross-sectional heterogeneity in\nretirement spending rates at any given chronological age. In sum, we argue that\nneither biological nor chronological age alone is a sufficient statistic for\nmaking economic decisions. Rather, both ages are required to behave rationally.\n"
    },
    {
        "paper_id": 1811.09932,
        "authors": "Moshe A. Milevsky, Thomas S. Salisbury, Alexander Chigodaev",
        "title": "The implied longevity curve: How long does the market think you are\n  going to live?",
        "comments": null,
        "journal-ref": "J. Investment Consulting 17 (2016), 11-21",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use life annuity prices to extract information about human longevity using\na framework that links the term structure of mortality and interest rates. We\ninvert the model and perform nonlinear least squares to obtain implied\nlongevity forecasts. Methodologically, we assume a Cox-Ingersoll-Ross (CIR)\nmodel for the underlying yield curve, and for mortality, a Gompertz-Makeham\n(GM) law that varies with the year of annuity purchase. Our main result is that\nover the last decade markets implied an improvement in longevity of of 6-7\nweeks per year for males and 1-3 weeks for females. In the year 2004 market\nprices implied a $40.1\\%$ probability of survival to the age 90 for a 75-year\nold male ($51.2\\%$ for a female) annuitant. By the year 2013 the implied\nsurvival probability had increased to $46.1\\%$ (and $53.1\\%$). The\ncorresponding implied life expectancy has increased (at the age of 75) from\n13.09 years for males (15.08 years for females) to 14.28 years (and 15.61\nyears.) Although these values are implied directly from markets, they are\nconsistent with demographic projections. Similar to implied volatility in\noption pricing, we believe that our implied survival probabilities (ISP) and\nimplied life expectancy (ILE) are relevant for the financial management of\nassets post-retirement and very important for the optimal timing and allocation\nto annuities; procrastinators are swimming against an uncertain but rather\nstrong longevity trend.\n"
    },
    {
        "paper_id": 1811.10041,
        "authors": "Zihao Zhang, Stefan Zohren, Stephen Roberts",
        "title": "BDLOB: Bayesian Deep Convolutional Neural Networks for Limit Order Books",
        "comments": "6 pages, 4 figures, 1 table, Third workshop on Bayesian Deep Learning\n  (NeurIPS 2018)",
        "journal-ref": "Third workshop on Bayesian Deep Learning (NeurIPS 2018)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We showcase how dropout variational inference can be applied to a large-scale\ndeep learning model that predicts price movements from limit order books\n(LOBs), the canonical data source representing trading and pricing movements.\nWe demonstrate that uncertainty information derived from posterior predictive\ndistributions can be utilised for position sizing, avoiding unnecessary trades\nand improving profits. Further, we test our models by using millions of\nobservations across several instruments and markets from the London Stock\nExchange. Our results suggest that those Bayesian techniques not only deliver\nuncertainty information that can be used for trading but also improve\npredictive performance as stochastic regularisers. To the best of our\nknowledge, we are the first to apply Bayesian networks to LOBs.\n"
    },
    {
        "paper_id": 1811.10109,
        "authors": "Jiahua Xu, Benjamin Livshits",
        "title": "The Anatomy of a Cryptocurrency Pump-and-Dump Scheme",
        "comments": null,
        "journal-ref": "Proceedings of the 28th USENIX Security Symposium (2019) 1609-1625",
        "doi": "10.5555/3361338.3361450",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While pump-and-dump schemes have attracted the attention of cryptocurrency\nobservers and regulators alike, this paper represents the first detailed\nempirical query of pump-and-dump activities in cryptocurrency markets. We\npresent a case study of a recent pump-and-dump event, investigate 412\npump-and-dump activities organized in Telegram channels from June 17, 2018 to\nFebruary 26, 2019, and discover patterns in crypto-markets associated with\npump-and-dump schemes. We then build a model that predicts the pump likelihood\nof all coins listed in a crypto-exchange prior to a pump. The model exhibits\nhigh precision as well as robustness, and can be used to create a simple, yet\nvery effective trading strategy, which we empirically demonstrate can generate\na return as high as 60% on small retail investments within a span of two and\nhalf months. The study provides a proof of concept for strategic crypto-trading\nand sheds light on the application of machine learning for crime detection.\n"
    },
    {
        "paper_id": 1811.10195,
        "authors": "Jonathan Manfield, Derek Lukacsko and Th\\'arsis T. P. Souza",
        "title": "Bull Bear Balance: A Cluster Analysis of Socially Informed Financial\n  Volatility",
        "comments": null,
        "journal-ref": "2017 IEEE Computing Conference, London, 2017, pp. 421-428",
        "doi": "10.1109/SAI.2017.8252134",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Using a method rooted in information theory, we present results that have\nidentified a large set of stocks for which social media can be informative\nregarding financial volatility. By clustering stocks based on the joint feature\nsets of social and financial variables, our research provides an important\ncontribution by characterizing the conditions in which social media signals can\nlead financial volatility. The results indicate that social media is most\ninformative about financial market volatility when the ratio of bullish to\nbearish sentiment is high, even when the number of messages is low. The\nrobustness of these findings is verified across 500 stocks from both NYSE and\nNASDAQ exchanges. The reported results are reproducible via an open-source\nlibrary for social-financial analysis made freely available.\n"
    },
    {
        "paper_id": 1811.10552,
        "authors": "Karol Horodecki, Maciej Stankiewicz",
        "title": "Semi-Device Independent Quantum Money",
        "comments": "26 pages, 4 figures, 1 table, added impossibility result",
        "journal-ref": "2020 New J. Phys. 22 023007",
        "doi": "10.1088/1367-2630/ab6872",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The seminal idea of quantum money not forgeable due to laws of Quantum\nMechanics proposed by Stephen Wiesner, has laid foundations for the Quantum\nInformation Theory in early '70s. Recently, several other schemes for quantum\ncurrencies have been proposed, all however relying on the assumption that the\nmint does not cooperate with the counterfeiter. Drawing inspirations from the\nsemi-device independent quantum key distribution protocol, we introduce the\nfirst scheme of quantum money with this assumption partially relaxed, along\nwith the proof of its unforgeability. Significance of this protocol is\nsupported by an impossibility result, which we prove, stating that there is no\nboth fully device independent and secure money scheme. Finally, we formulate a\nquantum analogue of the Oresme-Copernicus-Gresham's law of economy.\n"
    },
    {
        "paper_id": 1811.10935,
        "authors": "Paul Gassiat",
        "title": "On the martingale property in the rough Bergomi model",
        "comments": "8 pages, minor corrections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of fractional stochastic volatility models (including the\nso-called rough Bergomi model), where the volatility is a superlinear function\nof a fractional Gaussian process. We show that the stock price is a true\nmartingale if and only if the correlation $\\rho$ between the driving Brownian\nmotions of the stock and the volatility is nonpositive. We also show that for\neach $\\rho<0$ and $m> \\frac{1}{{1-\\rho^2}}$, the $m$-th moment of the stock\nprice is infinite at each positive time.\n"
    },
    {
        "paper_id": 1811.10993,
        "authors": "Peter Shnurkov, Daniil Novikov",
        "title": "Analysis of the problem of intervention control in the economy on the\n  basis of solving the problem of tuning",
        "comments": "15 pages, 1 figure. Keywords: controlled stochastic processes,\n  absorbing Markov chains, stochastic problem of tuning, mathematical models of\n  economic interventions, control in technical systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a new stochastic intervention control model conducted in\nvarious commodity and stock markets. The essence of the phenomenon of\nintervention is described in accordance with current economic theory. A review\nof papers on intervention research has been made. A general construction of the\nstochastic intervention model was developed as a Markov process with discrete\ntime, controlled at the time it hits the boundary of a given subset of a set of\nstates. Thus, the problem of optimal control of interventions is reduced to a\ntheoretical problem of control by the specified process or the problem of\ntuning. A general solution of the tuning problem for a model with discrete time\nis obtained. It is proved that the optimal control in such a problem is\ndeterministic and is determined by the global maximum point of the function of\ntwo discrete variables, for which an explicit analytical representation is\nobtained. It is noted that the solution of the stochastic tuning problem can be\nused as a basis for solving control problems of various technical systems in\nwhich there is a need to maintain some main parameter in a given set of its\nvalues.\n"
    },
    {
        "paper_id": 1811.11079,
        "authors": "Suproteem K. Sarkar, Kojin Oshiba, Daniel Giebisch, Yaron Singer",
        "title": "Robust Classification of Financial Risk",
        "comments": "NIPS 2018 Workshop on Challenges and Opportunities for AI in\n  Financial Services",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algorithms are increasingly common components of high-impact decision-making,\nand a growing body of literature on adversarial examples in laboratory settings\nindicates that standard machine learning models are not robust. This suggests\nthat real-world systems are also susceptible to manipulation or\nmisclassification, which especially poses a challenge to machine learning\nmodels used in financial services. We use the loan grade classification problem\nto explore how machine learning models are sensitive to small changes in\nuser-reported data, using adversarial attacks documented in the literature and\nan original, domain-specific attack. Our work shows that a robust optimization\nalgorithm can build models for financial services that are resistant to\nmisclassification on perturbations. To the best of our knowledge, this is the\nfirst study of adversarial attacks and defenses for deep learning in financial\nservices.\n"
    },
    {
        "paper_id": 1811.11265,
        "authors": "Claudio Bellani, Damiano Brigo, Alex Done and Eyal Neuman",
        "title": "Static vs Adaptive Strategies for Optimal Execution with Signals",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare optimal static and dynamic solutions in trade execution. An\noptimal trade execution problem is considered where a trader is looking at a\nshort-term price predictive signal while trading. When the trader creates an\ninstantaneous market impact, it is shown that transaction costs of optimal\nadaptive strategies are substantially lower than the corresponding costs of the\noptimal static strategy. In the same spirit, in the case of transient impact it\nis shown that strategies that observe the signal a finite number of times can\ndramatically reduce the transaction costs and improve the performance of the\noptimal static strategy.\n"
    },
    {
        "paper_id": 1811.11287,
        "authors": "Ben Moews, J. Michael Herrmann, Gbenga Ibikunle",
        "title": "Lagged correlation-based deep learning for directional trend change\n  prediction in financial time series",
        "comments": "11 pages, 4 figures",
        "journal-ref": "Expert Syst. Appl. 120 (2019) 197-206",
        "doi": "10.1016/j.eswa.2018.11.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trend change prediction in complex systems with a large number of noisy time\nseries is a problem with many applications for real-world phenomena, with stock\nmarkets as a notoriously difficult to predict example of such systems. We\napproach predictions of directional trend changes via complex lagged\ncorrelations between them, excluding any information about the target series\nfrom the respective inputs to achieve predictions purely based on such\ncorrelations with other series. We propose the use of deep neural networks that\nemploy step-wise linear regressions with exponential smoothing in the\npreparatory feature engineering for this task, with regression slopes as trend\nstrength indicators for a given time interval. We apply this method to\nhistorical stock market data from 2011 to 2016 as a use case example of lagged\ncorrelations between large numbers of time series that are heavily influenced\nby externally arising new information as a random factor. The results\ndemonstrate the viability of the proposed approach, with state-of-the-art\naccuracies and accounting for the statistical significance of the results for\nadditional validation, as well as important implications for modern financial\neconomics.\n"
    },
    {
        "paper_id": 1811.11301,
        "authors": "Matthew Norton, Valentyn Khokhlov, Stan Uryasev",
        "title": "Calculating CVaR and bPOE for Common Probability Distributions With\n  Application to Portfolio Optimization and Density Estimation",
        "comments": "Fixed typo in Proposition 5 (changed - to +) and added reference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR), also called the\nsuperquantile and quantile, are frequently used to characterize the tails of\nprobability distribution's and are popular measures of risk. Buffered\nProbability of Exceedance (bPOE) is a recently introduced characterization of\nthe tail which is the inverse of CVaR, much like the CDF is the inverse of the\nquantile. These quantities can prove very useful as the basis for a variety of\nrisk-averse parametric engineering approaches. Their use, however, is often\nmade difficult by the lack of well-known closed-form equations for calculating\nthese quantities for commonly used probability distribution's. In this paper,\nwe derive formulas for the superquantile and bPOE for a variety of common\nunivariate probability distribution's. Besides providing a useful collection\nwithin a single reference, we use these formulas to incorporate the\nsuperquantile and bPOE into parametric procedures. In particular, we consider\ntwo: portfolio optimization and density estimation. First, when portfolio\nreturns are assumed to follow particular distribution families, we show that\nfinding the optimal portfolio via minimization of bPOE has advantages over\nsuperquantile minimization. We show that, given a fixed threshold, a single\nportfolio is the minimal bPOE portfolio for an entire class of distribution's\nsimultaneously. Second, we apply our formulas to parametric density estimation\nand propose the method of superquantile's (MOS), a simple variation of the\nmethod of moment's (MM) where moment's are replaced by superquantile's at\ndifferent confidence levels. With the freedom to select various combinations of\nconfidence levels, MOS allows the user to focus the fitting procedure on\ndifferent portions of the distribution, such as the tail when fitting\nheavy-tailed asymmetric data.\n"
    },
    {
        "paper_id": 1811.11326,
        "authors": "Moshe A. Milevsky",
        "title": "Swimming with Wealthy Sharks: Longevity, Volatility and the Value of\n  Risk Pooling",
        "comments": "57 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Who {\\em values} life annuities more? Is it the healthy retiree who expects\nto live long and might become a centenarian, or is the unhealthy retiree with a\nshort life expectancy more likely to appreciate the pooling of longevity risk?\nWhat if the unhealthy retiree is pooled with someone who is much healthier and\nthus forced to pay an implicit loading? To answer these and related questions\nthis paper examines the empirical conditions under which retirees benefit (or\nmay not) from longevity risk pooling by linking the {\\em economics} of annuity\nequivalent wealth (AEW) to {\\em actuarially} models of aging. I focus attention\non the {\\em Compensation Law of Mortality} which implies that individuals with\nhigher relative mortality (e.g. lower income) age more slowly and experience\ngreater longevity uncertainty. Ergo, they place higher utility value on the\nannuity. The impetus for this research today is the increasing evidence on the\ngrowing disparity in longevity expectations between rich and poor.\n"
    },
    {
        "paper_id": 1811.11379,
        "authors": "Anindya Goswami, Omkar Manjarekar, and Anjana R",
        "title": "Option Pricing in a Regime Switching Jump Diffusion Model",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the solution to a European option pricing problem by\nconsidering a regime-switching jump diffusion model of the underlying financial\nasset price dynamics. The regimes are assumed to be the results of an observed\npure jump process, driving the values of interest rate and volatility\ncoefficient. The pure jump process is assumed to be a semi-Markov process on\nfinite state space. This consideration helps to incorporate a specific type of\nmemory influence in the asset price. Under this model assumption, the locally\nrisk minimizing price of the European type path-independent options is found.\nThe F\\\"{o}llmer-Schweizer decomposition is adopted to show that the option\nprice satisfies an evolution problem, as a function of time, stock price,\nmarket regime, and the stagnancy period. To be more precise, the evolution\nproblem involves a linear, parabolic, degenerate and non-local system of\nintegro-partial differential equations. We have established existence and\nuniqueness of classical solution to the evolution problem in an appropriate\nclass.\n"
    },
    {
        "paper_id": 1811.11476,
        "authors": "Thomas Kopp, Jan Salecker",
        "title": "Modelling Social Evolutionary Processes and Peer Effects in Agricultural\n  Trade Networks: the Rubber Value Chain in Indonesia",
        "comments": "Main part 16 pages; references + appendix: 15 pages; including 11\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding market participants' channel choices is important to policy\nmakers because it yields information on which channels are effective in\ntransmitting information. These channel choices are the result of a recursive\nprocess of social interactions and determine the observable trading networks.\nThey are characterized by feedback mechanisms due to peer interaction and\ntherefore need to be understood as complex adaptive systems (CAS). When\nmodelling CAS, conventional approaches like regression analyses face severe\ndrawbacks since endogeneity is omnipresent. As an alternative, process-based\nanalyses allow researchers to capture these endogenous processes and multiple\nfeedback loops. This paper applies an agent-based modelling approach (ABM) to\nthe empirical example of the Indonesian rubber trade. The feedback mechanisms\nare modelled via an innovative approach of a social matrix, which allows\ndecisions made in a specific period to feed back into the decision processes in\nsubsequent periods, and allows agents to systematically assign different\nweights to the decision parameters based on their individual characteristics.\nIn the validation against the observed network, uncertainty in the found\nestimates, as well as under determination of the model, are dealt with via an\napproach of evolutionary calibration: a genetic algorithm finds the combination\nof parameters that maximizes the similarity between the simulated and the\nobserved network. Results indicate that the sellers' channel choice decisions\nare mostly driven by physical distance and debt obligations, as well as\npeer-interaction. Within the social matrix, the most influential individuals\nare sellers that live close by to other traders, are active in social groups\nand belong to the ethnic majority in their village.\n"
    },
    {
        "paper_id": 1811.11618,
        "authors": "Eric Benhamou",
        "title": "Kalman filter demystified: from intuition to probabilistic graphical\n  model to real case in financial markets",
        "comments": "44 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we revisit the Kalman filter theory. After giving the\nintuition on a simplified financial markets example, we revisit the maths\nunderlying it. We then show that Kalman filter can be presented in a very\ndifferent fashion using graphical models. This enables us to establish the\nconnection between Kalman filter and Hidden Markov Models. We then look at\ntheir application in financial markets and provide various intuitions in terms\nof their applicability for complex systems such as financial markets. Although\nthis paper has been written more like a self contained work connecting Kalman\nfilter to Hidden Markov Models and hence revisiting well known and establish\nresults, it contains new results and brings additional contributions to the\nfield. First, leveraging on the link between Kalman filter and HMM, it gives\nnew algorithms for inference for extended Kalman filters. Second, it presents\nan alternative to the traditional estimation of parameters using EM algorithm\nthanks to the usage of CMA-ES optimization. Third, it examines the application\nof Kalman filter and its Hidden Markov models version to financial markets,\nproviding various dynamics assumptions and tests. We conclude by connecting\nKalman filter approach to trend following technical analysis system and showing\ntheir superior performances for trend following detection.\n"
    },
    {
        "paper_id": 1811.11621,
        "authors": "Christoph K\\\"uhn and Alexander Molitor",
        "title": "Prospective strict no-arbitrage and the fundamental theorem of asset\n  pricing under transaction costs",
        "comments": null,
        "journal-ref": "Finance Stoch. 23 (2019) 1049-1077",
        "doi": "10.1007/s00780-019-00403-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In discrete time markets with proportional transaction costs, Schachermayer\n(2004) shows that robust no-arbitrage is equivalent to the existence of a\nstrictly consistent price system.\n  In this paper, we introduce the concept of prospective strict no-arbitrage\nthat is a variant of the strict no-arbitrage property from Kabanov, R\\'asonyi,\nand Stricker (2002). The prospective strict no-arbitrage condition is slightly\nweaker than robust no-arbitrage, and it implies that the set of portfolios\nattainable from zero initial endowment is closed in probability. A weak version\nof prospective strict no-arbitrage turns out to be equivalent to the existence\nof a consistent price system. In contrast to the fundamental theorem of asset\npricing of Schachermayer (2004), the consistent frictionless prices may lie on\nthe boundary of the bid-ask spread.\n  On the technical level, a crucial difference to Schachermayer (2004) and\nKabanov-R\\'asonyi-Stricker (2003) is that we prove closedness without having at\nhand that the null-strategies form a linear space.\n"
    },
    {
        "paper_id": 1811.11664,
        "authors": "Mark Whitmeyer",
        "title": "Dynamic Competitive Persuasion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Two long-lived senders play a dynamic game of competitive persuasion. Each\nperiod, each provides information to a single short-lived receiver. When the\nsenders also set prices, we unearth a folk theorem: if they are sufficiently\npatient, virtually any vector of feasible and individually rational payoffs can\nbe sustained in a subgame perfect equilibrium. Without price-setting, there is\na unique subgame perfect equilibrium. In it, patient senders provide less\ninformation--maximally patient ones none.\n"
    },
    {
        "paper_id": 1811.12356,
        "authors": "Sean Ledger and Andreas Sojmark",
        "title": "Uniqueness for contagious McKean--Vlasov systems in the weak feedback\n  regime",
        "comments": "20 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1112/blms.12337",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple uniqueness argument for a collection of McKean-Vlasov\nproblems that have seen recent interest. Our first result shows that, in the\nweak feedback regime, there is global uniqueness for a very general class of\nrandom drivers. By weak feedback we mean the case where the contagion\nparameters are small enough to prevent blow-ups in solutions. Next, we\nspecialise to a Brownian driver and show how the same techniques can be\nextended to give short-time uniqueness after blow-ups, regardless of the\nfeedback strength. The heart of our approach is a surprisingly simple\nprobabilistic comparison argument that is robust in the sense that it does not\nask for any regularity of the solutions.\n"
    },
    {
        "paper_id": 1811.12491,
        "authors": "Mikhail Zhitlukhin",
        "title": "Survival investment strategies in a continuous-time market model with\n  competition",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic game-theoretic model of an investment market in\ncontinuous time with short-lived assets and study strategies, called survival,\nwhich guarantee that the relative wealth of an investor who uses such a\nstrategy remains bounded away from zero. The main results consist in obtaining\na sufficient condition for a strategy to be survival and showing that all\nsurvival strategies are asymptotically close to each other. It is also proved\nthat a survival strategy allows an investor to accumulate wealth in a certain\nsense faster than competitors.\n"
    },
    {
        "paper_id": 1812.00032,
        "authors": "Gabriel Khan, Jun Zhang",
        "title": "On the K\\\"ahler Geometry of Certain Optimal Transport Problems",
        "comments": "30 pages. In the previous versions, there was a switched index in the\n  curvature formulas. We have fixed the issue in this version",
        "journal-ref": "Pure Appl. Analysis 2 (2020) 397-426",
        "doi": "10.2140/paa.2020.2.397",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $X$ and $Y$ be domains of $\\mathbb{R}^n$ equipped with respective\nprobability measures $\\mu$ and $ \\nu$. We consider the problem of optimal\ntransport from $\\mu$ to $\\nu$ with respect to a cost function $c: X \\times Y\n\\to \\mathbb{R}$. To ensure that the solution to this problem is smooth, it is\nnecessary to make several assumptions about the structure of the domains and\nthe cost function. In particular, Ma, Trudinger, and Wang established\nregularity estimates when the domains are strongly \\textit{relatively\n$c$-convex} with respect to each other and cost function has non-negative\n\\textit{MTW tensor}. For cost functions of the form $c(x,y)= \\Psi(x-y)$ for\nsome convex function $\\Psi$, we find an associated K\\\"ahler manifold whose\northogonal anti-bisectional curvature is proportional to the MTW tensor. We\nalso show that relative $c$-convexity geometrically corresponds to geodesic\nconvexity with respect to a dual affine connection. Taken together, these\nresults provide a geometric framework for optimal transport which is\ncomplementary to the pseudo-Riemannian theory of Kim and McCann.\n  We provide several applications of this work. In particular, we find a\ncomplete K\\\"ahler surface with non-negative orthogonal bisectional curvature\nthat is not a Hermitian symmetric space or biholomorphic to $\\mathbb{C}^2$. We\nalso address a question in mathematical finance raised by Pal and Wong on the\nregularity of \\textit{pseudo-arbitrages}, or investment strategies which\noutperform the market.\n"
    },
    {
        "paper_id": 1812.00093,
        "authors": "Lorenz M. Roebers, Aras Selvi, Juan C. Vera",
        "title": "Using Column Generation to Solve Extensions to the Markowitz Model",
        "comments": "16 pages, 3 figures, 2 tables, 1 pseudocode",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a solution scheme for portfolio optimization problems with\ncardinality constraints. Typical portfolio optimization problems are extensions\nof the classical Markowitz mean-variance portfolio optimization model. We solve\nsuch type of problems using a method similar to column generation. In this\nscheme, the original problem is restricted to a subset of the assets resulting\nin a master convex quadratic problem. Then the dual information of the master\nproblem is used in a sub-problem to propose more assets to consider. We also\nconsider other extensions to the Markowitz model to diversify the portfolio\nselection within the given intervals for active weights.\n"
    },
    {
        "paper_id": 1812.00383,
        "authors": "Sean Sylvia, Xiaochen Ma, Yaojiang Shi, Scott Rozelle, and C.-Y.\n  Cynthia Lin Lawell",
        "title": "Ordeal Mechanisms, Information, and the Cost-Effectiveness of Subsidies:\n  Evidence from Subsidized Eyeglasses in Rural China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cost-effectiveness of policies providing subsidized goods is often\ncompromised by limited use of the goods provided. Through a randomized trial,\nwe test two approaches to improve the cost-effectiveness of a program\ndistributing free eyeglasses to myopic children in rural China. Requiring\nrecipients to undergo an ordeal better targeted eyeglasses to those who used\nthem without reducing usage relative to free delivery. An information campaign\nincreased use when eyeglasses were freely delivered but not under an ordeal.\nFree delivery plus information was determined to be the most socially\ncost-effective approach and obtained the highest rate of eyeglass use.\n"
    },
    {
        "paper_id": 1812.00501,
        "authors": "Soham R. Phade and Venkat Anantharam",
        "title": "Optimal Resource Allocation over Networks via Lottery-Based Mechanisms",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-030-16989-3_4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that, in a resource allocation problem, the ex ante aggregate utility\nof players with cumulative-prospect-theoretic preferences can be increased over\ndeterministic allocations by implementing lotteries. We formulate an\noptimization problem, called the system problem, to find the optimal lottery\nallocation. The system problem exhibits a two-layer structure comprised of a\npermutation profile and optimal allocations given the permutation profile. For\nany fixed permutation profile, we provide a market-based mechanism to find the\noptimal allocations and prove the existence of equilibrium prices. We show that\nthe system problem has a duality gap, in general, and that the primal problem\nis NP-hard. We then consider a relaxation of the system problem and derive some\nqualitative features of the optimal lottery structure.\n"
    },
    {
        "paper_id": 1812.00595,
        "authors": "Nikolaus Hautsch, Christoph Scheuch, Stefan Voigt",
        "title": "Building Trust Takes Time: Limits to Arbitrage for Blockchain-Based\n  Assets",
        "comments": "This paper replaces an earlier draft titled \"Limits to Arbitrage in\n  Markets with Stochastic Settlement Latency\". 49 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A blockchain replaces central counterparties with time-consuming consensus\nprotocols to record the transfer of ownership. This settlement latency slows\ncross-exchange trading, exposing arbitrageurs to price risk. Off-chain\nsettlement, instead, exposes arbitrageurs to costly default risk. We show with\nBitcoin network and order book data that cross-exchange price differences\ncoincide with periods of high settlement latency, asset flows chase arbitrage\nopportunities, and price differences across exchanges with low default risk are\nsmaller. Blockchain-based trading thus faces a dilemma: Reliable consensus\nprotocols require time-consuming settlement latency, leading to arbitrage\nlimits. Circumventing such arbitrage costs is possible only by reinstalling\ntrusted intermediation, which mitigates default risk.\n"
    },
    {
        "paper_id": 1812.00773,
        "authors": "Klaus Altendorfer, Thomas Felberbauer, Herbert Jodlbauer",
        "title": "Effects of forecast errors on optimal utilisation in aggregate\n  production planning with stochastic customer demand",
        "comments": "production planning, MRP II, simulation, aggregate production\n  planning, demand and forecast uncertainty",
        "journal-ref": "International Journal of Production Research, 54:12, 3718-3735\n  (2016)",
        "doi": "10.1080/00207543.2016.1162918",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The hierarchical structure of production planning has the advantage of\nassigning different decision variables to their respective time horizons and\ntherefore ensures their manageability. However, the restrictive structure of\nthis top-down approach implying that upper level decisions are the constraints\nfor lower level decisions also has its shortcomings. One problem that occurs is\nthat deterministic mixed integer decision problems are often used for long-term\nplanning, but the real production system faces a set of stochastic influences.\nTherefore, a planned utilisation factor has to be included into this\ndeterministic aggregate planning problem. In practice, this decision is often\nbased on past data and not consciously taken. In this paper, the effect of\nlong-term forecast error on the optimal planned utilisation factor is evaluated\nfor a production system facing stochastic demand and the benefit of exploiting\nthis decision's potential is discussed. Overall costs including capacity,\nbackorder and inventory costs, are determined with simulation for different\nmulti-stage and multi-item production system structures. The results show that\nthe planned utilisation factor used in the aggregate planning problem has a\nhigh influence on optimal costs. Additionally, the negative effect of forecast\nerrors is evaluated and discussed in detail for different production system\nenvironments.\n"
    },
    {
        "paper_id": 1812.00839,
        "authors": "Will Hicks",
        "title": "PT Symmetry, Non-Gaussian Path Integrals, and the Quantum Black-Scholes\n  Equation",
        "comments": "20 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.3390/e21020105",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Accardi-Boukas quantum Black-Scholes framework, provides a means by which\none can apply the Hudson-Parthasarathy quantum stochastic calculus to problems\nin finance. Solutions to these equations can be modelled using nonlocal\ndiffusion processes, via a Kramers-Moyal expansion, and this provides useful\ntools to understand their behaviour. In this paper we develop further links\nbetween quantum stochastic processes, and nonlocal diffusions, by inverting the\nquestion, and showing how certain nonlocal diffusions can be written as quantum\nstochastic processes. We then go on to show how one can use path integral\nformalism, and PT symmetric quantum mechanics, to build a non-Gaussian kernel\nfunction for the Accardi-Boukas quantum Black-Scholes. Behaviours observed in\nthe real market are a natural model output, rather than something that must be\ndeliberately included.\n"
    },
    {
        "paper_id": 1812.01102,
        "authors": "Greg Kirczenow, Masoud Hashemi, Ali Fathi and Matt Davison",
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to\n  Illiquid Corporate Bonds",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1806.01731",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an application of machine learning in extracting features\nfrom the historical market implied corporate bond yields. We consider an\nexample of a hypothetical illiquid fixed income market. After choosing a\nsurrogate liquid market, we apply the Denoising Autoencoder (DAE) algorithm to\nlearn the features of the missing yield parameters from the historical data of\nthe instruments traded in the chosen liquid market. The DAE algorithm is then\nchallenged by two \"point-in-time\" inpainting algorithms taken from the image\nprocessing and computer vision domain. It is observed that, when tested on\nunobserved rate surfaces, the DAE algorithm exhibits superior performance\nthanks to the features it has learned from the historical shapes of yield\ncurves.\n"
    },
    {
        "paper_id": 1812.01103,
        "authors": "Th\\'arsis T. P. Souza and Tomaso Aste",
        "title": "Predicting future stock market structure by combining social and\n  financial network information",
        "comments": "18 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.122343",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We demonstrate that future market correlation structure can be predicted with\nhigh out-of-sample accuracy using a multiplex network approach that combines\ninformation from social media and financial data. Market structure is measured\nby quantifying the co-movement of asset prices returns, while social structure\nis measured as the co-movement of social media opinion on those same assets.\nPredictions are obtained with a simple model that uses link persistence and\nlink formation by triadic closure across both financial and social media\nlayers. Results demonstrate that the proposed model can predict future market\nstructure with up to a 40\\% out-of-sample performance improvement compared to a\nbenchmark model that assumes a time-invariant financial correlation structure.\nSocial media information leads to improved models for all settings tested,\nparticularly in the long-term prediction of financial market structure.\nSurprisingly, financial market structure exhibited higher predictability than\nsocial opinion structure.\n"
    },
    {
        "paper_id": 1812.0127,
        "authors": "Giorgio Ferrari, Torben Koch",
        "title": "An Optimal Extraction Problem with Price Impact",
        "comments": "36 pages; 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A price-maker company extracts an exhaustible commodity from a reservoir, and\nsells it instantaneously in the spot market. In absence of any actions of the\ncompany, the commodity's spot price evolves either as a drifted Brownian motion\nor as an Ornstein-Uhlenbeck process. While extracting, the company affects the\nmarket price of the commodity, and its actions have an impact on the dynamics\nof the commodity's spot price. The company aims at maximizing the total\nexpected profits from selling the commodity, net of the total expected\nproportional costs of extraction. We model this problem as a two-dimensional\ndegenerate singular stochastic control problem with finite fuel. To determine\nits solution, we construct an explicit solution to the associated\nHamilton-Jacobi-Bellman equation, and then verify its actual optimality through\na verification theorem. On the one hand, when the (uncontrolled) price is a\ndrifted Brownian motion, it is optimal to extract whenever the current price\nlevel is larger or equal than an endogenously determined constant threshold. On\nthe other hand, when the (uncontrolled) price evolves as an Ornstein-Uhlenbeck\nprocess, we show that the optimal extraction rule is triggered by a curve\ndepending on the current level of the reservoir. Such a curve is a strictly\ndecreasing $C^{\\infty}$-function for which we are able to provide an explicit\nexpression. Finally, our study is complemented by a theoretical and numerical\nanalysis of the dependency of the optimal extraction strategy and value\nfunction on the model's parameters.\n"
    },
    {
        "paper_id": 1812.01341,
        "authors": "Xuan Lu, Li Huang, Kangjuan Lyu",
        "title": "Modelling China's Credit System with Complex Network Theory for\n  Systematic Credit Risk Control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The insufficient understanding of the credit network structure was recognized\nas a key factor for regulators' underestimation of the destructive systematic\nrisk during the financial crisis that started in 2007. The existing credit\nnetwork research either took a macro perspective to clarify the topological\nproperties of financial systems at a descriptive level or analyzed the risk\ntransmission path and characteristics of individual entities with much\npre-assumptions of the network. Here, we used the theory of complex network to\nmodel China's credit system from 2000 to 2014 based on actual financial data. A\nbipartite financial institution-firm network and its projected sub-networks\nwere constructed for an integrated analysis from both macro and micro\nperspectives, and the relationship between typological properties and\nsystematic credit risk control was also explored. The typological analysis of\nthe networks suggested that the financial institutions and firms were highly\nbut asymmetrically connected, and the credit network structure made local\nidiosyncratic shocks possible to proliferate through the whole economy. In\naddition, the Chinese credit market was still dominated by state-owned\nfinancial institutions with firms competing fiercely for financial resources in\nthe past fifteen years. Furthermore, the credit risk score (CRS) was introduced\nby simulation to identify the systematically important vertices in terms of\nsystematic risk control. The results indicated that the vertices with more\naccess to the credit market or less likelihood to be a bridge in the network\nwere the ones with higher systematically importance. The empirical results from\nthis study would provide specific policy suggestions to financial regulators on\nsupervisory approaches and optimizing the allocation of regulatory resources to\nenhance the robustness of credit systems in China and in other countries.\n"
    },
    {
        "paper_id": 1812.01707,
        "authors": "Afifa Alintissar, Abdelkader Intissar, Jean-karim Intissar",
        "title": "On dynamics of wage-price spiral and stagflation in some model economic\n  systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article aims to present an elementary analytical solution to the\nquestion of the formation of a structure of differentiation of rates of return\nin a classical gravitation model and in a model of the dynamics of price-wage\nspirals.\n"
    },
    {
        "paper_id": 1812.01914,
        "authors": "Ying Jiao, Chunhua Ma, Simone Scotti and Chao Zhou",
        "title": "The Alpha-Heston Stochastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an affine extension of the Heston model where the instantaneous\nvariance process contains a jump part driven by $\\alpha$-stable processes with\n$\\alpha\\in(1,2]$. In this framework, we examine the implied volatility and its\nasymptotic behaviors for both asset and variance options. Furthermore, we\nexamine the jump clustering phenomenon observed on the variance market and\nprovide a jump cluster decomposition which allows to analyse the cluster\nprocesses.\n"
    },
    {
        "paper_id": 1812.02298,
        "authors": "Anatoliy Swishchuk and Aiden Huffman",
        "title": "General Compound Hawkes Processes in Limit Order Books",
        "comments": "27 pages, 31 figues, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:1706.07459",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study various new Hawkes processes. Specifically, we\nconstruct general compound Hawkes processes and investigate their properties in\nlimit order books. With regards to these general compound Hawkes processes, we\nprove a Law of Large Numbers (LLN) and a Functional Central Limit Theorems\n(FCLT) for several specific variations. We apply several of these FCLTs to\nlimit order books to study the link between price volatility and order flow,\nwhere the volatility in mid-price changes is expressed in terms of parameters\ndescribing the arrival rates and mid-price process.\n"
    },
    {
        "paper_id": 1812.02311,
        "authors": "Dmitriy Volinskiy (1), Lana Cuthbertson (1), Omid Ardakanian (2) ((1)\n  ATB Financial, (2) University of Alberta)",
        "title": "In (Stochastic) Search of a Fairer Alife",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economies and societal structures in general are complex stochastic systems\nwhich may not lend themselves well to algebraic analysis. An addition of\nsubjective value criteria to the mechanics of interacting agents will further\ncomplicate analysis. The purpose of this short study is to demonstrate\ncapabilities of agent-based computational economics to be a platform for\nfairness or equity analysis in both a broad and practical sense.\n"
    },
    {
        "paper_id": 1812.0234,
        "authors": "Daniel Philps, Tillman Weyde, Artur d'Avila Garcez, Roy Batchelor",
        "title": "Continual Learning Augmented Investment Decisions",
        "comments": "NeurIPS 2018 Workshop on Challenges and Opportunities for AI in\n  Financial Services: the Impact of Fairness, Explainability, Accuracy, and\n  Privacy, Montreal, Canada. This is a non-archival publication - the authors\n  may submit revisions and extensions of this paper to other publication venues",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investment decisions can benefit from incorporating an accumulated knowledge\nof the past to drive future decision making. We introduce Continual Learning\nAugmentation (CLA) which is based on an explicit memory structure and a feed\nforward neural network (FFNN) base model and used to drive long term financial\ninvestment decisions. We demonstrate that our approach improves accuracy in\ninvestment decision making while memory is addressed in an explainable way. Our\napproach introduces novel remember cues, consisting of empirically learned\nchange points in the absolute error series of the FFNN. Memory recall is also\nnovel, with contextual similarity assessed over time by sampling distances\nusing dynamic time warping (DTW). We demonstrate the benefits of our approach\nby using it in an expected return forecasting task to drive investment\ndecisions. In an investment simulation in a broad international equity universe\nbetween 2003-2017, our approach significantly outperforms FFNN base models. We\nalso illustrate how CLA's memory addressing works in practice, using a worked\nexample to demonstrate the explainability of our approach.\n"
    },
    {
        "paper_id": 1812.02371,
        "authors": "Roland Rothenstein",
        "title": "Quantification of market efficiency based on informational-entropy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Since the 1960s, the question whether markets are efficient or not is\ncontroversially discussed. One reason for the difficulty to overcome the\ncontroversy is the lack of a universal, but also precise, quantitative\ndefinition of efficiency that is able to graduate between different states of\nefficiency. The main purpose of this article is to fill this gap by developing\na measure for the efficiency of markets that fulfill all the stated\nrequirements. It is shown that the new definition of efficiency, based on\ninformational-entropy, is equivalent to the two most used definitions of\nefficiency from Fama and Jensen. The new measure therefore enables steps to\nsettle the dispute over the state of efficiency in markets. Moreover, it is\nshown that inefficiency in a market can either arise from the possibility to\nuse information to predict an event with higher than chance level, or can\nemerge from wrong pricing/ quotes that do not reflect the right probabilities\nof possible events. Finally, the calculation of efficiency is demonstrated on a\nsimple game (of coin tossing), to show how one could exactly quantify the\nefficiency in any market-like system, if all probabilities are known.\n"
    },
    {
        "paper_id": 1812.02433,
        "authors": "Gunnhildur H. Steinbakk, Alex Lenkoski, Ragnar Bang Huseby, Anders\n  L{\\o}land, and Tor Arne {\\O}ig{\\aa}rd",
        "title": "Using published bid/ask curves to error dress spot electricity price\n  forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate forecasts of electricity spot prices are essential to the daily\noperational and planning decisions made by power producers and distributors.\nTypically, point forecasts of these quantities suffice, particularly in the\nNord Pool market where the large quantity of hydro power leads to price\nstability. However, when situations become irregular, deviations on the price\nscale can often be extreme and difficult to pinpoint precisely, which is a\nresult of the highly varying marginal costs of generating facilities at the\nedges of the load curve. In these situations it is useful to supplant a point\nforecast of price with a distributional forecast, in particular one whose tails\nare adaptive to the current production regime. This work outlines a methodology\nfor leveraging published bid/ask information from the Nord Pool market to\nconstruct such adaptive predictive distributions. Our methodology is a\nnon-standard application of the concept of error-dressing, which couples a\nfeature driven error distribution in volume space with a non-linear\ntransformation via the published bid/ask curves to obtain highly non-symmetric,\nadaptive price distributions. Using data from the Nord Pool market, we show\nthat our method outperforms more standard forms of distributional modeling. We\nfurther show how such distributions can be used to render `warning systems'\nthat issue reliable probabilities of prices exceeding various important\nthresholds.\n"
    },
    {
        "paper_id": 1812.02527,
        "authors": "Sonam Srivastava, Ritabratta Bhattacharya",
        "title": "Evaluating the Building Blocks of a Dynamically Adaptive Systematic\n  Trading Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets change their behaviours abruptly. The mean, variance and\ncorrelation patterns of stocks can vary dramatically, triggered by fundamental\nchanges in macroeconomic variables, policies or regulations. A trader needs to\nadapt her trading style to make the best out of the different phases in the\nstock markets. Similarly, an investor might want to invest in different asset\nclasses in different market regimes for a stable risk adjusted return profile.\nHere, we explore the use of State Switching Markov Autoregressive models for\nidentifying and predicting different market regimes loosely modeled on the\nWyckoff Price Regimes of accumulation, distribution, advance and decline. We\nexplore the behaviour of various asset classes and market sectors in the\nidentified regimes. We look at the trading strategies like trend following,\nrange trading, retracement trading and breakout trading in the given market\nregimes and tailor them for the specific regimes. We tie together the best\ntrading strategy and asset allocation for the identified market regimes to come\nup with a robust dynamically adaptive trading system to outperform simple\ntraditional alphas.\n"
    },
    {
        "paper_id": 1812.02726,
        "authors": "Maximilian Beikirch, Simon Cramer, Martin Frank, Philipp Otte, Emma\n  Pabich, Torsten Trimborn",
        "title": "Simulation of Stylized Facts in Agent-Based Computational Economic\n  Market Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1801.01811",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the qualitative and quantitative appearance of stylized facts in\nseveral agent-based computational economic market (ABCEM) models. We perform\nour simulations with the SABCEMM (Simulator for Agent-Based Computational\nEconomic Market Models) tool recently introduced by the authors (Trimborn et\nal. 2019). Furthermore, we present novel ABCEM models created by recombining\nexisting models and study them with respect to stylized facts as well. This can\nbe efficiently performed by the SABCEMM tool thanks to its object-oriented\nsoftware design. The code is available on GitHub (Trimborn et al. 2018), such\nthat all results can be reproduced by the reader.\n"
    },
    {
        "paper_id": 1812.02842,
        "authors": "Andres Gomez-Lievano and Oscar Patterson-Lomba",
        "title": "Estimating the drivers of urban economic complexity and their connection\n  to economic performance",
        "comments": "42 pages, 15 figures, 6 tables, 13 appendices",
        "journal-ref": null,
        "doi": "10.1098/rsos.210670",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating the capabilities, or inputs of production, that drive and\nconstrain the economic development of urban areas has remained a challenging\ngoal. We posit that capabilities are instantiated in the complexity and\nsophistication of urban activities, the knowhow of individual workers, and the\ncity-wide collective knowhow. We derive a model that indicates how the value of\nthese three quantities can be inferred from the probability that an individual\nin a city is employed in a given urban activity. We illustrate how to estimate\nempirically these variables using data on employment across industries and\nmetropolitan statistical areas in the US. We then show how the functional form\nof the probability function derived from our theory is statistically superior\nwhen compared to competing alternative models, and that it explains well-known\nresults in the urban scaling and economic complexity literature. Finally, we\nshow how the quantities are associated with metrics of economic performance,\nsuggesting our theory can provide testable implications for why some cities are\nmore prosperous than others.\n"
    },
    {
        "paper_id": 1812.03453,
        "authors": "Abdelali Gabih, Hakam Kondakji, Ralf Wunderlich",
        "title": "Asymptotic Filter Behavior for High-Frequency Expert Opinions in a\n  Market with Gaussian Drift",
        "comments": "Some changes of notation, minor revisions of Subsections 2.3, 3.1,\n  3.2 and 4.1, new Figure 1, 20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a financial market where stock returns depend on a\nhidden Gaussian mean reverting drift process. Information on the drift is\nobtained from returns and expert opinions in the form of noisy signals about\nthe current state of the drift arriving at the jump times of a homogeneous\nPoisson process. Drift estimates are based on Kalman filter techniques and\ndescribed by the conditional mean and covariance matrix of the drift given the\nobservations. We study the filter asymptotics for increasing arrival intensity\nof expert opinions and prove that the conditional mean is a consistent drift\nestimator, it converges in the mean-square sense to the hidden drift. Thus, in\nthe limit as the arrival intensity goes to infinity investors have full\ninformation about the drift.\n"
    },
    {
        "paper_id": 1812.03526,
        "authors": "Ivan Guo, Gregoire Loeper",
        "title": "Path Dependent Optimal Transport and Model Calibration on Exotic\n  Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce and develop the theory of semimartingale optimal\ntransport in a path dependent setting. Instead of the classical constraints on\nmarginal distributions, we consider a general framework of path dependent\nconstraints. Duality results are established, representing the solution in\nterms of path dependent partial differential equations (PPDEs). Moreover, we\nprovide a dimension reduction result based on the new notion of\n\"semifiltrations\", which identifies appropriate Markovian state variables based\non the constraints and the cost function. Our technique is then applied to the\nexact calibration of volatility models to the prices of general path dependent\nderivatives.\n"
    },
    {
        "paper_id": 1812.03534,
        "authors": "Charles D. Brummitt, Andres Gomez-Lievano, Ricardo Hausmann, and\n  Matthew H. Bonds",
        "title": "Machine-learned patterns suggest that diversification drives economic\n  development",
        "comments": "9 pages, 6 figures in the main text; 28 pages, 19 figures in the\n  supplement",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a machine-learning-based method, Principal Smooth-Dynamics\nAnalysis (PriSDA), to identify patterns in economic development and to automate\nthe development of new theory of economic dynamics. Traditionally, economic\ngrowth is modeled with a few aggregate quantities derived from simplified\ntheoretical models. Here, PriSDA identifies important quantities. Applied to 55\nyears of data on countries' exports, PriSDA finds that what most distinguishes\ncountries' export baskets is their diversity, with extra weight assigned to\nmore sophisticated products. The weights are consistent with previous measures\nof product complexity in the literature. The second dimension of variation is a\nproficiency in machinery relative to agriculture. PriSDA then couples these\nquantities with per-capita income and infers the dynamics of the system over\ntime. According to PriSDA, the pattern of economic development of countries is\ndominated by a tendency toward increased diversification. Moreover, economies\nappear to become richer after they diversify (i.e., diversity precedes growth).\nThe model predicts that middle-income countries with diverse export baskets\nwill grow the fastest in the coming decades, and that countries will converge\nonto intermediate levels of income and specialization. PriSDA is generalizable\nand may illuminate dynamics of elusive quantities such as diversity and\ncomplexity in other natural and social systems.\n"
    },
    {
        "paper_id": 1812.03771,
        "authors": "Christopher J. Boudreaux, Boris Nikolaev",
        "title": "Shattering the glass ceiling? How the institutional context mitigates\n  the gender gap in entrepreneurship",
        "comments": "34 pages, 2 tables, 3 figures, conference submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine how the institutional context affects the relationship between\ngender and opportunity entrepreneurship. To do this, we develop a multi-level\nmodel that connects feminist theory at the micro-level to institutional theory\nat the macro-level. It is hypothesized that the gender gap in opportunity\nentrepreneurship is more pronounced in low-quality institutional contexts and\nless pronounced in high-quality institutional contexts. Using data from the\nGlobal Entrepreneurship Monitor (GEM) and regulation data from the economic\nfreedom of the world index (EFW), we test our predictions and find evidence in\nsupport of our model. Our findings suggest that, while there is a gender gap in\nentrepreneurship, these disparities are reduced as the quality of the\ninstitutional context improves.\n"
    },
    {
        "paper_id": 1812.04184,
        "authors": "Tho V. Le, Junyi Zhang, Makoto Chikaraishi, Akimasa Fujiwara",
        "title": "Influence of High-Speed Railway System on Inter-city Travel Behavior in\n  Vietnam",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To analyze the influence of introducing the High-Speed Railway (HSR) system\non business and non-business travel behavior, this study develops an integrated\ninter-city travel demand model to represent trip generations, destination\nchoice, and travel mode choice behavior. The accessibility calculated from the\nRP/SP (Revealed Preference/Stated Preference) combined nested logit model of\ndestination and mode choices is used as an explanatory variable in the trip\nfrequency models. One of the important findings is that additional travel would\nbe induced by introducing HSR. Our simulation analyses also reveal that HSR and\nconventional airlines will be the main modes for middle distances and long\ndistances, respectively. The development of zones may highly influence the\ndestination choices for business purposes, while prices of HSR and Low-Cost\nCarriers affect choices for non-business purposes. Finally, the research\nreveals that people on non-business trips are more sensitive to changes in\ntravel time, travel cost and regional attributes than people on business trips.\n"
    },
    {
        "paper_id": 1812.04272,
        "authors": "Suren Harutyunyan, Adri\\`A Masip Borr\\`As",
        "title": "A Numerical Analysis of the Modified Kirk's Formula and Applications to\n  Spread Option Pricing Approximations a numerical analysis of the modified\n  kirk's formula and applications to spread option pricing approximations",
        "comments": "44 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper we study recent developments in the approximation of the spread\noption pricing. As the Kirk\\'s Approximation is extremely flawed in the cases\nwhen the correlation is very high, we explore a recent development that allows\napproximating with simplicity and accuracy the option price. To assess the\ngoodness of fit of the new method, we increase dramatically the number of\nsimulations and scenarios to test the new method and compare it with the\noriginal Kirk\\'s formula. The simulations confirmed that the Modified Kirk\\'s\nApproximation method is extremely accurate, improving Kirk\\'s approach for\ntwo-asset spread options.\n"
    },
    {
        "paper_id": 1812.04354,
        "authors": "Andreas H Hamel",
        "title": "Monetary Measures of Risk",
        "comments": "55 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey gives an introduction to monetary measures of risk as monotone\nand cash additive functions on spaces of univariate random variables. Primal\nand dual representation results as well as several examples are discussed.\nPrincipal ways to construct risk measures are given and extensions to more\ngeneral situations indicated.\n"
    },
    {
        "paper_id": 1812.04486,
        "authors": "David Saltiel and Eric Benhamou",
        "title": "Trade Selection with Supervised Learning and OCA",
        "comments": "7 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1811.12064",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, state-of-the-art methods for supervised learning have\nexploited increasingly gradient boosting techniques, with mainstream efficient\nimplementations such as xgboost or lightgbm. One of the key points in\ngenerating proficient methods is Feature Selection (FS). It consists in\nselecting the right valuable effective features. When facing hundreds of these\nfeatures, it becomes critical to select best features. While filter and\nwrappers methods have come to some maturity, embedded methods are truly\nnecessary to find the best features set as they are hybrid methods combining\nfeatures filtering and wrapping. In this work, we tackle the problem of finding\nthrough machine learning best a priori trades from an algorithmic strategy. We\nderive this new method using coordinate ascent optimization and using block\nvariables. We compare our method to Recursive Feature Elimination (RFE) and\nBinary Coordinate Ascent (BCA). We show on a real life example the capacity of\nthis method to select good trades a priori. Not only this method outperforms\nthe initial trading strategy as it avoids taking loosing trades, it also\nsurpasses other method, having the smallest feature set and the highest score\nat the same time. The interest of this method goes beyond this simple trade\nclassification problem as it is a very general method to determine the optimal\nfeature set using some information about features relationship as well as using\ncoordinate ascent optimization.\n"
    },
    {
        "paper_id": 1812.04528,
        "authors": "Shenhao Wang, Qingyi Wang, Jinhua Zhao",
        "title": "Deep Neural Networks for Choice Analysis: Extracting Complete Economic\n  Information for Interpretation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While deep neural networks (DNNs) have been increasingly applied to choice\nanalysis showing high predictive power, it is unclear to what extent\nresearchers can interpret economic information from DNNs. This paper\ndemonstrates that DNNs can provide economic information as complete as\nclassical discrete choice models (DCMs). The economic information includes\nchoice predictions, choice probabilities, market shares, substitution patterns\nof alternatives, social welfare, probability derivatives, elasticities,\nmarginal rates of substitution (MRS), and heterogeneous values of time (VOT).\nUnlike DCMs, DNNs can automatically learn the utility function and reveal\nbehavioral patterns that are not prespecified by domain experts. However, the\neconomic information obtained from DNNs can be unreliable because of the three\nchallenges associated with the automatic learning capacity: high sensitivity to\nhyperparameters, model non-identification, and local irregularity. To\ndemonstrate the strength and challenges of DNNs, we estimated the DNNs using a\nstated preference survey, extracted the full list of economic information from\nthe DNNs, and compared them with those from the DCMs. We found that the\neconomic information either aggregated over trainings or population is more\nreliable than the disaggregate information of the individual observations or\ntrainings, and that even simple hyperparameter searching can significantly\nimprove the reliability of the economic information extracted from the DNNs.\nFuture studies should investigate other regularizations and DNN architectures,\nbetter optimization algorithms, and robust DNN training methods to address\nDNNs' three challenges, to provide more reliable economic information from\nDNN-based choice models.\n"
    },
    {
        "paper_id": 1812.04603,
        "authors": "Alex Garivaltis",
        "title": "Game-Theoretic Optimal Portfolios for Jump Diffusions",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": "Games 10(1), 8 (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a two-person trading game in continuous time that\ngeneralizes Garivaltis (2018) to allow for stock prices that both jump and\ndiffuse. Analogous to Bell and Cover (1988) in discrete time, the players start\nby choosing fair randomizations of the initial dollar, by exchanging it for a\nrandom wealth whose mean is at most 1. Each player then deposits the resulting\ncapital into some continuously-rebalanced portfolio that must be adhered to\nover $[0,t]$. We solve the corresponding `investment $\\phi$-game,' namely the\nzero-sum game with payoff kernel\n$\\mathbb{E}[\\phi\\{\\textbf{W}_1V_t(b)/(\\textbf{W}_2V_t(c))\\}]$, where\n$\\textbf{W}_i$ is player $i$'s fair randomization, $V_t(b)$ is the final wealth\nthat accrues to a one dollar deposit into the rebalancing rule $b$, and\n$\\phi(\\bullet)$ is any increasing function meant to measure relative\nperformance. We show that the unique saddle point is for both players to use\nthe (leveraged) Kelly rule for jump diffusions, which is ordinarily defined by\nmaximizing the asymptotic almost-sure continuously-compounded capital growth\nrate. Thus, the Kelly rule for jump diffusions is the correct behavior for\npractically anybody who wants to outperform other traders (on any time frame)\nwith respect to practically any measure of relative performance.\n"
    },
    {
        "paper_id": 1812.04827,
        "authors": "Ruodu Wang, Ricardas Zitikis",
        "title": "Weak comonotonicity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical notion of comonotonicity has played a pivotal role when solving\ndiverse problems in economics, finance, and insurance. In various practical\nproblems, however, this notion of extreme positive dependence structure is\noverly restrictive and sometimes unrealistic. In the present paper, we put\nforward a notion of weak comonotonicity, which contains the classical notion of\ncomonotonicity as a special case, and gives rise to necessary and sufficient\nconditions for a number of optimization problems, such as those arising in\nportfolio diversification, risk aggregation, and premium calculation. In\nparticular, we show that a combination of weak comonotonicity and weak\nantimonotonicity with respect to some choices of measures is sufficient for the\nmaximization of Value-at-Risk aggregation, and weak comonotonicity is necessary\nand sufficient for the Expected Shortfall aggregation. Finally, with the help\nof weak comonotonicity acting as an intermediate notion of dependence between\nthe extreme cases of no dependence and strong comonotonicity, we give a natural\nsolution to a risk-sharing problem.\n"
    },
    {
        "paper_id": 1812.05093,
        "authors": "Benjam\\'in Leiva",
        "title": "Apropiaci\\'on privada de renta de recursos naturales? El caso del cobre\n  en Chile",
        "comments": "24 pages, in Spanish",
        "journal-ref": "El Trimestre Econ\\'omico, vol. LXXXIII (4), n\\'um. 332,\n  octubre-diciembre de 2016, pp. 549-572",
        "doi": "10.20430/ete.v83i332.233",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unexpected increases of natural resource prices can generate rents, value\nthat should be recovered by the State to minimize inefficiencies, avoid\narbitrary discrimination between citizens and keep a sustainable trajectory. As\na case study about private appropriation of natural resource rent, this work\nexplores the case of copper in Chile since 1990, empirically analyzing if the\n12 main private mining companies have recovered in present value more than\ntheir investment during their life cycle. The results of this exercise,\napplicable to other natural resources, indicate that some actually have,\ncapturing about US$ 40 billion up to 2012. Elaborating an adequate\ninstitutional framework for future deposits remain important challenges for\nChile to plentifully take advantage of its mining potential, as well as for any\ncountry with an abundant resource base to better enjoy its natural wealth. For\nthat purpose, a concession known as Least Present Value Revenue (LPVR) is\nproposed.\n"
    },
    {
        "paper_id": 1812.05315,
        "authors": "Henry Stone",
        "title": "Calibrating rough volatility models: a convolutional neural network\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we use convolutional neural networks to find the H\\\"older\nexponent of simulated sample paths of the rBergomi model, a recently proposed\nstock price model used in mathematical finance. We contextualise this as a\ncalibration problem, thereby providing a very practical and useful application.\n"
    },
    {
        "paper_id": 1812.05657,
        "authors": "David Rushing Dewhurst, Michael Vincent Arnold, Colin Michael Van Oort",
        "title": "Selection mechanisms affect volatility in evolving markets",
        "comments": "9 pages, 7 figures, to appear in proceedings of GECCO 2019 as a full\n  paper",
        "journal-ref": "In Proceedings of the Genetic and Evolutionary Computation\n  Conference (2019) 90-98",
        "doi": "10.1145/3321707.3321734",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial asset markets are sociotechnical systems whose constituent agents\nare subject to evolutionary pressure as unprofitable agents exit the\nmarketplace and more profitable agents continue to trade assets. Using a\npopulation of evolving zero-intelligence agents and a frequent batch auction\nprice-discovery mechanism as substrate, we analyze the role played by\nevolutionary selection mechanisms in determining macro-observable market\nstatistics. In particular, we show that selection mechanisms incorporating a\nlocal fitness-proportionate component are associated with high correlation\nbetween a micro, risk-aversion parameter and a commonly-used macro-volatility\nstatistic, while a purely quantile-based selection mechanism shows\nsignificantly less correlation.\n"
    },
    {
        "paper_id": 1812.05748,
        "authors": "Guanlong Ren and John Stachurski",
        "title": "Dynamic Programming with Recursive Preferences: Optimality and\n  Applications",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides new conditions for dynamic optimality in discrete time\nand uses them to establish fundamental dynamic programming results for several\ncommonly used recursive preference specifications. These include Epstein-Zin\npreferences, risk-sensitive preferences, narrow framing models and recursive\npreferences with sensitivity to ambiguity. The results obtained for these\napplications include (i) existence of optimal policies, (ii) uniqueness of\nsolutions to the Bellman equation, (iii) a complete characterization of optimal\npolicies via Bellman's principle of optimality, and (iv) a globally convergent\nmethod of computation via value function iteration.\n"
    },
    {
        "paper_id": 1812.05859,
        "authors": "Andrew Papanicolaou",
        "title": "Consistent Time-Homogeneous Modeling of SPX and VIX Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows how to recover a stochastic volatility model (SVM) from a\nmarket model of the VIX futures term structure. Market models have more\nflexibility for fitting of curves than do SVMs, and therefore are better suited\nfor pricing VIX futures and VIX derivatives. But the VIX itself is a derivative\nof the S&P500 (SPX) and it is common practice to price SPX derivatives using an\nSVM. Therefore, consistent modeling for both SPX and VIX should involve an SVM\nthat can be obtained by inverting the market model. This paper's main result is\na method for the recovery of a stochastic volatility function by solving an\ninverse problem where the input is the VIX function given by a market model.\nAnalysis will show conditions necessary for there to be a unique solution to\nthis inverse problem. The models are consistent if the recovered volatility\nfunction is non-negative. Examples are presented to illustrate the theory, to\nhighlight the issue of negativity in solutions, and to show the potential for\ninconsistency in non-Markov settings.\n"
    },
    {
        "paper_id": 1812.05893,
        "authors": "Erwan Koch and Christian Y. Robert",
        "title": "Stochastic derivative estimation for max-stable random fields",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider expected performances based on max-stable random fields and we\nare interested in their derivatives with respect to the spatial dependence\nparameters of those fields. Max-stable fields, such as the Brown--Resnick and\nSmith fields, are very popular in spatial extremes. We focus on the two most\npopular unbiased stochastic derivative estimation approaches: the likelihood\nratio method (LRM) and the infinitesimal perturbation analysis (IPA). LRM\nrequires the multivariate density of the max-stable field to be explicit, and\nIPA necessitates the computation of the derivative with respect to the\nparameters for each simulated value. We propose convenient and tractable\nconditions ensuring the validity of LRM and IPA in the cases of the\nBrown--Resnick and Smith field, respectively. Obtaining such conditions is\nintricate owing to the very structure of max-stable fields. Then we focus on\nrisk and dependence measures, which constitute one of the several frameworks\nwhere our theoretical results can be useful. We perform a simulation study\nwhich shows that both LRM and IPA perform well in various configurations, and\nprovide a real case study that is valuable for the insurance industry.\n"
    },
    {
        "paper_id": 1812.05916,
        "authors": "Achref Bachouch, C\\^ome Hur\\'e, Nicolas Langren\\'e, Huyen Pham",
        "title": "Deep neural networks algorithms for stochastic control problems on\n  finite horizon: numerical applications",
        "comments": "39 pages, 14 figures. Methodology and Computing in Applied\n  Probability, Springer Verlag, In press",
        "journal-ref": "Methodology and Computing in Applied Probability 24(1) 143-178\n  (2022)",
        "doi": "10.1007/s11009-019-09767-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents several numerical applications of deep learning-based\nalgorithms that have been introduced in [HPBL18]. Numerical and comparative\ntests using TensorFlow illustrate the performance of our different algorithms,\nnamely control learning by performance iteration (algorithms NNcontPI and\nClassifPI), control learning by hybrid iteration (algorithms Hybrid-Now and\nHybrid-LaterQ), on the 100-dimensional nonlinear PDEs examples from [EHJ17] and\non quadratic backward stochastic differential equations as in [CR16]. We also\nperformed tests on low-dimension control problems such as an option hedging\nproblem in finance, as well as energy storage problems arising in the valuation\nof gas storage and in microgrid management. Numerical results and comparisons\nto quantization-type algorithms Qknn, as an efficient algorithm to numerically\nsolve low-dimensional control problems, are also provided; and some\ncorresponding codes are available on https://github.com/comeh/.\n"
    },
    {
        "paper_id": 1812.06,
        "authors": "Ricardo T. Fernholz and Christoffer Koch",
        "title": "The Rank Effect",
        "comments": "47 pages, 10 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1810.12840",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We decompose returns for portfolios of bottom-ranked, lower-priced assets\nrelative to the market into rank crossovers and changes in the relative price\nof those bottom-ranked assets. This decomposition is general and consistent\nwith virtually any asset pricing model. Crossovers measure changes in rank and\nare smoothly increasing over time, while return fluctuations are driven by\nvolatile relative price changes. Our results imply that in a closed,\ndividend-free market in which the relative price of bottom-ranked assets is\napproximately constant, a portfolio of those bottom-ranked assets will\noutperform the market portfolio over time. We show that bottom-ranked relative\ncommodity futures prices have increased only slightly, and confirm the\nexistence of substantial excess returns predicted by our theory. If these\nexcess returns did not exist, then top-ranked relative prices would have had to\nbe much higher in 2018 than those actually observed -- this would imply a\nradically different commodity price distribution.\n"
    },
    {
        "paper_id": 1812.06166,
        "authors": "Hossein Nadeb, Hamzeh Torabi, Ali Dolati",
        "title": "Ordering the smallest claim amounts from two sets of interdependent\n  heterogeneous portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be a set of dependent and\nnon-negative random variables share a survival copula and let $Y_i=\nI_{p_i}X_{\\lambda_i}$, $i=1,\\ldots,n$, where $I_{p_1},\\ldots,I_{p_n}$ be\nindependent Bernoulli random variables independent of $X_{\\lambda_i}$'s, with\n${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial sciences, $Y_i$\ncorresponds to the claim amount in a portfolio of risks. This paper considers\ncomparing the smallest claim amounts from two sets of interdependent\nportfolios, in the sense of usual and likelihood ratio orders, when the\nvariables in one set have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and\n$p_1,\\ldots,p_n$ and the variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. Also, we present\nsome bounds for survival function of the smallest claim amount in a portfolio.\nTo illustrate validity of the results, we serve some applicable models.\n"
    },
    {
        "paper_id": 1812.06175,
        "authors": "Yaodong Yang, Alisa Kolesnikova, Stefan Lessmann, Tiejun Ma,\n  Ming-Chien Sung, Johnnie E.V. Johnson",
        "title": "Can Deep Learning Predict Risky Retail Investors? A Case Study in\n  Financial Risk Behavior Forecasting",
        "comments": "Within the \"equal\" contribution, Yaodong Yang contributed the core\n  deep learning algorithm along with its experimental results, and the first\n  draft of the manuscript (including Figure 1,2,3,4,7,8,9,11, and Table 3)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper examines the potential of deep learning to support decisions in\nfinancial risk management. We develop a deep learning model for predicting\nwhether individual spread traders secure profits from future trades. This task\nembodies typical modeling challenges faced in risk and behavior forecasting.\nConventional machine learning requires data that is representative of the\nfeature-target relationship and relies on the often costly development,\nmaintenance, and revision of handcrafted features. Consequently, modeling\nhighly variable, heterogeneous patterns such as trader behavior is challenging.\nDeep learning promises a remedy. Learning hierarchical distributed\nrepresentations of the data in an automatic manner (e.g. risk taking behavior),\nit uncovers generative features that determine the target (e.g., trader's\nprofitability), avoids manual feature engineering, and is more robust toward\nchange (e.g. dynamic market conditions). The results of employing a deep\nnetwork for operational risk forecasting confirm the feature learning\ncapability of deep learning, provide guidance on designing a suitable network\narchitecture and demonstrate the superiority of deep learning over machine\nlearning and rule-based benchmarks.\n"
    },
    {
        "paper_id": 1812.06185,
        "authors": "Fei Sun, Yijun Hu",
        "title": "Systemic risk measures with markets volatility",
        "comments": "arXiv admin note: text overlap with arXiv:1806.01166,\n  arXiv:1806.08701",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As systemic risk has become a hot topic in the financial markets, how to\nmeasure, allocate and regulate the systemic risk are becoming especially\nimportant. However, the financial markets are becoming more and more\ncomplicate, which makes the usual study of systemic risk to be restricted. In\nthis paper, we will study the systemic risk measures on a special space\n$L^{p(\\cdot)}$ where the variable exponent $p(\\cdot)$ is no longer a given real\nnumber like the space $L^{p}$, but a random variable, which reflects the\npossible volatility of the financial markets. Finally, the dual representation\nfor this new systemic risk measures will be studied. Our results show that\nevery this new systemic risk measure can be decomposed into a convex certain\nfunction and a simple-systemic risk measure, which provides a new ideas for\ndealing with the systemic risk.\n"
    },
    {
        "paper_id": 1812.066,
        "authors": "Brian Ning, Franco Ho Ting Lin, Sebastian Jaimungal",
        "title": "Double Deep Q-Learning for Optimal Execution",
        "comments": "20 pages, 7 figures, 1 table. Updated minor typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal trade execution is an important problem faced by essentially all\ntraders. Much research into optimal execution uses stringent model assumptions\nand applies continuous time stochastic control to solve them. Here, we instead\ntake a model free approach and develop a variation of Deep Q-Learning to\nestimate the optimal actions of a trader. The model is a fully connected Neural\nNetwork trained using Experience Replay and Double DQN with input features\ngiven by the current state of the limit order book, other trading signals, and\navailable execution actions, while the output is the Q-value function\nestimating the future rewards under an arbitrary action. We apply our model to\nnine different stocks and find that it outperforms the standard benchmark\napproach on most stocks using the measures of (i) mean and median\nout-performance, (ii) probability of out-performance, and (iii) gain-loss\nratios.\n"
    },
    {
        "paper_id": 1812.06679,
        "authors": "Bo Tranberg, Olivier Corradi, Bruno Lajoie, Thomas Gibon, Iain\n  Staffell, Gorm Bruun Andresen",
        "title": "Real-Time Carbon Accounting Method for the European Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.esr.2019.100367",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Electricity accounts for 25% of global greenhouse gas emissions. Reducing\nemissions related to electricity consumption requires accurate measurements\nreadily available to consumers, regulators and investors. In this case study,\nwe propose a new real-time consumption-based accounting approach based on flow\ntracing. This method traces power flows from producer to consumer thereby\nrepresenting the underlying physics of the electricity system, in contrast to\nthe traditional input-output models of carbon accounting. With this method we\nexplore the hourly structure of electricity trade across Europe in 2017, and\nfind substantial differences between production and consumption intensities.\nThis emphasizes the importance of considering cross-border flows for increased\ntransparency regarding carbon emission accounting of electricity.\n"
    },
    {
        "paper_id": 1812.06694,
        "authors": "Qing Yao, Tim Evans, Kim Christensen",
        "title": "How the network properties of shareholders vary with investor type and\n  country",
        "comments": null,
        "journal-ref": "PLOS ONE 14(8): e0220965 (2019)",
        "doi": "10.1371/journal.pone.0220965",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct two examples of shareholder networks in which shareholders are\nconnected if they have shares in the same company. We do this for the\nshareholders in Turkish companies and we compare this against the network\nformed from the shareholdings in Dutch companies. We analyse the properties of\nthese two networks in terms of the different types of shareholder. We create a\nsuitable randomised version of these networks to enable us to find significant\nfeatures in our networks. For that we find the roles played by different types\nof shareholder in these networks, and also show how these roles differ in the\ntwo countries we study.\n"
    },
    {
        "paper_id": 1812.06973,
        "authors": "Lorella Fatone and Francesca Mariani",
        "title": "Systemic risk governance in a dynamical model of a banking system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of governing systemic risk in a banking system model.\nThe banking system model consists in an initial value problem for a system of\nstochastic differential equations whose dependent variables are the\nlog-monetary reserves of the banks as functions of time. The banking system\nmodel considered generalizes previous models studied in [5], [4], [7] and\ndescribes an homogeneous population of banks. Two distinct mechanisms are used\nto model the cooperation among banks and the cooperation between banks and\nmonetary authority. These mechanisms are regulated respectively by the\nparameters $\\alpha$ and $\\gamma$. A bank fails when its log-monetary reserves\ngo below an assigned default level. We call systemic risk or systemic event in\na bounded time interval the fact that in that time interval at least a given\nfraction of the banks fails. The probability of systemic risk in a bounded time\ninterval is evaluated using statistical simulation. A method to govern the\nprobability of systemic risk in a bounded time interval is presented. The goal\nof the governance is to keep the probability of systemic risk in a bounded time\ninterval between two given thresholds. The governance is based on the choice of\nthe log-monetary reserves of a kind of \"ideal bank\" as a function of time and\non the solution of an optimal control problem for the mean field approximation\nof the banking system model. The solution of the optimal control problem\ndetermines the parameters $\\alpha$ and $\\gamma$ as functions of time, that is\ndefines the rules of the borrowing and lending activity among banks and between\nbanks and monetary authority. Some numerical examples are discussed. The\nsystemic risk governance is tested in absence and in presence of positive and\nnegative shocks acting on the banking system.\n"
    },
    {
        "paper_id": 1812.06975,
        "authors": "Olena Kostylenko, Helena Sofia Rodrigues, Delfim F. M. Torres",
        "title": "The risk of contagion spreading and its optimal control in the economy",
        "comments": "This is a preprint of a paper whose final and definite form is with\n  'Statistics Opt. Inform. Comput.', Vol. 7, No 2 (2019). See\n  [http://www.iapress.org/]. Submitted 01/Nov/2018; Revised 11/Dec/2018;\n  Accepted 16/Dec/2018",
        "journal-ref": "Stat. Optim. Inf. Comput. 7 (2019), no. 3, 578--587",
        "doi": "10.19139/soic-2310-5070-833",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The global crisis of 2008 provoked a heightened interest among scientists to\nstudy the phenomenon, its propagation and negative consequences. The process of\nmodelling the spread of a virus is commonly used in epidemiology. Conceptually,\nthe spread of a disease among a population is similar to the contagion process\nin economy. This similarity allows considering the contagion in the world\nfinancial system using the same mathematical model of infection spread that is\noften used in epidemiology. Our research focuses on the dynamic behaviour of\ncontagion spreading in the global financial network. The effect of infection by\na systemic spread of risks in the network of national banking systems of\ncountries is tested. An optimal control problem is then formulated to simulate\na control that may avoid significant financial losses. The results show that\nthe proposed approach describes well the reality of the world economy, and\nemphasizes the importance of international relations between countries on the\nfinancial stability.\n"
    },
    {
        "paper_id": 1812.07048,
        "authors": "Werner Kirsch, Wojciech S{\\l}omczy\\'nski, Dariusz Stolicki, Karol\n  \\.Zyczkowski",
        "title": "Double Majority and Generalized Brexit: Explaining Counterintuitive\n  Results",
        "comments": "32 pages, 10 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A mathematical analysis of the distribution of voting power in the Council of\nthe European Union operating according to the Treaty of Lisbon is presented. We\nstudy the effects of Brexit on the voting power of the remaining members,\nmeasured by the Penrose--Banzhaf Index. We note that the effects in question\nare non-monotonic with respect to voting weights, and that some member states\nwill lose power after Brexit. We use the normal approximation of the\nPenrose--Banzhaf Index in double-majority games to show that such\nnon-monotonicity is in most cases inherent in the double-majority system, but\nis strongly exacerbated by the peculiarities of the EU population vector.\nFurthermore, we investigate consequences of a hypothetical \"generalized\nBrexit\", i.e., NN-exit of another member state (from a 28-member Union), noting\nthat the effects on voting power are non-monotonic in most cases, but strongly\ndepend on the size of the country leaving the Union.\n"
    },
    {
        "paper_id": 1812.07295,
        "authors": "Luisa Bisaglia and Matteo Grigoletto",
        "title": "A new time-varying model for forecasting long-memory series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we propose a new class of long-memory models with time-varying\nfractional parameter. In particular, the dynamics of the long-memory\ncoefficient, $d$, is specified through a stochastic recurrence equation driven\nby the score of the predictive likelihood, as suggested by Creal et al. (2013)\nand Harvey (2013). We demonstrate the validity of the proposed model by a Monte\nCarlo experiment and an application to two real time series.\n"
    },
    {
        "paper_id": 1812.07318,
        "authors": "Francisco Blasques, Vladim\\'ir Hol\\'y and Petra Tomanov\\'a",
        "title": "Zero-Inflated Autoregressive Conditional Duration Model for Discrete\n  Trade Durations with Excessive Zeros",
        "comments": null,
        "journal-ref": "(2023) Studies in Nonlinear Dynamics & Econometrics",
        "doi": "10.1515/snde-2022-0008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In finance, durations between successive transactions are usually modeled by\nthe autoregressive conditional duration model based on a continuous\ndistribution omitting zero values. Zero or close-to-zero durations can be\ncaused by either split transactions or independent transactions. We propose a\ndiscrete model allowing for excessive zero values based on the zero-inflated\nnegative binomial distribution with score dynamics. This model allows to\ndistinguish between the processes generating split and standard transactions.\nWe use the existing theory on score models to establish the invertibility of\nthe score filter and verify that sufficient conditions hold for the consistency\nand asymptotic normality of the maximum likelihood of the model parameters. In\nan empirical study, we find that split transactions cause between 92 and 98\npercent of zero and close-to-zero values. Furthermore, the loss of decimal\nplaces in the proposed approach is less severe than the incorrect treatment of\nzero values in continuous models.\n"
    },
    {
        "paper_id": 1812.07369,
        "authors": "Sebastian M. Krause, Jonas A. Fiegen, Thomas Guhr",
        "title": "Emergence of stylized facts during the opening of stock markets",
        "comments": "7 pages, 7 figures, 1 supplemental table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets show a number of non-stationarities, ranging from\nvolatility fluctuations over ever changing technical and regulatory market\nconditions to seasonalities. On the other hand, financial markets show various\nstylized facts which are remarkably stable. It is thus an intriguing question\nto find out how these stylized facts emerge. As a first example, we here\ninvestigate how the bid-ask-spread between best sell and best buy offer for\nstocks develops during the trading day. For rescaled and properly smoothed data\nwe observe collapsing curves for many different NASDAQ stocks, with a slow\npower law decline of the spread during the whole trading day. This effect\nemerges robustly after a highly fluctuating opening period. Some so called\nlarge-tick stocks behave differently because of technical boundaries. Their\nspread closes to one tick shortly after the market opening. We use our findings\nfor identifying the duration of the market opening which we find to vary\nlargely from stock to stock.\n"
    },
    {
        "paper_id": 1812.07415,
        "authors": "K.E. Feldman",
        "title": "Change of Measure in Midcurve Pricing",
        "comments": "Quantitative Finance",
        "journal-ref": "Wilmott, Volume 2020, Issue106, March 2020, Pages 76-81",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive measure change formulae required to price midcurve swaptions in the\nforward swap annuity measure with stochastic annuities' ratios. We construct\nthe corresponding linear and exponential terminal swap rate pricing models and\nshow how they capture the midcurve swaption correlation skew.\n"
    },
    {
        "paper_id": 1812.07529,
        "authors": "Umut \\c{C}etin and Albina Danilova",
        "title": "On pricing rules and optimal strategies in general Kyle-Back models",
        "comments": "More examples and comparisons with earlier literature added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The folk result in Kyle-Back models states that the value function of the\ninsider remains unchanged when her admissible strategies are restricted to\nabsolutely continuous ones. In this paper we show that, for a large class of\npricing rules used in current literature, the value function of the insider can\nbe finite when her strategies are restricted to be absolutely continuous and\ninfinite when this restriction is not imposed. This implies that the folk\nresult doesn't hold for those pricing rules and that they are not consistent\nwith equilibrium. We derive the necessary conditions for a pricing rule to be\nconsistent with equilibrium and prove that, when a pricing rule satisfies these\nnecessary conditions, the insider's optimal strategy is absolutely continuous,\nthus obtaining the classical result in a more general setting.\n  This, furthermore, allows us to justify the standard assumption of absolute\ncontinuity of insider's strategies since one can construct a pricing rule\nsatisfying the necessary conditions derived in the paper that yield the same\nprice process as the pricing rules employed in the modern literature when\ninsider's strategies are absolutely continuous.\n"
    },
    {
        "paper_id": 1812.07635,
        "authors": "Mostafa Zandieh, Seyed Omid Mohaddesi",
        "title": "Portfolio Rebalancing under Uncertainty Using Meta-heuristic Algorithm",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we solve portfolio rebalancing problem when security returns\nare represented by uncertain variables considering transaction costs. The\nperformance of the proposed model is studied using constant-proportion\nportfolio insurance (CPPI) as rebalancing strategy. Numerical results showed\nthat uncertain parameters and different belief degrees will produce different\nefficient frontiers, and affect the performance of the proposed model.\nMoreover, CPPI strategy performs as an insurance mechanism and limits downside\nrisk in bear markets while it allows potential benefit in bull markets.\nFinally, using a globally optimization solver and genetic algorithm (GA) for\nsolving the model, we concluded that the problem size is an important factor in\nsolving portfolio rebalancing problem with uncertain parameters and to gain\nbetter results, it is recommended to use a meta-heuristic algorithm rather than\na global solver.\n"
    },
    {
        "paper_id": 1812.07645,
        "authors": "Konstantinos Spiliopoulos and Jia Yang",
        "title": "Network effects in default clustering for large systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a large collection of dynamically interacting components defined\non a weighted directed graph determining the impact of default of one component\nto another one. We prove a law of large numbers for the empirical measure\ncapturing the evolution of the different components in the pool and from this\nwe extract important information for quantities such as the loss rate in the\noverall pool as well as the mean impact on a given component from system wide\ndefaults. A singular value decomposition of the adjacency matrix of the graph\nallows to coarse-grain the system by focusing on the highest eigenvalues which\nalso correspond to the components with the highest contagion impact on the\npool. Numerical simulations demonstrate the theoretical findings.\n"
    },
    {
        "paper_id": 1812.07803,
        "authors": "Kaustav Das and Nicolas Langren\\'e",
        "title": "Closed-form approximations with respect to the mixing solution for\n  option pricing under stochastic volatility",
        "comments": "Stochastics 2021",
        "journal-ref": "Stochastics 94(5) 745-788 (2022)",
        "doi": "10.1080/17442508.2021.1993445",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider closed-form approximations for European put option prices within\nthe Heston and GARCH diffusion stochastic volatility models with time-dependent\nparameters. Our methodology involves writing the put option price as an\nexpectation of a Black-Scholes formula and performing a second-order Taylor\nexpansion around the mean of its argument. The difficulties then faced are\nsimplifying a number of expectations induced by the Taylor expansion. Under the\nassumption of piecewise-constant parameters, we derive closed-form pricing\nformulas and devise a fast calibration scheme. Furthermore, we perform a\nnumerical error and sensitivity analysis to investigate the quality of our\napproximation and show that the errors are well within the acceptable range for\napplication purposes. Lastly, we derive bounds on the remainder term generated\nby the Taylor expansion.\n"
    },
    {
        "paper_id": 1812.07827,
        "authors": "Alessio Muscillo, Paolo Pin, Tiziano Razzolini",
        "title": "Spreading of an infectious disease between different locations",
        "comments": "22 pages (and 10 pages for appendix); 11 figures (2 in appendix)",
        "journal-ref": null,
        "doi": "10.1016/j.jebo.2021.01.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The endogenous adaptation of agents, that may adjust their local contact\nnetwork in response to the risk of being infected, can have the perverse effect\nof increasing the overall systemic infectiveness of a disease. We study a\ndynamical model over two geographically distinct but interacting locations, to\nbetter understand theoretically the mechanism at play. Moreover, we provide\nempirical motivation from the Italian National Bovine Database, for the period\n2006-2013.\n"
    },
    {
        "paper_id": 1812.07959,
        "authors": "Constantin Udriste, Massimiliano Ferrara, Ionel Tevy, Dorel\n  Zugravescu, Florin Munteanu",
        "title": "Phase Diagram for Roegenian Economics",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We recall the similarities between the concepts and techniques of\nThermodynamics and Roegenian Economics. The Phase Diagram for a Roegenian\neconomic system highlights a triple point and a critical point, with related\nexplanations. These ideas can be used to improve our knowledge and\nunderstanding of the nature of development and evolution of Roegenian economic\nsystems.\n"
    },
    {
        "paper_id": 1812.0796,
        "authors": "Constantin Udriste, Vladimir Golubyatnikov, Ionel Tevy",
        "title": "Economic Cycles of Carnot Type",
        "comments": "Bicentennial of the University Politehnica of Bucharest 1818-2018\n  Centenary of Romanian Great Union 1818-2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Originally, the Carnot cycle is a theoretical thermodynamic cycle that\nprovides an upper limit on the efficiency that any classical thermodynamic\nengine can achieve during the conversion of heat into work, or conversely, the\nefficiency of a refrigeration system in creating a temperature difference by\nthe application of work to the system. The aim of this paper is to introduce\nand study the economic Carnot cycles into a Roegenian economy, using our\nThermodynamic-Economic Dictionary. Of course, the most difficult questions are:\nwhat is the economic significance of such a cycle? Roegenian economics is\nacceptable or not, in terms of practical applications? Our answer is yes for\nboth questions.\n"
    },
    {
        "paper_id": 1812.07961,
        "authors": "Constantin Udriste, Massimiliano Ferrara, Dorel Zugravescu, Florin\n  Munteanu, Ionel Tevy",
        "title": "Geobiodynamics and Roegenian Economic Systems",
        "comments": "Bicentennial of the University Politehnica of Bucharest 1818-2018\n  Centenary of Romanian Great Union 1818-2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This mathematical essay brings together ideas from Economics, Geobiodynamics\nand Thermodynamics. Its purpose is to obtain real models of complex\nevolutionary systems. More specifically, the essay defines Roegenian Economy\nand links Geobiodynamics and Roegenian Economy. In this context, we discuss the\nisomorphism between the concepts and techniques of Thermodynamics and\nEconomics. Then we describe a Roegenian economic system like a Carnot group.\nAfter we analyse the phase equilibrium for two heterogeneous economic systems.\nThe European Union Economics appears like Cartesian product of Roegenian\neconomic systems and its Balance is analysed in details. A Section at the end\ndescribes the \"economic black holes\" as small parts of a a global economic\nsystem in which national income is so great that it causes others poor\nenrichment. These ideas can be used to improve our knowledge and understanding\nof the nature of development and evolution of thermodynamic-economic systems.\n"
    },
    {
        "paper_id": 1812.08091,
        "authors": "Ariel Soto Caro, Roberto Herrera Cofre, Rodrigo Fuentes Solis",
        "title": "Social security and labor absenteeism in a regional health service",
        "comments": "in Spanish",
        "journal-ref": null,
        "doi": "10.4067/S0034-98872015000800004",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Background: Absenteism can generate important economic costs. Aim: To analyze\nthe determinants of the time off work for sick leaves granted to workers of a\nregional health service. Material and Methods: Information about 2033\nindividuals, working at a health service, that were granted at least one sick\nleave during 2012, was analyzed. Personal identification was censored. Special\nemphasis was given to the type of health insurance system of the workers\n(public or private). Results: Workers ascribed to the Chilean public health\ninsurance system (FONASA) had 11 days more off work than their counterparts\nascribed to private health insurance systems. A higher amount of time off work\nwas observed among older subjects and women. Conclusions: Age, gender and the\ntype of health insurance system influence the number of day off work due to\nsick leaves.\n"
    },
    {
        "paper_id": 1812.08099,
        "authors": "Hugo Salgado, Ariel Soto-Caro",
        "title": "Estimating biomass migration parameters by analyzing the spatial\n  behavior of the fishing fleet",
        "comments": null,
        "journal-ref": "RAE vol.31 no.1 Santiago Apr. 2016",
        "doi": "10.4067/S0718-88702016000100003",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this study, a method will be developed and applied for estimating\nbiological migration parameters of the biomass of a fishery resource by means\nof a decision analysis of the spatial behavior of the fleet. First, a model of\ndiscrete selection is estimated, together with patch capture function. This\nwill allow estimating the biomass availability on each patch. In the second\nregression, values of biomass are used in order to estimate a model of\nbiological migration between patches. This method is proven in the Chilean jack\nmackerel fishery. This will allow estimating statistically significant\nmigration parameters, identifying migration patterns.\n"
    },
    {
        "paper_id": 1812.08343,
        "authors": "Hossein Nadeb, Hamzeh Torabi, Ali Dolati",
        "title": "Stochastic comparisons of the largest claim amounts from two sets of\n  interdependent heterogeneous portfolios",
        "comments": "arXiv admin note: text overlap with arXiv:1812.06078 and\n  arXiv:1812.06166",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be dependent non-negative random\nvariables and $Y_i=I_{p_i} X_{\\lambda_i}$, $i=1,\\ldots,n$, where\n$I_{p_1},\\ldots,I_{p_n}$ are independent Bernoulli random variables independent\nof $X_{\\lambda_i}$'s, with ${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial\nsciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In\nthis paper, we compare the largest claim amounts of two sets of interdependent\nportfolios, in the sense of usual stochastic order, when the variables in one\nset have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and $p_1,\\ldots,p_n$ and\nthe variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. For\nillustration, we apply the results to some important models in actuary.\n"
    },
    {
        "paper_id": 1812.08435,
        "authors": "G.A. Delsing, M.R.H. Mandjes, P.J.C. Spreij, E.M.M. Winands",
        "title": "An optimization approach to adaptive multi-dimensional capital\n  management",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 84, 87-97 (2019)",
        "doi": "10.1016/j.insmatheco.2018.10.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Firms should keep capital to offer sufficient protection against the risks\nthey are facing. In the insurance context methods have been developed to\ndetermine the minimum capital level required, but less so in the context of\nfirms with multiple business lines including allocation. The individual capital\nreserve of each line can be represented by means of classical models, such as\nthe conventional Cram\\'{e}r-Lundberg model, but the challenge lies in soundly\nmodelling the correlations between the business lines. We propose a simple yet\nversatile approach that allows for dependence by introducing a common\nenvironmental factor. We present a novel Bayesian approach to calibrate the\nlatent environmental state distribution based on observations concerning the\nclaim processes. The calibration approach is adjusted for an environmental\nfactor that changes over time. The convergence of the calibration procedure\ntowards the true environmental state is deduced. We then point out how to\ndetermine the optimal initial capital of the different business lines under\nspecific constraints on the ruin probability of subsets of business lines. Upon\ncombining the above findings, we have developed an easy-to-implement approach\nto capital risk management in a multi-dimensional insurance risk model.\n"
    },
    {
        "paper_id": 1812.08486,
        "authors": "Martin Keller-Ressel, Martin Larsson, Sergio Pulido",
        "title": "Affine Rough Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this survey article is to explain and elucidate the affine\nstructure of recent models appearing in the rough volatility literature, and\nshow how it leads to exponential-affine transform formulas.\n"
    },
    {
        "paper_id": 1812.08533,
        "authors": "Christian Bayer, Chiheb Ben Hammouda and Raul Tempone",
        "title": "Hierarchical adaptive sparse grids and quasi Monte Carlo for option\n  pricing under the rough Bergomi model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2020.1744700",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rough Bergomi (rBergomi) model, introduced recently in [5], is a\npromising rough volatility model in quantitative finance. It is a parsimonious\nmodel depending on only three parameters, and yet remarkably fits with\nempirical implied volatility surfaces. In the absence of analytical European\noption pricing methods for the model, and due to the non-Markovian nature of\nthe fractional driver, the prevalent option is to use the Monte Carlo (MC)\nsimulation for pricing. Despite recent advances in the MC method in this\ncontext, pricing under the rBergomi model is still a time-consuming task. To\novercome this issue, we have designed a novel, hierarchical approach, based on\ni) adaptive sparse grids quadrature (ASGQ), and ii) quasi-Monte Carlo (QMC).\nBoth techniques are coupled with a Brownian bridge construction and a\nRichardson extrapolation on the weak error. By uncovering the available\nregularity, our hierarchical methods demonstrate substantial computational\ngains with respect to the standard MC method, when reaching a sufficiently\nsmall relative error tolerance in the price estimates across different\nparameter constellations, even for very small values of the Hurst parameter.\nOur work opens a new research direction in this field, i.e., to investigate the\nperformance of methods other than Monte Carlo for pricing and calibrating under\nthe rBergomi model.\n"
    },
    {
        "paper_id": 1812.08548,
        "authors": "Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z, Pawe{\\l}\n  O\\'swi\\c{e}cimka, Marek Stanuszek",
        "title": "Multifractal cross-correlations between the World Oil and other\n  Financial Markets in 2012-2017",
        "comments": null,
        "journal-ref": "Energy Economics 81, 874-885 (2019)",
        "doi": "10.1016/j.eneco.2019.05.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical and multiscaling characteristics of WTI Crude Oil prices\nexpressed in US dollar in relation to the most traded currencies as well as to\ngold futures and to the E-mini S$\\&$P500 futures prices on 5 min intra-day\nrecordings in the period January 2012 - December 2017 are studied. It is shown\nthat in most of the cases the tails of return distributions of the considered\nfinancial instruments follow the inverse cubic power law. The only exception is\nthe Russian ruble for which the distribution tail is heavier and scales with\nthe exponent close to 2. From the perspective of multiscaling the analysed time\nseries reveal the multifractal organization with the left-sided asymmetry of\nthe corresponding singularity spectra. Even more, all the considered financial\ninstruments appear to be multifractally cross-correlated with oil, especially\non the level of medium-size fluctuations, as the multifractal cross-correlation\nanalysis carried out by means of the multifractal cross-correlation analysis\n(MFCCA) and detrended cross-correlation coefficient $\\rho_q$ show. The degree\nof such cross-correlations is however varying among the financial instruments.\nThe strongest ties to the oil characterize currencies of the oil extracting\ncountries. Strength of this multifractal coupling appears to depend also on the\noil market trend. In the analysed time period the level of cross-correlations\nsystematically increases during the bear phase on the oil market and it\nsaturates after the trend reversal in 1st half of 2016. The same methodology is\nalso applied to identify possible causal relations between considered\nobservables. Searching for some related asymmetry in the information flow\nmediating cross-correlations indicates that it was the oil price that led the\nRussian ruble over the time period here considered rather than vice versa.\n"
    },
    {
        "paper_id": 1812.08913,
        "authors": "Aude Bernard, Martin Bell",
        "title": "Internal migration and education: A cross-national comparison",
        "comments": "66 pages",
        "journal-ref": "UNESCO, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Migration the main process shaping patterns of human settlement within and\nbetween countries. It is widely acknowledged to be integral to the process of\nhuman development as it plays a significant role in enhancing educational\noutcomes. At regional and national levels, internal migration underpins the\nefficient functioning of the economy by bringing knowledge and skills to the\nlocations where they are needed. It is the multi-dimensional nature of\nmigration that underlines its significance in the process of human development.\nHuman mobility extends in the spatial domain from local travel to international\nmigration, and in the temporal dimension from short-term stays to permanent\nrelocations. Classification and measurement of such phenomena is inevitably\ncomplex, which has severely hindered progress in comparative research, with\nvery few large-scale cross-national comparisons of migration. The linkages\nbetween migration and education have been explored in a separate line of\ninquiry that has predominantly focused on country-specific analyses as to the\nways in which migration affects educational outcomes and how educational\nattainment affects migration behaviour. A recurrent theme has been the\neducational selectivity of migrants, which in turn leads to an increase of\nhuman capital in some regions, primarily cities, at the expense of others.\nQuestions have long been raised as to the links between education and migration\nin response to educational expansion, but have not yet been fully answered\nbecause of the absence, until recently, of adequate data for comparative\nanalysis of migration. In this paper, we bring these two separate strands of\nresearch together to systematically explore links between internal migration\nand education across a global sample of 57 countries at various stages of\ndevelopment, using data drawn from the IPUMS database.\n"
    },
    {
        "paper_id": 1812.09067,
        "authors": "Stephan Grimm, Thomas Guhr",
        "title": "How spread changes affect the order book: Comparing the price responses\n  of order deletions and placements to trades",
        "comments": "11 pages, 12 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2019-90744-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We observe the effects of the three different events that cause spread\nchanges in the order book, namely trades, deletions and placement of limit\norders. By looking at the frequencies of the relative amounts of price changing\nevents, we discover that deletions of orders open the bid-ask spread of a stock\nmore often than trades do. We see that once the amount of spread changes due to\ndeletions exceeds the amount of the ones due to trades, other observables in\nthe order book change as well. We then look at how these spread changing events\naffect the prices of stocks, by means of the price response. We not only see\nthat the self-response of stocks is positive for both spread changing trades\nand deletions and negative for order placements, but also cross-response to\nother stocks and therefore the market as a whole. In addition, the\nself-response function of spread-changing trades is similar to that of all\ntrades. This leads to the conclusion that spread changing deletions and order\nplacements have a similar effect on the order book and stock prices over time\nas trades.\n"
    },
    {
        "paper_id": 1812.09081,
        "authors": "Micha{\\l} Narajewski and Florian Ziel",
        "title": "Econometric modelling and forecasting of intraday electricity prices",
        "comments": "Accepted for publication in the Journal of Commodity Markets",
        "journal-ref": null,
        "doi": "10.1016/j.jcomm.2019.100107",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the following paper, we analyse the ID$_3$-Price in the German Intraday\nContinuous electricity market using an econometric time series model. A\nmultivariate approach is conducted for hourly and quarter-hourly products\nseparately. We estimate the model using lasso and elastic net techniques and\nperform an out-of-sample, very short-term forecasting study. The model's\nperformance is compared with benchmark models and is discussed in detail.\nForecasting results provide new insights to the German Intraday Continuous\nelectricity market regarding its efficiency and to the ID$_3$-Price behaviour.\n"
    },
    {
        "paper_id": 1812.09302,
        "authors": "Emmanuel Chauvet",
        "title": "Growth, Industrial Externality, Prospect Dynamics, and Well-being on\n  Markets",
        "comments": "81 pages, 13 figures - last updates: Growth sustainability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Functions or 'functionings' enable to give a structure to any activity and\ntheir combinations constitute the capabilities which characterize economic\nassets such as work utility. The basic law of supply and demand naturally\nemerges from that structure while integrating this utility within frames of\nreference in which conditions of growth and associated inflation are identified\nin the exchange mechanisms. Growth sustainability is built step by step taking\ninto account functional and organizational requirements which are followed\nthrough a project up to a product delivery with different levels of\nexternalities. Entering the market through that structure leads to designing\nbasic equations of its dynamics and to finding canonical solutions, or\nparticular equilibria, after specifying the notion of maturity introduced in\norder to refine the basic model. This approach allows to tackle behavioral\nfoundations of Prospect Theory through a generalization of its probability\nweighting function for rationality analyses which apply to Western, Educated,\nIndustrialized, Rich, and Democratic societies as well as to the poorest ones.\nThe nature of reality and well-being appears then as closely related to the\nrelative satisfaction reached on the market, as it can be conceived by an\nagent, according to business cycles; this reality being the result of the\ncomplementary systems that govern human mind as structured by rational\npsychologists. The final concepts of growth integrate and extend the maturity\npart of the behavioral model into virtuous or erroneous sustainability.\n"
    },
    {
        "paper_id": 1812.09385,
        "authors": "Md Niaz Murshed Chowdhury, Md Mobarak Hossain",
        "title": "Poverty, Income Inequality and Growth in Bangladesh: Revisited Karl-Marx",
        "comments": "Working Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study tries to find the relationship among poverty inequality and\ngrowth. The major finding of this study is poverty has reduced significantly\nfrom 2000 to 2016, which is more than 100 percent but in recent time poverty\nreduction has slowed down. Slower and unequal household consumption growth\nmakes sloth the rate of poverty reduction. Average annual consumption fell from\n1.8 percent to 1.4 percent from 2010 to 2016 and poorer households experienced\nslower consumption growth compared to richer households.\n"
    },
    {
        "paper_id": 1812.09393,
        "authors": "Md Niaz Murshed Chowdhury, Md. Mobarak Hossain",
        "title": "Population Growth and Economic Development in Bangladesh: Revisited\n  Malthus",
        "comments": "Working Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Bangladesh is the 2nd largest growing country in the world in 2016 with 7.1%\nGDP growth. This study undertakes an econometric analysis to examine the\nrelationship between population growth and economic development. This result\nindicates population growth adversely related to per capita GDP growth, which\nmeans rapid population growth is a real problem for the development of\nBangladesh.\n"
    },
    {
        "paper_id": 1812.09407,
        "authors": "Lucia Cipolina-Kun, Ignacio Ruiz, Mariano Zero-Medina Laris",
        "title": "An Enhanced Initial Margin Methodology to Manage Warehoused Credit Risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.14536.37124",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The use of CVA to cover credit risk is widely spread, but has its\nlimitations. Namely, dealers face the problem of the illiquidity of instruments\nused for hedging it, hence forced to warehouse credit risk. As a result,\ndealers tend to offer a limited OTC derivatives market to highly risky\ncounterparties. Consequently, those highly risky entities rarely have access to\nhedging services precisely when they need them most. In this paper we propose a\nmethod to overcome this limitation. We propose to extend the CVA risk-neutral\nframework to compute an initial margin (IM) specific to each counterparty,\nwhich depends on the credit quality of the entity at stake, transforming the\neffective credit rating of a given netting set to AAA, regardless of the credit\nrating of the counterparty. By transforming CVA requirement into IM ones, as\nproposed in this paper, an institution could rely on the existing mechanisms\nfor posting and calling of IM, hence ensuring the operational viability of this\nnew form of managing warehoused risk. The main difference with the currently\nstandard framework is the creation of a Specific Initial Margin, that depends\nin the credit rating of the counterparty and the characteristics of the netting\nset in question. In this paper we propose a methodology for such transformation\nin a sound manner, and hence this method overcomes some of the limitations of\nthe CVA framework.\n"
    },
    {
        "paper_id": 1812.09452,
        "authors": "Pavel Ciaian and d'Artis Kancs and Miroslava Rajcaniova",
        "title": "The Price of BitCoin: GARCH Evidence from High Frequency Data",
        "comments": "Pavel Ciaian and d'Artis Kancs and Miroslava Rajcaniova The Price of\n  BitCoin: GARCH Evidence from High Frequency Data",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is the first paper that estimates the price determinants of BitCoin in a\nGeneralised Autoregressive Conditional Heteroscedasticity framework using high\nfrequency data. Derived from a theoretical model, we estimate BitCoin\ntransaction demand and speculative demand equations in a GARCH framework using\nhourly data for the period 2013-2018. In line with the theoretical model, our\nempirical results confirm that both the BitCoin transaction demand and\nspeculative demand have a statistically significant impact on the BitCoin price\nformation. The BitCoin price responds negatively to the BitCoin velocity,\nwhereas positive shocks to the BitCoin stock, interest rate and the size of the\nBitCoin economy exercise an upward pressure on the BitCoin price.\n"
    },
    {
        "paper_id": 1812.09619,
        "authors": "Victor H. Aguiar, Maria Jose Boccardi, Nail Kashaev, Jeongbin Kim",
        "title": "Random Utility and Limited Consideration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The random utility model (RUM, McFadden and Richter, 1990) has been the\nstandard tool to describe the behavior of a population of decision makers. RUM\nassumes that decision makers behave as if they maximize a rational preference\nover a choice set. This assumption may fail when consideration of all\nalternatives is costly. We provide a theoretical and statistical framework that\nunifies well-known models of random (limited) consideration and generalizes\nthem to allow for preference heterogeneity. We apply this methodology in a\nnovel stochastic choice dataset that we collected in a large-scale online\nexperiment. Our dataset is unique since it exhibits both choice set and\n(attention) frame variation. We run a statistical survival race between\ncompeting models of random consideration and RUM. We find that RUM cannot\nexplain the population behavior. In contrast, we cannot reject the hypothesis\nthat decision makers behave according to the logit attention model (Brade and\nRehbeck, 2016).\n"
    },
    {
        "paper_id": 1812.09637,
        "authors": "Lars Tyge Nielsen",
        "title": "Characterization of the Ito Integral",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides an existence-and-uniqueness theorem characterizing the\nstochastic integral with respect to a Wiener process. The integral is\nrepresented as a mapping from the space of measurable and adapted pathwise\nlocally integrable processes to the space of continuous adapted processes. It\nis characterized in terms of two properties: (1) how the stochastic integrals\nof simple processes are calculated and (2) how these integrals converge in\nprobability when the time integrals of the squared integrands converge in\nprobability.\n"
    },
    {
        "paper_id": 1812.09904,
        "authors": "Olesya Grishchenko, Xiao Han, Victor Nistor",
        "title": "A volatility-of-volatility expansion of the option prices in the SABR\n  stochastic volatility model",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general, very fast method to quickly approximate the solution of\na parabolic Partial Differential Equation (PDEs) with explicit formulas. Our\nmethod also provides equaly fast approximations of the derivatives of the\nsolution, which is a challenge for many other methods. Our approach is based on\na computable series expansion in terms of a \"small\" parameter. As an example,\nwe treat in detail the important case of the SABR PDE for $\\beta = 1$, namely\n$\\partial_{\\tau}u = \\sigma^2 \\big [ \\frac{1}{2} (\\partial^2_xu - \\partial_xu) +\n\\nu \\rho \\partial_x\\partial_\\sigma u + \\frac{1}{2} \\nu^2 \\partial^2_\\sigma u \\,\n\\big ] + \\kappa (\\theta - \\sigma) \\partial_\\sigma$, by choosing $\\nu$ as small\nparameter. This yields $u = u_0 + \\nu u_1 + \\nu^2 u_2 + \\ldots$, with $u_j$\nindependent of $\\nu$. The terms $u_j$ are explicitly computable, which is also\na challenge for many other, related methods. Truncating this expansion leads to\ncomputable approximations of $u$ that are in \"closed form,\" and hence can be\nevaluated very quickly. Most of the other related methods use the \"time\" $\\tau$\nas a small parameter. The advantage of our method is that it leads to shorter\nand hence easier to determine and to generalize formulas. We obtain also an\nexplicit expansion for the implied volatility in the SABR model in terms of\n$\\nu$, similar to Hagan's formula, but including also the {\\em mean reverting\nterm.} We provide several numerical tests that show the performance of our\nmethod. In particular, we compare our formula to the one due to Hagan. Our\nresults also behave well when used for actual market data and show the mean\nreverting property of the volatility.\n"
    },
    {
        "paper_id": 1812.10108,
        "authors": "Yaryna Kolomiytseva",
        "title": "Revisiting Transformation and Directional Technology Distance Functions",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the first part of the paper, we prove the equivalence of the unsymmetric\ntransformation function and an efficient joint production function (JPF) under\nstrong monotonicity conditions imposed on input and output correspondences.\nMonotonicity, continuity, and convexity properties sufficient for a symmetric\ntransformation function to be an efficient JPF are also stated. In the second\npart, we show that the most frequently used functional form for the directional\ntechnology distance function (DTDF), the quadratic, does not satisfy\nhomogeneity of degree $-1$ in the direction vector. This implies that the\nquadratic function is not the directional technology distance function. We\nprovide derivation of the DTDF from a symmetric transformation function and\nshow how this approach can be used to obtain functional forms that satisfy both\ntranslation property and homogeneity of degree $-1$ in the direction vector if\nthe optimal solution of an underlying optimization problem can be expressed in\nclosed form.\n"
    },
    {
        "paper_id": 1812.10183,
        "authors": "Babak Mahdavi-Damghani, Konul Mustafayeva, Stephen Roberts, Cristin\n  Buescu",
        "title": "Portfolio Optimization for Cointelated Pairs: SDEs vs. Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the recent rise of Machine Learning as a candidate to partially replace\nclassic Financial Mathematics methodologies, we investigate the performances of\nboth in solving the problem of dynamic portfolio optimization in\ncontinuous-time, finite-horizon setting for a portfolio of two assets that are\nintertwined.\n  In Financial Mathematics approach we model the asset prices not via the\ncommon approaches used in pairs trading such as a high correlation or\ncointegration, but with the cointelation model that aims to reconcile both\nshort-term risk and long-term equilibrium. We maximize the overall P&L with\nFinancial Mathematics approach that dynamically switches between a\nmean-variance optimal strategy and a power utility maximizing strategy. We use\na stochastic control formulation of the problem of power utility maximization\nand solve numerically the resulting HJB equation with the Deep Galerkin method.\n  We turn to Machine Learning for the same P&L maximization problem and use\nclustering analysis to devise bands, combined with in-band optimization.\nAlthough this approach is model agnostic, results obtained with data simulated\nfrom the same cointelation model as FM give an edge to ML.\n"
    },
    {
        "paper_id": 1812.10252,
        "authors": "Yagna Patel",
        "title": "Optimizing Market Making using Multi-Agent Reinforcement Learning",
        "comments": "10 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, reinforcement learning is applied to the problem of optimizing\nmarket making. A multi-agent reinforcement learning framework is used to\noptimally place limit orders that lead to successful trades. The framework\nconsists of two agents. The macro-agent optimizes on making the decision to\nbuy, sell, or hold an asset. The micro-agent optimizes on placing limit orders\nwithin the limit order book. For the context of this paper, the proposed\nframework is applied and studied on the Bitcoin cryptocurrency market. The goal\nof this paper is to show that reinforcement learning is a viable strategy that\ncan be applied to complex problems (with complex environments) such as market\nmaking.\n"
    },
    {
        "paper_id": 1812.10479,
        "authors": "Marcelo Sardelich and Suresh Manandhar",
        "title": "Multimodal deep learning for short-term stock volatility prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Stock market volatility forecasting is a task relevant to assessing market\nrisk. We investigate the interaction between news and prices for the\none-day-ahead volatility prediction using state-of-the-art deep learning\napproaches. The proposed models are trained either end-to-end or using sentence\nencoders transfered from other tasks. We evaluate a broad range of stock market\nsectors, namely Consumer Staples, Energy, Utilities, Heathcare, and Financials.\nOur experimental results show that adding news improves the volatility\nforecasting as compared to the mainstream models that rely only on price data.\nIn particular, our model outperforms the widely-recognized GARCH(1,1) model for\nall sectors in terms of coefficient of determination $R^2$, $MSE$ and $MAE$,\nachieving the best performance when training from both news and price data.\n"
    },
    {
        "paper_id": 1812.10619,
        "authors": "Reaz Chowdhury, M.R.C. Mahdy, Tanisha Nourin Alam, Golam Dastegir Al\n  Quaderi",
        "title": "Predicting the Stock Price of Frontier Markets Using Modified\n  Black-Scholes Option Pricing Model and Machine Learning",
        "comments": "41 pages along with the supplement article",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Scholes Option pricing model (BSOPM) has long been in use for\nvaluation of equity options to find the prices of stocks. In this work, using\nBSOPM, we have come up with a comparative analytical approach and numerical\ntechnique to find the price of call option and put option and considered these\ntwo prices as buying price and selling price of stocks of frontier markets so\nthat we can predict the stock price (close price). Changes have been made to\nthe model to find the parameters strike price and the time of expiration for\ncalculating stock price of frontier markets. To verify the result obtained\nusing modified BSOPM we have used machine learning approach using the software\nRapidminer, where we have adopted different algorithms like the decision tree,\nensemble learning method and neural network. It has been observed that, the\nprediction of close price using machine learning is very similar to the one\nobtained using BSOPM. Machine learning approach stands out to be a better\npredictor over BSOPM, because Black-Scholes-Merton equation includes risk and\ndividend parameter, which changes continuously. We have also numerically\ncalculated volatility. As the prices of the stocks goes high due to\noverpricing, volatility increases at a tremendous rate and when volatility\nbecomes very high market tends to fall, which can be observed and determined\nusing our modified BSOPM. The proposed modified BSOPM has also been explained\nbased on the analogy of Schrodinger equation (and heat equation) of quantum\nphysics.\n"
    },
    {
        "paper_id": 1812.10846,
        "authors": "Neng-Chieh Chang",
        "title": "Semiparametric Difference-in-Differences with Potentially Many Control\n  Variables",
        "comments": "63 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper discusses difference-in-differences (DID) estimation when there\nexist many control variables, potentially more than the sample size. In this\ncase, traditional estimation methods, which require a limited number of\nvariables, do not work. One may consider using statistical or machine learning\n(ML) methods. However, by the well-known theory of inference of ML methods\nproposed in Chernozhukov et al. (2018), directly applying ML methods to the\nconventional semiparametric DID estimators will cause significant bias and make\nthese DID estimators fail to be sqrt{N}-consistent. This article proposes three\nnew DID estimators for three different data structures, which are able to\nshrink the bias and achieve sqrt{N}-consistency and asymptotic normality with\nmean zero when applying ML methods. This leads to straightforward inferential\nprocedures. In addition, I show that these new estimators have the small bias\nproperty (SBP), meaning that their bias will converge to zero faster than the\npointwise bias of the nonparametric estimator on which it is based.\n"
    },
    {
        "paper_id": 1812.10876,
        "authors": "Ludovic Tangpi",
        "title": "Efficient hedging under ambiguity in continuous time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that the minimal superhedging price of a contingent claim is\ntoo high for practical use. In a continuous-time model uncertainty framework,\nwe consider a relaxed hedging criterion based on acceptable shortfall risks.\nCombining existing aggregation and convex dual representation theorems, we\nderive duality results for the minimal price on the set of upper semicontinuous\ndiscounted claims.\n"
    },
    {
        "paper_id": 1812.11201,
        "authors": "Laurence Carassus, Jan Obloj and Johannes Wiesel",
        "title": "The robust superreplication problem: a dynamic approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the frictionless discrete time financial market of Bouchard et al.(2015)\nwe consider a trader who, due to regulatory requirements or internal risk\nmanagement reasons, is required to hedge a claim $\\xi$ in a risk-conservative\nway relative to a family of probability measures $\\mathcal{P}$. We first\ndescribe the evolution of $\\pi_t(\\xi)$ - the superhedging price at time $t$ of\nthe liability $\\xi$ at maturity $T$ - via a dynamic programming principle and\nshow that $\\pi_t(\\xi)$ can be seen as a concave envelope of $\\pi_{t+1}(\\xi)$\nevaluated at today's prices. Then we consider an optimal investment problem for\na trader who is rolling over her robust superhedge and phrase this as a robust\nmaximisation problem, where the expected utility of inter-temporal consumption\nis optimised subject to a robust superhedging constraint. This utility\nmaximisation is carrried out under a new family of measures $\\mathcal{P}^u$,\nwhich no longer have to capture regulatory or institutional risk views but\nrather represent trader's subjective views on market dynamics. Under suitable\nassumptions on the trader's utility functions, we show that optimal investment\nand consumption strategies exist and further specify when, and in what sense,\nthese may be unique.\n"
    },
    {
        "paper_id": 1812.11226,
        "authors": "Li-Xin Wang",
        "title": "Fast Training Algorithms for Deep Convolutional Fuzzy Systems with\n  Application to Stock Index Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A deep convolutional fuzzy system (DCFS) on a high-dimensional input space is\na multi-layer connection of many low-dimensional fuzzy systems, where the input\nvariables to the low-dimensional fuzzy systems are selected through a moving\nwindow across the input spaces of the layers. To design the DCFS based on\ninput-output data pairs, we propose a bottom-up layer-by-layer scheme.\nSpecifically, by viewing each of the first-layer fuzzy systems as a weak\nestimator of the output based only on a very small portion of the input\nvariables, we design these fuzzy systems using the WM Method. After the\nfirst-layer fuzzy systems are designed, we pass the data through the first\nlayer to form a new data set and design the second-layer fuzzy systems based on\nthis new data set in the same way as designing the first-layer fuzzy systems.\nRepeating this process layer-by-layer we design the whole DCFS. We also propose\na DCFS with parameter sharing to save memory and computation. We apply the DCFS\nmodels to predict a synthetic chaotic plus random time-series and the real Hang\nSeng Index of the Hong Kong stock market.\n"
    },
    {
        "paper_id": 1812.11336,
        "authors": "Jamal Bouoiyour (CATT, IRMAPE), Refk Selmi (CATT, IRMAPE)",
        "title": "The gruesome murder of Jamal Khashoggi : Saudi Arabia's new economy\n  dream at risk ?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the horrific Jamal Khashoggi killing, Mohammed Bin Salman's image in the\ninternational community has been damaged. This study seeks to test whether\nKhashoggi murder discourage businesses from investing in Saudi Arabia. We use\nan event-study methodology and asset pricing model to assess, at sectoral\nlevel, the dynamics of stock prices surrounding the killing of the Saudi\njournalist on 2 October at the kingdom's consulate in Istanbul. A series of\nrobustness tests, including the Corrado ranking test and the non-parametric\nconditional distribution approach, have been conducted. We consistently show\nthat the khashoggi killing had the most adverse impact on banks and financial\nservices, materials, and technology. Oil and gas companies, however, were\nmoderately or insignificantly affected. Overall, our results suggest that the\ncrown prince's ambitious project for a Saudi Arabian economy moving beyond oil\nwealth are threatened as this recent event dampened foreign interest in\ninvesting in the kingdom.\n"
    },
    {
        "paper_id": 1812.11417,
        "authors": "Wolfgang Kuhle",
        "title": "Thought Viruses and Asset Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use insights from epidemiology, namely the SIR model, to study how agents\ninfect each other with \"investment ideas.\" Once an investment idea \"goes\nviral,\" equilibrium prices exhibit the typical \"fever peak,\" which is\ncharacteristic for speculative excesses. Using our model, we identify a time\nline of symptoms that indicate whether a boom is in its early or later stages.\nRegarding the market's top, we find that prices start to decline while the\nnumber of infected agents, who buy the asset, is still rising. Moreover, the\npresence of fully rational agents (i) accelerates booms (ii) lowers peak prices\nand (iii) produces broad, drawn-out, market tops.\n"
    },
    {
        "paper_id": 1812.1142,
        "authors": "Ali Kakhbod, Asuman Ozdaglar, Ian Schneider",
        "title": "Selling Wind",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We offer a parsimonious model to investigate how strategic wind producers\nsell energy under stochastic production constraints, where the extent of\nheterogeneity of wind energy availability varies according to wind farm\nlocations. The main insight of our analysis is that increasing heterogeneity in\nresource availability improves social welfare, as a function of its effects\nboth on improving diversification and on reducing withholding by firms. We show\nthat this insight is quite robust for any concave and downward-sloping inverse\ndemand function. The model is also used to analyze the effect of heterogeneity\non firm profits and opportunities for collusion. Finally, we analyze the\nimpacts of improving public information and weather forecasting; enhanced\npublic forecasting increases welfare, but it is not always in the best\ninterests of strategic producers.\n"
    },
    {
        "paper_id": 1812.11488,
        "authors": "Szabolcs Nagy",
        "title": "E-commerce in Hungary: A Market Analysis",
        "comments": null,
        "journal-ref": "Theory, Methodology, Practice, Vol. 12, No. 02, pp 25-32, (2016)",
        "doi": "10.18096/TMP.2016.03.03",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  E-commerce is on the rise in Hungary, with significantly growing numbers of\ncustomers shopping online. This paper aims to identify the direct and indirect\ndrivers of the double-digit growth rate, including the related macroeconomic\nindicators and the Digital Economy and Society Index (DESI). Moreover, this\nstudy provides a deep insight into industry trends and outlooks, including high\nindustry concentration and top industrial players. It also draws the profile of\nthe typical online shopper and the dominant characteristics of online\npurchases. Development of e-commerce is robust, but there is still plenty of\npotential for growth and progress in Hungary.\n"
    },
    {
        "paper_id": 1901.00191,
        "authors": "Lyudmyla Potrashkova, Diana Raiko, Leonid Tseitlin, Olga Savchenko,\n  Szabolcs Nagy",
        "title": "Methodological provisions for conducting empirical research of the\n  availability and implementation of the consumers socially responsible\n  intentions",
        "comments": null,
        "journal-ref": "Marketing and Management of Innovations 2018(3) pp 133-141",
        "doi": "10.21272/mmi.2018.3-11",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Social responsibility of consumers is one of the main conditions for the\nrecoupment of enterprises expenses associated with the implementation of social\nand ethical marketing tasks. Therefore, the enterprises, which plan to act on\nterms of social and ethical marketing, should monitor the social responsibility\nof consumers in the relevant markets. At the same time, special attention\nshould be paid to the analysis of factors that prevent consumers from\nimplementing their socially responsible intentions in the regions with a low\nlevel of social activity of consumers. The purpose of the article is to develop\nmethodological guidelines that determine the tasks and directions of conducting\nempirical studies aimed at assessing the gap between the socially responsible\nintentions of consumers and the actual implementation of these intentions, as\nwell as to identify the causes of this gap. An empirical survey of the sampled\nconsumers in Kharkiv was carried out in terms of the proposed methodological\nprovisions. It revealed a rather high level of respondents' willingness to\nsupport socially responsible enterprises and a rather low level of\nimplementation of these intentions due to the lack of consumers awareness. To\ntest the proposed methodological guidelines, an empirical study of the\nconsumers social responsibility was conducted in 2017 on a sample of students\nand professors of the Semen Kuznets Kharkiv National University of Economics\n(120 people). Questioning of the respondents was carried out using the Google\nForms. The finding allowed to make conclusion for existence of a high level of\nrespondents' willingness to support socially responsible and socially active\nenterprises. However, the study also revealed the existence of a significant\ngap between the intentions and actions of consumers, caused by the lack of\nawareness.\n"
    },
    {
        "paper_id": 1901.00227,
        "authors": "Shenhao Wang, Qingyi Wang, Jinhua Zhao",
        "title": "Multitask Learning Deep Neural Networks to Combine Revealed and Stated\n  Preference Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is an enduring question how to combine revealed preference (RP) and stated\npreference (SP) data to analyze travel behavior. This study presents a\nframework of multitask learning deep neural networks (MTLDNNs) for this\nquestion, and demonstrates that MTLDNNs are more generic than the traditional\nnested logit (NL) method, due to its capacity of automatic feature learning and\nsoft constraints. About 1,500 MTLDNN models are designed and applied to the\nsurvey data that was collected in Singapore and focused on the RP of four\ncurrent travel modes and the SP with autonomous vehicles (AV) as the one new\ntravel mode in addition to those in RP. We found that MTLDNNs consistently\noutperform six benchmark models and particularly the classical NL models by\nabout 5% prediction accuracy in both RP and SP datasets. This performance\nimprovement can be mainly attributed to the soft constraints specific to\nMTLDNNs, including its innovative architectural design and regularization\nmethods, but not much to the generic capacity of automatic feature learning\nendowed by a standard feedforward DNN architecture. Besides prediction, MTLDNNs\nare also interpretable. The empirical results show that AV is mainly the\nsubstitute of driving and AV alternative-specific variables are more important\nthan the socio-economic variables in determining AV adoption. Overall, this\nstudy introduces a new MTLDNN framework to combine RP and SP, and demonstrates\nits theoretical flexibility and empirical power for prediction and\ninterpretation. Future studies can design new MTLDNN architectures to reflect\nthe speciality of RP and SP and extend this work to other behavioral analysis.\n"
    },
    {
        "paper_id": 1901.00283,
        "authors": "Szabolcs Nagy",
        "title": "Digital Economy And Society. A Cross Country Comparison Of Hungary And\n  Ukraine",
        "comments": "ISSN 2519-4461 (print)",
        "journal-ref": "Visnyk Natsionalnogo Tekhichnogo Universytetu Kharkivskyj\n  Politekhnichnyj Instytut Ekonomichni Nauky: 46 (1267) pp. 174-179 (2017)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We live in the Digital Age in which both economy and society have been\ntransforming significantly. The Internet and the connected digital devices are\ninseparable parts of our daily life and the engine of the economic growth. In\nthis paper, first I analyzed the status of digital economy and society in\nHungary, then compared it with Ukraine and made conclusions regarding the\nfuture development tendencies. Using secondary data provided by the European\nCommission I investigated the five components of the Digital Economy and\nSociety Index of Hungary. I performed cross country analysis to find out the\nsignificant differences between Ukraine and Hungary in terms of access to the\nInternet and device use including smartphones, computers and tablets. Based on\nmy findings, I concluded that Hungary is more developed in terms of the\nsignificant parameters of the digital economy and society than Ukraine, but\neven Hungary is an emerging digital nation. Considering the high growth rate of\nInternet, tablet and smartphone penetration in both countries, I expect faster\nprogress in the development of the digital economy and society in Hungary and\nUkraine.\n"
    },
    {
        "paper_id": 1901.00345,
        "authors": "Ben-zhang Yang, Xinjiang He, Nan-jing Huang",
        "title": "Equilibrium price and optimal insider trading strategy under stochastic\n  liquidity with long memory",
        "comments": "21 pages; 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the Kyle model of insider trading is extended by\ncharacterizing the trading volume with long memory and allowing the noise\ntrading volatility to follow a general stochastic process. Under this newly\nrevised model, the equilibrium conditions are determined, with which the\noptimal insider trading strategy, price impact and price volatility are\nobtained explicitly. The volatility of the price volatility appears excessive,\nwhich is a result of the fact that a more aggressive trading strategy is chosen\nby the insider when uninformed volume is higher. The optimal trading strategy\nturns out to possess the property of long memory, and the price impact is also\naffected by the fractional noise.\n"
    },
    {
        "paper_id": 1901.00424,
        "authors": "Paolo Guasoni, Yu-Jui Huang",
        "title": "Consumption, Investment, and Healthcare with Aging",
        "comments": null,
        "journal-ref": "Finance and Stochastics, Vol. 23 (2019), Issue 2, pp 313-358",
        "doi": "10.1007/s00780-019-00383-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves the problem of optimal dynamic consumption, investment, and\nhealthcare spending with isoelastic utility, when natural mortality grows\nexponentially to reflect Gompertz' law and investment opportunities are\nconstant. Healthcare slows the natural growth of mortality, indirectly\nincreasing utility from consumption through longer lifetimes. Optimal\nconsumption and healthcare imply an endogenous mortality law that is\nasymptotically exponential in the old-age limit, with lower growth rate than\nnatural mortality. Healthcare spending steadily increases with age, both in\nabsolute terms and relative to total spending. The optimal stochastic control\nproblem reduces to a nonlinear ordinary differential equation with a unique\nsolution, which has an explicit expression in the old-age limit. The main\nresults are obtained through a novel version of Perron's method.\n"
    },
    {
        "paper_id": 1901.00495,
        "authors": "Shteryo Nozharov",
        "title": "The Institutional Economics of Collective Waste Recovery Systems: an\n  empirical investigation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of the study is to develop the model for transaction costs\nmeasurement in the Collective Waste Recovery Systems. The methodology of New\nInstitutional Economics is used in the research. The impact of the study is\nrelated both to the enlargement of the limits of the theory about the\ninteraction between transaction costs and social costs and to the\nidentification of institutional failures of the European concept for circular\neconomy. A new model for social costs measurement is developed. Keywords:\ncircular economy, transaction costs, extended producer responsibility JEL: A13,\nC51, D23, L22, Q53\n"
    },
    {
        "paper_id": 1901.00617,
        "authors": "Xue Cheng, Marina Di Giacinto, and Tai-Ho Wang",
        "title": "Optimal execution with dynamic risk adjustment",
        "comments": "34 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the problem of optimal liquidation of a position in a\nrisky security in a financial market, where price evolution are risky and\ntrades have an impact on price as well as uncertainty in the filling orders.\nThe problem is formulated as a continuous time stochastic optimal control\nproblem aiming at maximizing a generalized risk-adjusted profit and loss\nfunction. The expression of the risk adjustment is derived from the general\ntheory of dynamic risk measures and is selected in the class of $g$-conditional\nrisk measures. The resulting theoretical framework is nonclassical since the\ntarget function depends on backward components. We show that, under a quadratic\nspecification of the driver of a backward stochastic differential equation, it\nis possible to find a closed form solution and an explicit expression of the\noptimal liquidation policies. In this way it is immediate to quantify the\nimpact of risk-adjustment on the profit and loss and on the expression of the\noptimal liquidation policies.\n"
    },
    {
        "paper_id": 1901.00793,
        "authors": "Szabolcs Nagy",
        "title": "The Impact Of Country Of Origin In Mobile Phone Choice Of Generation Y\n  And Z",
        "comments": "14 p",
        "journal-ref": "Journal of Management and Training for Industries Vol.4, No.2 pp.\n  16-29. 2017",
        "doi": "10.12792/JMTI.4.2.16",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mobile phones play a very important role in our life. Mobile phone sales have\nbeen soaring over the last decade due to the growing acceptance of\ntechnological innovations, especially by Generations Y and Z. Understanding the\nchange in customers' requirement is the key to success in the smartphone\nbusiness. New, strong mobile phone models will emerge if the voice of the\ncustomer can be heard. Although it has been widely known that country of origin\nhas serious impact on the attitudes and purchase decisions of mobile phone\nconsumers, there lack substantial studies that investigate the mobile phone\npreference of young adults aged 18-25, members of late Generation Y and early\nGeneration Z. In order to investigate the role of country of origin in mobile\nphone choice of Generations Y and Z, an online survey with 228 respondents was\nconducted in Hungary in 2016. Besides the descriptive statistical methods,\ncrosstabs, ANOVA and Pearson correlation are used to analyze the collected data\nand find out significant relationships. Factor analysis (Principal Component\nAnalysis) is used for data reduction to create new factor components. The\nfindings of this exploratory study support the idea that country of origin\nplays a significant role in many respects related to young adults' mobile phone\nchoice. Mobile phone owners with different countries of origin attribute\ncrucial importance to the various product features including technical\nparameters, price, design, brand name, operating system, and memory size.\nCountry of origin has a moderating effect on the price sensitivity of consumers\nwith varied net income levels. It is also found that frequent buyers of mobile\nphones, especially US brand products, spend the most significant amount of\nmoney for their consumption in this aspect.\n"
    },
    {
        "paper_id": 1901.00814,
        "authors": "Anton Salikhmetov",
        "title": "Elementary Microeconomics of the Talmudic Rule",
        "comments": "4 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper takes a look at the Talmudic rule aka the 1/N rule aka the uniform\ninvestment strategy from the viewpoint of elementary microeconomics.\nSpecifically, we derive the cardinal utility function for a Talmud-obeying\nagent which happens to have the Cobb-Douglas form. Further, we investigate\nindividual supply and demand due to rebalancing and compare them to market\ndepth of an exchange. Finally, we discuss how operating as a liquidity provider\ncan benefit the Talmud-obeying agent with every exchange transaction in terms\nof the identified utility function.\n"
    },
    {
        "paper_id": 1901.00834,
        "authors": "Marcus Cordi, Damien Challet, Serge Kassibrakis",
        "title": "The market nanostructure origin of asset price time reversal asymmetry",
        "comments": "19 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a framework to infer lead-lag networks between the states of\nelements of complex systems, determined at different timescales. As such\nnetworks encode the causal structure of a system, infering lead-lag networks\nfor many pairs of timescales provides a global picture of the mutual influence\nbetween timescales. We apply our method to two trader-resolved FX data sets and\ndocument strong and complex asymmetric influence of timescales on the structure\nof lead-lag networks. Expectedly, this asymmetry extends to trader activity:\nfor institutional clients in our dataset, past activity on timescales longer\nthan 3 hours is more correlated with future activity at shorter timescales than\nthe opposite (Zumbach effect), while a reverse Zumbach effect is found for past\ntimescales shorter than 3 hours; retail clients have a totally different, and\nmuch more intricate, structure of asymmetric timescale influence. The causality\nstructures are clearly caused by markedly different behaviors of the two types\nof traders. Hence, market nanostructure, i.e., market dynamics at the\nindividual trader level, provides an unprecedented insight into the causality\nstructure of financial markets, which is much more complex than previously\nthought.\n"
    },
    {
        "paper_id": 1901.01486,
        "authors": "H. Dharma Kwon",
        "title": "Invest or Exit? Optimal Decisions in the Face of a Declining Profit\n  Stream",
        "comments": null,
        "journal-ref": "Operations Research, 58(3), 638-649 (2010)",
        "doi": "10.1287/opre.1090.0740",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Even in the face of deteriorating and highly volatile demand, firms often\ninvest in, rather than discard, aging technologies. In order to study this\nphenomenon, we model the firm's profit stream as a Brownian motion with\nnegative drift. At each point in time, the firm can continue operations, or it\ncan stop and exit the project. In addition, there is a one-time option to make\nan investment which boosts the project's profit rate. Using stochastic\nanalysis, we show that the optimal policy always exists and that it is\ncharacterized by three thresholds. There are investment and exit thresholds\nbefore investment, and there is a threshold for exit after investment. We also\neffect a comparative statics analysis of the thresholds with respect to the\ndrift and the volatility of the Brownian motion. When the profit boost upon\ninvestment is sufficiently large, we find a novel result: the investment\nthreshold decreases in volatility.\n"
    },
    {
        "paper_id": 1901.01751,
        "authors": "Adriano Koshiyama and Nick Firoozye and Philip Treleaven",
        "title": "Generative Adversarial Networks for Financial Trading Strategies\n  Fine-Tuning and Combination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systematic trading strategies are algorithmic procedures that allocate assets\naiming to optimize a certain performance criterion. To obtain an edge in a\nhighly competitive environment, the analyst needs to proper fine-tune its\nstrategy, or discover how to combine weak signals in novel alpha creating\nmanners. Both aspects, namely fine-tuning and combination, have been\nextensively researched using several methods, but emerging techniques such as\nGenerative Adversarial Networks can have an impact into such aspects.\nTherefore, our work proposes the use of Conditional Generative Adversarial\nNetworks (cGANs) for trading strategies calibration and aggregation. To this\npurpose, we provide a full methodology on: (i) the training and selection of a\ncGAN for time series data; (ii) how each sample is used for strategies\ncalibration; and (iii) how all generated samples can be used for ensemble\nmodelling. To provide evidence that our approach is well grounded, we have\ndesigned an experiment with multiple trading strategies, encompassing 579\nassets. We compared cGAN with an ensemble scheme and model validation methods,\nboth suited for time series. Our results suggest that cGANs are a suitable\nalternative for strategies calibration and combination, providing\noutperformance when the traditional techniques fail to generate any alpha.\n"
    },
    {
        "paper_id": 1901.01832,
        "authors": "Haibin Xie, Shouyang Wang",
        "title": "Timing the market: the economic value of price extremes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1186/s40854-018-0110-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By decomposing asset returns into potential maximum gain (PMG) and potential\nmaximum loss (PML) with price extremes, this study empirically investigated the\nrelationships between PMG and PML. We found significant asymmetry between PMG\nand PML. PML significantly contributed to forecasting PMG but not vice versa.\nWe further explored the power of this asymmetry for predicting asset returns\nand found it could significantly improve asset return predictability in both\nin-sample and out-of-sample forecasting. Investors who incorporate this\nasymmetry into their investment decisions can get substantial utility gains.\nThis asymmetry remains significant even when controlling for macroeconomic\nvariables, technical indicators, market sentiment, and skewness. Moreover, this\nasymmetry was found to be quite general across different countries.\n"
    },
    {
        "paper_id": 1901.01976,
        "authors": "Michele Starnini, Mari\\'an Bogu\\~n\\'a, and M. \\'Angeles Serrano",
        "title": "The interconnected wealth of nations: Shock propagation on global\n  trade-investment multiplex networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing integration of world economies, which organize in complex\nmultilayer networks of interactions, is one of the critical factors for the\nglobal propagation of economic crises. We adopt the network science approach to\nquantify shock propagation on the global trade-investment multiplex network. To\nthis aim, we propose a model that couples a Susceptible-Infected-Recovered\nepidemic spreading dynamics, describing how economic distress propagates\nbetween connected countries, with an internal contagion mechanism, describing\nthe spreading of such economic distress within a given country. At the local\nlevel, we find that the interplay between trade and financial interactions\ninfluences the vulnerabilities of countries to shocks. At the large scale, we\nfind a simple linear relation between the relative magnitude of a shock in a\ncountry and its global impact on the whole economic system, albeit the strength\nof internal contagion is country-dependent and the intercountry propagation\ndynamics is non-linear. Interestingly, this systemic impact can be predicted on\nthe basis of intra-layer and inter-layer scale factors that we name network\nmultipliers, that are independent of the magnitude of the initial shock. Our\nmodel sets-up a quantitative framework to stress-test the robustness of\nindividual countries and of the world economy to propagating crashes.\n"
    },
    {
        "paper_id": 1901.02246,
        "authors": "Giuseppe Orlando, Rosa Maria Mininni and Michele Bufalo",
        "title": "Forecasting interest rates through Vasicek and CIR models: a\n  partitioning approach",
        "comments": "Research artcile, 23 pages, 8 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to propose a new methodology that allows\nforecasting, through Vasicek and CIR models, of future expected interest rates\n(for each maturity) based on rolling windows from observed financial market\ndata. The novelty, apart from the use of those models not for pricing but for\nforecasting the expected rates at a given maturity, consists in an appropriate\npartitioning of the data sample. This allows capturing all the statistically\nsignificant time changes in volatility of interest rates, thus giving an\naccount of jumps in market dynamics. The performance of the new approach is\ncarried out for different term structures and is tested for both models. It is\nshown how the proposed methodology overcomes both the usual challenges (e.g.\nsimulating regime switching, volatility clustering, skewed tails, etc.) as well\nas the new ones added by the current market environment characterized by low to\nnegative interest rates.\n"
    },
    {
        "paper_id": 1901.02254,
        "authors": "Alexander Fromm",
        "title": "Evaluation of equity-based debt obligations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of participation rights, i.e. obligations issued by a\ncompany to investors who are interested in performance-based compensation.\nAlbeit having desirable economic properties equity-based debt obligations\n(EbDO) pose challenges in accounting and contract pricing. We formulate and\nsolve the associated mathematical problem in a discrete time, as well as a\ncontinuous time setting. In the latter case the problem is reduced to a\nforward-backward stochastic differential equation (FBSDE) and solved using the\nmethod of decoupling fields.\n"
    },
    {
        "paper_id": 1901.02327,
        "authors": "Alexander Barzykin and Fabrizio Lillo",
        "title": "Optimal VWAP execution under transient price impact",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve the problem of optimal liquidation with volume weighted average\nprice (VWAP) benchmark when the market impact is linear and transient. Our\nsetting is indeed more general as it considers the case when the trading\ninterval is not necessarily coincident with the benchmark interval:\nImplementation Shortfall and Target Close execution are shown to be particular\ncases of our setting. We find explicit solutions in continuous and discrete\ntime considering risk averse investors having a CARA utility function. Finally,\nwe show that, contrary to what is observed for Implementation Shortfall, the\noptimal VWAP solution contains both buy and sell trades also when the decay\nkernel is convex.\n"
    },
    {
        "paper_id": 1901.02384,
        "authors": "Juan Gonzalez-Blanco",
        "title": "Public Health and access to medicine. Pharmaceutical industry's role",
        "comments": "Paper written in Spanish",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Every year, 10 million people die from lack of access to treatment for\ncurable diseases, specially in developing countries. Meanwhile, legal but\nunsafe drugs cause 130 thousand deaths per year. How can this be happening in\n21st Century? What role does the pharmaceutical industry play in this tragedy?\nIn this research, WHO reports are analyzed and primary information gathered so\nas to answer this questions.\n"
    },
    {
        "paper_id": 1901.02391,
        "authors": "Bernardo Alves Furtado",
        "title": "Modeling tax distribution in metropolitan regions with PolicySpace",
        "comments": "10 pages, 1 figure, submitted to CAPS2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Brazilian executive body has consistently vetoed legislative initiatives\neasing creation and emancipation of municipalities. The literature lists\nevidence of the negative results of municipal fragmentation, especially so for\nmetropolitan regions. In order to provide evidences for the argument of\nmetropolitan union, this paper quantifies the quality of life of metropolitan\ncitizens in the face of four alternative rules of distribution of municipal tax\ncollection. Methodologically, a validated agent-based spatial model is\nsimulated. On top of that, econometric models are tested using real exogenous\nvariables and simulated data. Results suggest two central conclusions. First,\nthe progressiveness of the Municipal Participation Fund and its relevance to a\nbetter quality of life in metropolitan municipalities is confirmed. Second,\nmunicipal financial merging would improve citizens' quality of life, compared\nto the status quo for 23 Brazilian metropolises. Further, the paper presents\nquantitative evidence that allows comparing alternative tax distributions for\neach of the 40 simulated metropolises, identifying more efficient forms of\nfiscal distribution and contributing to the literature and to contemporary\nparliamentary debate.\n"
    },
    {
        "paper_id": 1901.02419,
        "authors": "Gordon V. Chavez",
        "title": "Dynamic tail inference with log-Laplace volatility",
        "comments": "Preprint, 27 pages, 7 figures, 3 tables",
        "journal-ref": "Extremes 23, 287-315 (2020)",
        "doi": "10.1007/s10687-019-00368-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a family of models that enable predictive estimation of\ntime-varying extreme event probabilities in heavy-tailed and nonlinearly\ndependent time series. The models are a white noise process with conditionally\nlog-Laplace stochastic volatility. In contrast to other, similar stochastic\nvolatility formalisms, this process has analytic expressions for its\nconditional probabilistic structure that enable straightforward estimation of\ndynamically changing extreme event probabilities. The process and volatility\nare conditionally Pareto-tailed, with tail exponent given by the reciprocal of\nthe log-volatility's mean absolute innovation. This formalism can accommodate a\nwide variety of nonlinear dependence, as well as conditional power law-tail\nbehavior ranging from weakly non-Gaussian to Cauchy-like tails. We provide a\ncomputationally straightforward estimation procedure that uses an asymptotic\napproximation of the process' dynamic large deviation probabilities. We\ndemonstrate the estimator's utility with a simulation study. We then show the\nmethod's predictive capabilities on a simulated nonlinear time series where the\nvolatility is driven by the chaotic Lorenz system. Lastly we provide an\nempirical application, which shows that this simple modeling method can be\neffectively used for dynamic and predictive tail inference in financial time\nseries.\n"
    },
    {
        "paper_id": 1901.02471,
        "authors": "Alexis Akira Toda and Yulong Wang",
        "title": "Efficient Minimum Distance Estimation of Pareto Exponent from Top Income\n  Shares",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/jae.2788",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an efficient estimation method for the income Pareto exponent when\nonly certain top income shares are observable. Our estimator is based on the\nasymptotic theory of weighted sums of order statistics and the efficient\nminimum distance estimator. Simulations show that our estimator has excellent\nfinite sample properties. We apply our estimation method to U.S. top income\nshare data and find that the Pareto exponent has been stable at around 1.5\nsince 1985, suggesting that the rise in inequality during the last three\ndecades is mainly driven by redistribution between the rich and poor, not among\nthe rich.\n"
    },
    {
        "paper_id": 1901.0248,
        "authors": "Chung-Han Hsieh, B. Ross Barmish, and John A. Gubner",
        "title": "On Positive Solutions of a Delay Equation Arising When Trading in\n  Financial Markets",
        "comments": "Accepted to IEEE Transactions on Automatic Control",
        "journal-ref": "IEEE Transactions on Automatic Control, AC-65, no. 7, pp.\n  3143-3149, 2020",
        "doi": "10.1109/TAC.2019.2945885",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete-time, linear state equation with delay which arises as\na model for a trader's account value when buying and selling a risky asset in a\nfinancial market. The state equation includes a nonnegative feedback gain\n$\\alpha$ and a sequence $v(k)$ which models asset returns which are within\nknown bounds but otherwise arbitrary. We introduce two thresholds, $\\alpha_-$\nand $\\alpha_+$, depending on these bounds, and prove that for $\\alpha <\n\\alpha_-$, state positivity is guaranteed for all time and all asset-return\nsequences; i.e., bankruptcy is ruled out and positive solutions of the state\nequation are continuable indefinitely. On the other hand, for $\\alpha >\n\\alpha_+$, we show that there is always a sequence of asset returns for which\nthe state fails to be positive for all time; i.e., along this sequence,\nbankruptcy is certain and the solution of the state equation ceases to be\nmeaningful after some finite time. Finally, this paper also includes a\nconjecture which says that for the \"gap\" interval $\\alpha_- \\leq \\alpha \\leq\n\\alpha_+,$ state positivity is also guaranteed for all time. Support for the\nconjecture, both theoretical and computational, is provided.\n"
    },
    {
        "paper_id": 1901.02505,
        "authors": "Roxana Dumitrescu",
        "title": "An optional decomposition of $\\mathscr{Y}^{g,\\xi}-submartingales$ and\n  applications to the hedging of American options in incomplete markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the recent paper \\cite{DESZ}, the notion of\n$\\mathscr{Y}^{g,\\xi}$-submartingale processes has been introduced. Within a\njump-diffusion model, we prove here that a process $X$ which satisfies the\nsimultaneous $\\mathscr{Y}^{\\mathbb{Q},g,\\xi}$ -submartingale property under a\nsuitable family of equivalent probability measures $\\mathbb{Q}$, admits a\n\\textit{nonlinear optional decomposition}. This is an analogous result to the\nwell known optional decomposition of simultaneous (classical and\n$\\mathscr{E}^g$-)supermartingales. We then apply this decomposition to the\nsuper-hedging problem of an American option in a jump-diffusion model, from the\nbuyer's point of view. We obtain an \\textit{infinitesimal characterization} of\nthe buyer's superhedging price, this result being completely new in the\nliterature. Indeed, it is well known that the seller's superheding price of an\nAmerican option admits an infinitesimal representation in terms of the\n\\textit{minimal supersolution of a constrained reflected BSDE}. To the best of\nour knowledge, no analogous result has been established for the buyer of the\nAmerican option in an incomplete market. Our results fill this gap, and show\nthat the buyer's super-hedging price admits an infinitesimal charcaterization\nin terms of the \\textit{maximal subsolution of a constrained reflected BSDE}.\n"
    },
    {
        "paper_id": 1901.02691,
        "authors": "Juho Kanniainen and Ye Yue",
        "title": "The Arrival of News and Return Jumps in Stock Markets: A Nonparametric\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a non-parametric framework to statistically examine how\nnews events, such as company or macroeconomic announcements, contribute to the\npre- and post-event jump dynamics of stock prices under the intraday\nseasonality of the news and jumps. We demonstrate our framework, which has\nseveral advantages over the existing methods, by using data for i) the S&P 500\nindex ETF, SPY, with macroeconomic announcements and ii) Nasdaq Nordic\nLarge-Cap stocks with scheduled and non-scheduled company announcements. We\nprovide strong evidence that non-scheduled company announcements and some\nmacroeconomic announcements contribute jumps that follow the releases and also\nsome evidence for pre-jumps that precede the scheduled arrivals of public\ninformation, which may indicate non-gradual information leakage. Especially\ninterim reports of Nordic large-cap companies are found containing important\ninformation to yield jumps in stock prices. Additionally, our results show that\nreleases of unexpected information are not reacted to uniformly across Nasdaq\nNordic markets, even if they are jointly operated and are based on the same\nexchange rules.\n"
    },
    {
        "paper_id": 1901.02715,
        "authors": "Yanling Chang, Eleftherios Iakovou and Weidong Shi4",
        "title": "Blockchain in Global Supply Chains and Cross Border Trade: A Critical\n  Synthesis of the State-of-the-Art, Challenges and Opportunities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain in supply chain management is expected to boom over the next five\nyears. It is estimated that the global blockchain supply chain market would\ngrow at a compound annual growth rate of 87% and increase from \\$45 million in\n2018 to \\$3,314.6 million by 2023. Blockchain will improve business for all\nglobal supply chain stakeholders by providing enhanced traceability,\nfacilitating digitisation, and securing chain-of-custody. This paper provides a\nsynthesis of the existing challenges in global supply chain and trade\noperations, as well as the relevant capabilities and potential of blockchain.\nWe further present leading pilot initiatives on applying blockchains to supply\nchains and the logistics industry to fulfill a range of needs. Finally, we\ndiscuss the implications of blockchain on customs and governmental agencies,\nsummarize challenges in enabling the wide scale deployment of blockchain in\nglobal supply chain management, and identify future research directions.\n"
    },
    {
        "paper_id": 1901.02995,
        "authors": "Oscar Lopez, Gerardo E. Oleaga, Alejandra Sanchez",
        "title": "Jump-telegraph models for the short rate: pricing and convexity\n  adjustments of zero coupon bonds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider a Markov-modulated model with jumps for short\nrate dynamics. We obtain closed formulas for the term structure and forward\nrates using the properties of the jump-telegraph process and the expectation\nhypothesis. The results are compared with the numerical solution of the\ncorresponding partial differential equation.\n"
    },
    {
        "paper_id": 1901.03021,
        "authors": "Kei Noba, Jos\\'e-Luis P\\'erez and Xiang Yu",
        "title": "On the bail-out dividend problem for spectrally negative Markov additive\n  models",
        "comments": "Final version, to appear in SIAM Journal on Control and Optimization.\n  Keywords: Refracted-reflected spectrally negative Levy process, capital\n  injection, absolutely continuous constraint, regime-switching, fixed point\n  argument",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the bail-out optimal dividend problem with regime\nswitching under the constraint that the cumulative dividend strategy is\nabsolutely continuous. We confirm the optimality of the regime-modulated\nrefraction-reflection strategy when the underlying risk model follows a general\nspectrally negative Markov additive process. To verify the conjecture of a\nbarrier type optimal control, we first introduce and study an auxiliary problem\nwith the final payoff at an exponential terminal time and characterize the\noptimal threshold explicitly using fluctuation identities of the\nrefracted-reflected Levy process. Second, we transform the problem with\nregime-switching into an equivalent local optimization problem with a final\npayoff up to the first regime switching time. The refraction-reflection\nstrategy with regime-modulated thresholds can be shown as optimal by using\nresults in the first step and some fixed point arguments for auxiliary\nrecursive iterations.\n"
    },
    {
        "paper_id": 1901.0303,
        "authors": "Jie Xiong, Zuo quan Xu and Jiayu Zheng",
        "title": "Mean-variance portfolio selection under partial information with drift\n  uncertainty",
        "comments": "23 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the mean-variance portfolio selection problem under\npartial information with drift uncertainty. First we show that the market model\nis complete even in this case while the information is not complete and the\ndrift is uncertain. Then, the optimal strategy based on partial information is\nderived, which reduces to solving a related backward stochastic differential\nequation (BSDE). Finally, we propose an efficient numerical scheme to\napproximate the optimal portfolio that is the solution of the BSDE mentioned\nabove. Malliavin calculus and the particle representation play important roles\nin this scheme.\n"
    },
    {
        "paper_id": 1901.03356,
        "authors": "Christopher Boudreaux",
        "title": "When does privatization spur entrepreneurial performance? The moderating\n  effect of institutional quality in an emerging market",
        "comments": "40 pages, 7 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore how institutional quality moderates the effectiveness of\nprivatization on entrepreneurs sales performance. To do this, we blend agency\ntheory and entrepreneurial cognition theory with insights from institutional\neconomics to develop a model of emerging market venture performance. Using data\nfrom the World Banks Enterprise Survey of entrepreneurs in China, our results\nsuggest that private-owned enterprises (POEs) outperform state-owned\nenterprises (SOEs) but only in environments with high-quality market\ninstitutions. In environments with low-quality market institutions, SOEs\noutperform POEs. These findings suggest that the effectiveness of privatization\non entrepreneurial performance is context-specific, which reveals more nuance\nthan previously has been attributed.\n"
    },
    {
        "paper_id": 1901.03478,
        "authors": "Ruimeng Hu",
        "title": "Deep Learning for Ranking Response Surfaces with Applications to Optimal\n  Stopping Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose deep learning algorithms for ranking response\nsurfaces, with applications to optimal stopping problems in financial\nmathematics. The problem of ranking response surfaces is motivated by\nestimating optimal feedback policy maps in stochastic control problems, aiming\nto efficiently find the index associated to the minimal response across the\nentire continuous input space $\\mathcal{X} \\subseteq \\mathbb{R}^d$. By\nconsidering points in $\\mathcal{X}$ as pixels and indices of the minimal\nsurfaces as labels, we recast the problem as an image segmentation problem,\nwhich assigns a label to every pixel in an image such that pixels with the same\nlabel share certain characteristics. This provides an alternative method for\nefficiently solving the problem instead of using sequential design in our\nprevious work [R. Hu and M. Ludkovski, SIAM/ASA Journal on Uncertainty\nQuantification, 5 (2017), 212--239].\n  Deep learning algorithms are scalable, parallel and model-free, i.e., no\nparametric assumptions needed on the response surfaces. Considering ranking\nresponse surfaces as image segmentation allows one to use a broad class of deep\nneural networks, e.g., UNet, SegNet, DeconvNet, which have been widely applied\nand numerically proved to possess high accuracy in the field. We also\nsystematically study the dependence of deep learning algorithms on the input\ndata generated on uniform grids or by sequential design sampling, and observe\nthat the performance of deep learning is {\\it not} sensitive to the noise and\nlocations (close to/away from boundaries) of training data. We present a few\nexamples including synthetic ones and the Bermudan option pricing problem to\nshow the efficiency and accuracy of this method.\n"
    },
    {
        "paper_id": 1901.03544,
        "authors": "Laura Abramovsky (1), Britta Augsburg (1), Melanie L\\\"uhrmann (2 and\n  1), Francisco Oteiza (3), Juan Pablo Rud (2 and 1) ((1) Centre for the\n  Evaluation of Social Policies (EDePo) Institute for Fiscal Studies, (2) Royal\n  Holloway Department of Economics, (3) UCL Institute of Education)",
        "title": "Community Matters: Heterogeneous Impacts of a Sanitation Intervention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effectiveness of a community-level information intervention\naimed at improving sanitation using a cluster-randomized controlled trial (RCT)\nin Nigerian communities. The intervention, Community-Led Total Sanitation\n(CLTS), is currently part of national sanitation policy in more than 25\ncountries. While average impacts are exiguous almost three years after\nimplementation at scale, the results hide important heterogeneity: the\nintervention has strong and lasting effects on sanitation practices in poorer\ncommunities. These are realized through increased sanitation investments. We\nshow that community wealth, widely available in secondary data, is a key\nstatistic for effective intervention targeting. Using data from five other\nsimilar randomized interventions in various contexts, we find that\ncommunity-level wealth heterogeneity can rationalize the wide range of impact\nestimates in the literature. This exercise provides plausible external validity\nto our findings, with implications for intervention scale-up. JEL Codes: O12,\nI12, I15, I18.\n"
    },
    {
        "paper_id": 1901.03645,
        "authors": "Nawapon Nakharutai, Camila C. S. Caiado, Matthias C. M. Troffaes",
        "title": "Evaluating betting odds and free coupons using desirability",
        "comments": "24 pages, 9 tables",
        "journal-ref": "International Journal of Approximate Reasoning 106 (2019) 128-145",
        "doi": "10.1016/j.ijar.2019.01.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the UK betting market, bookmakers often offer a free coupon to new\ncustomers. These free coupons allow the customer to place extra bets, at lower\nrisk, in combination with the usual betting odds. We are interested in whether\na customer can exploit these free coupons in order to make a sure gain, and if\nso, how the customer can achieve this. To answer this question, we evaluate the\nodds and free coupons as a set of desirable gambles for the bookmaker. We show\nthat we can use the Choquet integral to check whether this set of desirable\ngambles incurs sure loss for the bookmaker, and hence, results in a sure gain\nfor the customer. In the latter case, we also show how a customer can determine\nthe combination of bets that make the best possible gain, based on\ncomplementary slackness. As an illustration, we look at some actual betting\nodds in the market and find that, without free coupons, the set of desirable\ngambles derived from those odds avoids sure loss. However, with free coupons,\nwe identify some combinations of bets that customers could place in order to\nmake a guaranteed gain.\n"
    },
    {
        "paper_id": 1901.03691,
        "authors": "Jean-Philippe Bouchaud",
        "title": "Econophysics: Still fringe after 30 years?",
        "comments": "5 pages + references, 1 figure",
        "journal-ref": null,
        "doi": "10.1051/epn/2019103",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some personal reflections on the past and future of \"econophysics\", to appear\nin Europhysics News\n"
    },
    {
        "paper_id": 1901.03698,
        "authors": "Bent Flyvbjerg and Alexander Budzier",
        "title": "Report for the Commission of Inquiry Respecting the Muskrat Falls\n  Project",
        "comments": "arXiv admin note: text overlap with arXiv:1802.07312 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report was commissioned by the Commission of Inquiry Respecting the\nMuskrat Falls Project to provide the national and international context in\nwhich the Muskrat Falls Project took place. The Commission asked for the report\nto cover three specific topics of questions: (1) What is the national and\ninternational context of the Muskrat Falls Project with regards to cost overrun\nand schedule overrun? (What are the typical cost and schedule overruns of\nhydro-electric dam projects? How do hydro-electric dams compare to other\ncapital investment projects? How do Canadian projects compare to other\ncountries?), (2) What are the causes and root causes of cost and schedule\noverruns? (3) What are recommendations, based on international experience and\nresearch into capital investment projects, to prevent cost and schedule\noverruns in hydro-electric dam projects and other capital investment projects?\n  Keywords: Hydroelectric Dams, Megaprojects, Cost Overrun, Schedule Overrun,\nOptimism Bias, Strategic Misrepresentation, Infrastructure, Capital Investment\nProjects, Canada, Muskrat Falls\n"
    },
    {
        "paper_id": 1901.03843,
        "authors": "Alex A.T. Rathke",
        "title": "Fuzzy Profit Shifting: A Model for Optimal Tax-induced Transfer Pricing\n  with Fuzzy Arm's Length Parameter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a model of optimal tax-induced transfer pricing with a\nfuzzy arm's length parameter. Fuzzy numbers provide a suitable structure for\nmodelling the ambiguity that is intrinsic to the arm's length parameter. For\nthe usual conditions regarding the anti-shifting mechanisms, the optimal\ntransfer price becomes a maximising $\\alpha$-cut of the fuzzy arm's length\nparameter. Nonetheless, we show that it is profitable for firms to choose any\nmaximising transfer price if the probability of tax audit is sufficiently low,\neven if the chosen price is considered a completely non-arm's length price by\ntax authorities. In this case, we derive the necessary and sufficient\nconditions to prevent this extreme shifting strategy\n"
    },
    {
        "paper_id": 1901.03874,
        "authors": "Junbeom Lee and Stephan Sturm and Chao Zhou",
        "title": "A Risk-Sharing Framework of Bilateral Contracts",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a two-agent problem which is inspired by price asymmetry arising\nfrom funding difference. When two parties have different funding rates, the two\nparties deduce different fair prices for derivative contracts even under the\nsame pricing methodology and parameters. Thus, the two parties should enter the\nderivative contracts with a negotiated price, and we call the negotiation a\nrisk-sharing problem. This framework defines the negotiation as a problem that\nmaximizes the sum of utilities of the two parties. By the derived optimal\nprice, we provide a theoretical analysis on how the price is determined between\nthe two parties. As well as the price, the risk-sharing framework produces an\noptimal amount of collateral. The derived optimal collateral can be used for\ncontracts between financial firms and non-financial firms. However,\ninter-dealers markets are governed by regulations. As recommended in Basel III,\nit is a convention in inter-dealer contracts to pledge the full amount of a\nclose-out price as collateral. In this case, using the optimal collateral, we\ninterpret conditions for the full margin requirement to be indeed optimal.\n"
    },
    {
        "paper_id": 1901.03889,
        "authors": "Cl\\'ement Le Ludec, Paola Tubaro, Antonio A. Casilli",
        "title": "How many people microwork in France? Estimating the size of a new labor\n  force",
        "comments": "9 pages, 1 figure, 7 tables. The article is part of the DiPLab\n  (\"Digital Platform Labor\") research project. For more information:\n  http://diplab.eu/",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Microwork platforms allocate fragmented tasks to crowds of providers with\nremunerations as low as few cents. Instrumental to the development of today's\nartificial intelligence, these micro-tasks push to the extreme the logic of\ncasualization already observed in \"uberized\" workers. The present article uses\nthe results of the DiPLab study to estimate the number of people who microwork\nin France. We distinguish three categories of microworkers, corresponding to\ndifferent modes of engagement: a group of 14,903 \"very active\" microworkers,\nmost of whom are present on these platforms at least once a week; a second\nfeaturing 52,337 \"routine\" microworkers, more selective and present at least\nonce a month; a third circle of 266,126 \"casual\" microworkers, more\nheterogeneous and who alternate inactivity and various levels of work practice.\nOur results show that microwork is comparable to, and even larger than, the\nworkforce of ride-sharing and delivery platforms in France. It is therefore not\nan anecdotal phenomenon and deserves great attention from researchers, unions\nand policy-makers.\n"
    },
    {
        "paper_id": 1901.03951,
        "authors": "Simone Righi, Yuri Biondi",
        "title": "Inequality, mobility and the financial accumulation process: A\n  computational economic analysis",
        "comments": "31 pages, 10 figures. Accepted for publication in Journal of Economic\n  Interaction and Coordination",
        "journal-ref": "Journal of Economic Interaction and Coordination (2020), online",
        "doi": "10.1007/s11403-019-00236-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our computational economic analysis investigates the relationship between\ninequality, mobility and the financial accumulation process. Extending the\nbaseline model by Levy et al., we characterise the economic process through\nstylised return structures generating alternative evolutions of income and\nwealth through time. First, we explore the limited heuristic contribution of\none and two factors models comprising one single stock (capital wealth) and one\nsingle flow factor (labour) as pure drivers of income and wealth generation and\nallocation over time. Second, we introduce heuristic modes of taxation in line\nwith the baseline approach. Our computational economic analysis corroborates\nthat the financial accumulation process featuring compound returns plays a\nsignificant role as source of inequality, while institutional arrangements\nincluding taxation play a significant role in framing and shaping the aggregate\neconomic process that evolves over socioeconomic space and time.\n"
    },
    {
        "paper_id": 1901.0412,
        "authors": "H. Dharma Kwon, Steven A. Lippman",
        "title": "Acquisition of Project-Specific Assets with Bayesian Updating",
        "comments": null,
        "journal-ref": "Operations Research, 2011, Vol. 59, No. 5, Pages 1119 -1130",
        "doi": "10.1287/opre.1110.0949",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of learning on the optimal policy and the\ntime-to-decision in an infinite-horizon Bayesian sequential decision model with\ntwo irreversible alternatives, exit and expansion. In our model, a firm\nundertakes a small-scale pilot project so as to learn, via Bayesian updating,\nabout the project\\textquoteright s profitability, which is known to be in one\nof two possible states. The firm continuously observes the\nproject\\textquoteright s cumulative profit, but the true state of the\nprofitability is not immediately revealed because of the inherent noise in the\nprofit stream. The firm bases its exit or expansion decision on the posterior\nprobability distribution of the profitability. The optimal policy is\ncharacterized by a pair of thresholds for the posterior probability. We find\nthat the time-to-decision does not necessarily have a monotonic relation with\nthe arrival rate of new information.\n"
    },
    {
        "paper_id": 1901.042,
        "authors": "Dmitri Goloubentsev, Evgeny Lakshtanov",
        "title": "Remarks on stochastic automatic adjoint differentiation and financial\n  models calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we discuss the Automatic Adjoint Differentiation (AAD) for\nfunctions of the form $G=\\frac{1}{2}\\sum_1^m (Ey_i-C_i)^2$, which often appear\nin the calibration of stochastic models. { We demonstrate that it allows a\nperfect SIMD\\footnote{Single Input Multiple Data} parallelization and provide\nits relative computational cost. In addition we demonstrate that this\ntheoretical result is in concordance with numeric experiments.}\n"
    },
    {
        "paper_id": 1901.04265,
        "authors": "Ali Haeri and Abbas Arabmazar",
        "title": "Designing An Industrial Policy For Developing Countries: A New Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, the prevalent methodology for design of the industrial policy\nin developing countries was critically assessed, and it was shown that the\nmechanism and content of classical method is fundamentally contradictory to the\ngoals and components of the endogenous growth theories. This study, by\nproposing a new approach, along settling Schumpeter's economic growth theory as\na policy framework, designed the process of entering, analyzing and processing\ndata as the mechanism of the industrial policy in order to provide \"theoretical\nconsistency\" and \"technical and Statistical requirements\" for targeting the\ngrowth stimulant factor effectively.\n"
    },
    {
        "paper_id": 1901.04689,
        "authors": "Jan Dhaene, Roger J. A. Laeven, Yiying Zhang",
        "title": "Systemic Risk: Conditional Distortion Risk Measures",
        "comments": "41 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce the rich classes of conditional distortion (CoD)\nrisk measures and distortion risk contribution ($\\Delta$CoD) measures as\nmeasures of systemic risk and analyze their properties and representations. The\nclasses include the well-known conditional Value-at-Risk, conditional Expected\nShortfall, and risk contribution measures in terms of the VaR and ES as special\ncases. Sufficient conditions are presented for two random vectors to be ordered\nby the proposed CoD-risk measures and distortion risk contribution measures.\nThese conditions are expressed using the conventional stochastic dominance,\nincreasing convex/concave, dispersive, and excess wealth orders of the\nmarginals and canonical positive/negative stochastic dependence notions.\nNumerical examples are provided to illustrate our theoretical findings. This\npaper is the second in a triplet of papers on systemic risk by the same\nauthors. In \\cite{DLZorder2018a}, we introduce and analyze some new stochastic\norders related to systemic risk. In a third (forthcoming) paper, we attribute\nsystemic risk to the different participants in a given risky environment.\n"
    },
    {
        "paper_id": 1901.0477,
        "authors": "Nikolai Zaitsev",
        "title": "Empirical forward price distribution from Bitcoin option prices",
        "comments": "15 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Report presents analysis of empirical distribution of future returns of\nbitcoin (BTC) from BTUSD inverse option prices. Logistic pdf is chosen as\nunderlying distribution to fit option prices. The result is satisfactory and\nsuggests that these prices can be described with just three or even one\nparameter. Fitted Logistic pdf matches forward price movements upto a scaling\nfactor. Nevertheless, this observation stands alone and does not allow\nstochastic description of underlying prices with logistic pdf in similar\nfashion as it is done within Black-Scholes modelling framework. Put-call parity\nrelationship is derived connecting prices of vanilla inverse options and\nfutures.\n"
    },
    {
        "paper_id": 1901.04819,
        "authors": "Kai-Kristian Kemell, Xiaofeng Wang, Anh Nguyen-Duc, Jason Grendus,\n  Tuure Tuunanen and Pekka Abrahamsson",
        "title": "100+ Metrics for Software Startups - A Multi-Vocal Literature Review",
        "comments": "Published in the proceedings of The 1st Software-intensive Business\n  Workshop on Start-ups, Platforms and Ecosystems (SiBW 2018), Espoo, December\n  3rd, 2018. http://ceur-ws.org/Vol-2305/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Metrics can be used by businesses to make more objective decisions based on\ndata. Software startups in particular are characterized by the uncertain or\neven chaotic nature of the contexts in which they operate. Using data in the\nform of metrics can help software startups to make the right decisions amidst\nuncertainty and limited resources. However, whereas conventional business\nmetrics and software metrics have been studied in the past, metrics in the\nspe-cific context of software startup are not widely covered within academic\nliterature. To promote research in this area and to create a starting point for\nit, we have conducted a multi-vocal literature review focusing on practitioner\nliterature in order to compile a list of metrics used by software startups.\nSaid list is intended to serve as a basis for further research in the area, as\nthe metrics in it are based on suggestions made by practitioners and not\nempirically verified.\n"
    },
    {
        "paper_id": 1901.04928,
        "authors": "Tim Shuliar, Nikita Goldsmit",
        "title": "PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency\n  issuance protocol",
        "comments": "PDF, 9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we will describe a concept of a cryptocurrency issuance\nprotocol which supports digital currencies in a Proof-of-Work (< PoW >) like\nmanner. However, the methods assume alternative utilization of assets used for\ncryptocurrency creation (rather than purchasing electricity necessary for <\nmining >).\n"
    },
    {
        "paper_id": 1901.04945,
        "authors": "Sandhya Devi",
        "title": "Financial Portfolios based on Tsallis Relative Entropy as the Risk\n  Measure",
        "comments": "26 pages, 9 figures Abstract re-written - An error in Figure 3 for\n  relative sigma is corrected. More results added to Results section. Previous\n  results unchanged. Section 4 -Summary and Conclusions, re-written,\n  Conclusions unchanged",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/ab3bc5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Earlier studies have shown that stock market distributions can be well\ndescribed by distributions derived from Tsallis entropy, which is a\ngeneralization of Shannon entropy to non-extensive systems. In this paper,\nTsallis relative entropy (TRE), which is the generalization of Kullback-Leibler\nrelative entropy (KLRE) to non-extensive systems, is investigated as a possible\nrisk measure in constructing risk optimal portfolios whose returns beat market\nreturns. Portfolios are constructed by binning the risk values and allocating\nthe stocks to bins according to their risk values. The average return in excess\nof market returns for each bin is calculated to get the risk-return patterns of\nthe portfolios. The results are compared with those from three other risk\nmeasures: 1) the commonly used 'beta' of the Capital Asset Pricing Model\n(CAPM), 2) Kullback-Leibler relative entropy, and 3) the relative standard\ndeviation. Tests carried out for both long (~18 years) and shorter terms (~9\nyears), which include the dot-com bubble and the 2008 crash periods, show that\na linear fit can be obtained for the risk-excess return profiles of all four\nrisk measures. However, in all cases, the profiles from Tsallis relative\nentropy show a more consistent behavior in terms of both goodness of fit and\nthe variation of returns with risk, than the other three risk measures.\n"
    },
    {
        "paper_id": 1901.04967,
        "authors": "Higor Y. D. Sigaki, Matjaz Perc, Haroldo V. Ribeiro",
        "title": "Clustering patterns in efficiency and the coming-of-age of the\n  cryptocurrency market",
        "comments": "10 pages, 4 figures; accepted for publication in Scientific Reports",
        "journal-ref": "Sci. Rep. 9, 1440 (2019)",
        "doi": "10.1038/s41598-018-37773-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The efficient market hypothesis has far-reaching implications for financial\ntrading and market stability. Whether or not cryptocurrencies are\ninformationally efficient has therefore been the subject of intense recent\ninvestigation. Here, we use permutation entropy and statistical complexity over\nsliding time-windows of price log returns to quantify the dynamic efficiency of\nmore than four hundred cryptocurrencies. We consider that a cryptocurrency is\nefficient within a time-window when these two complexity measures are\nstatistically indistinguishable from their values obtained on randomly shuffled\ndata. We find that 37% of the cryptocurrencies in our study stay efficient over\n80% of the time, whereas 20% are informationally efficient in less than 20% of\nthe time. Our results also show that the efficiency is not correlated with the\nmarket capitalization of the cryptocurrencies. A dynamic analysis of\ninformational efficiency over time reveals clustering patterns in which\ndifferent cryptocurrencies with similar temporal patterns form four clusters,\nand moreover, younger currencies in each group appear poised to follow the\ntrend of their 'elders'. The cryptocurrency market thus already shows notable\nadherence to the efficient market hypothesis, although data also reveals that\nthe coming-of-age of digital currencies is in this regard still very much\nunderway.\n"
    }
]