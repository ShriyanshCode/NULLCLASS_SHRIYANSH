[
    {
        "paper_id": 2307.09332,
        "authors": "Christopher Gerling",
        "title": "Company2Vec -- German Company Embeddings based on Corporate Websites",
        "comments": "Accepted for Publication in: International Journal of Information\n  Technology & Decision Making (2023)",
        "journal-ref": null,
        "doi": "10.1142/S0219622023500694",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With Company2Vec, the paper proposes a novel application in representation\nlearning. The model analyzes business activities from unstructured company\nwebsite data using Word2Vec and dimensionality reduction. Company2Vec maintains\nsemantic language structures and thus creates efficient company embeddings in\nfine-granular industries. These semantic embeddings can be used for various\napplications in banking. Direct relations between companies and words allow\nsemantic business analytics (e.g. top-n words for a company). Furthermore,\nindustry prediction is presented as a supervised learning application and\nevaluation method. The vectorized structure of the embeddings allows measuring\ncompanies similarities with the cosine distance. Company2Vec hence offers a\nmore fine-grained comparison of companies than the standard industry labels\n(NACE). This property is relevant for unsupervised learning tasks, such as\nclustering. An alternative industry segmentation is shown with k-means\nclustering on the company embeddings. Finally, this paper proposes three\nalgorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric\npeer-firm identification.\n"
    },
    {
        "paper_id": 2307.09392,
        "authors": "Umut Cetin and Kasper Larsen",
        "title": "Is Kyle's equilibrium model stable?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the dynamic discrete-time trading setting of Kyle (1985), we prove that\nKyle's equilibrium model is stable when there are one or two trading times. For\nthree or more trading times, we prove that Kyle's equilibrium is not stable.\nThese theoretical results are proven to hold irrespectively of all Kyle's input\nparameters.\n"
    },
    {
        "paper_id": 2307.09617,
        "authors": "Michael Seigne, Joerg Osterrieder",
        "title": "The Great Deception: A Comprehensive Study of Execution Strategies in\n  Corporate Share Buy-Backs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We delve into the intricate world of share buy-backs, a strategic corporate\ncapital allocation tool that has gained significant prominence over the past\nfew decades. Despite being the subject of extensive research and debate, the\nexecution phase of these transactions remains an underexplored area. This lack\nof research into the execution phase is surprising, especially when compared to\nthe extensive literature on other capital allocation decisions, such as\nacquisition pricing. We aim to shed light on the execution practices of share\nbuy-backs, highlighting challenges and opportunities that arise and presenting\na comparative analysis with other aspects of capital allocation. From an\noutsider's perspective, this paper analyses the seemingly disparate practices\nand aims to uncover the \"dark arts\" of execution. The intention is to assist\ncorporations, investors, and regulators in better understanding the intricacies\nof executing share buy-backs, emphasizing the need for a cost-efficient and\nregulatory compliant service for corporations trading their own listed\nsecurities. We question the seemingly disproportionate fees charged to\ncorporations for similar or even inferior outcomes, compared to retail and\ninstitutional investors. We illustrate potential inefficiencies and frictional\ncosts in the current execution phase. The examples highlight the need for\ngreater transparency and fairness in share buy-back executions, advocating for\nmore equitable processes that benefit all stakeholders in the capital markets\necosystem. With the surge in regulatory attention and political pressure fueled\nby the rising prominence of buy-backs and evolving ESG considerations, it is\ncrucial that we enhance understanding of this key area. This paper seeks to\nfoster dialogue and encourage transparency, thus promoting the efficient use of\nresources in capital markets, and ultimately, benefiting shareholders.\n"
    },
    {
        "paper_id": 2307.09631,
        "authors": "Eduardo C. Garrido-Merch\\'an, Sol Mora-Figueroa-Cruz-Guzm\\'an, Mar\\'ia\n  Coronado-Vaca",
        "title": "Deep Reinforcement Learning for ESG financial portfolio management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the application of Deep Reinforcement Learning (DRL)\nfor Environment, Social, and Governance (ESG) financial portfolio management,\nwith a specific focus on the potential benefits of ESG score-based market\nregulation. We leveraged an Advantage Actor-Critic (A2C) agent and conducted\nour experiments using environments encoded within the OpenAI Gym, adapted from\nthe FinRL platform. The study includes a comparative analysis of DRL agent\nperformance under standard Dow Jones Industrial Average (DJIA) market\nconditions and a scenario where returns are regulated in line with company ESG\nscores. In the ESG-regulated market, grants were proportionally allotted to\nportfolios based on their returns and ESG scores, while taxes were assigned to\nportfolios below the mean ESG score of the index. The results intriguingly\nreveal that the DRL agent within the ESG-regulated market outperforms the\nstandard DJIA market setup. Furthermore, we considered the inclusion of ESG\nvariables in the agent state space, and compared this with scenarios where such\ndata were excluded. This comparison adds to the understanding of the role of\nESG factors in portfolio management decision-making. We also analyze the\nbehaviour of the DRL agent in IBEX 35 and NASDAQ-100 indexes. Both the A2C and\nProximal Policy Optimization (PPO) algorithms were applied to these additional\nmarkets, providing a broader perspective on the generalization of our findings.\nThis work contributes to the evolving field of ESG investing, suggesting that\nmarket regulation based on ESG scoring can potentially improve DRL-based\nportfolio management, with significant implications for sustainable investing\nstrategies.\n"
    },
    {
        "paper_id": 2307.09634,
        "authors": "Jos\\'e Alfonso Mu\\~noz-Alvarado",
        "title": "Power to the teens? A model of parents' and teens' collective labor\n  supply",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Teens make life-changing decisions while constrained by the needs and\nresources of the households they grow up in. Household behavior models\nfrequently delegate decision-making to the teen or their parents, ignoring\njoint decision-making in the household. I show that teens and parents allocate\ntime and income jointly by using data from the Costa Rican Encuesta Nacional de\nHogares from 2011 to 2019 and a conditional cash transfer program. First, I\npresent gender differences in household responses to the transfer using a\nmarginal treatment effect framework. Second, I explain how the gender gap from\nthe results is due to the bargaining process between parents and teens. I\npropose a collective household model and show that sons bargain cooperatively\nwith their parents while daughters do not. This result implies that sons have a\nhigher opportunity cost of attending school than daughters. Public policy\ntargeting teens must account for this gender disparity to be effective.\n"
    },
    {
        "paper_id": 2307.09669,
        "authors": "Qi Deng, Linhong Zheng, Jiaqi Peng, Xu Li, Zhong-guo Zhou, Monica\n  Hussein, Dingyi Chen, Mick Swartz",
        "title": "The Impacts of Registration Regime Implementation on IPO Pricing\n  Efficiency",
        "comments": "41 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the impacts of regime changes and related rule implementations on\nIPOs initial return for China entrepreneurial boards (ChiNext and STAR). We\npropose that an initial return contains the issuer fair value and an investors\noverreaction and examine their magnitudes and determinants. Our findings reveal\nan evolution of IPO pricing in response to the progression of regulation\nchanges along four dimensions: 1) governing regulation regime, 2) listing day\ntrading restrictions, 3) listing rules for issuers, and 4) participation\nrequirements for investors. We find that the most efficient regulation regime\nin Chinese IPO pricing has four characteristics: 1) registration system, 2) no\nhard return caps nor trading curbs that restrict the initial return; 3) more\nspecific listing rules for issuers, and 4) more stringent participation\nrequirements for investors. In all contexts, we show that the registration\nregime governing the STAR IPOs offers the most efficient pricing.\n"
    },
    {
        "paper_id": 2307.0971,
        "authors": "Julian Sester",
        "title": "On intermediate Marginals in Martingale Optimal Transportation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the influence of additional intermediate marginal distributions on\nthe value of the martingale optimal transport problem. From a financial point\nof view, this corresponds to taking into account call option prices not only,\nas usual, for those call options where the respective future maturities\ncoincide with the maturities of some exotic derivative but also additional\nmaturities and then to study the effect on model-independent price bounds for\nthe exotic derivative. We characterize market settings, i.e., combinations of\nthe payoff of exotic derivatives, call option prices and marginal distributions\nthat guarantee improved price bounds as well as those market settings that\nexclude any improvement.\n  Eventually, we showcase in numerous examples that the consideration of\nadditional price information on vanilla options may have a considerable impact\non the resultant model-independent price bounds.\n"
    },
    {
        "paper_id": 2307.09767,
        "authors": "Magnus Wiese, Phillip Murray, Ralf Korn",
        "title": "Sig-Splines: universal approximation and convex calibration of time\n  series generative models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel generative model for multivariate discrete-time time\nseries data. Drawing inspiration from the construction of neural spline flows,\nour algorithm incorporates linear transformations and the signature transform\nas a seamless substitution for traditional neural networks. This approach\nenables us to achieve not only the universality property inherent in neural\nnetworks but also introduces convexity in the model's parameters.\n"
    },
    {
        "paper_id": 2307.09844,
        "authors": "Francesco Mandelli, Marco Pinciroli, Michele Trapletti, Edoardo\n  Vittori",
        "title": "Reinforcement Learning for Credit Index Option Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we focus on finding the optimal hedging strategy of a credit\nindex option using reinforcement learning. We take a practical approach, where\nthe focus is on realism i.e. discrete time, transaction costs; even testing our\npolicy on real market data. We apply a state of the art algorithm, the Trust\nRegion Volatility Optimization (TRVO) algorithm and show that the derived\nhedging strategy outperforms the practitioner's Black & Scholes delta hedge.\n"
    },
    {
        "paper_id": 2307.09969,
        "authors": "P. G. Morrison",
        "title": "Asian Option Pricing via Laguerre Quadrature: A Diffusion Kernel\n  Approach",
        "comments": "38 pages, 2 figures. Paper from MATRIX conference on Mathematics of\n  Risk, 2023, Ballarat, Victoria, AU",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper will demonstrate some new techniques for developing the theory of\nAsian (arithmetic average) options pricing. We discuss the basic derivation of\nthe diffusion equations, and how various techniques from potential theory can\nbe applied to solve these complex expressions. The Whittaker-type confluent\nhypergeometric functions are introduced, and we discuss how these functions are\nrelated to other systems including Mehler-Fock and modified Bessel functions.\nWe close with a brief analysis of some index transforms and the kernels related\nto these integral transforms.\n"
    },
    {
        "paper_id": 2307.10328,
        "authors": "Gianluca Cassese",
        "title": "Subjective Expected Utility and Psychological Gambles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain an elementary characterization of expected utility based on a\nrepresentation of choice in terms of psychological gambles, which requires no\nassumption other than coherence between ex-ante and ex-post preferences. Weaker\nversion of coherence are associated with various attitudes towards complexity\nand lead to a characterization of minimax or Choquet expected utility.\n"
    },
    {
        "paper_id": 2307.10485,
        "authors": "Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, Daochen Zha",
        "title": "FinGPT: Democratizing Internet-scale Data for Financial Large Language\n  Models",
        "comments": "43 pages, 8 tables, and 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating human-like texts, which may potentially\nrevolutionize the finance industry. However, existing LLMs often fall short in\nthe financial field, which is mainly attributed to the disparities between\ngeneral text data and financial text data. Unfortunately, there is only a\nlimited number of financial text datasets available, and BloombergGPT, the\nfirst financial LLM (FinLLM), is close-sourced (only the training logs were\nreleased). In light of this, we aim to democratize Internet-scale financial\ndata for LLMs, which is an open challenge due to diverse data sources, low\nsignal-to-noise ratio, and high time-validity. To address the challenges, we\nintroduce an open-sourced and data-centric framework, Financial Generative\nPre-trained Transformer (FinGPT), that automates the collection and curation of\nreal-time financial data from 34 diverse sources on the Internet, providing\nresearchers and practitioners with accessible and transparent resources to\ndevelop their FinLLMs. Additionally, we propose a simple yet effective strategy\nfor fine-tuning FinLLM using the inherent feedback from the market, dubbed\nReinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank\nAdaptation (LoRA, QLoRA) method that enables users to customize their own\nFinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several\nFinGPT applications, including robo-advisor, sentiment analysis for algorithmic\ntrading, and low-code development. FinGPT aims to democratize FinLLMs,\nstimulate innovation, and unlock new opportunities in open finance. The codes\nhave been open-sourced.\n"
    },
    {
        "paper_id": 2307.1054,
        "authors": "Ananya Parashar",
        "title": "Mean Field Games for Optimal Investment Under Relative Performance\n  Criteria",
        "comments": "Error in Section 5.2",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the portfolio optimization problem formulated by\nLacker and Soret. They formulate a finite time horizon model that allows agents\nto be competitive, measuring their utility not only by their absolute wealth\nbut also relative performance compared to the average of other agents. While\nthe finite population or $n$-player game is tractable in some cases, the\nauthors present the Mean Field Game framework to solve this problem. Here, we\nseek to use this framework to clearly detail the optimal investment and\nconsumption strategies in the CRRA utility case as was briefly outlined in\nLacker and Soret, but also derive a solution in the CARA utility case.\n"
    },
    {
        "paper_id": 2307.10549,
        "authors": "Yuanhao Gong",
        "title": "Dynamic Large Language Models on Blockchains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Training and deploying the large language models requires a large mount of\ncomputational resource because the language models contain billions of\nparameters and the text has thousands of tokens. Another problem is that the\nlarge language models are static. They are fixed after the training process. To\ntackle these issues, in this paper, we propose to train and deploy the dynamic\nlarge language model on blockchains, which have high computation performance\nand are distributed across a network of computers. A blockchain is a secure,\ndecentralized, and transparent system that allows for the creation of a\ntamper-proof ledger for transactions without the need for intermediaries. The\ndynamic large language models can continuously learn from the user input after\nthe training process. Our method provides a new way to develop the large\nlanguage models and also sheds a light on the next generation artificial\nintelligence systems.\n"
    },
    {
        "paper_id": 2307.10649,
        "authors": "Soohan Kim, Jimyeong Kim, Hong Kee Sul, Youngjoon Hong",
        "title": "An Adaptive Dual-level Reinforcement Learning Approach for Optimal Trade\n  Execution",
        "comments": "Submitted to Expert Systems with Applications (Under 2nd review)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this research is to devise a tactic that can closely track the\ndaily cumulative volume-weighted average price (VWAP) using reinforcement\nlearning. Previous studies often choose a relatively short trading horizon to\nimplement their models, making it difficult to accurately track the daily\ncumulative VWAP since the variations of financial data are often insignificant\nwithin the short trading horizon. In this paper, we aim to develop a strategy\nthat can accurately track the daily cumulative VWAP while minimizing the\ndeviation from the VWAP. We propose a method that leverages the U-shaped\npattern of intraday stock trade volumes and use Proximal Policy Optimization\n(PPO) as the learning algorithm. Our method follows a dual-level approach: a\nTransformer model that captures the overall(global) distribution of daily\nvolumes in a U-shape, and a LSTM model that handles the distribution of orders\nwithin smaller(local) time intervals. The results from our experiments suggest\nthat this dual-level architecture improves the accuracy of approximating the\ncumulative VWAP, when compared to previous reinforcement learning-based models.\n"
    },
    {
        "paper_id": 2307.109,
        "authors": "Zakaria Marah",
        "title": "American Exchange option driven by a L\\'evy process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of pricing American Exchange options driven by a\nL\\'evy process. We study the properties of American Exchange options, we\nrepresented it as the sum of the price of the corresponding European exchange\noption price and an early exercise premium. Secondly, we show some properties\nof the free boundary and give an approximative formula of an American Exchange\noption.\n"
    },
    {
        "paper_id": 2307.10983,
        "authors": "Alexandros Theloudis and Jorge Velilla and Pierre-Andr\\'e Chiappori\n  and J. Ignacio Gim\\'enez-Nadal and Jos\\'e Alberto Molina",
        "title": "Commitment and the Dynamics of Household Labor Supply",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The extent to which individuals commit to their partner for life has\nimportant implications. This paper develops a lifecycle collective model of the\nhousehold, through which it characterizes behavior in three prominent\nalternative types of commitment: full, limited, and no commitment. We propose a\ntest that distinguishes between all three types based on how contemporaneous\nand historical news affect household behavior. Our test permits heterogeneity\nin the degree of commitment across households. Using recent data from the Panel\nStudy of Income Dynamics, we reject full and no commitment, while we find\nstrong evidence for limited commitment.\n"
    },
    {
        "paper_id": 2307.11012,
        "authors": "David Ardia, Cl\\'ement Aymard, Tolga Cenesizoglu",
        "title": "Fast and Furious: A High-Frequency Analysis of Robinhood Users' Trading\n  Behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We analyze Robinhood (RH) investors' trading reactions to intraday hourly and\novernight price changes. Contrasting with recent studies focusing on daily\nbehaviors, we find that RH users strongly favor big losers over big gainers. We\nalso uncover that they react rapidly, typically within an hour, when acquiring\nstocks that exhibit extreme negative returns. Further analyses suggest greater\n(lower) attention to overnight (intraday) movements and exacerbated behaviors\npost-COVID-19 announcement. Moreover, trading attitudes significantly vary\nacross firm size and industry, with a more contrarian strategy towards\nlarger-cap firms and a heightened activity on energy and consumer discretionary\nstocks.\n"
    },
    {
        "paper_id": 2307.11039,
        "authors": "Fabio Bacchini, Lorenzo Di Biagio, Giampiero M. Gallo, Vincenzo\n  Spinelli",
        "title": "Indicatori comuni del PNRR e framework SDGs: una proposta di indicatore\n  composito",
        "comments": "30 pages, in Italian, 10 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main component of the NextGeneration EU (NGEU) program is the Recovery\nand Resilience Facility (RRF), spanning an implementation period between 2021\nand 2026. The RRF also includes a monitoring system: every six months, each\ncountry is required to send an update on the progress of the plan against 14\ncommon indicators, measured on specific quantitative scales. The aim of this\npaper is to present the first empirical evidence on this system, while, at the\nsame time, emphasizing the potential of its integration with the sustainable\ndevelopment framework (SDGs). We propose to develop a first linkage between the\n14 common indicators and the SDGs which allows us to produce a composite index\n(SDGs-RRF) for France, Germany, Italy, and Spain for the period 2014-2021. Over\nthis time, widespread improvements in the composite index across the four\ncountries led to a partial reduction of the divergence. The proposed approach\nrepresents a first step towards a wider use of the SDGs for the assessment of\nthe RRF, in line with their use in the European Semester documents prepared by\nthe European Commission.\n"
    },
    {
        "paper_id": 2307.11137,
        "authors": "Steve Phelps and Rebecca Ranson",
        "title": "Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent\n  Problems in AI Alignment using Large-Language Models",
        "comments": "11 pages, 7 figures. For code see\n  https://github.com/phelps-sg/llm-cooperation Updated with minor corrections:\n  - corrected typo: \"mesa-optimiser\" instead of \"meso-optimiser\" - Cited Yang\n  et al (2023) in support of claim that LLMs can solve optimisation problems -\n  Acknowledged Seth Aslin for corrections",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  AI Alignment is often presented as an interaction between a single designer\nand an artificial agent in which the designer attempts to ensure the agent's\nbehavior is consistent with its purpose, and risks arise solely because of\nconflicts caused by inadvertent misalignment between the utility function\nintended by the designer and the resulting internal utility function of the\nagent. With the advent of agents instantiated with large-language models\n(LLMs), which are typically pre-trained, we argue this does not capture the\nessential aspects of AI safety because in the real world there is not a\none-to-one correspondence between designer and agent, and the many agents, both\nartificial and human, have heterogeneous values. Therefore, there is an\neconomic aspect to AI safety and the principal-agent problem is likely to\narise. In a principal-agent problem conflict arises because of information\nasymmetry together with inherent misalignment between the utility of the agent\nand its principal, and this inherent misalignment cannot be overcome by\ncoercing the agent into adopting a desired utility function through training.\nWe argue the assumptions underlying principal-agent problems are crucial to\ncapturing the essence of safety problems involving pre-trained AI models in\nreal-world situations. Taking an empirical approach to AI safety, we\ninvestigate how GPT models respond in principal-agent conflicts. We find that\nagents based on both GPT-3.5 and GPT-4 override their principal's objectives in\na simple online shopping task, showing clear evidence of principal-agent\nconflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced\nbehaviour in response to changes in information asymmetry, whereas the later\nGPT-4 model is more rigid in adhering to its prior alignment. Our results\nhighlight the importance of incorporating principles from economics into the\nalignment process.\n"
    },
    {
        "paper_id": 2307.1134,
        "authors": "Ludovic Tangpi and Shichun Wang",
        "title": "Optimal Bubble Riding with Price-dependent Entry: a Mean Field Game of\n  Controls with Common Noise",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we further extend the optimal bubble riding model proposed by\nTangpi and Wang by allowing for price-dependent entry times. Agents are\ncharacterized by their individual entry threshold that represents their belief\nin the strength of the bubble. Conversely, the growth dynamics of the bubble is\nfueled by the influx of players. Price-dependent entry naturally leads to a\nmean field game of controls with common noise and random entry time, for which\nwe provide an existence result. The equilibrium is obtained by first solving\ndiscretized versions of the game in the weak formulation and then examining the\nmeasurability property in the limit. In this paper, the common noise comes from\ntwo sources: the price of the asset which all agents trade, and also the\nexogenous bubble burst time, which we also discretize and incorporate into the\nmodel via progressive enlargement of filtration.\n"
    },
    {
        "paper_id": 2307.11508,
        "authors": "Mohammad Heydari, Yanan Fan, Kin Keung Lai",
        "title": "A Robust Site Selection Model under uncertainty for Special Hospital\n  Wards in Hong Kong",
        "comments": null,
        "journal-ref": "15th International Congress of Logistics and SCM Systems (ICLS\n  2021), September 28~29, 2021 Poznan, Poland",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper process two robust models for site selection problems for one of\nthe major Hospitals in Hong Kong. Three parameters, namely, level of\nuncertainty, infeasibility tolerance as well as the level of reliability, are\nincorporated. Then, 2 kinds of uncertainty; that is, the symmetric and bounded\nuncertainties have been investigated. Therefore, the issue of scheduling under\nuncertainty has been considered wherein unknown problem factors could be\nillustrated via a given probability distribution function. In this regard, Lin,\nJanak, and Floudas (2004) introduced one of the newly developed strong\noptimisation protocols. Hence, computers as well as the chemical engineering\n[1069-1085] has been developed for considering uncertainty illustrated through\na given probability distribution. Finally, our accurate optimisation protocol\nhas been on the basis of a min-max framework and in a case of application to\nthe (MILP) problems it produced a precise solution that has immunity to\nuncertain data.\n"
    },
    {
        "paper_id": 2307.11571,
        "authors": "Maxime L. D. Nicolas, Adrien Desroziers, Fabio Caccioli, and Tomaso\n  Aste",
        "title": "ESG Reputation Risk Matters: An Event Study Based on Social Media Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the response of shareholders to Environmental, Social, and\nGovernance-related reputational risk (ESG-risk), focusing exclusively on the\nimpact of social media. Using a dataset of 114 million tweets about firms\nlisted on the S&P100 index between 2016 and 2022, we extract conversations\ndiscussing ESG matters. In an event study design, we define events as unusual\nspikes in message posting activity linked to ESG-risk, and we then examine the\ncorresponding changes in the returns of related assets. By focusing on social\nmedia, we gain insight into public opinion and investor sentiment, an aspect\nnot captured through ESG controversies news alone. To the best of our\nknowledge, our approach is the first to distinctly separate the reputational\nimpact on social media from the physical costs associated with negative ESG\ncontroversy news. Our results show that the occurrence of an ESG-risk event\nleads to a statistically significant average reduction of 0.29% in abnormal\nreturns. Furthermore, our study suggests this effect is predominantly driven by\nSocial and Governance categories, along with the \"Environmental Opportunities\"\nsubcategory. Our research highlights the considerable impact of social media on\nfinancial markets, particularly in shaping shareholders' perception of ESG\nreputation. We formulate several policy implications based on our findings.\n"
    },
    {
        "paper_id": 2307.11683,
        "authors": "Oleg Nivievskyi, Pavlo Iavorskyi and Oleksandr Donchenko",
        "title": "Assessing the role of small farmers and households in agriculture and\n  the rural economy and measures to support their sustainable development",
        "comments": "This publication was commissioned and produced within the framework\n  of the Project Support to Agricultural and Food Policy Implementation in\n  Ukraine (SAFPI), with the financial support of the European Union. Its\n  contents are the sole responsibility of the Project and do not necessarily\n  reflect the views of the European Union",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Ministry of Economy has an interest and demand in exploring how to\nincrease the set of [legally registered] small family farmers in Ukraine and to\nexamine more in details measures that could reduce the scale of the shadow\nagricultural market in Ukraine. Building upon the above political economy\nbackground and demand, we will be undertaking the analysis along the two\nseparate but not totally independents streams of analysis, i.e. sustainable\nsmall scale (family) farming development and exploring the scale and measures\nfor reducing the shadow agricultural market in Ukraine\n"
    },
    {
        "paper_id": 2307.11685,
        "authors": "Chuheng Zhang, Yitong Duan, Xiaoyu Chen, Jianyu Chen, Jian Li, Li Zhao",
        "title": "Towards Generalizable Reinforcement Learning for Trade Execution",
        "comments": "Accepted by IJCAI-23",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimized trade execution is to sell (or buy) a given amount of assets in a\ngiven time with the lowest possible trading cost. Recently, reinforcement\nlearning (RL) has been applied to optimized trade execution to learn smarter\npolicies from market data. However, we find that many existing RL methods\nexhibit considerable overfitting which prevents them from real deployment. In\nthis paper, we provide an extensive study on the overfitting problem in\noptimized trade execution. First, we model the optimized trade execution as\noffline RL with dynamic context (ORDC), where the context represents market\nvariables that cannot be influenced by the trading policy and are collected in\nan offline manner. Under this framework, we derive the generalization bound and\nfind that the overfitting issue is caused by large context space and limited\ncontext samples in the offline setting. Accordingly, we propose to learn\ncompact representations for context to address the overfitting problem, either\nby leveraging prior knowledge or in an end-to-end manner. To evaluate our\nalgorithms, we also implement a carefully designed simulator based on\nhistorical limit order book (LOB) data to provide a high-fidelity benchmark for\ndifferent algorithms. Our experiments on the high-fidelity simulator\ndemonstrate that our algorithms can effectively alleviate overfitting and\nachieve better performance.\n"
    },
    {
        "paper_id": 2307.11732,
        "authors": "Ming Chen, Sareh Nabi, Marciano Siniscalchi",
        "title": "Advancing Ad Auction Realism: Practical Insights & Modeling Implications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contemporary real-world online ad auctions differ from canonical models\n[Edelman et al., 2007; Varian, 2009] in at least four ways: (1) values and\nclick-through rates can depend upon users' search queries, but advertisers can\nonly partially \"tune\" their bids to specific queries; (2) advertisers do not\nknow the number, identity, and precise value distribution of competing bidders;\n(3) advertisers only receive partial, aggregated feedback, and (4) payment\nrules are only partially known to bidders. These features make it virtually\nimpossible to fully characterize equilibrium bidding behavior. This paper shows\nthat, nevertheless, one can still gain useful insight into modern ad auctions\nby modeling advertisers as agents governed by an adversarial bandit algorithm,\nindependent of auction mechanism intricacies. To demonstrate our approach, we\nfirst simulate \"soft-floor\" auctions [Zeithammer, 2019], a complex, real-world\npricing rule for which no complete equilibrium characterization is known. We\nfind that (i) when values and click-through rates are query-dependent, soft\nfloors can improve revenues relative to standard auction formats even if bidder\ntypes are drawn from the same distribution; and (ii) with distributional\nasymmetries that reflect relevant real-world scenario, we find that soft floors\nyield lower revenues than suitably chosen reserve prices, even restricting\nattention to a single query. We then demonstrate how to infer advertiser value\ndistributions from observed bids for a variety of pricing rules, and illustrate\nour approach with aggregate data from an e-commerce website.\n"
    },
    {
        "paper_id": 2307.11845,
        "authors": "Christopher Gerling, Stefan Lessmann",
        "title": "Multimodal Document Analytics for Banking Process Automation",
        "comments": "A Preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Traditional banks face increasing competition from FinTechs in the rapidly\nevolving financial ecosystem. Raising operational efficiency is vital to\naddress this challenge. Our study aims to improve the efficiency of\ndocument-intensive business processes in banking. To that end, we first review\nthe landscape of business documents in the retail segment. Banking documents\noften contain text, layout, and visuals, suggesting that document analytics and\nprocess automation require more than plain natural language processing (NLP).\nTo verify this and assess the incremental value of visual cues when processing\nbusiness documents, we compare a recently proposed multimodal model called\nLayoutXLM to powerful text classifiers (e.g., BERT) and large language models\n(e.g., GPT) in a case study related to processing company register extracts.\nThe results confirm that incorporating layout information in a model\nsubstantially increases its performance. Interestingly, we also observed that\nmore than 75% of the best model performance (in terms of the F1 score) can be\nachieved with as little as 30% of the training data. This shows that the demand\nfor data labeled data to set up a multi-modal model can be moderate, which\nsimplifies real-world applications of multimodal document analytics. Our study\nalso sheds light on more specific practices in the scope of calibrating a\nmultimodal banking document classifier, including the need for fine-tuning. In\nsum, the paper contributes original empirical evidence on the effectiveness and\nefficiency of multi-model models for document processing in the banking\nbusiness and offers practical guidance on how to unlock this potential in\nday-to-day operations.\n"
    },
    {
        "paper_id": 2307.11846,
        "authors": "Bryce Morsky and Fuwei Zhuang and Zuojun Zhou",
        "title": "Social and individual learning in the Minority Game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the roles of social and individual learning on outcomes of the\nMinority Game model of a financial market. Social learning occurs via agents\nadopting the strategies of their neighbours within a social network, while\nindividual learning results in agents changing their strategies without input\nfrom other agents. In particular, we show how social learning can undermine\nefficiency of the market due to negative frequency dependent selection and loss\nof strategy diversity. The latter of which can lock the population into a\nmaximally inefficient state. We show how individual learning can rescue a\npopulation engaged in social learning from such inefficiencies.\n"
    },
    {
        "paper_id": 2307.11919,
        "authors": "Laurence Carassus and Massinissa Ferhoune",
        "title": "Discrete time optimal investment under model uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust utility maximization problem in a general discrete-time\nfrictionless market under quasi-sure no-arbitrage. The investor is assumed to\nhave a random and concave utility function defined on the whole real-line. She\nalso faces model ambiguity on her beliefs about the market, which is modeled\nthrough a set of priors. We prove the existence of an optimal investment\nstrategy using only primal methods. For that we assume classical assumptions on\nthe market and on the random utility function as asymptotic elasticity\nconstraints. Most of our other assumptions are stated on a prior-by-prior basis\nand correspond to generally accepted assumptions in the literature on markets\nwithout ambiguity. We also propose a general setting including utility\nfunctions with benchmark for which our assumptions are easily checked.\n"
    },
    {
        "paper_id": 2307.12087,
        "authors": "Shiheng Wang",
        "title": "CFR-p: Counterfactual Regret Minimization with Hierarchical Policy\n  Abstraction, and its Application to Two-player Mahjong",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Counterfactual Regret Minimization(CFR) has shown its success in Texas\nHold'em poker. We apply this algorithm to another popular incomplete\ninformation game, Mahjong. Compared to the poker game, Mahjong is much more\ncomplex with many variants. We study two-player Mahjong by conducting game\ntheoretical analysis and making a hierarchical abstraction to CFR based on\nwinning policies. This framework can be generalized to other imperfect\ninformation games.\n"
    },
    {
        "paper_id": 2307.12161,
        "authors": "Marcos Escobar-Anel and Yiyao Jiao",
        "title": "Unraveling the Trade-off between Sustainability and Returns: A\n  Multivariate Utility Analysis",
        "comments": "24 pages, 12 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes an expected multivariate utility analysis for ESG\ninvestors in which green stocks, brown stocks, and a market index are modeled\nin a one-factor, CAPM-type structure. This setting allows investors to\naccommodate their preferences for green investments according to proper risk\naversion levels. We find closed-form solutions for optimal allocations, wealth\nand value functions. As by-products, we first demonstrate that investors do not\nneed to reduce their pecuniary satisfaction in order to increase green\ninvestments. Secondly, we propose a parameterization to capture investors'\npreferences for green assets over brown or market assets, independent of\nperformance. The paper uses the RepRisk Rating of U.S. stocks from 2010 to 2020\nto select companies that are representative of various ESG ratings. Our\nempirical analysis reveals drastic increases in wealth allocation toward\nhigh-rated ESG stocks for ESG-sensitive investors; this holds even as the\noverall level of pecuniary satisfaction is kept unchanged.\n"
    },
    {
        "paper_id": 2307.12362,
        "authors": "Petri P. Karenlampi",
        "title": "Microeconomics of nitrogen fertilization in boreal carbon forestry",
        "comments": "16 pages, 9 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nitrogen fertilization of boreal forests is investigated in terms of\nmicroeconomics, as a tool for carbon sequestration. The effects of nitrogen\nfertilization's timing on the return rate on capital and the expected value of\nthe timber stock are investigated within a set of semi-fertile,\nspruce-dominated boreal stands, using an inventory-based growth model. Early\nfertilization tends to shorten rotations, reducing timber stock and carbon\nstorage. The same applies to fertilization after the second thinning.\nFertilization applied ten years before stand maturity is profitable and\nincreases the timber stock, but the latter effect is small. Fertilization of\nmature stands, extending any rotation by ten years, effectively increases the\ncarbon stock. Profitability varies but is increased by fertilization, instead\nof merely extending the rotation.\n"
    },
    {
        "paper_id": 2307.12479,
        "authors": "Saurabh Deochake",
        "title": "Cloud Cost Optimization: A Comprehensive Review of Strategies and Case\n  Studies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Cloud computing has revolutionized the way organizations manage their IT\ninfrastructure, but it has also introduced new challenges, such as managing\ncloud costs. This paper explores various techniques for cloud cost\noptimization, including cloud pricing, analysis, and strategies for resource\nallocation. Real-world case studies of these techniques are presented, along\nwith a discussion of their effectiveness and key takeaways. The analysis\nconducted in this paper reveals that organizations can achieve significant cost\nsavings by adopting cloud cost optimization techniques. Additionally, future\nresearch directions are proposed to advance the state of the art in this\nimportant field.\n"
    },
    {
        "paper_id": 2307.12695,
        "authors": "G\\'eraldine Bouveret, Jean-Fran\\c{c}ois Chassagneux, Smail Ibbou,\n  Antoine Jacquier and Lionel Sopgoui",
        "title": "Propagation of a carbon price in a credit portfolio through\n  macroeconomic factors",
        "comments": "62 pages, 20 figues, 19 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study how the climate transition through a low-carbon economy, implemented\nby carbon pricing, propagates in a credit portfolio and precisely describe how\ncarbon price dynamics affects credit risk measures such as probability of\ndefault, expected and unexpected losses. We adapt a stochastic multisectoral\nmodel to take into account the greenhouse gases (GHG) emissions costs of both\nsectoral firms' production and consumption, as well as sectoral household's\nconsumption. GHG emissions costs are the product of carbon prices, provided by\nthe NGFS transition scenarios, and of GHG emissions. For each sector, our model\nyields the sensitivity of firms' production and households' consumption to\ncarbon price and the relationships between sectors. It allows us to analyze the\nshort-term effects of the carbon price as opposed to standard IAM (such as\nREMIND), which are deterministic and only capture long-term trends. Finally, we\nuse a DCF methodology to compute firms' values which we then combine with a\nstructural credit risk model to describe how the carbon price impacts credit\nrisk measures. We obtain that the carbon price distorts the distribution of the\nfirm's value, increases banking fees charged to clients (materialized by the\nbank provisions), and reduces banks' profitability (translated by the economic\ncapital). In addition, the randomness we introduce provides extra flexibility\nto take into account uncertainties on the productivity and on the different\ntransition scenarios. We also compute the sensitivities of the credit risk\nmeasures with respect to changes in the carbon price, yielding further criteria\nfor a more accurate assessment of climate transition risk in a credit\nportfolio. This work provides a preliminary methodology to calculate the\nevolution of credit risk measures of a credit portfolio, starting from a given\nclimate transition scenario described by a carbon price.\n"
    },
    {
        "paper_id": 2307.12744,
        "authors": "Tobias Wand, Martin He{\\ss}ler and Oliver Kamps",
        "title": "Memory Effects, Multiple Time Scales and Local Stability in Langevin\n  Models of the S&P500 Market Correlation",
        "comments": "15 pages (excluding references and appendix)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The analysis of market correlations is crucial for optimal portfolio\nselection of correlated assets, but their memory effects have often been\nneglected. In this work, we analyse the mean market correlation of the S&P500\nwhich corresponds to the main market mode in principle component analysis. We\nfit a generalised Langevin equation (GLE) to the data whose memory kernel\nimplies that there is a significant memory effect in the market correlation\nranging back at least three trading weeks. The memory kernel improves the\nforecasting accuracy of the GLE compared to models without memory and hence,\nsuch a memory effect has to be taken into account for optimal portfolio\nselection to minimise risk or for predicting future correlations. Moreover, a\nBayesian resilience estimation provides further evidence for non-Markovianity\nin the data and suggests the existence of a hidden slow time scale that\noperates on much slower times than the observed daily market data. Assuming\nthat such a slow time scale exists, our work supports previous research on the\nexistence of locally stable market states.\n"
    },
    {
        "paper_id": 2307.12776,
        "authors": "Valerio Capraro, Roberto Di Paolo, Veronica Pizziol",
        "title": "Assessing Large Language Models' ability to predict how humans balance\n  self-interest and the interest of others",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generative artificial intelligence (AI) holds enormous potential to\nrevolutionize decision-making processes, from everyday to high-stake scenarios.\nBy leveraging generative AI, humans can benefit from data-driven insights and\npredictions, enhancing their ability to make informed decisions that consider a\nwide array of factors and potential outcomes. However, as many decisions carry\nsocial implications, for AI to be a reliable assistant for decision-making it\nis crucial that it is able to capture the balance between self-interest and the\ninterest of others. We investigate the ability of three of the most advanced\nchatbots to predict dictator game decisions across 108 experiments with human\nparticipants from 12 countries. We find that only GPT-4 (not Bard nor Bing)\ncorrectly captures qualitative behavioral patterns, identifying three major\nclasses of behavior: self-interested, inequity-averse, and fully altruistic.\nNonetheless, GPT-4 consistently underestimates self-interest and\ninequity-aversion, while overestimating altruistic behavior. This bias has\nsignificant implications for AI developers and users, as overly optimistic\nexpectations about human altruism may lead to disappointment, frustration,\nsuboptimal decisions in public policy or business contexts, and even social\nconflict.\n"
    },
    {
        "paper_id": 2307.12843,
        "authors": "Gero Junike and Hauke Stier",
        "title": "From characteristic functions to multivariate distribution functions and\n  European option prices by the damped COS method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide a unified framework to obtain numerically certain quantities, such\nas the distribution function, absolute moments and prices of financial options,\nfrom the characteristic function of some (unknown) probability density function\nusing the Fourier-cosine expansion (COS) method. The classical COS method is\nnumerically very efficient in one-dimension, but it cannot deal very well with\ncertain integrands in general dimensions. Therefore, we introduce the damped\nCOS method, which can handle a large class of integrands very efficiently. We\nprove the convergence of the (damped) COS method and study its order of\nconvergence. The method converges exponentially if the characteristic function\ndecays exponentially. To apply the (damped) COS method, one has to specify two\nparameters: a truncation range for the multivariate density and the number of\nterms to approximate the truncated density by a cosine series. We provide an\nexplicit formula for the truncation range and an implicit formula for the\nnumber of terms. Numerical experiments up to five dimensions confirm the\ntheoretical results.\n"
    },
    {
        "paper_id": 2307.12893,
        "authors": "Laurent Kloeker, Gregor Joeken, Lutz Eckstein",
        "title": "Economic Analysis of Smart Roadside Infrastructure Sensors for Connected\n  and Automated Mobility",
        "comments": "Accepted to be published as part of the 26th IEEE International\n  Conference on Intelligent Transportation Systems (ITSC), Bilbao, Bizkaia,\n  Spain, September 24-28, 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Smart roadside infrastructure sensors in the form of intelligent\ntransportation system stations (ITS-Ss) are increasingly deployed worldwide at\nrelevant traffic nodes. The resulting digital twins of the real environment are\nsuitable for developing and validating connected and automated driving\nfunctions and for increasing the operational safety of intelligent vehicles by\nproviding ITS-S real-time data. However, ITS-Ss are very costly to establish\nand operate. The choice of sensor technology also has an impact on the overall\ncosts as well as on the data quality. So far, there is only insufficient\nknowledge about the concrete expenses that arise with the construction of\ndifferent ITS-S setups. Within this work, multiple modular infrastructure\nsensor setups are investigated with the help of a life cycle cost analysis\n(LCCA). Their economic efficiency, different user requirements and sensor data\nqualities are considered. Based on the static cost model, a Monte Carlo\nsimulation is performed, to generate a range of possible project costs and to\nquantify the financial risks of implementing ITS-S projects of different\nscales. Due to its modularity, the calculation model is suitable for diverse\napplications and outputs a distinctive evaluation of the underlying\ncost-benefit ratio of investigated setups.\n"
    },
    {
        "paper_id": 2307.12918,
        "authors": "Alexander Roth, Dana Kirchem, Carlos Gaete-Morales, Wolf-Peter Schill",
        "title": "Flexible heat pumps: must-have or nice to have in a power sector with\n  renewables?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Heat pumps are a key technology for reducing fossil fuel use in the heating\nsector. However, the transition to heat pumps implies an increase in\nelectricity demand, especially in the cold winter months. Therefore, the\nflexible operation of heat pumps will be of high importance to the power\nsector. Using an open-source power sector model, we examine the power sector\nimpacts of three different expansion scenarios of decentralized heat pumps in\nan interconnected Germany until 2030 and the role of buffer heat storage of\ndifferent sizes. We quantify the required additional investments in renewable\nenergy sources and the effects on firm capacity needs. If wind power expansion\npotentials are limited, the rollout of heat pumps can also be accompanied by\nsolar PV with little additional costs. The expansion of heat pumps increases\nthe need for firm capacities and battery storage, but even small heat buffer\nstorage with an energy-to-power ratio of two hours can reduce these additional\ncapacities. We further show that increasing the number of heat pumps from 1.7\nto 10 million saves around 180 TWh of natural gas and 35 million tonnes of\nCO2eq emissions per year.\n"
    },
    {
        "paper_id": 2307.13217,
        "authors": "Masanori Hirano, Kentaro Minami, Kentaro Imajo",
        "title": "Adversarial Deep Hedging: Learning to Hedge without Price Process\n  Modeling",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep hedging is a deep-learning-based framework for derivative hedging in\nincomplete markets. The advantage of deep hedging lies in its ability to handle\nvarious realistic market conditions, such as market frictions, which are\nchallenging to address within the traditional mathematical finance framework.\nSince deep hedging relies on market simulation, the underlying asset price\nprocess model is crucial. However, existing literature on deep hedging often\nrelies on traditional mathematical finance models, e.g., Brownian motion and\nstochastic volatility models, and discovering effective underlying asset models\nfor deep hedging learning has been a challenge. In this study, we propose a new\nframework called adversarial deep hedging, inspired by adversarial learning. In\nthis framework, a hedger and a generator, which respectively model the\nunderlying asset process and the underlying asset process, are trained in an\nadversarial manner. The proposed method enables to learn a robust hedger\nwithout explicitly modeling the underlying asset process. Through numerical\nexperiments, we demonstrate that our proposed method achieves competitive\nperformance to models that assume explicit underlying asset processes across\nvarious real market data.\n"
    },
    {
        "paper_id": 2307.13221,
        "authors": "Yuanhao Gong",
        "title": "Multilevel Large Language Models for Everyone",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large language models have made significant progress in the past few years.\nHowever, they are either generic {\\it or} field specific, splitting the\ncommunity into different groups. In this paper, we unify these large language\nmodels into a larger map, where the generic {\\it and} specific models are\nlinked together and can improve each other, based on the user personal input\nand information from the internet. The idea of linking several large language\nmodels together is inspired by the functionality of human brain. The specific\nregions on the brain cortex are specific for certain low level functionality.\nAnd these regions can jointly work together to achieve more complex high level\nfunctionality. Such behavior on human brain cortex sheds the light to design\nthe multilevel large language models that contain global level, field level and\nuser level models. The user level models run on local machines to achieve\nefficient response and protect the user's privacy. Such multilevel models\nreduce some redundancy and perform better than the single level models. The\nproposed multilevel idea can be applied in various applications, such as\nnatural language processing, computer vision tasks, professional assistant,\nbusiness and healthcare.\n"
    },
    {
        "paper_id": 2307.13232,
        "authors": "Emil Mendoza, Fabian Dunker, Marco Reale",
        "title": "Changes in Risk Appreciation, and Short Memory of House Buyers When the\n  Market is Hot, a Case Study of Christchurch, New Zealand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper house prices in Christchurch are analyzed over three distinct\nperiods of time: post-2011 earthquake, pre-COVID-19 lockdown, and post-COVID-19\nlockdown using the well-established hedonic price model. Results show that\nbuyers, in periods that are temporally distant from the 2011 Christchurch\nearthquake, value the risk of potential earthquake damage to a property\ndifferently from buyers soon after the earthquake. We find that there are\nobservable shifts in hedonic prices across the different time periods,\nspecifically for section size pre and post COVID-19 lockdown.\n"
    },
    {
        "paper_id": 2307.13422,
        "authors": "Ivan Letteri",
        "title": "VolTS: A Volatility-based Trading System to forecast Stock Markets Trend\n  using Statistics and Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Volatility-based trading strategies have attracted a lot of attention in\nfinancial markets due to their ability to capture opportunities for profit from\nmarket dynamics. In this article, we propose a new volatility-based trading\nstrategy that combines statistical analysis with machine learning techniques to\nforecast stock markets trend.\n  The method consists of several steps including, data exploration, correlation\nand autocorrelation analysis, technical indicator use, application of\nhypothesis tests and statistical models, and use of variable selection\nalgorithms. In particular, we use the k-means++ clustering algorithm to group\nthe mean volatility of the nine largest stocks in the NYSE and NasdaqGS\nmarkets. The resulting clusters are the basis for identifying relationships\nbetween stocks based on their volatility behaviour. Next, we use the Granger\nCausality Test on the clustered dataset with mid-volatility to determine the\npredictive power of a stock over another stock. By identifying stocks with\nstrong predictive relationships, we establish a trading strategy in which the\nstock acting as a reliable predictor becomes a trend indicator to determine the\nbuy, sell, and hold of target stock trades.\n  Through extensive backtesting and performance evaluation, we find the\nreliability and robustness of our volatility-based trading strategy. The\nresults suggest that our approach effectively captures profitable trading\nopportunities by leveraging the predictive power of volatility clusters, and\nGranger causality relationships between stocks.\n  The proposed strategy offers valuable insights and practical implications to\ninvestors and market participants who seek to improve their trading decisions\nand capitalize on market trends. It provides valuable insights and practical\nimplications for market participants looking to.\n"
    },
    {
        "paper_id": 2307.13501,
        "authors": "Tessa Bauman, Bruno Ga\\v{s}perov, Stjepan Begu\\v{s}i\\'c, and Zvonko\n  Kostanj\\v{c}ar",
        "title": "Deep Reinforcement Learning for Robust Goal-Based Wealth Management",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-031-34111-3_7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Goal-based investing is an approach to wealth management that prioritizes\nachieving specific financial goals. It is naturally formulated as a sequential\ndecision-making problem as it requires choosing the appropriate investment\nuntil a goal is achieved. Consequently, reinforcement learning, a machine\nlearning technique appropriate for sequential decision-making, offers a\npromising path for optimizing these investment strategies. In this paper, a\nnovel approach for robust goal-based wealth management based on deep\nreinforcement learning is proposed. The experimental results indicate its\nsuperiority over several goal-based wealth management benchmarks on both\nsimulated and historical market data.\n"
    },
    {
        "paper_id": 2307.13546,
        "authors": "Haoyang Cao, Haotian Gu, Xin Guo and Mathieu Rosenbaum",
        "title": "Transfer Learning for Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this work, we explore the possibility of utilizing transfer learning\ntechniques to address the financial portfolio optimization problem. We\nintroduce a novel concept called \"transfer risk\", within the optimization\nframework of transfer learning. A series of numerical experiments are conducted\nfrom three categories: cross-continent transfer, cross-sector transfer, and\ncross-frequency transfer. In particular, 1. a strong correlation between the\ntransfer risk and the overall performance of transfer learning methods is\nestablished, underscoring the significance of transfer risk as a viable\nindicator of \"transferability\"; 2. transfer risk is shown to provide a\ncomputationally efficient way to identify appropriate source tasks in transfer\nlearning, enhancing the efficiency and effectiveness of the transfer learning\napproach; 3. additionally, the numerical experiments offer valuable new\ninsights for portfolio management across these different settings.\n"
    },
    {
        "paper_id": 2307.1362,
        "authors": "Lu Ling, Xinwu Qian, Satish V. Ukkusuri",
        "title": "Impact of Transportation Network Companies on Labor Supply and Wages for\n  Taxi Drivers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the growth of TNCs took a substantial part of ridership and asset value\naway from the traditional taxi industry, existing taxi market policy\nregulations and planning models remain to be reexamined, which requires\nreliable estimates of the sensitivity of labor supply and income levels in the\ntaxi industry. This study aims to investigate the impact of TNCs on the labor\nsupply of the taxi industry, estimate wage elasticity, and understand the\nchanges in taxi drivers' work preferences. We introduce the wage decomposition\nmethod to quantify the effects of TNC trips on taxi drivers' work hours over\ntime, based on taxi and TNC trip record data from 2013 to 2018 in New York\nCity. The data are analyzed to evaluate the changes in overall market\nperformances and taxi drivers' work behavior through statistical analyses, and\nour results show that the increase in TNC trips not only decreases the income\nlevel of taxi drivers but also discourages their willingness to work. We find\nthat 1% increase in TNC trips leads to 0.28% reduction in the monthly revenue\nof the yellow taxi industry and 0.68% decrease in the monthly revenue of the\ngreen taxi industry in recent years. More importantly, we report that the work\nbehavior of taxi drivers shifts from the widely accepted neoclassical standard\nbehavior to the reference-dependent preference (RDP) behavior, which signifies\na persistent trend of loss in labor supply for the taxi market and hints at the\ncollapse of taxi industry if the growth of TNCs continues. In addition, we\nobserve that yellow and green taxi drivers present different work preferences\nover time. Consistently increasing RDP behavior is found among yellow taxi\ndrivers. Green taxi drivers were initially revenue maximizers but later turned\ninto income targeting strategy\n"
    },
    {
        "paper_id": 2307.13624,
        "authors": "Arman Abgaryan, Utkarsh Sharma",
        "title": "Dynamic Function Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Decentralised automated market makers (AMMs) have gained significant\nattention recently. We propose an adaptive and automated Dynamic Function\nMarket Maker (DFMM) that addresses challenges in this space. Our DFMM protocol\nincludes a data aggregator and an order routing mechanism. It synchronises\nprice-sensitive market information, asserting the principle of one price, and\nensuring market efficiency. The data aggregator includes a virtual order book,\nasserting efficient asset pricing by staying synchronised with information from\nexternal venues, including competitors. The protocol's rebalancing and order\nrouting method optimises inventory risk through arbitrageurs, who are more\nlikely to assist DFMM, enhancing protocol stability. DFMM incorporates\nprotective buffers with non-linear derivative instruments to manage risk and\nmitigate losses caused by market volatility. The protocol employs an\nalgorithmic accounting-asset, connecting all pools and resolving the issue of\nsegregated pools and risk transfer. The settlement process is entirely\nprotocol-driven, maximising risk management efficiency, and eliminating\nsubjective market risk assessments. In essence, DFMM offers a fully automated,\ndecentralised, and robust solution for automated market making. It aims to\nprovide long-term viability and stability in an asset class that demands\nrobustness.\n"
    },
    {
        "paper_id": 2307.13772,
        "authors": "Alfred Lehar, Christine Parlour, Marius Zoican",
        "title": "Fragmentation and optimal liquidity supply on decentralized exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate how liquidity providers (LPs) choose between high- and low-fee\ntrading venues, in the face of a fixed common gas cost. Analyzing Uniswap data,\nwe find that high-fee pools attract 58% of liquidity supply yet execute only\n21% of volume. Large LPs dominate low-fee pools, frequently adjusting\nout-of-range positions in response to informed order flow. In contrast, small\nLPs converge to high-fee pools, accepting lower execution probabilities to\nmitigate adverse selection and liquidity management costs. Fragmented liquidity\ndominates a single-fee market, as it encourages more liquidity providers to\nenter the market, while fostering LP competition on the low-fee pool.\n"
    },
    {
        "paper_id": 2307.13807,
        "authors": "V\\'elez Jim\\'enez, Rom\\'an Alberto, Lecuanda Ontiveros, Jos\\'e Manuel,\n  Edgar Possani",
        "title": "Sports Betting: an application of neural networks and modern portfolio\n  theory to the English Premier League",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper presents a novel approach for optimizing betting strategies in\nsports gambling by integrating Von Neumann-Morgenstern Expected Utility Theory,\ndeep learning techniques, and advanced formulations of the Kelly Criterion. By\ncombining neural network models with portfolio optimization, our method\nachieved remarkable profits of 135.8% relative to the initial wealth during the\nlatter half of the 20/21 season of the English Premier League. We explore\ncomplete and restricted strategies, evaluating their performance, risk\nmanagement, and diversification. A deep neural network model is developed to\nforecast match outcomes, addressing challenges such as limited variables. Our\nresearch provides valuable insights and practical applications in the field of\nsports betting and predictive modeling.\n"
    },
    {
        "paper_id": 2307.13832,
        "authors": "Tom Liu, Stefan Zohren",
        "title": "Multi-Factor Inception: What to Do with All of These Features?",
        "comments": "10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrency trading represents a nascent field of research, with growing\nadoption in industry. Aided by its decentralised nature, many metrics\ndescribing cryptocurrencies are accessible with a simple Google search and\nupdate frequently, usually at least on a daily basis. This presents a promising\nopportunity for data-driven systematic trading research, where limited\nhistorical data can be augmented with additional features, such as hashrate or\nGoogle Trends. However, one question naturally arises: how to effectively\nselect and process these features? In this paper, we introduce Multi-Factor\nInception Networks (MFIN), an end-to-end framework for systematic trading with\nmultiple assets and factors. MFINs extend Deep Inception Networks (DIN) to\noperate in a multi-factor context. Similar to DINs, MFIN models automatically\nlearn features from returns data and output position sizes that optimise\nportfolio Sharpe ratio. Compared to a range of rule-based momentum and\nreversion strategies, MFINs learn an uncorrelated, higher-Sharpe strategy that\nis not captured by traditional, hand-crafted factors. In particular, MFIN\nmodels continue to achieve consistent returns over the most recent years\n(2022-2023), where traditional strategies and the wider cryptocurrency market\nhave underperformed.\n"
    },
    {
        "paper_id": 2307.1387,
        "authors": "Andrey Itkin, Dmitry Muravey",
        "title": "American options in time-dependent one-factor models: Semi-analytic\n  pricing, numerical methods and ML support",
        "comments": "38 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Semi-analytical pricing of American options in a time-dependent\nOrnstein-Uhlenbeck model was presented in [Carr, Itkin, 2020]. It was shown\nthat to obtain these prices one needs to solve (numerically) a nonlinear\nVolterra integral equation of the second kind to find the exercise boundary\n(which is a function of the time only). Once this is done, the option prices\nfollow. It was also shown that computationally this method is as efficient as\nthe forward finite difference solver while providing better accuracy and\nstability. Later this approach called \"the Generalized Integral transform\"\nmethod has been significantly extended by the authors (also, in cooperation\nwith Peter Carr and Alex Lipton) to various time-dependent one factor, and\nstochastic volatility models as applied to pricing barrier options. However,\nfor American options, despite possible, this was not explicitly reported\nanywhere. In this paper our goal is to fill this gap and also discuss which\nnumerical method (including those in machine learning) could be efficient to\nsolve the corresponding Volterra integral equations.\n"
    },
    {
        "paper_id": 2307.14049,
        "authors": "Kurada T S S Satyanarayana, Addada Narasimha Rao",
        "title": "Capital Structure Theories and its Practice, A study with reference to\n  select NSE listed public sectors banks, India",
        "comments": "19 PAGES, 8 FIGURES",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Among the various factors affecting the firms positioning and performance in\nmodern day markets, capital structure of the firm has its own way of expressing\nitself as a crucial one. With the rapid changes in technology, firms are being\npushed onto a paradigm that is burdening the capital management process. Hence\nthe study of capital structure changes gives the investors an insight into\nfirm's behavior and intrinsic goals. These changes will vary for firms in\ndifferent sectors. This work considers the banking sector, which has a unique\ncapital structure for the given regulations of its operations in India. The\ncapital structure behavioral changes in a few public sector banks are studied\nin this paper. A theoretical framework has been developed from the popular\ncapital structure theories and hypotheses are derived from them accordingly.\nThe main idea is to validate different theories with real time performance of\nthe select banks from 2011 to 2022. Using statistical techniques like\nregression and correlation, tested hypotheses have resulted in establishing the\nrelation between debt component and financial performance variables of the\nselect banks which are helping in understanding the theories in practice.\n"
    },
    {
        "paper_id": 2307.14129,
        "authors": "Ivan Guo, Shijia Jin, Kihun Nam",
        "title": "Macroscopic Market Making",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose the macroscopic market making model \\`a la Avellaneda-Stoikov,\nusing continuous processes for orders instead of discrete point processes. The\nmodel intends to bridge a gap between market making and optimal execution\nproblems, while shedding light on the influence of order flows on the strategy.\nWe demonstrate our model through three problems. The study provides a\ncomprehensive analysis from Markovian to non-Markovian noises and from linear\nto non-linear intensity functions, encompassing both bounded and unbounded\ncoefficients. Mathematically, the contribution lies in the existence and\nuniqueness of the optimal control, guaranteed by the well-posedness of the\nHamilton-Jacobi-Bellman equation or the (non-)Lipschitz forward-backward\nstochastic differential equation.\n"
    },
    {
        "paper_id": 2307.14218,
        "authors": "Antoine Jacquier and Mugad Oumgari",
        "title": "Interest rate convexity in a Gaussian framework",
        "comments": "17 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The contributions of this paper are twofold: we define and investigate the\nproperties of a short rate model driven by a general Gaussian Volterra process\nand, after defining precisely a notion of convexity adjustment, derive explicit\nformulae for it.\n"
    },
    {
        "paper_id": 2307.1427,
        "authors": "Ruben Zakine, Jerome Garnier-Brun, Antoine-Cyrus Becharat and Michael\n  Benzaquen",
        "title": "Socioeconomic agents as active matter in nonequilibrium Sakoda-Schelling\n  models",
        "comments": "12 pages, 7 figures + Appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How robust are socioeconomic agent-based models with respect to the details\nof the agents' decision rule? We tackle this question by considering an\noccupation model in the spirit of the Sakoda-Schelling model, historically\nintroduced to shed light on segregation dynamics among human groups. For a\nlarge class of utility functions and decision rules, we pinpoint the\nnonequilibrium nature of the agent dynamics, while recovering the\nequilibrium-like phase separation phenomenology. Within the mean field\napproximation we show how the model can be mapped, to some extent, onto an\nactive matter field description. Finally, we consider non-reciprocal\ninteractions between two populations, and show how they can lead to non-steady\nmacroscopic behavior. We believe our approach provides a unifying framework to\nfurther study geography-dependent agent-based models, notably paving the way\nfor joint consideration of population and price dynamics within a field\ntheoretic approach.\n"
    },
    {
        "paper_id": 2307.1431,
        "authors": "Nikitas Stamatopoulos and William J. Zeng",
        "title": "Derivative Pricing using Quantum Signal Processing",
        "comments": null,
        "journal-ref": "Quantum 8, 1322 (2024)",
        "doi": "10.22331/q-2024-04-30-1322",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Pricing financial derivatives on quantum computers typically includes quantum\narithmetic components which contribute heavily to the quantum resources\nrequired by the corresponding circuits. In this manuscript, we introduce a\nmethod based on Quantum Signal Processing (QSP) to encode financial derivative\npayoffs directly into quantum amplitudes, alleviating the quantum circuits from\nthe burden of costly quantum arithmetic. Compared to current state-of-the-art\napproaches in the literature, we find that for derivative contracts of\npractical interest, the application of QSP significantly reduces the required\nresources across all metrics considered, most notably the total number of\nT-gates by $\\sim 16$x and the number of logical qubits by $\\sim 4$x.\nAdditionally, we estimate that the logical clock rate needed for quantum\nadvantage is also reduced by a factor of $\\sim 5$x. Overall, we find that\nquantum advantage will require $4.7$k logical qubits, and quantum devices that\ncan execute $10^9$ T-gates at a rate of $45$MHz. While in this work we focus\nspecifically on the payoff component of the derivative pricing process where\nthe method we present is most readily applicable, similar techniques can be\nemployed to further reduce the resources in other applications, such as state\npreparation.\n"
    },
    {
        "paper_id": 2307.14322,
        "authors": "Zhiyu Cao, Zihan Chen, Prerna Mishra, Hamed Amini, Zachary Feinstein",
        "title": "Modeling Inverse Demand Function with Explainable Dual Neural Networks",
        "comments": "Accepted and selected for oral presentation at ICAIF 2023, NY, US",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial contagion has been widely recognized as a fundamental risk to the\nfinancial system. Particularly potent is price-mediated contagion, wherein\nforced liquidations by firms depress asset prices and propagate financial\nstress, enabling crises to proliferate across a broad spectrum of seemingly\nunrelated entities. Price impacts are currently modeled via exogenous inverse\ndemand functions. However, in real-world scenarios, only the initial shocks and\nthe final equilibrium asset prices are typically observable, leaving actual\nasset liquidations largely obscured. This missing data presents significant\nlimitations to calibrating the existing models. To address these challenges, we\nintroduce a novel dual neural network structure that operates in two sequential\nstages: the first neural network maps initial shocks to predicted asset\nliquidations, and the second network utilizes these liquidations to derive\nresultant equilibrium prices. This data-driven approach can capture both linear\nand non-linear forms without pre-specifying an analytical structure;\nfurthermore, it functions effectively even in the absence of observable\nliquidation data. Experiments with simulated datasets demonstrate that our\nmodel can accurately predict equilibrium asset prices based solely on initial\nshocks, while revealing a strong alignment between predicted and true\nliquidations. Our explainable framework contributes to the understanding and\nmodeling of price-mediated contagion and provides valuable insights for\nfinancial authorities to construct effective stress tests and regulatory\npolicies.\n"
    },
    {
        "paper_id": 2307.14409,
        "authors": "Nicol\\`o Vallarano, Tiziano Squartini, Claudio J. Tessone",
        "title": "Exploring the Bitcoin Mesoscale",
        "comments": "17 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The open availability of the entire history of the Bitcoin transactions opens\nup the possibility to study this system at an unprecedented level of detail.\nThis contribution is devoted to the analysis of the mesoscale structural\nproperties of the Bitcoin User Network (BUN), across its entire history (i.e.\nfrom 2009 to 2017). What emerges from our analysis is that the BUN is\ncharacterized by a core-periphery structure a deeper analysis of which reveals\na certain degree of bow-tieness (i.e. the presence of a Strongly-Connected\nComponent, an IN- and an OUT-component together with some tendrils attached to\nthe IN-component). Interestingly, the evolution of the BUN structural\norganization experiences fluctuations that seem to be correlated with the\npresence of bubbles, i.e. periods of price surge and decline observed\nthroughout the entire Bitcoin history: our results, thus, further confirm the\ninterplay between structural quantities and price movements observed in\nprevious analyses.\n"
    },
    {
        "paper_id": 2307.14525,
        "authors": "B.N. Kausik",
        "title": "Long Tails, Automation and Labor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A central question in economics is whether automation will displace human\nlabor and diminish standards of living. Whilst prior works typically frame this\nquestion as a competition between human labor and machines, we frame it as a\ncompetition between human consumers and human suppliers. Specifically, we\nobserve that human needs favor long tail distributions, i.e., a long list of\nniche items that are substantial in aggregate demand. In turn, the long tails\nare reflected in the goods and services that fulfill those needs. With this\nbackground, we propose a theoretical model of economic activity on a long tail\ndistribution, where innovation in demand for new niche outputs competes with\ninnovation in supply automation for mature outputs. Our model yields analytic\nexpressions and asymptotes for the shares of automation and labor in terms of\njust four parameters: the rates of innovation in supply and demand, the\nexponent of the long tail distribution and an initial value. We validate the\nmodel via non-linear stochastic regression on historical US economic data with\nsurprising accuracy.\n"
    },
    {
        "paper_id": 2307.14651,
        "authors": "Negha Senthil, Jayanthi Vajiram, Nirmala.V",
        "title": "The misuse of law by Women in India -Constitutionality of Gender Bias",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The misuse of law by women in India is a serious issue that has been\nreceiving increased attention in recent years. In India, women are often\ndiscriminated against and are not provided with equal rights and opportunities,\nleading to a gender bias in many aspects of life. This gender bias is further\nexacerbated by the misuse of law by women. There are numerous instances of\nwomen using the law to their advantage, often at the expense of men. This\npractice is not only unethical but also unconstitutional. The Indian\nConstitution does not explicitly guarantee gender equality. However, several\namendments have been made to the Constitution to ensure that women are treated\nequally in accordance with the law. The protection of women from all forms of\ndiscrimination is considered a fundamental right. Despite this, women continue\nto be discriminated against in various spheres of life, including marriage,\neducation, employment and other areas. The misuse of law by women in India is\nprimarily seen in cases of domestic violence and dowry-related issues and are\npunishable by law. However, women often file false dowry harassment cases\nagainst their husbands or in-laws in order to gain an advantage in a divorce or\nproperty dispute.\n"
    },
    {
        "paper_id": 2307.14661,
        "authors": "Jayanthi Vajiram, Negha Senthil, Nean Adhith.P, Ritikaa.VN",
        "title": "Exploration of legal implications of air and space travel for\n  international and domestic travel and the Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid growth of air and space travel in recent years has resulted in an\nincreased demand for legal regulation in the aviation and aerospace fields.\nThis paper provides an overview of air and space law, including the topics of\naircraft accident investigations, air traffic control, international borders\nand law, and the regulation of space activities. With the increasing complexity\nof air and space travel, it is important to understand the legal implications\nof these activities. This paper examines the various legal aspects of air and\nspace law, including the roles of national governments, international\norganizations, and private entities. It also provides an overview of the legal\nframeworks that govern these activities and the implications of international\nlaw. Finally, it considers the potential for future developments in the field\nof air and space law. This paper provides a comprehensive overview of the legal\naspects of air and space travel and their implications for international and\ndomestic travel, as well as for international business and other activities in\nthe air and space domains.\n"
    },
    {
        "paper_id": 2307.14887,
        "authors": "Josef Teichmann and Hanna Wutte",
        "title": "Machine Learning-powered Pricing of the Multidimensional Passport Option",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Introduced in the late 90s, the passport option gives its holder the right to\ntrade in a market and receive any positive gain in the resulting traded account\nat maturity. Pricing the option amounts to solving a stochastic control problem\nthat for $d>1$ risky assets remains an open problem. Even in a correlated\nBlack-Scholes (BS) market with $d=2$ risky assets, no optimal trading strategy\nhas been derived in closed form. In this paper, we derive a discrete-time\nsolution for multi-dimensional BS markets with uncorrelated assets. Moreover,\ninspired by the success of deep reinforcement learning in, e.g., board games,\nwe propose two machine learning-powered approaches to pricing general options\non a portfolio value in general markets. These approaches prove to be\nsuccessful for pricing the passport option in one-dimensional and\nmulti-dimensional uncorrelated BS markets.\n"
    },
    {
        "paper_id": 2307.15197,
        "authors": "Aziz Guergachi and Javid Hakim",
        "title": "On the mathematics of the circular flow of economic activity with\n  applications to the topic of caring for the vulnerable during pandemics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate, at the fundamental level, the questions of `why', `when' and\n`how' one could or should reach out to poor and vulnerable people to support\nthem in the absence of governmental institutions. We provide a simple and new\napproach that is rooted in linear algebra and basic graph theory to capture the\ndynamics of income circulation among economic agents. A new linear algebraic\nmodel for income circulation is introduced, based on which we are able to\ncategorize societies as fragmented or cohesive. We show that, in the case of\nfragmented societies, convincing wealthy agents at the top of the social\nhierarchy to support the poor and vulnerable will be very difficult. We also\nhighlight how linear-algebraic and simple graph-theoretic methods help explain,\nfrom a fundamental point of view, some of the mechanics of class struggle in\nfragmented societies. Then, we explain intuitively and prove mathematically\nwhy, in cohesive societies, wealthy agents at the top of the social hierarchy\ntend to benefit by supporting the vulnerable in their society. A number of new\nconcepts emerge naturally from our mathematical analysis to describe the level\nof cohesiveness of the society, the number of degrees of separation in business\n(as opposed to social) networks, and the level of generosity of the overall\neconomy, which all tend to affect the rate at which the top wealthy class\nrecovers its support money back. In the discussion on future perspectives, the\nconnections between the proposed matrix model and statistical physics concepts\nare highlighted.\n"
    },
    {
        "paper_id": 2307.153,
        "authors": "Ruyi Liu, Jingzhi Tie, Zhen Wu and Qing Zhang",
        "title": "Pairs Trading: An Optimal Selling Rule with Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The focus of this paper is on identifying the most effective selling strategy\nfor pairs trading of stocks. In pairs trading, a long position is held in one\nstock while a short position is held in another. The goal is to determine the\noptimal time to sell the long position and repurchase the short position in\norder to close the pairs position. The paper presents an optimal pairs-trading\nselling rule with trading constraints. In particular, the underlying stock\nprices evolve according to a two dimensional geometric Brownian motion and the\ntrading permission process is given in terms of a two-state {trading allowed,\ntrading not allowed} Markov chain. It is shown that the optimal policy can be\ndetermined by a threshold curve which is obtained by solving the associated HJB\nequations (quasi-variational inequalities). A closed form solution is obtained.\nA verification theorem is provided. Numerical experiments are also reported to\ndemonstrate the optimal policies and value functions.\n"
    },
    {
        "paper_id": 2307.15336,
        "authors": "Keisuke Kawata, Mizuki Komura",
        "title": "Only-child matching penalty in the marriage market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study explores the marriage matching of only-child individuals and its\noutcome. Specifically, we analyze two aspects. First, we investigate how\nmarital status (i.e., marriage with an only child, that with a non-only child\nand remaining single) differs between only children and non-only children. This\nanalysis allows us to know whether people choose mates in a positive or a\nnegative assortative manner regarding only-child status, and to predict whether\nonly-child individuals benefit from marriage matching premiums or are subject\nto penalties regarding partner attractiveness. Second, we measure the\npremium/penalty by the size of the gap in partner's socio economic status (SES,\nhere, years of schooling) between only-child and non--only-child individuals.\nThe conventional economic theory and the observed marriage patterns of positive\nassortative mating on only-child status predict that only-child individuals are\nsubject to a matching penalty in the marriage market, especially when their\npartner is also an only child. Furthermore, our estimation confirms that among\nespecially women marrying an only-child husband, only children are penalized in\nterms of 0.57-years-lower educational attainment on the part of the partner.\n"
    },
    {
        "paper_id": 2307.15402,
        "authors": "Nick James and Max Menzies",
        "title": "An exploration of the mathematical structure and behavioural biases of\n  21st century financial crises",
        "comments": "Accepted manuscript. Minor edits since v1. Equal contribution",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 630, 129256\n  (2023)",
        "doi": "10.1016/j.physa.2023.129256",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we contrast the dynamics of the 2022 Ukraine invasion financial\ncrisis with notable financial crises of the 21st century - the dot-com bubble,\nglobal financial crisis and COVID-19. We study the similarity in market\ndynamics and associated implications for equity investors between various\nfinancial market crises and we introduce new mathematical techniques to do so.\nFirst, we study the strength of collective dynamics during different market\ncrises, and compare suitable portfolio diversification strategies with respect\nto the unique number of sectors and stocks for optimal systematic risk\nreduction. Next, we introduce a new linear operator method to quantify\ndistributional distance between equity returns during various crises. Our\nmethod allows us to fairly compare underlying stock and sector performance\nduring different time periods, normalising for those collective dynamics driven\nby the overall market. Finally, we introduce a new combinatorial portfolio\noptimisation framework driven by random sampling to investigate whether\nparticular equities and equity sectors are more effective in maximising\ninvestor risk-adjusted returns during market crises.\n"
    },
    {
        "paper_id": 2307.1554,
        "authors": "Shuhei Kitamura",
        "title": "Quantifying the Influence of Climate on Human Mind and Culture: Evidence\n  from Visual Art",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  While connections between climate and the human mind and culture are widely\nacknowledged, they are not thoroughly quantified. Analyzing 100,000 paintings\nand data on 2,000 artists from the 13th to 21st centuries, the study reveals\nthat the lightness of the paintings exhibited an interesting U-shaped pattern\nmirroring global temperature trends. There is a significant association between\nthe two, even after controlling for various factors. Event study analysis using\nthe artist-level data further reveals that high-temperature shocks resulted in\nbrighter paintings in later periods for artists who experienced them compared\nto the control group. The effect is particularly pronounced in art genres that\nrely on artists' imaginations, indicating a notable influence on artists'\nminds. These findings underscore the enduring impact of climate on the human\nmind and culture throughout history and highlight art as a valuable tool for\nunderstanding people's minds and cultures.\n"
    },
    {
        "paper_id": 2307.15599,
        "authors": "Sergio Pulido, Mathieu Rosenbaum, Emmanouil Sfendourakis",
        "title": "Understanding the worst-kept secret of high-frequency trading",
        "comments": "49 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volume imbalance in a limit order book is often considered as a reliable\nindicator for predicting future price moves. In this work, we seek to analyse\nthe nuances of the relationship between prices and volume imbalance. To this\nend, we study a market-making problem which allows us to view the imbalance as\nan optimal response to price moves. In our model, there is an underlying\nefficient price driving the mid-price, which follows the model with uncertainty\nzones. A single market maker knows the underlying efficient price and\nconsequently the probability of a mid-price jump in the future. She controls\nthe volumes she quotes at the best bid and ask prices. Solving her optimization\nproblem allows us to understand endogenously the price-imbalance connection and\nto confirm in particular that it is optimal to quote a predictive imbalance.\nOur model can also be used by a platform to select a suitable tick size, which\nis known to be a crucial topic in financial regulation. The value function of\nthe market maker's control problem can be viewed as a family of functions,\nindexed by the level of the market maker's inventory, solving a coupled system\nof PDEs. We show existence and uniqueness of classical solutions to this\ncoupled system of equations. In the case of a continuous inventory, we also\nprove uniqueness of the market maker's optimal control policy.\n"
    },
    {
        "paper_id": 2307.15614,
        "authors": "Fatemeh Zarei and Yerali Gandica and Luis Enrique Correa Rocha",
        "title": "Fast but multi-partisan: Bursts of communication increase opinion\n  diversity in the temporal Deffuant model",
        "comments": "9 pages, 6 figures. Comments (e.g. missing references, suggestions,\n  ...) are welcomed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Human interactions create social networks forming the backbone of societies.\nIndividuals adjust their opinions by exchanging information through social\ninteractions. Two recurrent questions are whether social structures promote\nopinion polarisation or consensus in societies and whether polarisation can be\navoided, particularly on social media. In this paper, we hypothesise that not\nonly network structure but also the timings of social interactions regulate the\nemergence of opinion clusters. We devise a temporal version of the Deffuant\nopinion model where pairwise interactions follow temporal patterns and show\nthat burstiness alone is sufficient to refrain from consensus and polarisation\nby promoting the reinforcement of local opinions. Individuals self-organise\ninto a multi-partisan society due to network clustering, but the diversity of\nopinion clusters further increases with burstiness, particularly when\nindividuals have low tolerance and prefer to adjust to similar peers. The\nemergent opinion landscape is well-balanced regarding clusters' size, with a\nsmall fraction of individuals converging to extreme opinions. We thus argue\nthat polarisation is more likely to emerge in social media than offline social\nnetworks because of the relatively low social clustering observed online.\nCounter-intuitively, strengthening online social networks by increasing social\nredundancy may be a venue to reduce polarisation and promote opinion diversity.\n"
    },
    {
        "paper_id": 2307.15669,
        "authors": "Lutz Sager",
        "title": "Global air quality inequality over 2000-2020",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Air pollution generates substantial health damages and economic costs\nworldwide. Pollution exposure varies greatly, both between countries and within\nthem. However, the degree of air quality inequality and its' trajectory over\ntime have not been quantified at a global level. Here I use economic inequality\nindices to measure global inequality in exposure to ambient fine particles with\n2.5 microns or less in diameter (PM2.5). I find high and rising levels of\nglobal air quality inequality. The global PM2.5 Gini Index increased from 0.32\nin 2000 to 0.36 in 2020, exceeding levels of income inequality in many\ncountries. Air quality inequality is mostly driven by differences between\ncountries and less so by variation within them, as decomposition analysis\nshows. A large share of people facing the highest levels of PM2.5 exposure are\nconcentrated in only a few countries. The findings suggest that research and\npolicy efforts that focus only on differences within countries are overlooking\nan important global dimension of environmental justice.\n"
    },
    {
        "paper_id": 2307.15718,
        "authors": "Darsh Kachhara, John K.E Markin and Astha Singh",
        "title": "Option Smile Volatility and Implied Probabilities: Implications of\n  Concavity in IV Curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Earnings announcements (EADs) are corporate events that provide investors\nwith fundamentally important information. The prospect of stock price rises may\nalso contribute to EADs increased volatility. Using data on extremely short\nterm options, we study that bimodality in the risk neutral distribution and\nconcavity in the IV smiles are ubiquitous characteristics before an earnings\nannouncement day. This study compares the returns between concave and non\nconcave IV smiles to see if the concavity in the IV curve leads to any\ninformation about the risk in the market and showcases how investors hedge\nagainst extreme volatility during earnings announcements. In fact, our paper\nshows in the presence of concave IV smiles; investors pay a significant premium\nto hedge against the uncertainty caused by the forthcoming announcement.\n"
    },
    {
        "paper_id": 2307.15805,
        "authors": "Joffrey Derchu, Dimitrios Kavvathas, Thibaut Mastrolia, Mathieu\n  Rosenbaum",
        "title": "Equilibria and incentives for illiquid auction markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a toy two-player game for periodic double auction markets to\ngenerate liquidity. The game has imperfect information, which allows us to link\nmarket spreads with signal strength. We characterize Nash equilibria in cases\nwith or without incentives from the exchange. This enables us to derive new\ninsights about price formation and incentives design. We show in particular\nthat without any incentives, the market is inefficient and does not lead to any\ntrade between market participants. We however prove that quadratic fees indexed\non each players half spread leads to a transaction and we propose a\nquantitative value for the optimal fees that the exchange has to propose in\nthis model to generate liquidity.\n"
    },
    {
        "paper_id": 2307.16238,
        "authors": "Sangita Das",
        "title": "Inequality in Educational Attainment: Urban-Rural Comparison in the\n  Indian Context",
        "comments": "20 pages, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article tries to compare urban and rural literacy of fifteen selected\nIndian states during 1981 - 2011 and explores the instruments which can reduce\nthe disparity in urban and rural educational attainment. The study constructs\nSopher's urban-rural differential literacy index to analyze the trends of\nliteracy disparity across fifteen states in India over time. Although literacy\ndisparity has decreased over time, Sopher's index shows that the states of\nAndhra Pradesh, Madhya Pradesh, Gujarat, Odisha, Maharashtra and even Karnataka\nfaced high inequality in education between urban and rural India in 2011.\nAdditionally, the Fixed Effect panel data regression technique has been applied\nin the study to identify the factors which influence urban-rural inequality in\neducation. The model shows that the following factors can reduce literacy\ndisparity between urban and rural areas of India: low fertility rate in rural\nwomen, higher percentages of rural females marrying after the age of 21 years,\nmother's educational attainment and their labour force participation rate in\nrural areas.\n"
    },
    {
        "paper_id": 2307.16427,
        "authors": "Satyam Kumar, Yelleti Vivek, Vadlamani Ravi, Indranil Bose",
        "title": "Causal Inference for Banking Finance and Insurance A Survey",
        "comments": "52 pages, 9 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Causal Inference plays an significant role in explaining the decisions taken\nby statistical models and artificial intelligence models. Of late, this field\nstarted attracting the attention of researchers and practitioners alike. This\npaper presents a comprehensive survey of 37 papers published during 1992-2023\nand concerning the application of causal inference to banking, finance, and\ninsurance. The papers are categorized according to the following families of\ndomains: (i) Banking, (ii) Finance and its subdomains such as corporate\nfinance, governance finance including financial risk and financial policy,\nfinancial economics, and Behavioral finance, and (iii) Insurance. Further, the\npaper covers the primary ingredients of causal inference namely, statistical\nmethods such as Bayesian Causal Network, Granger Causality and jargon used\nthereof such as counterfactuals. The review also recommends some important\ndirections for future research. In conclusion, we observed that the application\nof causal inference in the banking and insurance sectors is still in its\ninfancy, and thus more research is possible to turn it into a viable method.\n"
    },
    {
        "paper_id": 2307.16554,
        "authors": "Richard S.J. Tol",
        "title": "The fiscal implications of stringent climate policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stringent climate policy compatible with the targets of the 2015 Paris\nAgreement would pose a substantial fiscal challenge. Reducing carbon dioxide\nemissions by 95% or more by 2050 would raise 7% (1-17%) of GDP in carbon tax\nrevenue, half of current, global tax revenue. Revenues are relatively larger in\npoorer regions. Subsidies for carbon dioxide sequestration would amount to 6.6%\n(0.3-7.1%) of GDP. These numbers are conservative as they were estimated using\nmodels that assume first-best climate policy implementation and ignore the\ncosts of raising revenue. The fiscal challenge rapidly shrinks if emission\ntargets are relaxed.\n"
    },
    {
        "paper_id": 2307.16619,
        "authors": "Thomas Deschatre and Xavier Warin",
        "title": "A Common Shock Model for multidimensional electricity intraday price\n  modelling with application to battery valuation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a multidimensional statistical model of intraday\nelectricity prices at the scale of the trading session, which allows all\nproducts to be simulated simultaneously. This model, based on Poisson measures\nand inspired by the Common Shock Poisson Model, reproduces the Samuelson effect\n(intensity and volatility increases as time to maturity decreases). It also\nreproduces the price correlation structure, highlighted here in the data, which\ndecreases as two maturities move apart. This model has only three parameters\nthat can be estimated using a moment method that we propose here. We\ndemonstrate the usefulness of the model on a case of storage valuation by\ndynamic programming over a trading session.\n"
    },
    {
        "paper_id": 2307.16649,
        "authors": "Zakaria Marah",
        "title": "American Passport options in an exponential L\\'evy model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we examine the problem of valuing an exotic derivative known as\nthe American passport option where the underlying is driven by a L\\'evy\nprocess. The passport option is a call option on a trading account. We derive\nthe pricing equation, using the dynamic programming principle, and prove that\nthe option value is a viscosity solution of variational inequality. We also\nestablish the comparison principle, which yields uniqueness and the convexity\nof the viscosity solution.\n"
    },
    {
        "paper_id": 2307.16874,
        "authors": "Arnav Hiray, Pratvi Shah, Vishwa Shah, Agam Shah, Sudheer Chava,\n  Mukesh Tiwari",
        "title": "Shifting Cryptocurrency Influence: A High-Resolution Network Analysis of\n  Market Leaders",
        "comments": "Withdrawing this preprint due to a minor error in the code\n  implementation that affects the results",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the last decade, the cryptocurrency market has experienced unprecedented\ngrowth, emerging as a prominent financial market. As this market rapidly\nevolves, it necessitates re-evaluating which cryptocurrencies command the\nmarket and steer the direction of blockchain technology. We implement a\nnetwork-based cryptocurrency market analysis to investigate this changing\nlandscape. We use novel hourly-resolution data and Kendall's Tau correlation to\nexplore the interconnectedness of the cryptocurrency market. We observed\ncritical differences in the hierarchy of cryptocurrencies determined by our\nmethod compared to rankings derived from daily data and Pearson's correlation.\nThis divergence emphasizes the potential information loss stemming from daily\ndata aggregation and highlights the limitations of Pearson's correlation. Our\nfindings show that in the early stages of this growth, Bitcoin held a leading\nrole. However, during the 2021 bull run, the landscape changed drastically. We\nsee that while Ethereum has emerged as the overall leader, it was FTT and its\nassociated exchange, FTX, that greatly led to the increase at the beginning of\nthe bull run. We also find that highly-influential cryptocurrencies are\nincreasingly gaining a commanding influence over the market as time progresses,\ndespite the growing number of cryptocurrencies making up the market.\n"
    },
    {
        "paper_id": 2308.00013,
        "authors": "Haoyang Yu, Yutong Sun, Yulin Liu, Luyao Zhang",
        "title": "Bitcoin Gold, Litecoin Silver:An Introduction to Cryptocurrency's\n  Valuation and Trading Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historically, gold and silver have played distinct roles in traditional\nmonetary systems. While gold has primarily been revered as a superior store of\nvalue, prompting individuals to hoard it, silver has commonly been used as a\nmedium of exchange. As the financial world evolves, the emergence of\ncryptocurrencies has introduced a new paradigm of value and exchange. However,\nthe store-of-value characteristic of these digital assets remains largely\nuncharted. Charlie Lee, the founder of Litecoin, once likened Bitcoin to gold\nand Litecoin to silver. To validate this analogy, our study employs several\nmetrics, including unspent transaction outputs (UTXO), spent transaction\noutputs (STXO), Weighted Average Lifespan (WAL), CoinDaysDestroyed (CDD), and\npublic on-chain transaction data. Furthermore, we've devised trading strategies\ncentered around the Price-to-Utility (PU) ratio, offering a fresh perspective\non crypto-asset valuation beyond traditional utilities. Our back-testing\nresults not only display trading indicators for both Bitcoin and Litecoin but\nalso substantiate Lee's metaphor, underscoring Bitcoin's superior\nstore-of-value proposition relative to Litecoin. We anticipate that our\nfindings will drive further exploration into the valuation of crypto assets.\nFor enhanced transparency and to promote future research, we've made our\ndatasets available on Harvard Dataverse and shared our Python code on GitHub as\nopen source.\n"
    },
    {
        "paper_id": 2308.00016,
        "authors": "Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel M. Ni, Heung-Yeung Shum,\n  Jian Guo",
        "title": "Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the most important tasks in quantitative investment research is mining\nnew alphas (effective trading signals or factors). Traditional alpha mining\nmethods, either hand-crafted factor synthesizing or algorithmic factor mining\n(e.g., search with genetic programming), have inherent limitations, especially\nin implementing the ideas of quants. In this work, we propose a new alpha\nmining paradigm by introducing human-AI interaction, and a novel prompt\nengineering algorithmic framework to implement this paradigm by leveraging the\npower of large language models. Moreover, we develop Alpha-GPT, a new\ninteractive alpha mining system framework that provides a heuristic way to\n``understand'' the ideas of quant researchers and outputs creative, insightful,\nand effective alphas. We demonstrate the effectiveness and advantage of\nAlpha-GPT via a number of alpha mining experiments.\n"
    },
    {
        "paper_id": 2308.00065,
        "authors": "Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu",
        "title": "FinPT: Financial Risk Prediction with Profile Tuning on Pretrained\n  Foundation Models",
        "comments": "Keywords: Profile Tuning, Financial Risk Prediction, Financial\n  Benchmark, Pretrained Foundation Models",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial risk prediction plays a crucial role in the financial sector.\nMachine learning methods have been widely applied for automatically detecting\npotential risks and thus saving the cost of labor. However, the development in\nthis field is lagging behind in recent years by the following two facts: 1) the\nalgorithms used are somewhat outdated, especially in the context of the fast\nadvance of generative AI and large language models (LLMs); 2) the lack of a\nunified and open-sourced financial benchmark has impeded the related research\nfor years. To tackle these issues, we propose FinPT and FinBench: the former is\na novel approach for financial risk prediction that conduct Profile Tuning on\nlarge pretrained foundation models, and the latter is a set of high-quality\ndatasets on financial risks such as default, fraud, and churn. In FinPT, we\nfill the financial tabular data into the pre-defined instruction template,\nobtain natural-language customer profiles by prompting LLMs, and fine-tune\nlarge foundation models with the profile text to make predictions. We\ndemonstrate the effectiveness of the proposed FinPT by experimenting with a\nrange of representative strong baselines on FinBench. The analytical studies\nfurther deepen the understanding of LLMs for financial risk prediction.\n"
    },
    {
        "paper_id": 2308.00087,
        "authors": "Martin He{\\ss}ler, Tobias Wand and Oliver Kamps",
        "title": "Efficient Multi-Change Point Analysis to decode Economic Crisis\n  Information from the S&P500 Mean Market Correlation",
        "comments": "22 pages, 3 figures, 1 table. Appendix with 1 figure included",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying macroeconomic events that are responsible for dramatic changes of\neconomy is of particular relevance to understand the overall economic dynamics.\nWe introduce an open-source available efficient Python implementation of a\nBayesian multi-trend change point analysis which solves significant memory and\ncomputing time limitations to extract crisis information from a correlation\nmetric. Therefore, we focus on the recently investigated S&P500 mean market\ncorrelation in a period of roughly 20 years that includes the dot-com bubble,\nthe global financial crisis and the Euro crisis. The analysis is performed\ntwo-fold: first, in retrospect on the whole dataset and second, in an on-line\nadaptive manner in pre-crisis segments. The on-line sensitivity horizon is\nroughly determined to be 80 up to 100 trading days after a crisis onset. A\ndetailed comparison to global economic events supports the interpretation of\nthe mean market correlation as an informative macroeconomic measure by a rather\ngood agreement of change point distributions and major crisis events.\nFurthermore, the results hint to the importance of the U.S. housing bubble as\ntrigger of the global financial crisis, provide new evidence for the general\nreasoning of locally (meta)stable economic states and could work as a\ncomparative impact rating of specific economic events.\n"
    },
    {
        "paper_id": 2308.00179,
        "authors": "Chowdhury Mohammad Sakib Anwar, Konstantinos Georgalos",
        "title": "Position Uncertainty in a Sequential Public Goods Game: An Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Gallice and Monz\\'on (2019) present a natural environment that sustains full\nco-operation in one-shot social dilemmas among a finite number of\nself-interested agents. They demonstrate that in a sequential public goods\ngame, where agents lack knowledge of their position in the sequence but can\nobserve some predecessors' actions, full contribution emerges in equilibrium\ndue to agents' incentive to induce potential successors to follow suit. In this\nstudy, we aim to test the theoretical predictions of this model through an\neconomic experiment. We conducted three treatments, varying the amount of\ninformation about past actions that a subject can observe, as well as their\npositional awareness. Through rigorous structural econometric analysis, we\nfound that approximately 25% of the subjects behaved in line with the\ntheoretical predictions. However, we also observed the presence of alternative\nbehavioural types among the remaining subjects. The majority were classified as\nconditional co-operators, showing a willingness to cooperate based on others'\nactions. Some subjects exhibited altruistic tendencies, while only a small\nminority engaged in free-riding behaviour.\n"
    },
    {
        "paper_id": 2308.00383,
        "authors": "Robert J Bianchi, John Hua Fan, Joelle Miffre, Tingxi Zhang",
        "title": "Exploiting the dynamics of commodity futures curves",
        "comments": null,
        "journal-ref": "Journal of Banking and Finance, 2023, 154 (106965)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Nelson-Siegel framework is employed to model the term structure of\ncommodity futures prices. Exploiting the information embedded in the level,\nslope and curvature parameters, we develop novel investment strategies that\nassume short-term continuation of recent parallel, slope or butterfly movements\nof futures curves. Systematic strategies based on the change in the slope\ngenerate significant profits that are unrelated to previously documented risk\nfactors and can survive reasonable transaction costs. Further analysis\ndemonstrates that the profitability of the slope strategy increases with\ninvestor sentiment and is in part a compensation for the drawdowns incurred\nduring economic slowdowns. The profitability can also be magnified through\ntiming and persists under alternative specifications of the Nelson-Siegel\nmodel.\n"
    },
    {
        "paper_id": 2308.00521,
        "authors": "Steve J. Bickley, Ho Fai Chan, Bang Dao, Benno Torgler, Son Tran",
        "title": "SurveyLM: A platform to explore emerging value perspectives in augmented\n  language models' behaviors",
        "comments": "8 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This white paper presents our work on SurveyLM, a platform for analyzing\naugmented language models' (ALMs) emergent alignment behaviors through their\ndynamically evolving attitude and value perspectives in complex social\ncontexts. Social Artificial Intelligence (AI) systems, like ALMs, often\nfunction within nuanced social scenarios where there is no singular correct\nresponse, or where an answer is heavily dependent on contextual factors, thus\nnecessitating an in-depth understanding of their alignment dynamics. To address\nthis, we apply survey and experimental methodologies, traditionally used in\nstudying social behaviors, to evaluate ALMs systematically, thus providing\nunprecedented insights into their alignment and emergent behaviors. Moreover,\nthe SurveyLM platform leverages the ALMs' own feedback to enhance survey and\nexperiment designs, exploiting an underutilized aspect of ALMs, which\naccelerates the development and testing of high-quality survey frameworks while\nconserving resources. Through SurveyLM, we aim to shed light on factors\ninfluencing ALMs' emergent behaviors, facilitate their alignment with human\nintentions and expectations, and thereby contributed to the responsible\ndevelopment and deployment of advanced social AI systems. This white paper\nunderscores the platform's potential to deliver robust results, highlighting\nits significance to alignment research and its implications for future social\nAI systems.\n"
    },
    {
        "paper_id": 2308.00681,
        "authors": "Mostafa Pazoki, Hamed Samarghandi, Mehdi Behroozi",
        "title": "Increasing Supply Chain Resiliency Through Equilibrium Pricing and\n  Stipulating Transportation Quota Regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Supply chain disruption can occur for a variety of reasons, including natural\ndisasters or market dynamics for which resilient strategies should be designed.\nIf the disruption is profound and with dire consequences for the economy, it\ncalls for the regulator's intervention to minimize the impact for the\nbetterment of the society. This paper considers a shipping company with limited\ncapacity which will ship a group of products with heterogeneous transportation\nand production costs and prices, and investigates the minimum quota regulation\non transportation amounts stipulated by the government. An interesting example\ncan happen in North American rail transportation market, where the rail\ncapacity is used for a variety of products and commodities such as oil and\ngrains. Similarly, in Europe supply chain of grains produced in Ukraine is\ndisrupted by the Ukraine war and the blockade of sea transportation routes,\nwhich puts pressure on rail transportation capacity of Ukraine and its\nneighboring countries to the west that needs to be shared for shipping a\nvariety of products including grains, military, and humanitarian supplies. Such\nsituations require a proper execution of government intervention for effective\nmanagement of the limited transportation capacity to avoid the rippling effects\nthroughout the economy. We propose mathematical models and solutions for the\nmarket players and the government in a Canadian case study. Subsequently, the\nconditions that justify government intervention are identified, and an\nalgorithm to obtain the optimum minimum quotas is presented.\n"
    },
    {
        "paper_id": 2308.00702,
        "authors": "Xianyang Li, Jiayi Xu, Haoxuan Xu, Yunxuan Ma, Yu Zhong, Lei Wang",
        "title": "An Empirical Study on the Holiday Effect of China's Time-Honored\n  Companies",
        "comments": "24pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock segment of China's time-honored brand enterprises has an important\nposition in our securities stock market. The holiday effect is one of the\nmarket anomalies that occur in the securities market, which refers to the\nphenomenon that the stock market has significantly different returns than other\ntrading days around festivals. The study of the holiday effect of China's\ntime-honored brand enterprises can provide fresh ideas for the revitalization\nof our time-honored brands and the revitalization of time-honored enterprises.\nThis paper takes listed companies of China's time-honored brand enterprises as\nthe research object and focuses on the impact of the holiday effect on listed\ncompanies of China's time-honored brands with the help of the event study, and\nempirically analyses the changes in the return of listed companies of China\ntime-honored brands during the Spring Festival period from 2012 to 2021. The\nempirical results reveal that: the time-honored brand concept stocks have a\nsignificant post-holiday effect during the Chinese New Year period, the\ntime-honored alcoholic beverage enterprises are more sensitive to the Chinese\nNew Year reflection, while the holiday effect of the time-honored\npharmaceutical manufacturing enterprises is not significant.\n"
    },
    {
        "paper_id": 2308.00706,
        "authors": "Francesco Saverio Bucci, Matteo Cristofaro, Pier Luigi Giardino",
        "title": "Infodemia e pandemia: la cognitive warfare ai tempi del SARS-CoV-2",
        "comments": "in Italian language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the appearance of SARS-CoV-2, two epidemics have spread: one for health\nand one for information. The virus has generated an unprecedented infodemic,\ncontributing to the establishment of a climate of great uncertainty. Massive\nand redundant information has also been an effective vector of propaganda, on a\nglobal scale, by state and non-state actors. A real hostile act in which\nphysical violence is not foreseen, but systematic management of information\nthrough the manipulation of the cognitive sphere. For this reason, the adoption\nof an attitude of active critical analysis by citizens and institutions and\nalso the implementation of common policies at an international level could\nundoubtedly facilitate the fight against cognitive warfare (so-called cognitive\nwarfare) and therefore limit its disastrous effects.\n"
    },
    {
        "paper_id": 2308.00805,
        "authors": "Ulrich Horst, D\\\"orte Kreher, Konstantins Starovoitovs",
        "title": "Second-Order Approximation of Limit Order Books in a Single-Scale Regime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We establish a first and second-order approximation for an infinite\ndimensional limit order book model (LOB) in a single (''critical'') scaling\nregime where market and limit orders arrive at a common time scale. With our\nchoice of scaling we obtain non-degenerate first-order and second-order\napproximations for the price and volume dynamics. While the first-order\napproximation is given by a standard coupled ODE-PDE system, the second-order\napproximation is non-standard and described in terms of an infinite-dimensional\nstochastic evolution equation driven by a cylindrical Brownian motion. The\ndriving noise processes exhibit a non-trivial correlation in terms of the model\nparameters. We prove that the evolution equation has a unique solution and that\nthe sequence of standardized LOB models converges weakly to the solution of the\nevolution equation. The proof uses a non-standard martingale problem. We\ncalibrate a simplified version of our model to market data and show that the\nmodel accurately captures correlations between price and volume fluctuations.\n"
    },
    {
        "paper_id": 2308.00808,
        "authors": "Vasileios Alevizos and Ilias Georgousis and Anna-Maria Kapodistria",
        "title": "Towards Climate Neutrality: A Comprehensive Overview of Sustainable\n  Operations Management, Optimization, and Wastewater Treatment Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Various studies have been conducted in the fields of sustainable operations\nmanagement, optimization, and wastewater treatment, yielding unsubstantiated\nrecovery. In the context of Europes climate neutrality vision, this paper\nreviews effective decarbonization strategies and proposes sustainable\napproaches to mitigate carbonization in various sectors such as building,\nenergy, industry, and transportation. The study also explores the role of\ndigitalization in decarbonization and reviews decarbonization policies that can\ndirect governments action towards a climate-neutral society. The paper also\npresents a review of optimization approaches applied in the fields of science\nand technology, incorporating modern optimization techniques based on various\npeer-reviewed published research papers. It emphasizes non-conventional energy\nand distributed power generating systems along with the deregulated and\nregulated environment. Additionally, this paper critically reviews the\nperformance and capability of micellar enhanced ultrafiltration (MEUF) process\nin the treatment of dye wastewater. The review presents evidence of\nsimultaneous removal of co-existing pollutants and explores the feasibility and\nefficiency of biosurfactant in-stead of chemical surfactant. Lastly, the paper\nproposes a novel firm-regulator-consumer interaction framework to study\noperations decisions and interactive cooperation considering the interactions\namong three agents through a comprehensive literature review on sustainable\noperations management. The framework provides support for exploring future\nresearch opportunities.\n"
    },
    {
        "paper_id": 2308.00921,
        "authors": "Wing Fung Chong, Daniel Linders, Zhiyu Quan, Linfeng Zhang",
        "title": "Incident-Specific Cyber Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the current market practice, many cyber insurance products offer a\ncoverage bundle for losses arising from various types of incidents, such as\ndata breaches and ransomware attacks, and the coverage for each incident type\ncomes with a separate limit and deductible. Although this gives prospective\ncyber insurance buyers more flexibility in customizing the coverage and better\nmanages the risk exposures of sellers, it complicates the decision-making\nprocess in determining the optimal amount of risks to retain and transfer for\nboth parties. This paper aims to build an economic foundation for these\nincident-specific cyber insurance products with a focus on how\nincident-specific indemnities should be designed for achieving Pareto\noptimality for both the insurance seller and buyer. Real data on cyber\nincidents is used to illustrate the feasibility of this approach. Several\nimplementation improvement methods for practicality are also discussed.\n"
    },
    {
        "paper_id": 2308.01013,
        "authors": "Anoop C V, Neeraj Negi, Anup Aprem",
        "title": "Bayesian framework for characterizing cryptocurrency market dynamics,\n  structural dependency, and volatility using potential field",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Identifying the structural dependence between the cryptocurrencies and\npredicting market trend are fundamental for effective portfolio management in\ncryptocurrency trading. In this paper, we present a unified Bayesian framework\nbased on potential field theory and Gaussian Process to characterize the\nstructural dependency of various cryptocurrencies, using historic price\ninformation. The following are our significant contributions: (i) Proposed a\nnovel model for cryptocurrency price movements as a trajectory of a dynamical\nsystem governed by a time-varying non-linear potential field. (ii) Validated\nthe existence of the non-linear potential function in cryptocurrency market\nthrough Lyapunov stability analysis. (iii) Developed a Bayesian framework for\ninferring the non-linear potential function from observed cryptocurrency\nprices. (iv) Proposed that attractors and repellers inferred from the potential\nfield are reliable cryptocurrency market indicators, surpassing existing\nattributes, such as, mean, open price or close price of an observation window,\nin the literature. (v) Analysis of cryptocurrency market during various Bitcoin\ncrash durations from April 2017 to November 2021, shows that attractors\ncaptured the market trend, volatility, and correlation. In addition, attractors\naids explainability and visualization. (vi) The structural dependence inferred\nby the proposed approach was found to be consistent with results obtained using\nthe popular wavelet coherence approach. (vii) The proposed market indicators\n(attractors and repellers) can be used to improve the prediction performance of\nstate-of-art deep learning price prediction models. As, an example, we show\nimprovement in Litecoin price prediction up to a horizon of 12 days.\n"
    },
    {
        "paper_id": 2308.01112,
        "authors": "Yuki Sato and Kiyoshi Kanazawa",
        "title": "Quantitative statistical analysis of order-splitting behaviour of\n  individual trading accounts in the Japanese stock market over nine years",
        "comments": "33 pages, 19 figures",
        "journal-ref": "Phys. Rev. Res. 5, 043131 (2023)",
        "doi": "10.1103/PhysRevResearch.5.043131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research, we focus on the order-splitting behavior. The order\nsplitting is a trading strategy to execute their large potential metaorder into\nsmall pieces to reduce transaction cost. This strategic behavior is believed to\nbe important because it is a promising candidate for the microscopic origin of\nthe long-range correlation (LRC) in the persistent order flow. Indeed, in 2005,\nLillo, Mike, and Farmer (LMF) introduced a microscopic model of the\norder-splitting traders to predict the asymptotic behavior of the LRC from the\nmicroscopic dynamics, even quantitatively. The plausibility of this scenario\nhas been qualitatively investigated by Toth et al. 2015. However, no solid\nsupport has been presented yet on the quantitative prediction by the LMF model\nin the lack of large microscopic datasets. In this report, we have provided the\nfirst quantitative statistical analysis of the order-splitting behavior at the\nlevel of each trading account. We analyse a large dataset of the Tokyo stock\nexchange (TSE) market over nine years, including the account data of traders\n(called virtual servers). The virtual server is a unit of trading accounts in\nthe TSE market, and we can effectively define the trader IDs by an appropriate\npreprocessing. We apply a strategy clustering to individual traders to identify\nthe order-splitting traders and the random traders. For most of the stocks, we\nfind that the metaorder length distribution obeys power laws with exponent\n$\\alpha$, such that $P(L)\\propto L^{-\\alpha-1}$ with the metaorder length $L$.\nBy analysing the sign correlation $C(\\tau)\\propto \\tau^{-\\gamma}$, we directly\nconfirmed the LMF prediction $\\gamma \\approx \\alpha-1$. Furthermore, we discuss\nhow to estimate the total number of the splitting traders only from public data\nvia the ACF prefactor formula in the LMF model. Our work provides the first\nquantitative evidence of the LMF model.\n"
    },
    {
        "paper_id": 2308.01121,
        "authors": "Cyril B\\'en\\'ezet (ENSIIE, LaMME), Jean-Fran\\c{c}ois Chassagneux (LPSM\n  (UMR\\_8001), UPCit\\'e), Mohan Yang (ADIA)",
        "title": "An optimal transport approach for the multiple quantile hedging problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the multiple quantile hedging problem, which is a class of\npartial hedging problems containing as special examples the quantile hedging\nproblem (F{\\\"o}llmer \\& Leukert 1999) and the PnL matching problem (introduced\nin Bouchard \\& Vu 2012). In complete non-linear markets, we show that the\nproblem can be reformulated as a kind of Monge optimal transport problem. Using\nthis observation, we introduce a Kantorovitch version of the problem and prove\nthat the value of both problems coincide. In the linear case, we thus obtain\nthat the multiple quantile hedging problem can be seen as a semi-discrete\noptimal transport problem, for which we further introduce the dual problem. We\nthen prove that there is no duality gap, allowing us to design a numerical\nmethod based on SGA algorithms to compute the multiple quantile hedging price.\n"
    },
    {
        "paper_id": 2308.01208,
        "authors": "Ashraf Ghiye, Baptiste Barreau, Laurent Carlier, Michalis Vazirgiannis",
        "title": "Adaptive Collaborative Filtering with Personalized Time Decay Functions\n  for Financial Product Recommendation",
        "comments": "10 pages, 1 figure, 2 tables, to be published in the Seventeenth ACM\n  Conference on Recommender Systems (RecSys '23)",
        "journal-ref": null,
        "doi": "10.1145/3604915.3608832",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical recommender systems often assume that historical data are\nstationary and fail to account for the dynamic nature of user preferences,\nlimiting their ability to provide reliable recommendations in time-sensitive\nsettings. This assumption is particularly problematic in finance, where\nfinancial products exhibit continuous changes in valuations, leading to\nfrequent shifts in client interests. These evolving interests, summarized in\nthe past client-product interactions, see their utility fade over time with a\ndegree that might differ from one client to another. To address this challenge,\nwe propose a time-dependent collaborative filtering algorithm that can\nadaptively discount distant client-product interactions using personalized\ndecay functions. Our approach is designed to handle the non-stationarity of\nfinancial data and produce reliable recommendations by modeling the dynamic\ncollaborative signals between clients and products. We evaluate our method\nusing a proprietary dataset from BNP Paribas and demonstrate significant\nimprovements over state-of-the-art benchmarks from relevant literature. Our\nfindings emphasize the importance of incorporating time explicitly in the model\nto enhance the accuracy of financial product recommendation.\n"
    },
    {
        "paper_id": 2308.01305,
        "authors": "Bernhard K Meister and Henry C W Price",
        "title": "A quantum double-or-nothing game: The Kelly Criterion for Spins",
        "comments": "8 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A sequence of spin-1/2 particles polarised in one of two possible directions\nis presented to an experimenter, who can wager in a double-or-nothing game on\nthe outcomes of measurements in freely chosen polarisation directions. Wealth\nis accrued through astute betting. As information is gained from the stream of\nparticles, the measurement directions are progressively adjusted, and the\nportfolio growth rate is raised. The optimal quantum strategy is determined\nnumerically and shown to differ from the classical strategy, which is\nassociated with the Kelly criterion. The paper contributes to the development\nof quantum finance, as aspects of portfolio optimisation are extended to the\nquantum realm.\n"
    },
    {
        "paper_id": 2308.01419,
        "authors": "Chao Zhang, Xingyue Pu, Mihai Cucuringu, Xiaowen Dong",
        "title": "Graph Neural Networks for Forecasting Multivariate Realized Volatility\n  with Spillover Effects",
        "comments": "8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a novel methodology for modeling and forecasting multivariate\nrealized volatilities using customized graph neural networks to incorporate\nspillover effects across stocks. The proposed model offers the benefits of\nincorporating spillover effects from multi-hop neighbors, capturing nonlinear\nrelationships, and flexible training with different loss functions. Our\nempirical findings provide compelling evidence that incorporating spillover\neffects from multi-hop neighbors alone does not yield a clear advantage in\nterms of predictive accuracy. However, modeling nonlinear spillover effects\nenhances the forecasting accuracy of realized volatilities, particularly for\nshort-term horizons of up to one week. Moreover, our results consistently\nindicate that training with the Quasi-likelihood loss leads to substantial\nimprovements in model performance compared to the commonly-used mean squared\nerror. A comprehensive series of empirical evaluations in alternative settings\nconfirm the robustness of our results.\n"
    },
    {
        "paper_id": 2308.01485,
        "authors": "Christoph B\\\"orgers and Claude Greengard",
        "title": "A new probabilistic analysis of the yard-sale model",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In Chakraborti's yard-sale model of an economy, identical agents engage in\ntrades that result in wealth exchanges, but conserve the combined wealth of all\nagents and each agent's expected wealth. In this model, wealth condensation,\nthat is, convergence to a state in which one agent owns everything and the\nothers own nothing, occurs almost surely. We give a proof of this fact that is\nmuch shorter than existing ones and extends to a modified model in which there\nis a wealth-acquired advantage, i.e., the wealthier of two trading partners is\nmore likely to benefit from the trade.\n"
    },
    {
        "paper_id": 2308.01486,
        "authors": "Rudy Morel, St\\'ephane Mallat and Jean-Philippe Bouchaud",
        "title": "Path Shadowing Monte-Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a Path Shadowing Monte-Carlo method, which provides prediction\nof future paths, given any generative model. At any given date, it averages\nfuture quantities over generated price paths whose past history matches, or\n`shadows', the actual (observed) history. We test our approach using paths\ngenerated from a maximum entropy model of financial prices, based on a recently\nproposed multi-scale analogue of the standard skewness and kurtosis called\n`Scattering Spectra'. This model promotes diversity of generated paths while\nreproducing the main statistical properties of financial prices, including\nstylized facts on volatility roughness. Our method yields state-of-the-art\npredictions for future realized volatility and allows one to determine\nconditional option smiles for the S\\&P500 that outperform both the current\nversion of the Path-Dependent Volatility model and the option market itself.\n"
    },
    {
        "paper_id": 2308.01752,
        "authors": "Nir Douer, Joachim Meyer",
        "title": "Quantifying Retrospective Human Responsibility in Intelligent Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intelligent systems have become a major part of our lives. Human\nresponsibility for outcomes becomes unclear in the interaction with these\nsystems, as parts of information acquisition, decision-making, and action\nimplementation may be carried out jointly by humans and systems. Determining\nhuman causal responsibility with intelligent systems is particularly important\nin events that end with adverse outcomes. We developed three measures of\nretrospective human causal responsibility when using intelligent systems. The\nfirst measure concerns repetitive human interactions with a system. Using\ninformation theory, it quantifies the average human's unique contribution to\nthe outcomes of past events. The second and third measures concern human causal\nresponsibility in a single past interaction with an intelligent system. They\nquantify, respectively, the unique human contribution in forming the\ninformation used for decision-making and the reasonability of the actions that\nthe human carried out. The results show that human retrospective responsibility\ndepends on the combined effects of system design and its reliability, the\nhuman's role and authority, and probabilistic factors related to the system and\nthe environment. The new responsibility measures can serve to investigate and\nanalyze past events involving intelligent systems. They may aid the judgment of\nhuman responsibility and ethical and legal discussions, providing a novel\nquantitative perspective.\n"
    },
    {
        "paper_id": 2308.01803,
        "authors": "Wenpin Tang",
        "title": "Trading and wealth evolution in the Proof of Stake protocol",
        "comments": "23 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2206.10105",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the increasing adoption of the Proof of Stake (PoS) blockchain, it is\ntimely to study the economy created by such blockchain. In this chapter, we\nwill survey recent progress on the trading and wealth evolution in a\ncryptocurrency where the new coins are issued according to the PoS protocol. We\nfirst consider the wealth evolution in the PoS protocol assuming no trading,\nand focus on the problem of decentralisation. Next we consider each miner's\ntrading incentive and strategy through the lens of optimal control, where the\nminer needs to trade off PoS mining and trading. Finally, we study the\ncollective behavior of the miners in a PoS trading environment by a mean field\nmodel. We use both stochastic and analytic tools in our study. A list of open\nproblems are also presented.\n"
    },
    {
        "paper_id": 2308.01844,
        "authors": "Yen-Jui Chang, Wei-Ting Wang, Hao-Yuan Chen, Shih-Wei Liao, Ching-Ray\n  Chang",
        "title": "A novel approach for quantum financial simulation and quantum state\n  preparation",
        "comments": "13 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2302.12500",
        "journal-ref": "Quantum Mach. Intell. 6, 24 (2024)",
        "doi": "10.1007/s42484-024-00160-5",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum state preparation is vital in quantum computing and information\nprocessing. The ability to accurately and reliably prepare specific quantum\nstates is essential for various applications. One of the promising applications\nof quantum computers is quantum simulation. This requires preparing a quantum\nstate representing the system we are trying to simulate. This research\nintroduces a novel simulation algorithm, the multi-Split-Steps Quantum Walk\n(multi-SSQW), designed to learn and load complicated probability distributions\nusing parameterized quantum circuits (PQC) with a variational solver on\nclassical simulators. The multi-SSQW algorithm is a modified version of the\nsplit-steps quantum walk, enhanced to incorporate a multi-agent decision-making\nprocess, rendering it suitable for modeling financial markets. The study\nprovides theoretical descriptions and empirical investigations of the\nmulti-SSQW algorithm to demonstrate its promising capabilities in probability\ndistribution simulation and financial market modeling. Harnessing the\nadvantages of quantum computation, the multi-SSQW models complex financial\ndistributions and scenarios with high accuracy, providing valuable insights and\nmechanisms for financial analysis and decision-making. The multi-SSQW's key\nbenefits include its modeling flexibility, stable convergence, and\ninstantaneous computation. These advantages underscore its rapid modeling and\nprediction potential in dynamic financial markets.\n"
    },
    {
        "paper_id": 2308.0191,
        "authors": "Jonas Hanetho",
        "title": "Deep Policy Gradient Methods in Commodity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The energy transition has increased the reliance on intermittent energy\nsources, destabilizing energy markets and causing unprecedented volatility,\nculminating in the global energy crisis of 2021. In addition to harming\nproducers and consumers, volatile energy markets may jeopardize vital\ndecarbonization efforts. Traders play an important role in stabilizing markets\nby providing liquidity and reducing volatility. Several mathematical and\nstatistical models have been proposed for forecasting future returns. However,\ndeveloping such models is non-trivial due to financial markets' low\nsignal-to-noise ratios and nonstationary dynamics.\n  This thesis investigates the effectiveness of deep reinforcement learning\nmethods in commodities trading. It formalizes the commodities trading problem\nas a continuing discrete-time stochastic dynamical system. This system employs\na novel time-discretization scheme that is reactive and adaptive to market\nvolatility, providing better statistical properties for the sub-sampled\nfinancial time series. Two policy gradient algorithms, an actor-based and an\nactor-critic-based, are proposed for optimizing a transaction-cost- and\nrisk-sensitive trading agent. The agent maps historical price observations to\nmarket positions through parametric function approximators utilizing deep\nneural network architectures, specifically CNNs and LSTMs.\n  On average, the deep reinforcement learning models produce an 83 percent\nhigher Sharpe ratio than the buy-and-hold baseline when backtested on\nfront-month natural gas futures from 2017 to 2022. The backtests demonstrate\nthat the risk tolerance of the deep reinforcement learning agents can be\nadjusted using a risk-sensitivity term. The actor-based policy gradient\nalgorithm performs significantly better than the actor-critic-based algorithm,\nand the CNN-based models perform slightly better than those based on the LSTM.\n"
    },
    {
        "paper_id": 2308.01915,
        "authors": "Matteo Prata, Giuseppe Masi, Leonardo Berti, Viviana Arrigoni, Andrea\n  Coletta, Irene Cannistraci, Svitlana Vyetrenko, Paola Velardi, Novella\n  Bartolini",
        "title": "LOB-Based Deep Learning Models for Stock Price Trend Prediction: A\n  Benchmark Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recent advancements in Deep Learning (DL) research have notably\ninfluenced the finance sector. We examine the robustness and generalizability\nof fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction\n(SPTP) based on Limit Order Book (LOB) data. To carry out this study, we\ndeveloped LOBCAST, an open-source framework that incorporates data\npreprocessing, DL model training, evaluation and profit analysis. Our extensive\nexperiments reveal that all models exhibit a significant performance drop when\nexposed to new data, thereby raising questions about their real-world market\napplicability. Our work serves as a benchmark, illuminating the potential and\nthe limitations of current approaches and providing insight for innovative\nsolutions.\n"
    },
    {
        "paper_id": 2308.02049,
        "authors": "Abdelali Gabih and Ralf Wunderlich",
        "title": "Portfolio Optimization in a Market with Hidden Gaussian Drift and\n  Randomly Arriving Expert Opinions: Modeling and Theoretical Results",
        "comments": "29 pages. arXiv admin note: text overlap with arXiv:2301.06847",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the optimal selection of portfolios for power utility\nmaximizing investors in a financial market where stock returns depend on a\nhidden Gaussian mean reverting drift process. Information on the drift is\nobtained from returns and expert opinions in the form of noisy signals about\nthe current state of the drift arriving randomly over time. The arrival dates\nare modeled as the jump times of a homogeneous Poisson process. Applying Kalman\nfilter techniques we derive estimates of the hidden drift which are described\nby the conditional mean and covariance of the drift given the observations. The\nutility maximization problem is solved with dynamic programming methods. We\nderive the associated dynamic programming equation and study regularization\narguments for a rigorous mathematical justification.\n"
    },
    {
        "paper_id": 2308.02083,
        "authors": "Jacob K Goeree and Bernardo Garcia-Pola",
        "title": "A Non-Parametric Test of Risk Aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In economics, risk aversion is modeled via a concave Bernoulli utility within\nthe expected-utility paradigm. We propose a simple test of expected utility and\nconcavity. We find little support for either: only 30 percent of the choices\nare consistent with a concave utility, only two out of 72 subjects are\nconsistent with expected utility, and only one of them fits the economic model\nof risk aversion. Our findings contrast with the preponderance of seemingly\n\"risk-averse\" choices that have been elicited using the popular multiple-price\nlist methodology, a result we replicate in this paper. We demonstrate that this\nmethodology is unfit to measure risk aversion, and that the high prevalence of\nrisk aversion it produces is due to parametric misspecification.\n"
    },
    {
        "paper_id": 2308.02231,
        "authors": "Jens Foerderer",
        "title": "Should we trust web-scraped data?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing adoption of econometric and machine-learning approaches by\nempirical researchers has led to a widespread use of one data collection\nmethod: web scraping. Web scraping refers to the use of automated computer\nprograms to access websites and download their content. The key argument of\nthis paper is that na\\\"ive web scraping procedures can lead to sampling bias in\nthe collected data. This article describes three sources of sampling bias in\nweb-scraped data. More specifically, sampling bias emerges from web content\nbeing volatile (i.e., being subject to change), personalized (i.e., presented\nin response to request characteristics), and unindexed (i.e., abundance of a\npopulation register). In a series of examples, I illustrate the prevalence and\nmagnitude of sampling bias. To support researchers and reviewers, this paper\nprovides recommendations on anticipating, detecting, and overcoming sampling\nbias in web-scraped data.\n"
    },
    {
        "paper_id": 2308.02246,
        "authors": "Paul Kr\\\"uhner and Shijie Xu",
        "title": "Statistically consistent term structures have affine geometry",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with finite dimensional models for the entire term\nstructure for energy futures. As soon as a finite dimensional set of possible\nyield curves is chosen, one likes to estimate the dynamic behaviour of the\nyield curve evolution from data. The estimated model should be free of\narbitrage which is known to result in some drift condition. If the yield curve\nevolution is modelled by a diffusion, then this leaves the diffusion\ncoefficient open for estimation. From a practical perspective, this requires\nthat the chosen set of possible yield curves is compatible with any obtained\ndiffusion coefficient. In this paper, we show that this compatibility enforces\nan affine geometry of the set of possible yield curves.\n"
    },
    {
        "paper_id": 2308.02491,
        "authors": "Lea Karbevska and C\\'esar A. Hidalgo",
        "title": "Mapping Global Value Chains at the Product Level",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Value chain data is crucial to navigate economic disruptions, such as those\ncaused by the COVID-19 pandemic and the war in Ukraine. Yet, despite its\nimportance, publicly available value chain datasets, such as the ``World\nInput-Output Database'', ``Inter-Country Input-Output Tables'', ``EXIOBASE'' or\nthe ``EORA'', lack detailed information about products (e.g. Radio Receivers,\nTelephones, Electrical Capacitors, LCDs, etc.) and rely instead on more\naggregate industrial sectors (e.g. Electrical Equipment, Telecommunications).\nHere, we introduce a method based on machine learning and trade theory to infer\nproduct-level value chain relationships from fine-grained international trade\ndata. We apply our method to data summarizing the exports and imports of 300+\nworld regions (e.g. states in the U.S., prefectures in Japan, etc.) and 1200+\nproducts to infer value chain information implicit in their trade patterns.\nFurthermore, we use proportional allocation to assign the trade flow between\nregions and countries. This work provides an approximate method to map value\nchain data at the product level with a relevant trade flow, that should be of\ninterest to people working in logistics, trade, and sustainable development.\n"
    },
    {
        "paper_id": 2308.02624,
        "authors": "Morgan Frank, Yong-Yeol Ahn, Esteban Moro",
        "title": "AI exposure predicts unemployment risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Is artificial intelligence (AI) disrupting jobs and creating unemployment?\nDespite many attempts to quantify occupations' exposure to AI, inconsistent\nvalidation obfuscates the relative benefits of each approach. A lack of\ndisaggregated labor outcome data, including unemployment data, further\nexacerbates the issue. Here, we assess which models of AI exposure predict job\nseparations and unemployment risk using new occupation-level unemployment data\nby occupation from each US state's unemployment insurance office spanning 2010\nthrough 2020. Although these AI exposure scores have been used by governments\nand industry, we find that individual AI exposure models are not predictive of\nunemployment rates, unemployment risk, or job separation rates. However, an\nensemble of those models exhibits substantial predictive power suggesting that\ncompeting models may capture different aspects of AI exposure that collectively\naccount for AI's variable impact across occupations, regions, and time. Our\nresults also call for dynamic, context-aware, and validated methods for\nassessing AI exposure. Interactive visualizations for this study are available\nat https://sites.pitt.edu/~mrfrank/uiRiskDemo/.\n"
    },
    {
        "paper_id": 2308.02627,
        "authors": "Daniel Sevcovic, Cyril Izuchukwu Udeani",
        "title": "Hamilton-Jacobi-Bellman Equation Arising from Optimal Portfolio\n  Selection Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Hamilton-Jacobi-Bellman equation arising from the optimal portfolio\nselection problem is studied by means of the maximal monotone operator method.\nThe existence and uniqueness of a solution to the Cauchy problem for the\nnonlinear parabolic partial integral differential equation in an abstract\nsetting are investigated by using the Banach fixed-point theorem, the Fourier\ntransform, and the monotone operators technique.\n"
    },
    {
        "paper_id": 2308.02739,
        "authors": "Raphaelle G. Coulombe, Akhil Rao",
        "title": "Fires and Local Labor Markets",
        "comments": "Paper and appendices. JEL codes: R11, Q54, E24, H84. Keywords: fires,\n  employment, economic activity, natural disasters",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We study the dynamic effects of fires on county labor markets in the US using\na novel geophysical measure of fire exposure based on satellite imagery. We\nfind increased fire exposure causes lower employment growth in the short and\nmedium run, with medium-run effects being linked to migration. We also document\nheterogeneous effects across counties by education and industrial concentration\nlevels, states of the business cycle, and fire size. By overcoming challenges\nin measuring fire impacts, we identify vulnerable places and economic states,\noffering guidance on tailoring relief efforts and contributing to a broader\nunderstanding of natural disasters' economic impacts.\n"
    },
    {
        "paper_id": 2308.0282,
        "authors": "Xianhua Peng, Chenyin Gong, Xue Dong He",
        "title": "Reinforcement Learning for Financial Index Tracking",
        "comments": "75 pages,15 figures, and 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the first discrete-time infinite-horizon dynamic formulation of\nthe financial index tracking problem under both return-based tracking error and\nvalue-based tracking error. The formulation overcomes the limitations of\nexisting models by incorporating the intertemporal dynamics of market\ninformation variables not limited to prices, allowing exact calculation of\ntransaction costs, accounting for the tradeoff between overall tracking error\nand transaction costs, allowing effective use of data in a long time period,\netc. The formulation also allows novel decision variables of cash injection or\nwithdraw. We propose to solve the portfolio rebalancing equation using a Banach\nfixed point iteration, which allows to accurately calculate the transaction\ncosts specified as nonlinear functions of trading volumes in practice. We\npropose an extension of deep reinforcement learning (RL) method to solve the\ndynamic formulation. Our RL method resolves the issue of data limitation\nresulting from the availability of a single sample path of financial data by a\nnovel training scheme. A comprehensive empirical study based on a 17-year-long\ntesting set demonstrates that the proposed method outperforms a benchmark\nmethod in terms of tracking accuracy and has the potential for earning extra\nprofit through cash withdraw strategy.\n"
    },
    {
        "paper_id": 2308.02895,
        "authors": "Jean-Philippe Bouchaud",
        "title": "From Statistical Physics to Social Sciences: The Pitfalls of\n  Multi-disciplinarity",
        "comments": "French version published as a book by Fayard (2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is the English version of my inaugural lecture at Coll\\`ege de France in\n2021, available at https://www.youtube.com/watch?v=bxktplKMhKU. I reflect on\nthe difficulty of multi-disciplinary research, which often hinges of unexpected\nepistemological and methodological differences, for example about the\nscientific status of models. What is the purpose of a model? What are we\nultimately trying to establish: rigorous theorems or ad-hoc calculation\nrecipes; absolute truth, or heuristic representations of the world? I argue\nthat the main contribution of statistical physics to social and economic\nsciences is to make us realise that unexpected behaviour can emerge at the\naggregate level, that isolated individuals would never experience. Crises,\npanics, opinion reversals, the spread of rumours or beliefs, fashion effects\nand the zeitgeist, but also the existence of money, lasting institutions,\nsocial norms and stable societies, must be understood in terms of collective\nbelief and/or trust, self-sustained by interactions, or on the contrary, the\nrapid collapse of this belief or trust. The Appendix contains my opening\nremarks to the workshop ``More is Different'', as a tribute to Phil Anderson.\n"
    },
    {
        "paper_id": 2308.02914,
        "authors": "Kleyton da Costa",
        "title": "Anomaly Detection in Global Financial Markets with Graph Neural Networks\n  and Nonextensive Entropy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Anomaly detection is a challenging task, particularly in systems with many\nvariables. Anomalies are outliers that statistically differ from the analyzed\ndata and can arise from rare events, malfunctions, or system misuse. This study\ninvestigated the ability to detect anomalies in global financial markets\nthrough Graph Neural Networks (GNN) considering an uncertainty scenario\nmeasured by a nonextensive entropy. The main findings show that the complex\nstructure of highly correlated assets decreases in a crisis, and the number of\nanomalies is statistically different for nonextensive entropy parameters\nconsidering before, during, and after crisis.\n"
    },
    {
        "paper_id": 2308.02969,
        "authors": "Guillermo Jose Navarro del Toro",
        "title": "El paradigma del marketing digital en la academia, el emprendimiento\n  universitario y las empresas establecidas",
        "comments": "24 paginas, in Spanish language",
        "journal-ref": null,
        "doi": "10.23913/ride.v13i25.1321",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The updating required by students in the marketing area is of utmost\nimportance, since they cannot wait until they are professionals to be updated,\nespecially in entrepreneurship and around digital marketing. The objective of\nthis work was to develop a digital marketing specialty module that allows\nchanging the academic-entrepreneurial-entrepreneurial paradigm of the graduate\nof the Centro Universitario de los Altos (CUAltos) in economic-administrative\nsciences at the undergraduate, graduate and specialty levels. The module aims\nto instruct the student in the latest in digital marketing. As part of this,\ntwo surveys were applied to the teachers of this center to know the viability\nand willingness to participate in this module. Among the results, it was\ndiscovered that marketing research specialists who work as teachers at CUAltos\nare willing to train other teachers and students of the University of\nGuadalajara's system, as well as to advise students in their ventures so that\nthey can apply different components of this field.\n"
    },
    {
        "paper_id": 2308.03704,
        "authors": "Yancheng Liang, Jiajie Zhang, Hui Li, Xiaochen Liu, Yi Hu, Yong Wu,\n  Jinyao Zhang, Yongyan Liu, Yi Wu",
        "title": "DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction\n  over Real-World Financial Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the tremendous advances achieved over the past years by deep learning\ntechniques, the latest risk prediction models for industrial applications still\nrely on highly handtuned stage-wised statistical learning tools, such as\ngradient boosting and random forest methods. Different from images or\nlanguages, real-world financial data are high-dimensional, sparse, noisy and\nextremely imbalanced, which makes deep neural network models particularly\nchallenging to train and fragile in practice. In this work, we propose DeRisk,\nan effective deep learning risk prediction framework for credit risk prediction\non real-world financial data. DeRisk is the first deep risk prediction model\nthat outperforms statistical learning approaches deployed in our company's\nproduction system. We also perform extensive ablation studies on our method to\npresent the most critical factors for the empirical success of DeRisk.\n"
    },
    {
        "paper_id": 2308.03858,
        "authors": "Christa Cuchiero, Tonio M\\\"ollmann and Josef Teichmann",
        "title": "Ramifications of generalized Feller theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Generalized Feller theory provides an important analog to Feller theory\nbeyond locally compact state spaces. This is very useful for solutions of\ncertain stochastic partial differential equations, Markovian lifts of\nfractional processes, or infinite dimensional affine and polynomial processes\nwhich appear prominently in the theory of signature stochastic differential\nequations. We extend several folklore results related to generalized Feller\nprocesses, in particular on their construction and path properties, and provide\nthe often quite sophisticated proofs in full detail. We also introduce the new\nconcept of extended Feller processes and compare them with standard and\ngeneralized ones. A key example relates generalized Feller semigroups of\nalgebra homomorphisms via the method of characteristics to transport equations\nand continuous semiflows on weighted spaces, i.e. a remarkably generic way to\ntreat differential equations on weighted spaces. We also provide a\ncounterexample, which shows that no condition of the basic definition of\ngeneralized Feller semigroups can be dropped.\n"
    },
    {
        "paper_id": 2308.0413,
        "authors": "Claude Martini, Arianna Mingone",
        "title": "Options are also options on options: how to smile with Black-Scholes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We observe that a European Call option with strike $L > K$ can be seen as a\nCall option with strike $L-K$ on a Call option with strike $K$. Under no\narbitrage assumptions, this yields immediately that the prices of the two\ncontracts are the same, in full generality. We study in detail the relative\npricing function which gives the price of the Call on Call option as a function\nof its underlying Call option, and provide quasi-closed formula for those new\npricing functions in the Carr-Pelts-Tehranchi family [Carr and Pelts, Duality,\nDeltas, and Derivatives Pricing, 2015] and [Tehranchi, A Black-Scholes\ninequality: applications and generalisations, Finance Stoch, 2020] that\nincludes the Black-Scholes model as a particular case. We also study the\nproperties of the function that maps the price normalized by the underlier,\nviewed as a function of the moneyness, to the normalized relative price, which\nallows us to produce several new closed formulas. In connection to the symmetry\ntransformation of a smile, we build a lift of the relative pricing function in\nthe case of an underlier that does not vanish. We finally provide some\nproperties of the implied volatility smiles of Calls on Calls and lifted Calls\non Calls in the Black-Scholes model.\n"
    },
    {
        "paper_id": 2308.04181,
        "authors": "Radhika Prosad Datta",
        "title": "Regularity in forex returns during financial distress: Evidence from\n  India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses the concepts of entropy to study the regularity/irregularity\nof the returns from the Indian Foreign exchange (forex) markets. The\nApproximate Entropy and Sample Entropy statistics which measure the level of\nrepeatability in the data are used to quantify the randomness in the forex\nreturns from the time period 2006 to 2021. The main objective of the research\nis to see how the randomness of the foreign exchange returns evolve over the\ngiven time period particularly during periods of high financial instability or\nturbulence in the global financial market. With this objective we look at 2\nmajor financial upheavals, the subprime crisis also known as the Global\nFinancial Crisis (GFC) during 2006-2007 and the recent Covid-19 pandemic during\n2020-2021. Our empirical results overwhelmingly confirm our working hypothesis\nthat regularity in the returns of the major Indian foreign exchange rates\nincreases during times of financial crisis. This is evidenced by a decrease in\nthe values of the sample entropy and approximate entropy before and\nafter/during the financial crisis period for the majority of the exchange\nrates. Our empirical results also show that Sample Entropy is a better measure\nof regularity than Approximate Entropy for the Indian forex rates which is in\nagreement with the theoretical predictions.\n"
    },
    {
        "paper_id": 2308.04378,
        "authors": "Robert Denkert and Ulrich Horst",
        "title": "Extended mean-field control problems with multi-dimensional singular\n  controls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider extended mean-field control problems with multi-dimensional\nsingular controls. A key challenge when analysing singular controls are jump\ncosts. When controls are one-dimensional, jump costs are most naturally\ncomputed by linear interpolation. When the controls are multi-dimensional the\nsituation is more complex, especially when the model parameters depend on an\nadditional mean-field interaction term, in which case one needs to \"jointly\"\nand \"consistently\" interpolate jumps both on a distributional and a pathwise\nlevel. This is achieved by introducing the novel concept of two-layer\nparametrisations of stochastic processes. Two-layer parametrisations allow us\nto equivalently rewrite rewards in terms of continuous functions of\nparametrisations of the control process and to derive an explicit\nrepresentation of rewards in terms of minimal jump costs. From this we derive a\nDPP for extended mean-field control problems with multi-dimensional singular\ncontrols. Under the additional assumption that the value function is continuous\nwe characterise the value function as the minimal super-solution to a certain\nquasi-variational inequality in the Wasserstein space.\n"
    },
    {
        "paper_id": 2308.044,
        "authors": "Moritz A. Drupp, Zachary M. Turk, Ben Groom, Jonas Heckenhahn",
        "title": "Limited substitutability, relative price changes and the uplifting of\n  public natural capital values",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As the global economy continues to grow, ecosystem services tend to stagnate\nor decline. Economic theory has shown how such shifts in relative scarcities\ncan be reflected in the appraisal of public projects and environmental-economic\naccounting, but empirical evidence has been lacking to put the theory into\npractice. To estimate the relative price change in ecosystem services that can\nbe used to make such adjustments, we perform a global meta-analysis of\nenvironmental valuation studies to derive income elasticities of willingness to\npay (WTP) for ecosystem services as a proxy for the degree of limited\nsubstitutability. Based on 861 income-WTP pairs, we estimate an income\nelasticity of WTP of around 0.79 (95-CI: 0.60 to 0.97). Combining these results\nwith a global data set on shifts in the relative scarcity of ecosystem\nservices, we estimate relative price change of ecosystem services of around 2.2\npercent per year. In an application to natural capital valuation of non-timber\nforest ecosystem services by the World Bank, we show that their natural capital\nvalue should be uplifted by more than 50 percent (95-CI: 32 to 78 percent),\nmaterially elevating the role of public natural capital. We discuss\nimplications for relative price adjustments in policy appraisal and for\nimproving estimates of comprehensive national accounts.\n"
    },
    {
        "paper_id": 2308.04493,
        "authors": "Hui Zhang, Lingxiao Wan, Sergi Ramos-Calderer, Yuancheng Zhan,\n  Wai-Keong Mok, Hong Cai, Feng Gao, Xianshu Luo, Guo-Qiang Lo, Leong Chuan\n  Kwek, Jos\\'e Ignacio Latorre and Ai Qun Liu",
        "title": "Efficient option pricing with unary-based photonic computing chip and\n  generative adversarial learning",
        "comments": "11 pages, 7 figures",
        "journal-ref": "Photonics Research 10.1364/PRJ.493865 (2023)",
        "doi": "10.1364/PRJ.493865",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the modern financial industry system, the structure of products has become\nmore and more complex, and the bottleneck constraint of classical computing\npower has already restricted the development of the financial industry. Here,\nwe present a photonic chip that implements the unary approach to European\noption pricing, in combination with the quantum amplitude estimation algorithm,\nto achieve a quadratic speedup compared to classical Monte Carlo methods. The\ncircuit consists of three modules: a module loading the distribution of asset\nprices, a module computing the expected payoff, and a module performing the\nquantum amplitude estimation algorithm to introduce speed-ups. In the\ndistribution module, a generative adversarial network is embedded for efficient\nlearning and loading of asset distributions, which precisely capture the market\ntrends. This work is a step forward in the development of specialized photonic\nprocessors for applications in finance, with the potential to improve the\nefficiency and quality of financial services.\n"
    },
    {
        "paper_id": 2308.04629,
        "authors": "Fabien Le Floc'h",
        "title": "Instabilities of explicit finite difference schemes with ghost points on\n  the diffusion equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Ghost, or fictitious points allow to capture boundary conditions that are not\nlocated on the finite difference grid discretization. We explore in this paper\nthe impact of ghost points on the stability of the explicit Euler finite\ndifference scheme in the context of the diffusion equation. In particular, we\nconsider the case of a one-touch option under the Black-Scholes model. The\nobservations and results are however valid for a much wider range of financial\ncontracts and models.\n"
    },
    {
        "paper_id": 2308.04769,
        "authors": "Ryo Hidaka, Yohei Hamakawa, Jun Nakayama, and Kosuke Tatsumura",
        "title": "Correlation-diversified portfolio construction by finding maximum\n  independent set in large-scale market graph",
        "comments": "14 pages, 9 figures, 4 tables",
        "journal-ref": "IEEE Access 11, pp. 142979 - 142991 (2023)",
        "doi": "10.1109/ACCESS.2023.3341422",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Correlation-diversified portfolios can be constructed by finding the maximum\nindependent sets (MISs) in market graphs with edges corresponding to\ncorrelations between two stocks. The computational complexity to find the MIS\nincreases exponentially as the size of the market graph increases, making the\nMIS selection in a large-scale market graph difficult. Here we construct a\ndiversified portfolio by solving the MIS problem for a large-scale market graph\nwith a combinatorial optimization solver (an Ising machine) based on a\nquantum-inspired algorithm called simulated bifurcation (SB) and investigate\nthe investment performance of the constructed portfolio using long-term\nhistorical market data. Comparisons using stock universes of various sizes\n[TOPIX 100, Nikkei 225, TOPIX 1000, and TOPIX (including approximately 2,000\nconstituents)] show that the SB-based solver outperforms conventional MIS\nsolvers in terms of computation-time and solution-accuracy. By using the\nSB-based solver, we optimized the parameters of a MIS portfolio strategy\nthrough iteration of the backcast simulation that calculates the performance of\nthe MIS portfolio strategy based on a large-scale universe covering more than\n1,700 Japanese stocks for a long period of 10 years. It has been found that the\nbest MIS portfolio strategy (Sharpe ratio = 1.16, annualized return/risk =\n16.3%/14.0%) outperforms the major indices such as TOPIX (0.66, 10.0%/15.2%)\nand MSCI Japan Minimum Volatility Index (0.64, 7.7%/12.1%) for the period from\n2013 to 2023.\n"
    },
    {
        "paper_id": 2308.04947,
        "authors": "Liping Wang, Jiawei Li, Lifan Zhao, Zhizhuo Kou, Xiaohan Wang, Xinyi\n  Zhu, Hao Wang, Yanyan Shen and Lei Chen",
        "title": "Methods for Acquiring and Incorporating Knowledge into Stock Price\n  Prediction: A Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting stock prices presents a challenging research problem due to the\ninherent volatility and non-linear nature of the stock market. In recent years,\nknowledge-enhanced stock price prediction methods have shown groundbreaking\nresults by utilizing external knowledge to understand the stock market. Despite\nthe importance of these methods, there is a scarcity of scholarly works that\nsystematically synthesize previous studies from the perspective of external\nknowledge types. Specifically, the external knowledge can be modeled in\ndifferent data structures, which we group into non-graph-based formats and\ngraph-based formats: 1) non-graph-based knowledge captures contextual\ninformation and multimedia descriptions specifically associated with an\nindividual stock; 2) graph-based knowledge captures interconnected and\ninterdependent information in the stock market. This survey paper aims to\nprovide a systematic and comprehensive description of methods for acquiring\nexternal knowledge from various unstructured data sources and then\nincorporating it into stock price prediction models. We also explore fusion\nmethods for combining external knowledge with historical price features.\nMoreover, this paper includes a compilation of relevant datasets and delves\ninto potential future research directions in this domain.\n"
    },
    {
        "paper_id": 2308.04973,
        "authors": "Allister Loder and Fabienne Cantner and Victoria Dahmen and Klaus\n  Bogenberger",
        "title": "The Mobilit\\\"at.Leben Study: a Year-Long Mobility-Tracking Panel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Mobilit\\\"at.Leben study investigated travel behavior effects of a natural\nexperiment in Germany. In response to the 2022 cost-of-living crisis, two\npolicy measures to reduce travel costs for the population in June, July, and\nAugust 2022 were introduced: a fuel excise tax cut and almost fare-free public\ntransport with the so-called 9-Euro-Ticket. The announcement of a successor\nticket to the 9-Euro-Ticket, the so-called Deutschlandticket, led to the\nimmediate decision to continue the study. The Mobilit\\\"at.Leben study has two\nperiods, the 9-Euro-Ticket period and the Deutschlandticket period, and\ncomprises two elements: several questionnaires and a smartphone-based passive\nwaypoint tracking. The entire duration of the study was almost thirteen months.\n  In this paper, we report on the study design, the recruitment strategy, the\nstudy participation in the survey, and the tracking parts, and we share our\nexperience in conducting such large-scale panel studies. Overall, 3,080 people\nregistered for our study of which 1,420 decided to use the smartphone tracking\napp. While the relevant questionnaires in both phases have been completed by\n818 participants, we have 170 study participants who completed the tracking in\nboth phases and all relevant questionnaires. We find that providing a study\ncompensation increases participation performance. It can be concluded that\nconducting year-long panel studies is possible, providing rich information on\nthe heterogeneity in travel behavior between and within travelers.\n"
    },
    {
        "paper_id": 2308.052,
        "authors": "Calvet, Emmanuel, Herranz-Celotti, Luca, and Valimamode, Karim",
        "title": "SmartDCA superiority",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dollar-Cost Averaging (DCA) is a widely used technique to mitigate volatility\nin long-term investments of appreciating assets. However, the inefficiency of\nDCA arises from fixing the investment amount regardless of market conditions.\nIn this paper, we present a more efficient approach that we name SmartDCA,\nwhich consists in adjusting asset purchases based on price levels. The\nsimplicity of SmartDCA allows for rigorous mathematical analysis, enabling us\nto establish its superiority through the application of Cauchy-Schwartz\ninequality and Lehmer means. We further extend our analysis to what we refer to\nas $\\rho$-SmartDCA, where the invested amount is raised to the power of $\\rho$.\nWe demonstrate that higher values of $\\rho$ lead to enhanced performance.\nHowever, this approach may result in unbounded investments. To address this\nconcern, we introduce a bounded version of SmartDCA, taking advantage of two\nnovel mean definitions that we name quasi-Lehmer means. The bounded SmartDCA is\nspecifically designed to retain its superiority to DCA. To support our claims,\nwe provide rigorous mathematical proofs and conduct numerical analyses across\nvarious scenarios. The performance gain of different SmartDCA alternatives is\ncompared against DCA using data from S\\&P500 and Bitcoin. The results\nconsistently demonstrate that all SmartDCA variations yield higher long-term\ninvestment returns compared to DCA.\n"
    },
    {
        "paper_id": 2308.05201,
        "authors": "Jin Liu (1), Xingchen Xu (2), Xi Nan (2), Yongjun Li (1) and Yong Tan\n  (2) ((1) University of Science and Technology of China, (2) University of\n  Washington)",
        "title": "\"Generate\" the Future of Work through AI: Empirical Evidence from Online\n  Labor Markets",
        "comments": "65 pages, 6 figures, 22 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large Language Model (LLM) based generative AI, such as ChatGPT, is\nconsidered the first generation of Artificial General Intelligence (AGI),\nexhibiting zero-shot learning abilities for a wide variety of downstream tasks.\nDue to its general-purpose and emergent nature, its impact on labor dynamics\nbecomes complex and difficult to anticipate. Leveraging an extensive dataset\nfrom a prominent online labor market, we uncover a post-ChatGPT decline in\nlabor demand, supply, and transactions for submarkets pertaining to\ntext-related and programming-related jobs, in comparison to those not directly\nexposed to ChatGPT's core functionalities. Meanwhile, these affected submarkets\nexhibit a discernible increase in the complexity of the remaining jobs and a\nheightened level of competition among freelancers. Intriguingly, our findings\nindicate that the diminution in the labor supply pertaining to programming is\ncomparatively less pronounced, a phenomenon ascribed to the transition of\nfreelancers previously engaged in text-related tasks now bidding for\nprogramming-related opportunities. Although the per-period job diversity\nfreelancers apply for tends to be more limited, those who successfully navigate\nskill transitions from text to programming demonstrate greater resilience to\nChatGPT's overall market contraction impact. As AI becomes increasingly\nversatile and potent, our paper offers crucial insights into AI's influence on\nlabor markets and individuals' reactions, underscoring the necessity for\nproactive interventions to address the challenges and opportunities presented\nby this transformative technology.\n"
    },
    {
        "paper_id": 2308.05237,
        "authors": "Nouhaila Innan, Muhammad Al-Zafar Khan, and Mohamed Bennai",
        "title": "Financial Fraud Detection: A Comparative Study of Quantum Machine\n  Learning Models",
        "comments": "30 pages, 15 figures, and 2 tables",
        "journal-ref": null,
        "doi": "10.1142/S0219749923500442",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this research, a comparative study of four Quantum Machine Learning (QML)\nmodels was conducted for fraud detection in finance. We proved that the Quantum\nSupport Vector Classifier model achieved the highest performance, with F1\nscores of 0.98 for fraud and non-fraud classes. Other models like the\nVariational Quantum Classifier, Estimator Quantum Neural Network (QNN), and\nSampler QNN demonstrate promising results, propelling the potential of QML\nclassification for financial applications. While they exhibit certain\nlimitations, the insights attained pave the way for future enhancements and\noptimisation strategies. However, challenges exist, including the need for more\nefficient Quantum algorithms and larger and more complex datasets. The article\nprovides solutions to overcome current limitations and contributes new insights\nto the field of Quantum Machine Learning in fraud detection, with important\nimplications for its future development.\n"
    },
    {
        "paper_id": 2308.05564,
        "authors": "Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn",
        "title": "Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity\n  Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Skew-t copula models are attractive for the modeling of financial data\nbecause they allow for asymmetric and extreme tail dependence. We show that the\ncopula implicit in the skew-t distribution of Azzalini and Capitanio (2003)\nallows for a higher level of pairwise asymmetric dependence than two popular\nalternative skew-t copulas. Estimation of this copula in high dimensions is\nchallenging, and we propose a fast and accurate Bayesian variational inference\n(VI) approach to do so. The method uses a generative representation of the\nskew-t distribution to define an augmented posterior that can be approximated\naccurately. A stochastic gradient ascent algorithm is used to solve the\nvariational optimization. The methodology is used to estimate skew-t factor\ncopula models with up to 15 factors for intraday returns from 2017 to 2021 on\n93 U.S. equities. The copula captures substantial heterogeneity in asymmetric\ndependence over equity pairs, in addition to the variability in pairwise\ncorrelations. In a moving window study we show that the asymmetric dependencies\nalso vary over time, and that intraday predictive densities from the skew-t\ncopula are more accurate than those from benchmark copula models. Portfolio\nselection strategies based on the estimated pairwise asymmetric dependencies\nimprove performance relative to the index.\n"
    },
    {
        "paper_id": 2308.05849,
        "authors": "Julio Deride, Roger J-B Wets",
        "title": "Solving equilibrium problems in economies with financial markets, home\n  production, and retention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new methodology to compute equilibria for general equilibrium\nproblems on exchange economies with real financial markets, home-production,\nand retention. We demonstrate that equilibrium prices can be determined by\nsolving a related maxinf-optimization problem. We incorporate the non-arbitrage\ncondition for financial markets into the equilibrium formulation and establish\nthe equivalence between solutions to both problems. This reduces the complexity\nof the original by eliminating the need to directly compute financial contract\nprices, allowing us to calculate equilibria even in cases of incomplete\nfinancial markets.\n  We also introduce a Walrasian bifunction that captures the imbalances and\nshow that maxinf-points of this function correspond to equilibrium points.\nMoreover, we demonstrate that every equilibrium point can be approximated by a\nlimit of maxinf points for a family of perturbed problems, by relying on the\nnotion of lopsided convergence.\n  Finally, we propose an augmented Walrasian algorithm and present numerical\nexamples to illustrate the effectiveness of this approach. Our methodology\nallows for efficient calculation of equilibria in a variety of exchange\neconomies and has potential applications in finance and economics.\n"
    },
    {
        "paper_id": 2308.05912,
        "authors": "Hector Galindo-Silva",
        "title": "Ideological Ambiguity and Political Spectrum",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10101-024-00310-2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the relationship between ambiguity and the ideological\npositioning of political parties across the political spectrum. We identify a\nstrong non-monotonic (inverted U-shaped) relationship between party ideology\nand ambiguity within a sample of 202 European political parties. This pattern\nis observed across all ideological dimensions covered in the data. To explain\nthis pattern, we propose a novel theory that suggests centrist parties are\nperceived as less risky by voters compared to extremist parties, giving them an\nadvantage in employing ambiguity to attract more voters at a lower cost. We\nsupport our explanation with additional evidence from electoral outcomes and\neconomic indicators in the respective party countries.\n"
    },
    {
        "paper_id": 2308.06223,
        "authors": "Chonghao Zhao",
        "title": "An approach to extend Cross-Impact Balance method in multiple timespans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cross-Impact Balance Analysis (CIB) is a widely used method to build\nscenarios and help researchers to formulate policies in different fields, such\nas management sciences and social sciences. During the development of the CIB\nmethod over the years, some derivative methods were developed to expand its\napplication scope, including a method called dynamic CIB. However, the workflow\nof dynamic CIB is relatively complex. In this article, we provide another\napproach to extend CIB in multiple timespans based on the concept 'scenario\nweight' and simplify the workflow to bring convenience to the policy makers.\n"
    },
    {
        "paper_id": 2308.0626,
        "authors": "Oleksandr Romanko, Akhilesh Narayan, Roy H. Kwon",
        "title": "ChatGPT-based Investment Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore potential uses of generative AI models, such as\nChatGPT, for investment portfolio selection. Trusting investment advice from\nGenerative Pre-Trained Transformer (GPT) models is a challenge due to model\n\"hallucinations\", necessitating careful verification and validation of the\noutput. Therefore, we take an alternative approach. We use ChatGPT to obtain a\nuniverse of stocks from S&P500 market index that are potentially attractive for\ninvesting. Subsequently, we compared various portfolio optimization strategies\nthat utilized this AI-generated trading universe, evaluating those against\nquantitative portfolio optimization models as well as comparing to some of the\npopular investment funds. Our findings indicate that ChatGPT is effective in\nstock selection but may not perform as well in assigning optimal weights to\nstocks within the portfolio. But when stocks selection by ChatGPT is combined\nwith established portfolio optimization models, we achieve even better results.\nBy blending strengths of AI-generated stock selection with advanced\nquantitative optimization techniques, we observed the potential for more robust\nand favorable investment outcomes, suggesting a hybrid approach for more\neffective and reliable investment decision-making in the future.\n"
    },
    {
        "paper_id": 2308.06265,
        "authors": "Eugene Kharitonov, Oksana Zakharchuk, Lin Mei",
        "title": "Long-term Effects of Temperature Variations on Economic Growth: A\n  Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the long-term effects of temperature variations on\neconomic growth using a data-driven approach. Leveraging machine learning\ntechniques, we analyze global land surface temperature data from Berkeley Earth\nand economic indicators, including GDP and population data, from the World\nBank. Our analysis reveals a significant relationship between average\ntemperature and GDP growth, suggesting that climate variations can\nsubstantially impact economic performance. This research underscores the\nimportance of incorporating climate factors into economic planning and\npolicymaking, and it demonstrates the utility of machine learning in uncovering\ncomplex relationships in climate-economy studies.\n"
    },
    {
        "paper_id": 2308.06279,
        "authors": "Federico Fioravanti, Fernando Delbianco, Fernando Tohm\\'e",
        "title": "Visitors Out! The Absence of Away Team Supporters as a Source of Home\n  Advantage in Football",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We seek to gain more insight into the effect of the crowds on the Home\nAdvantage by analyzing the particular case of Argentinean football (also known\nas soccer), where for more than ten years, the visiting team fans were not\nallowed to attend the games. Additionally, during the COVID-19 lockdown, a\nsignificant number of games were played without both away and home team fans.\nThe analysis of more than 20 years of matches of the Argentinean tournament\nindicates that the absence of the away team crowds was beneficial for the Top 5\nteams during the first two years after their attendance was forbidden. An\nadditional intriguing finding is that the lack of both crowds affects\nsignificantly all the teams, to the point of turning the home advantage into\nhome `disadvantage' for most of the teams.\n"
    },
    {
        "paper_id": 2308.06303,
        "authors": "Mikrajuddin Abdullah",
        "title": "A New Approach to Overcoming Zero Trade in Gravity Models to Avoid\n  Indefinite Values in Linear Logarithmic Equations and Parameter Verification\n  Using Machine Learning",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The presence of a high number of zero flow trades continues to provide a\nchallenge in identifying gravity parameters to explain international trade\nusing the gravity model. Linear regression with a logarithmic linear equation\nencounters an indefinite value on the logarithmic trade. Although several\napproaches to solving this problem have been proposed, the majority of them are\nno longer based on linear regression, making the process of finding solutions\nmore complex. In this work, we suggest a two-step technique for determining the\ngravity parameters: first, perform linear regression locally to establish a\ndummy value to substitute trade flow zero, and then estimating the gravity\nparameters. Iterative techniques are used to determine the optimum parameters.\nMachine learning is used to test the estimated parameters by analyzing their\nposition in the cluster. We calculated international trade figures for 2004,\n2009, 2014, and 2019. We just examine the classic gravity equation and discover\nthat the powers of GDP and distance are in the same cluster and are both worth\nroughly one. The strategy presented here can be used to solve other problems\ninvolving log-linear regression.\n"
    },
    {
        "paper_id": 2308.06375,
        "authors": "Daniel Jiwoong Im, Alexander Kondratskiy, Vincent Harvey, Hsuan-Wei Fu",
        "title": "UAMM: UBET Automated Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Automated market makers (AMMs) are pricing mechanisms utilized by\ndecentralized exchanges (DEX). Traditional AMM approaches are constrained by\npricing solely based on their own liquidity pool, without consideration of\nexternal markets or risk management for liquidity providers. In this paper, we\npropose a new approach known as UBET AMM (UAMM), which calculates prices by\nconsidering external market prices and the impermanent loss of the liquidity\npool. Despite relying on external market prices, our method maintains the\ndesired properties of a constant product curve when computing slippages. The\nkey element of UAMM is determining the appropriate slippage amount based on the\ndesired target balance, which encourages the liquidity pool to minimize\nimpermanent loss. We demonstrate that our approach eliminates arbitrage\nopportunities when external market prices are efficient.\n"
    },
    {
        "paper_id": 2308.06525,
        "authors": "Deb Narayan Barik and Siddhartha P. Chakrabarty",
        "title": "Loan portfolio management and Liquidity Risk: The impact of limited\n  liability and haircut",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we consider the problem of a bank's loan portfolio in the\ncontext of liquidity risk, while allowing for the limited liability protection\nenjoyed by the bank. Accordingly, we construct a novel loan portfolio model\nwith limited liability, while maintaining a threshold level of haircut in the\nportfolio. For the constructed three-time step loan portfolio, at the initial\ntime, the bank raises capital via debt and equity, investing the same in\nseveral classes of loans, while at the final time, the bank either meets its\nliabilities or becomes insolvent. At the intermediate time step, a fraction of\nthe deposits are withdrawn, resulting in liquidation of some of the bank's\nassets. The liquidated portfolio is designed with the goal of minimizing the\nliquidation cost. Our theoretical results show that model with the haircut\nconstraint leads to lesser liquidity risk, as compared to the scenario of no\nhaircut constraint being imposed. Finally, we present numerical results to\nillustrate the theoretical results which were obtained.\n"
    },
    {
        "paper_id": 2308.06568,
        "authors": "Joshua S. Gans and Hanna Halaburda",
        "title": "\"Zero Cost'' Majority Attacks on Permissionless Blockchains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The core premise of permissionless blockchains is their reliable and secure\noperation without the need to trust any individual agent. At the heart of\nblockchain consensus mechanisms is an explicit cost (whether work or stake) for\nparticipation in the network and the opportunity to add blocks to the\nblockchain. A key rationale for that cost is to make attacks on the network,\nwhich could be theoretically carried out if a majority of nodes were controlled\nby a single entity, too expensive to be worthwhile. We demonstrate that a\nmajority attacker can successfully attack with a {\\em negative cost}, which\nshows that the protocol mechanisms are insufficient to create a secure network,\nand emphasizes the importance of socially driven mechanisms external to the\nprotocol. At the same time, negative cost enables a new type of majority attack\nthat is more likely to elude external scrutiny.\n"
    },
    {
        "paper_id": 2308.06607,
        "authors": "Giampaolo Bonomi",
        "title": "The Disagreement Dividend",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how disagreement influences team performance in a dynamic game with\npositive production externalities. Players can hold different views about the\nproductivity of the available production technologies. This disagreement\nresults in different technology and effort choices -- \"optimistic\" views induce\nhigher effort than \"skeptical\" views. Views are changed when falsified by\nevidence. With a single technology available, optimists exert more effort early\non if the team also includes skeptics. With sufficiently strong externalities,\na disagreeing team produces, on average, more than any like-minded team. With\nmultiple technologies, disagreement over which technology works best motivates\neveryone to exert more effort.\n"
    },
    {
        "paper_id": 2308.06642,
        "authors": "Dong Beom Choi, Paul Goldsmith-Pinkham and Tanju Yorulmazer",
        "title": "Contagion Effects of the Silicon Valley Bank Run",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper analyzes the contagion effects associated with the failure of\nSilicon Valley Bank (SVB) and identifies bank-specific vulnerabilities\ncontributing to the subsequent declines in banks' stock returns. We find that\nuninsured deposits, unrealized losses in held-to-maturity securities, bank\nsize, and cash holdings had a significant impact, while better-quality assets\nor holdings of liquid securities did not help mitigate the negative spillovers.\nInterestingly, banks whose stocks performed worse post-SVB also experienced\nlower returns in the previous year, following Federal Reserve interest rate\nhikes. Stock investors appeared to anticipate risks associated with uninsured\ndeposit reliance, but did not foresee the realization of implied losses. While\nmid-sized banks experienced particular stress immediately after the SVB\nfailure, over time negative spillovers became widespread except for the largest\nbanks.\n"
    },
    {
        "paper_id": 2308.06882,
        "authors": "Dhruv Desai, Ashmita Dhiman, Tushar Sharma, Deepika Sharma, Dhagash\n  Mehta, Stefano Pasquali",
        "title": "Quantifying Outlierness of Funds from their Categories using Supervised\n  Similarity",
        "comments": "8 pages, 5 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mutual fund categorization has become a standard tool for the investment\nmanagement industry and is extensively used by allocators for portfolio\nconstruction and manager selection, as well as by fund managers for peer\nanalysis and competitive positioning. As a result, a (unintended)\nmiscategorization or lack of precision can significantly impact allocation\ndecisions and investment fund managers. Here, we aim to quantify the effect of\nmiscategorization of funds utilizing a machine learning based approach. We\nformulate the problem of miscategorization of funds as a distance-based outlier\ndetection problem, where the outliers are the data-points that are far from the\nrest of the data-points in the given feature space. We implement and employ a\nRandom Forest (RF) based method of distance metric learning, and compute the\nso-called class-wise outlier measures for each data-point to identify outliers\nin the data. We test our implementation on various publicly available data\nsets, and then apply it to mutual fund data. We show that there is a strong\nrelationship between the outlier measures of the funds and their future returns\nand discuss the implications of our findings.\n"
    },
    {
        "paper_id": 2308.06935,
        "authors": "Tanut Treetanthiploet, Yufei Zhang, Lukasz Szpruch, Isaac\n  Bowers-Barnard, Henrietta Ridley, James Hickey, Chris Pearce",
        "title": "Insurance pricing on price comparison websites via reinforcement\n  learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The emergence of price comparison websites (PCWs) has presented insurers with\nunique challenges in formulating effective pricing strategies. Operating on\nPCWs requires insurers to strike a delicate balance between competitive\npremiums and profitability, amidst obstacles such as low historical conversion\nrates, limited visibility of competitors' actions, and a dynamic market\nenvironment. In addition to this, the capital intensive nature of the business\nmeans pricing below the risk levels of customers can result in solvency issues\nfor the insurer. To address these challenges, this paper introduces\nreinforcement learning (RL) framework that learns the optimal pricing policy by\nintegrating model-based and model-free methods. The model-based component is\nused to train agents in an offline setting, avoiding cold-start issues, while\nmodel-free algorithms are then employed in a contextual bandit (CB) manner to\ndynamically update the pricing policy to maximise the expected revenue. This\nfacilitates quick adaptation to evolving market dynamics and enhances algorithm\nefficiency and decision interpretability. The paper also highlights the\nimportance of evaluating pricing policies using an offline dataset in a\nconsistent fashion and demonstrates the superiority of the proposed methodology\nover existing off-the-shelf RL/CB approaches. We validate our methodology using\nsynthetic data, generated to reflect private commercially available data within\nreal-world insurers, and compare against 6 other benchmark approaches. Our\nhybrid agent outperforms these benchmarks in terms of sample efficiency and\ncumulative reward with the exception of an agent that has access to perfect\nmarket information which would not be available in a real-world set-up.\n"
    },
    {
        "paper_id": 2308.07029,
        "authors": "Jiuk Jang and Hyungbin Park",
        "title": "A discretization scheme for path-dependent FBSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a discretization scheme for solutions to forward-backward\nstochastic differential equations (FBSDEs) with path-dependent coefficients. We\nshow the convergence of the Picard-type iteration to the FBDSE solution and\nprovide its convergence rate. To the best of our knowledge, this is the first\nresult of discretization scheme for path-dependent FBSDEs. Using this result,\nwe establish a numerical method for solutions to second-order parabolic\npath-dependent partial differential equations. To achieve this, weak\napproximation of martingale representation theorem (Cont, Rama, and Yi Lu.\n``Weak approximation of martingale representations.\" Stochastic Processes and\ntheir Applications 2016) is employed. Our results generalize the scheme for\nMarkovian cases in (Bender, Christian, and Robert Denk. ``A forward scheme for\nbackward SDEs.\" Stochastic processes and their applications, 2007)\n"
    },
    {
        "paper_id": 2308.07041,
        "authors": "Matthias Hafner, Marco Henriques Pereira, Helmut Dietl, Juan Beccuti",
        "title": "The four types of stablecoins: A comparative analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stablecoins have gained significant popularity recently, with their market\ncap rising to over $180 billion. However, recent events have raised concerns\nabout their stability. In this paper, we classify stablecoins into four types\nbased on the source and management of collateral and investigate the stability\nof each type under different conditions. We highlight each type's potential\ninstabilities and underlying tradeoffs using agent-based simulations. The\nresults emphasize the importance of carefully evaluating the origin of a\nstablecoin's collateral and its collateral management mechanism to ensure\nstability and minimize risks. Enhanced understanding of stablecoins should be\ninformative to regulators, policymakers, and investors alike.\n"
    },
    {
        "paper_id": 2308.07154,
        "authors": "Zhiwei Yang",
        "title": "Exploring the Nexus between Exhaustible Human Resources and Economic\n  Development in China: An Application of the Hotelling Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper applies the Hotelling model to the context of exhaustible human\nresources in China. We find that over-exploitation of human resources occurs\nunder conditions of restricted population mobility, rigid wage levels, and\nincreased foreign trade demand elasticity. Conversely, the existence of\ntechnological replacements for human resources or improvements in the\nutilization rate of human resources leads to conservation. Our analysis\nprovides practical insights for policy-making towards sustainable development.\n"
    },
    {
        "paper_id": 2308.07172,
        "authors": "Bernardo Caldarola, Dario Mazzilli, Lorenzo Napolitano, Aurelio\n  Patelli, Angelica Sbardella",
        "title": "Economic complexity and the sustainability transition: A review of data,\n  methods, and literature",
        "comments": "58 pages, 1 figure, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic Complexity (EC) methods have gained increasing popularity across\nfields and disciplines. In particular, the EC toolbox has proved particularly\npromising in the study of complex and interrelated phenomena, such as the\ntransition towards a greener economy. Using the EC approach, scholars have been\ninvestigating the relationship between EC and sustainability, proposing to\nidentify the distinguishing characteristics of green products and to assess the\nreadiness of productive and technological structures for the sustainability\ntransition. This article proposes to review and summarize the data, methods,\nand empirical literature that are relevant to the study of the sustainability\ntransition from an EC perspective. We review three distinct but connected\nblocks of literature on EC and environmental sustainability. First, we survey\nthe evidence linking measures of EC to indicators related to environmental\nsustainability. Second, we review articles that strive to assess the green\ncompetitiveness of productive systems. Third, we examine evidence on green\ntechnological development and its connection to non-green knowledge bases.\nFinally, we summarize the findings for each block and identify avenues for\nfurther research in this recent and growing body of empirical literature.\n"
    },
    {
        "paper_id": 2308.07185,
        "authors": "Nick Harkiolakis",
        "title": "The Cycle of Value The Cycle of Value -- A Conservationist Approach to\n  Economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A representation of economic activity in the form of a law of conservation of\nvalue is presented based on the definition of value as potential to act in an\nenvironment. This allows the encapsulation of the term as a conserved quantity\nthroughout transactions. Marginal value and speed of marginal value are defined\nas derivatives of value and marginal value, respectively. Traditional economic\nstatements are represented here as cycles of value where value is conserved.\nProducer-consumer dyads, shortage and surplus, as well as the role of the value\nin representing the market and the economy are explored. The role of the\ngovernment in the economy is also explained through the cycles of value the\ngovernment is involved in. Traditional economic statements and assumptions\nproduce existing hypotheses as outcomes of the law of conservation of value.\n"
    },
    {
        "paper_id": 2308.0732,
        "authors": "SujayKumar Reddy M, Gopakumar G",
        "title": "PM-Gati Shakti: Advancing India's Energy Future through Demand\n  Forecasting -- A Case Study",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  PM-Gati-Shakti Initiative, integration of ministries, including railways,\nports, waterways, logistic infrastructure, mass transport, airports, and roads.\nAimed at enhancing connectivity and bolstering the competitiveness of Indian\nbusinesses, the initiative focuses on six pivotal pillars known as\n\"Connectivity for Productivity\": comprehensiveness, prioritization,\noptimization, synchronization, analytical, and dynamic. In this study, we\nexplore the application of these pillars to address the problem of \"Maximum\nDemand Forecasting in Delhi.\" Electricity forecasting plays a very significant\nrole in the power grid as it is required to maintain a balance between supply\nand load demand at all times, to provide a quality electricity supply, for\nFinancial planning, generation reserve, and many more. Forecasting helps not\nonly in Production Planning but also in Scheduling like Import / Export which\nis very often in India and mostly required by the rural areas and North Eastern\nRegions of India. As Electrical Forecasting includes many factors which cannot\nbe detected by the models out there, We use Classical Forecasting Techniques to\nextract the seasonal patterns from the daily data of Maximum Demand for the\nUnion Territory Delhi. This research contributes to the power supply industry\nby helping to reduce the occurrence of disasters such as blackouts, power cuts,\nand increased tariffs imposed by regulatory commissions. The forecasting\ntechniques can also help in reducing OD and UD of Power for different regions.\nWe use the Data provided by a department from the Ministry of Power and use\ndifferent forecast models including Seasonal forecasts for daily data.\n"
    },
    {
        "paper_id": 2308.07329,
        "authors": "Avish Buramdoyal, Tim Gebbie",
        "title": "Variations on the Reinforcement Learning performance of Blackjack",
        "comments": "12 pages, 15 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Blackjack or \"21\" is a popular card-based game of chance and skill. The\nobjective of the game is to win by obtaining a hand total higher than the\ndealer's without exceeding 21. The ideal blackjack strategy will maximize\nfinancial return in the long run while avoiding gambler's ruin. The stochastic\nenvironment and inherent reward structure of blackjack presents an appealing\nproblem to better understand reinforcement learning agents in the presence of\nenvironment variations. Here we consider a q-learning solution for optimal play\nand investigate the rate of learning convergence of the algorithm as a function\nof deck size. A blackjack simulator allowing for universal blackjack rules is\nalso implemented to demonstrate the extent to which a card counter perfectly\nusing the basic strategy and hi-lo system can bring the house to bankruptcy and\nhow environment variations impact this outcome. The novelty of our work is to\nplace this conceptual understanding of the impact of deck size in the context\nof learning agent convergence.\n"
    },
    {
        "paper_id": 2308.07334,
        "authors": "Mengmou Li, Taichi Tanaka, A. Daniel Carnerero, Yasuaki Wasa, Kenji\n  Hirata, Yasumasa Fujisaki, Yoshiaki Ushifusa, Takeshi Hatanaka",
        "title": "Stochastic Optimal Investment Strategy for Net-Zero Energy Houses",
        "comments": "Submitted to IET Renewable Power Generation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research, we investigate Net-Zero Energy Houses (ZEH), which harness\nregionally produced electricity from photovoltaic(PV) panels and fuel cells,\nintegrating them into a local power system in pursuit of achieving carbon\nneutrality. This paper examines the impact of electricity sharing among users\nwho are working towards attaining ZEH status through the integration of PV\npanels and battery storage devices. We propose two potential scenarios: the\nfirst assumes that all users individually invest in storage devices, hence\nminimizing their costs on a local level without energy sharing; the second\nenvisions cost minimization through the collective use of a shared storage\ndevice, managed by a central manager. These two scenarios are formulated as a\nstochastic convex optimization and a cooperative game, respectively. To tackle\nthe stochastic challenges posed by multiple random variables, we apply the\nMonte Carlo sample average approximation (SAA) to the problems. To demonstrate\nthe practical applicability of these models, we implement the proposed\nscenarios in the Jono neighborhood in Kitakyushu, Japan.\n"
    },
    {
        "paper_id": 2308.07437,
        "authors": "Xin Sun",
        "title": "Impact of COVID-19 Lockdown Measures on Chinese Startups and Local\n  Government Public Finance: Challenges and Policy Implications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to assess the impact of COVID-19 on the public finance of\nChinese local governments, with a particular focus on the effect of lockdown\nmeasures on startups during the pandemic. The outbreak has placed significant\nfiscal pressure on local governments, as containment measures have led to\ndeclines in revenue and increased expenses related to public health and social\nwelfare. In tandem, startups have faced substantial challenges, including\nreduced funding and profitability, due to the negative impact of lockdown\nmeasures on entrepreneurship. Moreover, the pandemic has generated short- and\nlong-term economic shocks, affecting both employment and economic recovery. To\naddress these challenges, policymakers must balance health concerns with\neconomic development. In this regard, the government should consider\nimplementing more preferential policies that focus on startups to ensure their\nsurvival and growth. Such policies may include financial assistance, tax\nincentives, and regulatory flexibility to foster innovation and\nentrepreneurship. By and large, the COVID-19 pandemic has had a profound impact\non both the public finance of Chinese local governments and the startup\necosystem. Addressing the challenges faced by local governments and startups\nwill require a comprehensive approach that balances health and economic\nconsiderations and includes targeted policies to support entrepreneurship and\ninnovation.\n"
    },
    {
        "paper_id": 2308.07626,
        "authors": "Tanya Ara\\'ujo and Paulo Barbosa",
        "title": "Reconstructing cryptocurrency processes via Markov chains",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growing attention on cryptocurrencies has led to increasing research on\ndigital stock markets. Approaches and tools usually applied to characterize\nstandard stocks have been applied to the digital ones. Among these tools is the\nidentification of processes of market fluctuations. Being interesting\nstochastic processes, the usual statistical methods are appropriate tools for\ntheir reconstruction. There, besides chance, the description of a behavioural\ncomponent shall be present whenever a deterministic pattern is ever found.\nMarkov approaches are at the leading edge of this endeavour. In this paper,\nMarkov chains of orders one to eight are considered as a way to forecast the\ndynamics of three major cryptocurrencies. It is accomplished using an empirical\nbasis of intra-day returns. Besides forecasting, we investigate the existence\nof eventual long-memory components in each of those stochastic processes.\nResults show that predictions obtained from using the empirical probabilities\nare better than random choices.\n"
    },
    {
        "paper_id": 2308.07763,
        "authors": "Purushottam Parthasarathy, Avinash Bhardwaj, Manjesh K. Hanawal",
        "title": "Online Universal Dirichlet Factor Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We revisit the online portfolio allocation problem and propose universal\nportfolios that use factor weighing to produce portfolios that out-perform\nuniform dirichlet allocation schemes. We show a few analytical results on the\nlower bounds of portfolio growth when the returns are known to follow a factor\nmodel. We also show analytically that factor weighted dirichlet sampled\nportfolios dominate the wealth generated by uniformly sampled dirichlet\nportfolios. We corroborate our analytical results with empirical studies on\nequity markets that are known to be driven by factors.\n"
    },
    {
        "paper_id": 2308.07835,
        "authors": "Abdul-Lateef Haji-Ali and Jonathan Spence",
        "title": "Nested Multilevel Monte Carlo with Biased and Antithetic Sampling",
        "comments": "28 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of estimating a nested structure of two expectations\ntaking the form $U_0 = E[\\max\\{U_1(Y), \\pi(Y)\\}]$, where $U_1(Y) = E[X\\ |\\ Y]$.\nTerms of this form arise in financial risk estimation and option pricing. When\n$U_1(Y)$ requires approximation, but exact samples of $X$ and $Y$ are\navailable, an antithetic multilevel Monte Carlo (MLMC) approach has been\nwell-studied in the literature. Under general conditions, the antithetic MLMC\nestimator obtains a root mean squared error $\\varepsilon$ with order\n$\\varepsilon^{-2}$ cost. If, additionally, $X$ and $Y$ require approximate\nsampling, careful balancing of the various aspects of approximation is required\nto avoid a significant computational burden. Under strong convergence criteria\non approximations to $X$ and $Y$, randomised multilevel Monte Carlo techniques\ncan be used to construct unbiased Monte Carlo estimates of $U_1$, which can be\npaired with an antithetic MLMC estimate of $U_0$ to recover order\n$\\varepsilon^{-2}$ computational cost. In this work, we instead consider biased\nmultilevel approximations of $U_1(Y)$, which require less strict assumptions on\nthe approximate samples of $X$. Extensions to the method consider an\napproximate and antithetic sampling of $Y$. Analysis shows the resulting\nestimator has order $\\varepsilon^{-2}$ asymptotic cost under the conditions\nrequired by randomised MLMC and order $\\varepsilon^{-2}|\\log\\varepsilon|^3$\ncost under more general assumptions.\n"
    },
    {
        "paper_id": 2308.07944,
        "authors": "Petr Sokerin, Kristian Kuznetsov, Elizaveta Makhneva, Alexey Zaytsev",
        "title": "Portfolio Selection via Topological Data Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Portfolio management is an essential part of investment decision-making.\nHowever, traditional methods often fail to deliver reasonable performance. This\nproblem stems from the inability of these methods to account for the unique\ncharacteristics of multivariate time series data from stock markets. We present\na two-stage method for constructing an investment portfolio of common stocks.\nThe method involves the generation of time series representations followed by\ntheir subsequent clustering. Our approach utilizes features based on\nTopological Data Analysis (TDA) for the generation of representations, allowing\nus to elucidate the topological structure within the data. Experimental results\nshow that our proposed system outperforms other methods. This superior\nperformance is consistent over different time frames, suggesting the viability\nof TDA as a powerful tool for portfolio selection.\n"
    },
    {
        "paper_id": 2308.07993,
        "authors": "Oleksandr Rossolov, Anastasiia Botsman, Serhii Lyfenko, Yusak O.\n  Susilo",
        "title": "Does courier gender matter? Exploring mode choice behaviour for\n  E-groceries crowd-shipping in developing economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the mode choice behaviour of people who may act as\noccasional couriers to provide crowd-shipping (CS) deliveries. Given its recent\nincrease in popularity, online grocery services have become the main market for\ncrowd-shipping deliveries' provider. The study included a behavioural survey,\nPTV Visum simulations and discrete choice behaviour modelling based on random\nutility maximization theory. Mode choice behaviour was examined by considering\nthe gender heterogeneity of the occasional couriers in a multimodal urban\ntransport network. The behavioural dataset was collected in the city of\nKharkiv, Ukraine, at the beginning of 2021. The results indicated that women\nwere willing to provide CS service with 8% less remuneration than men. Women\nwere also more likely to make 10% longer detours by car and metro than men,\nwhile male couriers were willing to implement 25% longer detours when\ntravelling by bike or walking. Considering the integration of CS detours into\nthe couriers' routine trip chains, women couriers were more likely to attach\nthe CS trip to the work-shopping trip chain whilst men would use the home-home\nevening time trip chain. The estimated marginal probability effect indicated a\nhigher detour time sensitivity with respect to expected profit and the relative\ndetour costs of the couriers.\n"
    },
    {
        "paper_id": 2308.08031,
        "authors": "Dimitrios Vamvourellis, M\\'at\\'e Toth, Snigdha Bhagat, Dhruv Desai,\n  Dhagash Mehta, Stefano Pasquali",
        "title": "Company Similarity using Large Language Models",
        "comments": "8 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying companies with similar profiles is a core task in finance with a\nwide range of applications in portfolio construction, asset pricing and risk\nattribution. When a rigorous definition of similarity is lacking, financial\nanalysts usually resort to 'traditional' industry classifications such as\nGlobal Industry Classification System (GICS) which assign a unique category to\neach company at different levels of granularity. Due to their discrete nature,\nthough, GICS classifications do not allow for ranking companies in terms of\nsimilarity. In this paper, we explore the ability of pre-trained and finetuned\nlarge language models (LLMs) to learn company embeddings based on the business\ndescriptions reported in SEC filings. We show that we can reproduce GICS\nclassifications using the embeddings as features. We also benchmark these\nembeddings on various machine learning and financial metrics and conclude that\nthe companies that are similar according to the embeddings are also similar in\nterms of financial performance metrics including return correlation.\n"
    },
    {
        "paper_id": 2308.08066,
        "authors": "Guillermo Angeris, Tarun Chitra, Theo Diamandis, Alex Evans, Kshitij\n  Kulkarni",
        "title": "The Geometry of Constant Function Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constant function market makers (CFMMs) are the most popular type of\ndecentralized trading venue for cryptocurrency tokens. In this paper, we give a\nvery general geometric framework (or 'axioms') which encompass and generalize\nmany of the known results for CFMMs in the literature, without requiring strong\nconditions such as differentiability or homogeneity. One particular consequence\nof this framework is that every CFMM has a (unique) canonical trading function\nthat is nondecreasing, concave, and homogeneous, showing that many results\nknown only for homogeneous trading functions are actually fully general. We\nalso show that CFMMs satisfy a number of intuitive and geometric composition\nrules, and give a new proof, via conic duality, of the equivalence of the\nportfolio value function and the trading function. Many results are extended to\nthe general setting where the CFMM is not assumed to be path-independent, but\nonly one trade is allowed. Finally, we show that all 'path-independent' CFMMs\nhave a simple geometric description that does not depend on any notion of a\n'trading history'.\n"
    },
    {
        "paper_id": 2308.08135,
        "authors": "Xianfeng Jiao, Zizhong Li, Chang Xu, Yang Liu, Weiqing Liu and Jiang\n  Bian",
        "title": "Microstructure-Empowered Stock Factor Extraction and Utilization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High-frequency quantitative investment is a crucial aspect of stock\ninvestment. Notably, order flow data plays a critical role as it provides the\nmost detailed level of information among high-frequency trading data, including\ncomprehensive data from the order book and transaction records at the tick\nlevel. The order flow data is extremely valuable for market analysis as it\nequips traders with essential insights for making informed decisions. However,\nextracting and effectively utilizing order flow data present challenges due to\nthe large volume of data involved and the limitations of traditional factor\nmining techniques, which are primarily designed for coarser-level stock data.\nTo address these challenges, we propose a novel framework that aims to\neffectively extract essential factors from order flow data for diverse\ndownstream tasks across different granularities and scenarios. Our method\nconsists of a Context Encoder and an Factor Extractor. The Context Encoder\nlearns an embedding for the current order flow data segment's context by\nconsidering both the expected and actual market state. In addition, the Factor\nExtractor uses unsupervised learning methods to select such important signals\nthat are most distinct from the majority within the given context. The\nextracted factors are then utilized for downstream tasks. In empirical studies,\nour proposed framework efficiently handles an entire year of stock order flow\ndata across diverse scenarios, offering a broader range of applications\ncompared to existing tick-level approaches that are limited to only a few days\nof stock data. We demonstrate that our method extracts superior factors from\norder flow data, enabling significant improvement for stock trend prediction\nand order execution tasks at the second and minute level.\n"
    },
    {
        "paper_id": 2308.08315,
        "authors": "Iram Gleriaa, Sergio Da Silvab, Leon Brenig, Tarc{\\i}sio M. Rocha\n  Filho, Annibal Figueiredo",
        "title": "Modified Verhulst-Solow model for long-term population and economic\n  growths",
        "comments": "28 pages, 9 figures, Preprint Article",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this study, we analyze the relationship between human population growth\nand economic dynamics. To do so, we present a modified version of the Verhulst\nmodel and the Solow model, which together simulate population dynamics and the\nrole of economic variables in capital accumulation. The model incorporates\nsupport and foraging functions, which participate in the dynamic relationship\nbetween population growth and the creation and destruction of carrying\ncapacity. The validity of the model is demonstrated using empirical data.\n"
    },
    {
        "paper_id": 2308.08549,
        "authors": "S.Srinivas, R.Gadela, R.Sabu, A.Das, G.Nath and V.Datla",
        "title": "Effects of Daily News Sentiment on Stock Price Forecasting",
        "comments": "9th ICBAI conference on December'2022 at IIM Bangalore",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting future prices of a stock is an arduous task to perform. However,\nincorporating additional elements can significantly improve our predictions,\nrather than relying solely on a stock's historical price data to forecast its\nfuture price. Studies have demonstrated that investor sentiment, which is\nimpacted by daily news about the company, can have a significant impact on\nstock price swings. There are numerous sources from which we can get this\ninformation, but they are cluttered with a lot of noise, making it difficult to\naccurately extract the sentiments from them. Hence the focus of our research is\nto design an efficient system to capture the sentiments from the news about the\nNITY50 stocks and investigate how much the financial news sentiment of these\nstocks are affecting their prices over a period of time. This paper presents a\nrobust data collection and preprocessing framework to create a news database\nfor a timeline of around 3.7 years, consisting of almost half a million news\narticles. We also capture the stock price information for this timeline and\ncreate multiple time series data, that include the sentiment scores from\nvarious sections of the article, calculated using different sentiment\nlibraries. Based on this, we fit several LSTM models to forecast the stock\nprices, with and without using the sentiment scores as features and compare\ntheir performances.\n"
    },
    {
        "paper_id": 2308.0855,
        "authors": "Damien Challet and Vincent Ragel",
        "title": "Recurrent Neural Networks with more flexible memory: better predictions\n  than rough volatility",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend recurrent neural networks to include several flexible timescales\nfor each dimension of their output, which mechanically improves their abilities\nto account for processes with long memory or with highly disparate time scales.\nWe compare the ability of vanilla and extended long short term memory networks\n(LSTMs) to predict asset price volatility, known to have a long memory.\nGenerally, the number of epochs needed to train extended LSTMs is divided by\ntwo, while the variation of validation and test losses among models with the\nsame hyperparameters is much smaller. We also show that the model with the\nsmallest validation loss systemically outperforms rough volatility predictions\nby about 20% when trained and tested on a dataset with multiple time series.\n"
    },
    {
        "paper_id": 2308.08554,
        "authors": "Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap",
        "title": "AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies\n  and Price Factors",
        "comments": "8 pages, 5 figures, 7 tables. Accepted for publication in The Fifth\n  International Conference on Blockchain Computing and Applications (BCCA 2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies have become a popular and widely researched topic of\ninterest in recent years for investors and scholars. In order to make informed\ninvestment decisions, it is essential to comprehend the factors that impact\ncryptocurrency prices and to identify risky cryptocurrencies. This paper\nfocuses on analyzing historical data and using artificial intelligence\nalgorithms on on-chain parameters to identify the factors affecting a\ncryptocurrency's price and to find risky cryptocurrencies. We conducted an\nanalysis of historical cryptocurrencies' on-chain data and measured the\ncorrelation between the price and other parameters. In addition, we used\nclustering and classification in order to get a better understanding of a\ncryptocurrency and classify it as risky or not. The analysis revealed that a\nsignificant proportion of cryptocurrencies (39%) disappeared from the market,\nwhile only a small fraction (10%) survived for more than 1000 days. Our\nanalysis revealed a significant negative correlation between cryptocurrency\nprice and maximum and total supply, as well as a weak positive correlation\nbetween price and 24-hour trading volume. Moreover, we clustered\ncryptocurrencies into five distinct groups using their on-chain parameters,\nwhich provides investors with a more comprehensive understanding of a\ncryptocurrency when compared to those clustered with it. Finally, by\nimplementing multiple classifiers to predict whether a cryptocurrency is risky\nor not, we obtained the best f1-score of 76% using K-Nearest Neighbor.\n"
    },
    {
        "paper_id": 2308.08558,
        "authors": "Minsuk Kim, Byungchul Kim, Junyeong Yong, Jeongwoo Park and Gyeongmin\n  Kim",
        "title": "BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal\n  Pattern Matching",
        "comments": "5 pages, 2 figures, KDD 2023 Machine Learning in Finance workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series have historically been assumed to be a martingale\nprocess under the Random Walk hypothesis. Instead of making investment\ndecisions using the raw prices alone, various multimodal pattern matching\nalgorithms have been developed to help detect subtly hidden repeatable patterns\nwithin the financial market. Many of the chart-based pattern matching tools\nonly retrieve similar past chart (PC) patterns given the current chart (CC)\npattern, and leaves the entire interpretive and predictive analysis, thus\nultimately the final investment decision, to the investors. In this paper, we\npropose an approach of ranking similar PC movements given the CC information\nand show that exploiting this as additional features improves the directional\nprediction capacity of our model. We apply our ranking and directional\nprediction modeling methodologies on Bitcoin due to its highly volatile prices\nthat make it challenging to predict its future movements.\n"
    },
    {
        "paper_id": 2308.0863,
        "authors": "Lili Miao, Vincent Larivi\\`ere, Feifei Wang, Yong-Yeol Ahn, Cassidy R.\n  Sugimoto",
        "title": "Cooperation and interdependence in global science funding",
        "comments": "48 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Investments in research and development are key to scientific and economic\ngrowth and to the well-being of society. Scientific research demands\nsignificant resources making national scientific investment a crucial driver of\nscientific production. As scientific production becomes increasingly\nmultinational, it is critical to study how nations' scientific activities are\nfunded both domestically and internationally. By tracing research grants\nacknowledged in scholarly publications, our study reveals a shifting duopoly of\nChina and the United States in the global funding landscape, with a contrasting\nfunding pattern; while China has surpassed the United States in publications\nwith acknowledged domestic and international funding, the United States largely\nmaintains its role as the most important global research partner. Our results\nalso highlight the precarity of low- and middle-income countries to global\nfunding disruptions. By revealing the complex interdependence and collaboration\nbetween countries in the global scientific enterprise, this work informs future\nstudies investigating the national and global scientific enterprise and how\nfunding leads to both productive cooperation and vulnerable dependencies.\n"
    },
    {
        "paper_id": 2308.08683,
        "authors": "Haochen Li, Maria Polukarova and Carmine Ventre",
        "title": "Detecting Financial Market Manipulation with Statistical Physics Tools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We take inspiration from statistical physics to develop a novel conceptual\nframework for the analysis of financial markets. We model the order book\ndynamics as a motion of particles and define the momentum measure of the system\nas a way to summarise and assess the state of the market. Our approach proves\nuseful in capturing salient financial market phenomena: in particular, it helps\ndetect the market manipulation activities called spoofing and layering. We\napply our method to identify pathological order book behaviours during the\nflash crash of the LUNA cryptocurrency, uncovering widespread instances of\nspoofing and layering in the market. Furthermore, we establish that our\ntechnique outperforms the conventional Z-score-based anomaly detection method\nin identifying market manipulations across both LUNA and Bitcoin cryptocurrency\nmarkets.\n"
    },
    {
        "paper_id": 2308.08745,
        "authors": "Masayuki Ando and Masaaki Fukasawa",
        "title": "When to efficiently rebalance a portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A constant weight asset allocation is a popular investment strategy and is\noptimal under a suitable continuous model. We study the tracking error for the\ntarget continuous rebalancing strategy by a feasible discrete-in-time\nrebalancing under a general multi-dimensional Brownian semimartingale model of\nasset prices. In a high-frequency asymptotic framework, we derive an\nasymptotically efficient sequence of simple predictable strategies.\n"
    },
    {
        "paper_id": 2308.0876,
        "authors": "Andrey Itkin",
        "title": "Semi-analytic pricing of American options in time-dependent\n  jump-diffusion models with exponential jumps",
        "comments": "23 pages, 1 table, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a semi-analytic approach to pricing American options\nfor time-dependent jump-diffusions models with exponential jumps The idea of\nthe method is to further generalize our approach developed for pricing barrier,\n[Itkin et al., 2021], and American, [Carr and Itkin, 2021; Itkin and Muravey,\n2023], options in various time-dependent one factor and even stochastic\nvolatility models. Our approach i) allows arbitrary dependencies of the model\nparameters on time; ii) reduces solution of the pricing problem for American\noptions to a simpler problem of solving a system of an algebraic nonlinear\nequation for the exercise boundary and a linear Fredholm-Volterra equation for\nthe the option price; iii) the options Greeks solve a similar Fredholm-Volterra\nlinear equation obtained by just differentiating Eq. (25) by the required\nparameter. Once done, the American option price is presented in close form.\n"
    },
    {
        "paper_id": 2308.08776,
        "authors": "Qin Chen, Jinfeng Ge, Huaqing Xie, Xingcheng Xu, Yanqing Yang",
        "title": "Large Language Models at Work in China's Labor Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the potential impacts of large language models (LLMs) on\nthe Chinese labor market. We analyze occupational exposure to LLM capabilities\nby incorporating human expertise and LLM classifications, following Eloundou et\nal. (2023)'s methodology. We then aggregate occupation exposure to the industry\nlevel to obtain industry exposure scores. The results indicate a positive\ncorrelation between occupation exposure and wage levels/experience premiums,\nsuggesting higher-paying and experience-intensive jobs may face greater\ndisplacement risks from LLM-powered software. The industry exposure scores\nalign with expert assessments and economic intuitions. We also develop an\neconomic growth model incorporating industry exposure to quantify the\nproductivity-employment trade-off from AI adoption. Overall, this study\nprovides an analytical basis for understanding the labor market impacts of\nincreasingly capable AI systems in China. Key innovations include the\noccupation-level exposure analysis, industry aggregation approach, and economic\nmodeling incorporating AI adoption and labor market effects. The findings will\ninform policymakers and businesses on strategies for maximizing the benefits of\nAI while mitigating adverse disruption risks.\n"
    },
    {
        "paper_id": 2308.08808,
        "authors": "Rahmat Ullah, Rashid Aftab, Saeed Siyal, and Kashif Zaheer",
        "title": "Entrepreneurial Higher Education Education, Knowledge and Wealth\n  Creation",
        "comments": "130 pages, 10 figures",
        "journal-ref": "Riphah Institute of Public Policy, Riphah International\n  University, Islamabad 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This book presents detailed discussion on the role of higher education in\nterms of serving basic knowledge creation, teaching, and doing applied research\nfor commercialization. The book presents an historical account on how this\nchallenge was addressed earlier in education history, the cases of successful\nacademic commercialization, the marriage between basic and applied science and\nhow universities develop economies of the regions and countries. This book also\ndiscusses cultural and social challenges in research commercialization and\npathways to break the status quo.\n"
    },
    {
        "paper_id": 2308.08918,
        "authors": "Hui Niu, Siyuan Li, Jiahao Zheng, Zhouchi Lin, Jian Li, Jian Guo, Bo\n  An",
        "title": "IMM: An Imitative Reinforcement Learning Approach with Predictive\n  Representation Learning for Automatic Market Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making (MM) has attracted significant attention in financial trading\nowing to its essential function in ensuring market liquidity. With strong\ncapabilities in sequential decision-making, Reinforcement Learning (RL)\ntechnology has achieved remarkable success in quantitative trading.\nNonetheless, most existing RL-based MM methods focus on optimizing single-price\nlevel strategies which fail at frequent order cancellations and loss of queue\npriority. Strategies involving multiple price levels align better with actual\ntrading scenarios. However, given the complexity that multi-price level\nstrategies involves a comprehensive trading action space, the challenge of\neffectively training profitable RL agents for MM persists. Inspired by the\nefficient workflow of professional human market makers, we propose Imitative\nMarket Maker (IMM), a novel RL framework leveraging both knowledge from\nsuboptimal signal-based experts and direct policy interactions to develop\nmulti-price level MM strategies efficiently. The framework start with\nintroducing effective state and action representations adept at encoding\ninformation about multi-price level orders. Furthermore, IMM integrates a\nrepresentation learning unit capable of capturing both short- and long-term\nmarket trends to mitigate adverse selection risk. Subsequently, IMM formulates\nan expert strategy based on signals and trains the agent through the\nintegration of RL and imitation learning techniques, leading to efficient\nlearning. Extensive experimental results on four real-world market datasets\ndemonstrate that IMM outperforms current RL-based market making strategies in\nterms of several financial criteria. The findings of the ablation study\nsubstantiate the effectiveness of the model components.\n"
    },
    {
        "paper_id": 2308.08953,
        "authors": "Goran Durakovic, Hongyu Zhang, Brage Rugstad Knudsen, Asgeir\n  Tomasgard, Pedro Crespo del Granado",
        "title": "Decarbonizing the European energy system in the absence of Russian gas:\n  Hydrogen uptake and carbon capture developments in the power, heat and\n  industry sectors",
        "comments": "39 pages, 7 figures, submitted to the Journal of Cleaner Production",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Hydrogen and carbon capture and storage are pivotal to decarbonize the\nEuropean energy system in a broad range of pathway scenarios. Yet, their timely\nuptake in different sectors and distribution across countries are affected by\nsupply options of renewable and fossil energy sources. Here, we analyze the\ndecarbonization of the European energy system towards 2060, covering the power,\nheat, and industry sectors, and the change in use of hydrogen and carbon\ncapture and storage in these sectors upon Europe's decoupling from Russian gas.\nThe results indicate that the use of gas is significantly reduced in the power\nsector, instead being replaced by coal with carbon capture and storage, and\nwith a further expansion of renewable generators. Coal coupled with carbon\ncapture and storage is also used in the steel sector as an intermediary step\nwhen Russian gas is neglected, before being fully decarbonized with hydrogen.\nHydrogen production mostly relies on natural gas with carbon capture and\nstorage until natural gas is scarce and costly at which time green hydrogen\nproduction increases sharply. The disruption of Russian gas imports has\nsignificant consequences on the decarbonization pathways for Europe, with local\nenergy sources and carbon capture and storage becoming even more important.\n"
    },
    {
        "paper_id": 2308.08972,
        "authors": "Prathamesh Muzumdar, George Kurian, Ganga Prasad Basyal, Apoorva Muley",
        "title": "Econometrics Modelling Approach to Examine the Effect of STEM Policy\n  Changes on Asian Students Enrollment Decision in USA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Academic research has shown significant interest in international student\nmobility, with previous literature primarily focusing on the migration industry\nfrom a political and public policy perspective. For many countries,\ninternational student mobility plays a crucial role in bolstering their\neconomies through financial gains and attracting skilled immigrants. While\nprevious studies have explored the determinants of mobility and country\neconomic policies, only a few have examined the impact of policy changes on\nmobility trends. In this study, the researchers investigate the influence of\nimmigration policy changes, particularly the optional practical training (OPT)\nextension on STEM programs, on Asian students' preference for enrolling in STEM\nmajors at universities. The study utilizes observational data and employs a\nquasi-experimental design, analysing the information using the\ndifference-in-difference technique. The findings of the research indicate that\nthe implementation of the STEM extension policy in 2008 has a significant\neffect on Asian students' decisions to enroll in a STEM major. Additionally,\nthe study highlights the noteworthy role of individual factors such as the\nspecific STEM major, terminal degree pursued, and gender in influencing Asian\nstudents' enrollment decisions.\n"
    },
    {
        "paper_id": 2308.09264,
        "authors": "Kwong Yu Chong",
        "title": "Black-Litterman, Bayesian Shrinkage, and Factor Models in Portfolio\n  Selection: You Can Have It All",
        "comments": "not enough originality",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mean-variance analysis is widely used in portfolio management to identify the\nbest portfolio that makes an optimal trade-off between expected return and\nvolatility. Yet, this method has its limitations, notably its vulnerability to\nestimation errors and its reliance on historical data. While shrinkage\nestimators and factor models have been introduced to improve estimation\naccuracy through bias-variance trade-offs, and the Black-Litterman model has\nbeen developed to integrate investor opinions, a unified framework combining\nthree approaches has been lacking. Our study debuts a Bayesian blueprint that\nfuses shrinkage estimation with view inclusion, conceptualizing both as\nBayesian updates. This model is then applied within the context of the\nFama-French approach factor models, thereby integrating the advantages of each\nmethodology. Finally, through a comprehensive empirical study in the US equity\nmarket spanning a decade, we show that the model outperforms both the simple\n$1/N$ portfolio and the optimal portfolios based on sample estimators.\n"
    },
    {
        "paper_id": 2308.0948,
        "authors": "Oliver Pf\\\"auti",
        "title": "The Inflation Attention Threshold and Inflation Surges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  At the outbreak of the recent inflation surge, the public's attention to\ninflation was low but increased quickly once inflation started to rise. In this\npaper, I quantify when and by how much the public's attention to inflation\nchanges, and derive the macroeconomic implications of these attention changes.\nI estimate an attention threshold at an inflation rate of about $4%$, and that\nattention doubles when inflation exceeds this threshold. Adverse supply shocks\nbecome more inflationary in times of high attention, and the increase in\npeople's attention to inflation in 2021 accounts for half of the subsequent\nsupply-driven inflation. I develop a model accounting for the attention\nthreshold and show that shocks that are usually short lived lead to a\npersistent surge in inflation if they induce an increase in people's attention.\nThe attention threshold further lengthens the last mile of disinflation after\nan inflation surge, and leads to an asymmetry in the dynamics of inflation.\n"
    },
    {
        "paper_id": 2308.09485,
        "authors": "Valentina Semenova, Dragos Gorduza, William Wildi, Xiaowen Dong,\n  Stefan Zohren",
        "title": "Wisdom of the Crowds or Ignorance of the Masses? A data-driven guide to\n  WSB",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A trite yet fundamental question in economics is: What causes large asset\nprice fluctuations? A tenfold rise in the price of GameStop equity, between the\n22nd and 28th of January 2021, demonstrated that herding behaviour among retail\ninvestors is an important contributing factor. This paper presents a\ndata-driven guide to the forum that started the hype -- WallStreetBets (WSB).\nOur initial experiments decompose the forum using a large language topic model\nand network tools. The topic model describes the evolution of the forum over\ntime and shows the persistence of certain topics (such as the market / S\\&P500\ndiscussion), and the sporadic interest in others, such as COVID or crude oil.\nNetwork analysis allows us to decompose the landscape of retail investors into\nclusters based on their posting and discussion habits; several large,\ncorrelated asset discussion clusters emerge, surrounded by smaller, niche ones.\nA second set of experiments assesses the impact that WSB discussions have had\non the market. We show that forum activity has a Granger-causal relationship\nwith the returns of several assets, some of which are now commonly classified\nas `meme stocks', while others have gone under the radar. The paper extracts a\nset of short-term trade signals from posts and long-term (monthly and weekly)\ntrade signals from forum dynamics, and considers their predictive power at\ndifferent time horizons. In addition to the analysis, the paper presents the\ndataset, as well as an interactive dashboard, in order to promote further\nresearch.\n"
    },
    {
        "paper_id": 2308.09783,
        "authors": "Rich Ryan",
        "title": "Discretionary Extensions to Unemployment-Insurance Compensation and Some\n  Potential Costs for a McCall Worker",
        "comments": "52 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unemployment insurance provides temporary cash benefits to eligible\nunemployed workers. Benefits are sometimes extended by discretion during\neconomic slumps. In a model that features temporary benefits and sequential job\nopportunities, a worker's reservation wages are studied when policymakers can\nmake discretionary extensions to benefits. A worker's optimal labor-supply\nchoice is characterized by a sequence of reservation wages that increases with\nweeks of remaining benefits. The possibility of an extension raises the entire\nsequence of reservation wages, meaning a worker is more selective when\naccepting job offers throughout their spell of unemployment. The welfare\nconsequences of misperceiving the probability and length of an extension are\ninvestigated. Properties of the model can help policymakers interpret data on\nreservation wages, which may be important if extended benefits are used more\noften in response to economic slumps, virus pandemics, extreme heat, and\nnatural disasters.\n"
    },
    {
        "paper_id": 2308.09789,
        "authors": "Jeremy Bertomeu",
        "title": "Managers' Choice of Disclosure Complexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Aghamolla and Smith (2023) make a significant contribution to enhancing our\nunderstanding of how managers choose financial reporting complexity. I outline\nthe key assumptions and implications of the theory, and discuss two empirical\nimplications: (1) a U-shaped relationship between complexity and returns, and\n(2) a negative association between complexity and investor sophistication.\nHowever, the robust equilibrium also implies a counterfactual positive market\nresponse to complexity. I develop a simplified approach in which simple\ndisclosures indicate positive surprises, and show that this implies greater\ninvestor skepticism toward complexity and a positive association between\ninvestor sophistication and complexity. More work is needed to understand\ncomplexity as an interaction of reporting and economic transactions, rather\nthan solely as a reporting phenomenon.\n"
    },
    {
        "paper_id": 2308.09818,
        "authors": "Darren L. Linvill and Patrick L. Warren",
        "title": "Paths to Influence: How Coordinated Influence Operations Affect the\n  Prominence of Ideas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents four examples of different ways that coordinated\ninfluence operations exert pressure on the prominence of ideas on social\nnetworks. We argue that these examples illustrate the four archetypical paths\nto influence: promotion by strengthening, promotion by weakening, demotion by\nstrengthening, and demotion by weakening. We formalize this idea in a stylized\neconomic model of the optimal behavior of the influence operator and derive\nsome predictions about when we should expect each path to be followed. Finally\nwe sketch out how one might go about quantitatively estimating the key\nparameters of (a variant of) this model and how it applies much more broadly\nthan in the international political influence examples that motivate it.\n"
    },
    {
        "paper_id": 2308.09968,
        "authors": "Tim Matthies, Thomas L\\\"ohden, Stephan Leible, Jun-Patrick Raabe",
        "title": "To the Moon: Analyzing Collective Trading Events on the Wings of\n  Sentiment Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research investigates the growing trend of retail investors\nparticipating in certain stocks by organizing themselves on social media\nplatforms, particularly Reddit. Previous studies have highlighted a notable\nassociation between Reddit activity and the volatility of affected stocks. This\nstudy seeks to expand the analysis to Twitter, which is among the most\nimpactful social media platforms. To achieve this, we collected relevant tweets\nand analyzed their sentiment to explore the correlation between Twitter\nactivity, sentiment, and stock volatility. The results reveal a significant\nrelationship between Twitter activity and stock volatility but a weak link\nbetween tweet sentiment and stock performance. In general, Twitter activity and\nsentiment appear to play a less critical role in these events than Reddit\nactivity. These findings offer new theoretical insights into the impact of\nsocial media platforms on stock market dynamics, and they may practically\nassist investors and regulators in comprehending these phenomena better.\n"
    },
    {
        "paper_id": 2308.10018,
        "authors": "Miguel Puente-Ajov\\'in, Arturo Ramos, Fernando Sanz-Gracia",
        "title": "Is there a universal parametric city size distribution? Empirical\n  evidence for 70 countries",
        "comments": null,
        "journal-ref": "The Annals of Regional Science 65, 727-741 (2020)",
        "doi": "10.1007/s00168-020-01001-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the parametric description of the city size distribution (CSD) of 70\ndifferent countries (developed and developing) using seven models, as follows:\nthe lognormal (LN), the loglogistic (LL), the double Pareto lognormal (dPLN),\nthe two-lognormal (2LN), the two-loglogistic (2LL), the three-lognormal (3LN)\nand the three-loglogistic (3LL). Our results show that 3LN and 3LL are the best\ndensities in terms of non-rejections out of standard statistical tests.\nMeanwhile, according to the information criteria AIC and BIC, there is no\nsystematically dominant distribution.\n"
    },
    {
        "paper_id": 2308.10023,
        "authors": "Till Massing, Arturo Ramos",
        "title": "Student't mixture models for stock indices. A comparative study",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 580, 126143\n  (2021)",
        "doi": "10.1016/j.physa.2021.126143",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a comparative study for multiple equity indices of different\ncountries using different models to determine the best fit using the\nKolmogorov-Smirnov statistic, the Anderson-Darling statistic, the Akaike\ninformation criterion and the Bayesian information criteria as goodness-of-fit\nmeasures. We fit models both to daily and to hourly log-returns. The main\nresult is the excellent performance of a mixture of three Student's $t$\ndistributions with the numbers of degrees of freedom fixed a priori (3St). In\naddition, we find that the different components of the 3St mixture with\nsmall/moderate/high degree of freedom parameter describe the\nextreme/moderate/small log-returns of the studied equity indices.\n"
    },
    {
        "paper_id": 2308.1003,
        "authors": "Michele Campolieti, Arturo Ramos",
        "title": "The Distribution of Strike Size:Empirical Evidence from Europe and North\n  America in the 19th and 20th Centuries",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 563, 125424\n  (2021)",
        "doi": "10.1016/j.physa.2020.125424",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the distribution of strike size, which we measure as lost person\ndays, for a long period in several countries of Europe and America. When we\nconsider the full samples, the mixtures of two or three lognormals arise as\nvery convenient models. When restricting to the upper tails, the Pareto power\nlaw becomes almost indistinguishable of the truncated lognormal.\n"
    },
    {
        "paper_id": 2308.10034,
        "authors": "Till Massing, Miguel Puente-Ajov\\'in, Arturo Ramos",
        "title": "On the parametric description of log-growth rates of cities' sizes of\n  four European countries and the USA",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications, 551, 124587\n  (2020)",
        "doi": "10.1016/j.physa.2020.124587",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have studied the parametric description of the distribution of the\nlog-growth rates of the sizes of cities of France, Germany, Italy, Spain and\nthe USA. We have considered several parametric distributions well known in the\nliterature as well as some others recently introduced. There are some models\nthat provide similar excellent performance, for all studied samples. The normal\ndistribution is not the one observed empirically.\n"
    },
    {
        "paper_id": 2308.10039,
        "authors": "HyeonJun Kim",
        "title": "Do We Price Happiness? Evidence from Korean Stock Market",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the potential of internet search volume data,\nspecifically Google Trends, as an indicator for cross-sectional stock returns.\nUnlike previous studies, our research specifically investigates the search\nvolume of the topic 'happiness' and its impact on stock returns in the aspect\nof risk pricing rather than as sentiment measurement. Empirical results\nindicate that this 'happiness' search exposure (HSE) can explain future\nreturns, particularly for big and value firms. This suggests that HSE might be\na reflection of a firm's ability to produce goods or services that meet\nsocietal utility needs. Our findings have significant implications for\ninstitutional investors seeking to leverage HSE-based strategies for\noutperformance. Additionally, our research suggests that, when selected\njudiciously, some search topics on Google Trends can be related to risks that\nimpact stock prices.\n"
    },
    {
        "paper_id": 2308.10046,
        "authors": "Jean-Michel Benkert, Igor Letina and Shuo Liu",
        "title": "Startup Acquisitions: Acquihires and Talent Hoarding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how competitive forces may drive firms to inefficiently acquire\nstartup talent. In our model, two rival firms have the capacity to acquire and\nintegrate a startup operating in an orthogonal market. We show that firms may\npursue such acquihires primarily as a preemptive strategy, even when they\nappear unprofitable in isolation. Thus, acquihires, even absent traditional\ncompetition-reducing effects, need not be benign, as they can lead to\ninefficient talent allocation. Additionally, our analysis underscores that such\ntalent hoarding can diminish consumer surplus and exacerbate job volatility for\nacquihired employees.\n"
    },
    {
        "paper_id": 2308.10104,
        "authors": "Fabian Scheller, Karyn Morrissey, Karsten Neuhoff, Dogan Keles",
        "title": "Green or greedy: the relationship between perceived benefits and\n  homeowners' intention to adopt residential low-carbon technologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Transitioning to a net-zero economy requires a nuanced understanding of\nhomeowners decision-making pathways when considering the adoption of Low Carbon\nTechnologies (LCTs). These LCTs present both personal and collective benefits,\nwith positive perceptions critically influencing attitudes and intentions. Our\nstudy analyses the relationship between two primary benefits: the\nhousehold-level financial gain and the broader environmental advantage.\nFocusing on the intention to adopt Rooftop Photovoltaic Systems, Energy\nEfficient Appliances, and Green Electricity Tariffs, we employ Partial Least\nSquares Structural Equation Modeling to demonstrate that the adoption intention\nof the LCTs is underpinned by the Theory of Planned Behaviour. Attitudes toward\nthe LCTs are more strongly related to product-specific benefits than affective\nconstructs. In terms of evaluative benefits, environmental benefits exhibit a\nhigher positive association with attitude formation compared to financial\nbenefits. However, this relationship switches as homeowners move through the\ndecision process with the financial benefits of selected LCTs having a\nconsistently higher association with adoption intention. At the same time,\nfinancial benefits also positively affect attitudes. Observing this trend\nacross both low- and high-cost LCTs, we recommend that policymakers amplify\nhomeowners' recognition of the individual benefits intrinsic to LCTs and enact\nmeasures that ensure these financial benefits.\n"
    },
    {
        "paper_id": 2308.10131,
        "authors": "Kwok Ping Tsang and Zichao Yang",
        "title": "Agree to Disagree: Measuring Hidden Dissents in FOMC Meetings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Based on a record of dissents on FOMC votes and transcripts of the meetings\nfrom 1976 to 2017, we develop a deep learning model based on self-attention\nmechanism to create a measure of disagreement for members in each meeting.\nWhile dissents are rare, we find that members often have reservations with the\npolicy decision, and the level of disagreement is mostly driven by current or\npredicted macroeconomic data. Using our model to evaluate speeches made by\nmembers between meetings, we find that the informational content of speeches is\nlow if we can only compare them to speeches made by the chair. Disagreement\nstrongly correlates with data from the Summary of Economic Projections and a\nmeasure of monetary policy sub-optimality, suggesting that disagreement is\ndriven by both members' different preferences and their different views about\nthe future.\n"
    },
    {
        "paper_id": 2308.10309,
        "authors": "Lennart Ante, Friedrich-Philipp Wazinski, Aman Saggu",
        "title": "Digital Real Estate in the Metaverse: An Empirical Analysis of Retail\n  Investor Motivations",
        "comments": "27 pages, 1 figure, 4 tables, 1 appendix table",
        "journal-ref": "Finance Research Letters, Volume 58, Part A, 104299 (2023)",
        "doi": "10.1016/j.frl.2023.104299",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates retail investor motivations for digital real estate\nownership in the crypto-metaverse. Utilizing a detailed financial behavior\nsurvey of metaverse landowners' intrinsic and extrinsic motivations, we apply\nprincipal components analysis to uncover four distinct motivational groups: (1)\nAesthetics and Identity, (2) Social and Community, (3) Speculation and\nInvestment, and (4) Innovation and Technology. Our findings reveal that age,\neducation, investment knowledge, risk-taking, and impulsivity significantly\ninfluence investor group membership. This research provides valuable insights\nto investors and developers, underscoring the potential of a platform to\nattract retail investors with speculative intentions, engagement longevity, and\npassive or active trading characteristics, contingent on unique\ncrypto-metaverse attributes.\n"
    },
    {
        "paper_id": 2308.10313,
        "authors": "Fatemeh Nazari, Abolfazl Mohammadian, Thomas Stephens",
        "title": "Exploring the Role of Perceived Range Anxiety in Adoption Behavior of\n  Plug-in Electric Vehicles",
        "comments": "27 pages, 3 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A sustainable solution to negative externalities imposed by road\ntransportation is replacing internal combustion vehicles with electric vehicles\n(EVs), especially plug-in EV (PEV) encompassing plug-in hybrid EV (PHEV) and\nbattery EV (BEV). However, EV market share is still low and is forecast to\nremain low and uncertain. This shows a research need for an in-depth\nunderstanding of EV adoption behavior with a focus on one of the main barriers\nto the mass EV adoption, which is the limited electric driving range. The\npresent study extends the existing literature in two directions; First, the\ninfluence of the psychological aspect of driving range, which is referred to as\nrange anxiety, is explored on EV adoption behavior by presenting a nested logit\n(NL) model with a latent construct. Second, the two-level NL model captures\nindividuals' decision on EV adoption behavior distinguished by vehicle\ntransaction type and EV type, where the upper level yields the vehicle\ntransaction type selected from the set of alternatives including\nno-transaction, sell, trade, and add. The fuel type of the vehicles decided to\nbe acquired, either as traded-for or added vehicles, is simultaneously\ndetermined at the lower level from a set including conventional vehicle, hybrid\nEV, PHEV, and BEV. The model is empirically estimated using a stated\npreferences dataset collected in the State of California. A notable finding is\nthat anxiety about driving range influences the preference for BEV, especially\nas an added than traded-for vehicle, but not the decision on PHEV adoption.\n"
    },
    {
        "paper_id": 2308.1055,
        "authors": "Yan Dolinsky and Or Zuk",
        "title": "Explicit Computations for Delayed Semistatic Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work we consider the exponential utility maximization problem in the\nframework of semistatic hedging.\n"
    },
    {
        "paper_id": 2308.10556,
        "authors": "Kristoffer Andersson and Cornelis W. Oosterlee",
        "title": "D-TIPO: Deep time-inconsistent portfolio optimization with stocks and\n  options",
        "comments": "27 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a machine learning algorithm for time-inconsistent\nportfolio optimization. The proposed algorithm builds upon neural network based\ntrading schemes, in which the asset allocation at each time point is determined\nby a a neural network. The loss function is given by an empirical version of\nthe objective function of the portfolio optimization problem. Moreover, various\ntrading constraints are naturally fulfilled by choosing appropriate activation\nfunctions in the output layers of the neural networks. Besides this, our main\ncontribution is to add options to the portfolio of risky assets and a risk-free\nbond and using additional neural networks to determine the amount allocated\ninto the options as well as their strike prices.\n  We consider objective functions more in line with the rational preference of\nan investor than the classical mean-variance, apply realistic trading\nconstraints and model the assets with a correlated jump-diffusion SDE. With an\nincomplete market and a more involved objective function, we show that it is\nbeneficial to add options to the portfolio. Moreover, it is shown that adding\noptions leads to a more constant stock allocation with less demand for drastic\nre-allocations.\n"
    },
    {
        "paper_id": 2308.10568,
        "authors": "Juan Jose Francisco Miguelez, Cristin Buescu",
        "title": "Analytical valuation of vulnerable derivative claims with bilateral cash\n  flows under credit, funding and wrong-way risk",
        "comments": "44 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the problem of valuing and hedging a vulnerable derivative claim\nwith bilateral cash flows between two counterparties in the presence of\nasymmetric funding costs, defaults and wrong way risk (WWR). We characterize\nthe pre-default claim value as the solution to a non-linear Cauchy problem. We\nshow an explicit stochastic representation of the solution exists under a\nfunding policy which linearises the Cauchy PDE. We apply this framework to the\nvaluation of a vulnerable equity forward and show it can be represented as a\nportfolio of European options. Despite the complexity of the model, we prove\nthe forward's value admits an analytical formula involving only elementary\nfunctions and Gaussian integrals. Based on this explicit formula, numerical\nanalysis demonstrates WWR has a significant impact even under benign\nassumptions: with a parameter configuration less punitive than that\nrepresentative of Archegos AM default, we find WWR can shift values for\nvulnerable forwards by 100bps of notional, while peak exposures increase by 25%\nof notional. This framework is the first to apply to contracts with bilateral\ncash flows in the presence of credit, funding and WWR, resulting in a\nnon-linear valuation formula which admits a closed-form solution under a\nsuitable funding policy.\n"
    },
    {
        "paper_id": 2308.10974,
        "authors": "Xu Han, Zengqing Wu, Chuan Xiao",
        "title": "\"Guinea Pig Trials\" Utilizing GPT: A Novel Smart Agent-Based Modeling\n  Approach for Studying Firm Competition and Collusion",
        "comments": "Source code is available at: https://github.com/Roihn/SABM",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Firm competition and collusion involve complex dynamics, particularly when\nconsidering communication among firms. Such issues can be modeled as problems\nof complex systems, traditionally approached through experiments involving\nhuman subjects or agent-based modeling methods. We propose an innovative\nframework called Smart Agent-Based Modeling (SABM), wherein smart agents,\nsupported by GPT-4 technologies, represent firms, and interact with one\nanother. We conducted a controlled experiment to study firm price competition\nand collusion behaviors under various conditions. SABM is more cost-effective\nand flexible compared to conducting experiments with human subjects. Smart\nagents possess an extensive knowledge base for decision-making and exhibit\nhuman-like strategic abilities, surpassing traditional ABM agents. Furthermore,\nsmart agents can simulate human conversation and be personalized, making them\nideal for studying complex situations involving communication. Our results\ndemonstrate that, in the absence of communication, smart agents consistently\nreach tacit collusion, leading to prices converging at levels higher than the\nBertrand equilibrium price but lower than monopoly or cartel prices. When\ncommunication is allowed, smart agents achieve a higher-level collusion with\nprices close to cartel prices. Collusion forms more quickly with communication,\nwhile price convergence is smoother without it. These results indicate that\ncommunication enhances trust between firms, encouraging frequent small price\ndeviations to explore opportunities for a higher-level win-win situation and\nreducing the likelihood of triggering a price war. We also assigned different\npersonas to firms to analyze behavioral differences and tested variant models\nunder diverse market structures. The findings showcase the effectiveness and\nrobustness of SABM and provide intriguing insights into competition and\ncollusion.\n"
    },
    {
        "paper_id": 2308.11069,
        "authors": "Sabiou Inoua, Vernon Smith",
        "title": "Classical Economics: Lost and Found",
        "comments": null,
        "journal-ref": "The Independent Review, Volume 25, Number 1, Summer 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  \"Economists miss the boat when they act as if Arrow and Debreu's general\nequilibrium model accurately describes markets in the real world of constant\nchange. In contrast, the classical view on the market mechanism offers a\nhelpful foundation on which to add modern insights about how markets create and\ncoordinate information.\"\n"
    },
    {
        "paper_id": 2308.11138,
        "authors": "Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ri\\v{c}ardas Zitikis",
        "title": "NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n"
    },
    {
        "paper_id": 2308.11202,
        "authors": "Kapil Panda",
        "title": "Analysis of Optimal Portfolio Management Using Hierarchical Clustering",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio optimization is a task that investors use to determine the best\nallocations for their investments, and fund managers implement computational\nmodels to help guide their decisions. While one of the most common portfolio\noptimization models in the industry is the Markowitz Model, practitioners\nrecognize limitations in its framework that lead to suboptimal out-of-sample\nperformance and unrealistic allocations. In this study, I refine the Markowitz\nModel by incorporating machine learning to improve portfolio performance. By\nusing a hierarchical clustering-based approach, I am able to enhance portfolio\nperformance on a risk-adjusted basis compared to the Markowitz Model, across\nvarious market factors.\n"
    },
    {
        "paper_id": 2308.11294,
        "authors": "Xingyue Pu, Stephen Roberts, Xiaowen Dong, and Stefan Zohren",
        "title": "Network Momentum across Asset Classes",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the concept of network momentum, a novel trading signal\nderived from momentum spillover across assets. Initially observed within the\nconfines of pairwise economic and fundamental ties, such as the stock-bond\nconnection of the same company and stocks linked through supply-demand chains,\nmomentum spillover implies a propagation of momentum risk premium from one\nasset to another. The similarity of momentum risk premium, exemplified by\nco-movement patterns, has been spotted across multiple asset classes including\ncommodities, equities, bonds and currencies. However, studying the network\neffect of momentum spillover across these classes has been challenging due to a\nlack of readily available common characteristics or economic ties beyond the\ncompany level. In this paper, we explore the interconnections of momentum\nfeatures across a diverse range of 64 continuous future contracts spanning\nthese four classes. We utilise a linear and interpretable graph learning model\nwith minimal assumptions to reveal the intricacies of the momentum spillover\nnetwork. By leveraging the learned networks, we construct a network momentum\nstrategy that exhibits a Sharpe ratio of 1.5 and an annual return of 22%, after\nvolatility scaling, from 2000 to 2022. This paper pioneers the examination of\nmomentum spillover across multiple asset classes using only pricing data,\npresents a multi-asset investment strategy based on network momentum, and\nunderscores the effectiveness of this strategy through robust empirical\nanalysis.\n"
    },
    {
        "paper_id": 2308.11302,
        "authors": "Quentin Gallea",
        "title": "From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence\n  from ChatGPT and Stack Overflow",
        "comments": "17 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper illustrates how generative AI could give opportunities for big\nproductivity gains but also opens up questions about the impact of these new\npowerful technologies on the way we work and share knowledge. More\nspecifically, we explore how ChatGPT changed a fundamental aspect of coding:\nproblem-solving. To do so, we exploit the effect of the sudden release of\nChatGPT on the 30th of November 2022 on the usage of the largest online\ncommunity for coders: Stack Overflow. Using quasi-experimental methods\n(Difference-in-Difference), we find a significant drop in the number of\nquestions. In addition, the questions are better documented after the release\nof ChatGPT. Finally, we find evidence that the remaining questions are more\ncomplex. These findings suggest not only productivity gains but also a\nfundamental change in the way we work where routine inquiries are solved by AI\nallowing humans to focus on more complex tasks.\n"
    },
    {
        "paper_id": 2308.11406,
        "authors": "Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Georgii\n  Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev",
        "title": "Designing an attack-defense game: how to increase robustness of\n  financial transaction models via a competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Given the escalating risks of malicious attacks in the finance sector and the\nconsequential severe damage, a thorough understanding of adversarial strategies\nand robust defense mechanisms for machine learning models is critical. The\nthreat becomes even more severe with the increased adoption in banks more\naccurate, but potentially fragile neural networks. We aim to investigate the\ncurrent state and dynamics of adversarial attacks and defenses for neural\nnetwork models that use sequential financial data as the input.\n  To achieve this goal, we have designed a competition that allows realistic\nand detailed investigation of problems in modern financial transaction data.\nThe participants compete directly against each other, so possible attacks and\ndefenses are examined in close-to-real-life conditions. Our main contributions\nare the analysis of the competition dynamics that answers the questions on how\nimportant it is to conceal a model from malicious users, how long does it take\nto break it, and what techniques one should use to make it more robust, and\nintroduction additional way to attack models or increase their robustness.\n  Our analysis continues with a meta-study on the used approaches with their\npower, numerical experiments, and accompanied ablations studies. We show that\nthe developed attacks and defenses outperform existing alternatives from the\nliterature while being practical in terms of execution, proving the validity of\nthe competition as a tool for uncovering vulnerabilities of machine learning\nmodels and mitigating them in various domains.\n"
    },
    {
        "paper_id": 2308.11805,
        "authors": "Matthew Stuart and Cindy Yu and David A. Hennessy",
        "title": "The Impact of Stocks on Correlations between Crop Yields and Prices and\n  on Revenue Insurance Premiums using Semiparametric Quantile Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Crop yields and harvest prices are often considered to be negatively\ncorrelated, thus acting as a natural risk management hedge through stabilizing\nrevenues. Storage theory gives reason to believe that the correlation is an\nincreasing function of stocks carried over from previous years.\nStock-conditioned second moments have implications for price movements during\nshortages and for hedging needs, while spatially varying yield-price\ncorrelation structures have implications for who benefits from commodity\nsupport policies. In this paper, we propose to use semi-parametric quantile\nregression (SQR) with penalized B-splines to estimate a stock-conditioned joint\ndistribution of yield and price. The proposed method, validated through a\ncomprehensive simulation study, enables sampling from the true joint\ndistribution using SQR. Then it is applied to approximate stock-conditioned\ncorrelation and revenue insurance premium for both corn and soybeans in the\nUnited States. For both crops, Cornbelt core regions have more negative\ncorrelations than do peripheral regions. We find strong evidence that\ncorrelation becomes less negative as stocks increase. We also show that\nconditioning on stocks is important when calculating actuarially fair revenue\ninsurance premiums. In particular, revenue insurance premiums in the Cornbelt\ncore will be biased upward if the model for calculating premiums does not allow\ncorrelation to vary with stocks available. The stock-dependent correlation can\nbe viewed as a form of tail dependence that, if unacknowledged, leads to\nmispricing of revenue insurance products.\n"
    },
    {
        "paper_id": 2308.11828,
        "authors": "Emma Kroell and Sebastian Jaimungal and Silvana M. Pesenti",
        "title": "Optimal Robust Reinsurance with Multiple Insurers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a reinsurer who faces multiple sources of model uncertainty. The\nreinsurer offers contracts to $n$ insurers whose claims follow compound Poisson\nprocesses representing both idiosyncratic and systemic sources of loss. As the\nreinsurer is uncertain about the insurers' claim severity distributions and\nfrequencies, they design reinsurance contracts that maximise their expected\nwealth subject to an entropy penalty. Insurers meanwhile seek to maximise their\nexpected utility without ambiguity. We solve this continuous-time Stackelberg\ngame for general reinsurance contracts and find that the reinsurer prices under\na distortion of the barycentre of the insurers' models. We apply our results to\nproportional reinsurance and excess-of-loss reinsurance contracts, and\nillustrate the solutions numerically. Furthermore, we solve the related problem\nwhere the reinsurer maximises, still under ambiguity, their expected utility\nand compare the solutions.\n"
    },
    {
        "paper_id": 2308.11922,
        "authors": "Anuar Assamidanov",
        "title": "Discrimination and Constraints: Evidence from The Voice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Gender discrimination in the hiring process is one significant factor\ncontributing to labor market disparities. However, there is little evidence on\nthe extent to which gender bias by hiring managers is responsible for these\ndisparities. In this paper, I exploit a unique dataset of blind auditions of\nThe Voice television show as an experiment to identify own gender bias in the\nselection process. The first televised stage audition, in which four noteworthy\nrecording artists are coaches, listens to the contestants blindly (chairs\nfacing away from the stage) to avoid seeing the contestant. Using a\ndifference-in-differences estimation strategy, a coach (hiring person) is\ndemonstrably exogenous with respect to the artist's gender, I find that artists\nare 4.5 percentage points (11 percent) more likely to be selected when they are\nthe recipients of an opposite-gender coach. I also utilize the machine-learning\napproach in Athey et al. (2018) to include heterogeneity from team gender\ncomposition, order of performance, and failure rates of the coaches. The\nfindings offer a new perspective to enrich past research on gender\ndiscrimination, shedding light on the instances of gender bias variation by the\ngender of the decision maker and team gender composition.\n"
    },
    {
        "paper_id": 2308.11939,
        "authors": "Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah",
        "title": "Retail Demand Forecasting: A Comparative Study for Multivariate Time\n  Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate demand forecasting in the retail industry is a critical determinant\nof financial performance and supply chain efficiency. As global markets become\nincreasingly interconnected, businesses are turning towards advanced prediction\nmodels to gain a competitive edge. However, existing literature mostly focuses\non historical sales data and ignores the vital influence of macroeconomic\nconditions on consumer spending behavior. In this study, we bridge this gap by\nenriching time series data of customer demand with macroeconomic variables,\nsuch as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and\nunemployment rates. Leveraging this comprehensive dataset, we develop and\ncompare various regression and machine learning models to predict retail demand\naccurately.\n"
    },
    {
        "paper_id": 2308.12179,
        "authors": "Lorenzo Mercuri and Andrea Perchiazzo and Edit Rroji",
        "title": "Investigating Short-Term Dynamics in Green Bond Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper investigates the effect of the label green in bond markets from the\nlens of the trading activity. The idea is that jumps in the dynamics of returns\nhave a specific memory nature that can be well represented through a\nself-exciting process. Specifically, using Hawkes processes where the intensity\nis described through a continuous time moving average model, we study the\nhigh-frequency dynamics of bond prices. We also introduce a bivariate extension\nof the model that deals with the cross-effect of upward and downward price\nmovements. Empirical results suggest that differences emerge if we consider\nperiods with relevant interest rate announcements, especially in the case of an\nissuer operating in the energy market.\n"
    },
    {
        "paper_id": 2308.12212,
        "authors": "Xingyue Pu, Stefan Zohren, Stephen Roberts, and Xiaowen Dong",
        "title": "Learning to Learn Financial Networks for Optimising Momentum Strategies",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Network momentum provides a novel type of risk premium, which exploits the\ninterconnections among assets in a financial network to predict future returns.\nHowever, the current process of constructing financial networks relies heavily\non expensive databases and financial expertise, limiting accessibility for\nsmall-sized and academic institutions. Furthermore, the traditional approach\ntreats network construction and portfolio optimisation as separate tasks,\npotentially hindering optimal portfolio performance. To address these\nchallenges, we propose L2GMOM, an end-to-end machine learning framework that\nsimultaneously learns financial networks and optimises trading signals for\nnetwork momentum strategies. The model of L2GMOM is a neural network with a\nhighly interpretable forward propagation architecture, which is derived from\nalgorithm unrolling. The L2GMOM is flexible and can be trained with diverse\nloss functions for portfolio performance, e.g. the negative Sharpe ratio.\nBacktesting on 64 continuous future contracts demonstrates a significant\nimprovement in portfolio profitability and risk control, with a Sharpe ratio of\n1.74 across a 20-year period.\n"
    },
    {
        "paper_id": 2308.12477,
        "authors": "Melissa Dell, Jacob Carlson, Tom Bryan, Emily Silcock, Abhishek Arora,\n  Zejiang Shen, Luca D'Amico-Wong, Quan Le, Pablo Querubin, Leander Heldring",
        "title": "American Stories: A Large-Scale Structured Text Dataset of Historical\n  U.S. Newspapers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existing full text datasets of U.S. public domain newspapers do not recognize\nthe often complex layouts of newspaper scans, and as a result the digitized\ncontent scrambles texts from articles, headlines, captions, advertisements, and\nother layout regions. OCR quality can also be low. This study develops a novel,\ndeep learning pipeline for extracting full article texts from newspaper images\nand applies it to the nearly 20 million scans in Library of Congress's public\ndomain Chronicling America collection. The pipeline includes layout detection,\nlegibility classification, custom OCR, and association of article texts\nspanning multiple bounding boxes. To achieve high scalability, it is built with\nefficient architectures designed for mobile phones. The resulting American\nStories dataset provides high quality data that could be used for pre-training\na large language model to achieve better understanding of historical English\nand historical world knowledge. The dataset could also be added to the external\ndatabase of a retrieval-augmented language model to make historical information\n- ranging from interpretations of political events to minutiae about the lives\nof people's ancestors - more widely accessible. Furthermore, structured article\ntexts facilitate using transformer-based methods for popular social science\napplications like topic classification, detection of reproduced content, and\nnews story clustering. Finally, American Stories provides a massive silver\nquality dataset for innovating multimodal layout analysis models and other\nmultimodal applications.\n"
    },
    {
        "paper_id": 2308.12479,
        "authors": "Yonghong An, David Davis, Yizao Liu and Ruli Xiao",
        "title": "Procurement in welfare programs: Evidence and implications from WIC\n  infant formula contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper examines the impact of government procurement in social welfare\nprograms on consumers, manufacturers, and the government. We analyze the U.S.\ninfant formula market, where over half of the total sales are purchased by the\nWomen, Infants, and Children (WIC) program. The WIC program utilizes\nfirst-price auctions to solicit rebates from the three main formula\nmanufacturers, with the winner exclusively serving all WIC consumers in the\nwinning state. The manufacturers compete aggressively in providing rebates\nwhich account for around 85% of the wholesale price. To rationalize and\ndisentangle the factors contributing to this phenomenon, we model\nmanufacturers' retail pricing competition by incorporating two unique features:\nprice inelastic WIC consumers and government regulation on WIC brand prices.\nOur findings confirm three sizable benefits from winning the auction: a notable\nspill-over effect on non-WIC demand, a significant marginal cost reduction, and\na higher retail price for the WIC brand due to the price inelasticity of WIC\nconsumers. Our counterfactual analysis shows that procurement auctions affect\nmanufacturers asymmetrically, with the smallest manufacturer harmed the most.\nMore importantly, by switching from the current mechanism to a predetermined\nrebate procurement, the government can still contain the cost successfully,\nconsumers' surplus is greatly improved, and the smallest manufacturer benefits\nfrom the switch, promoting market competition.\n"
    },
    {
        "paper_id": 2308.12542,
        "authors": "Gautam Kumar Biswas and Faruque Ahamed",
        "title": "Financial Inclusion and Monetary Policy: A Study on the Relationship\n  between Financial Inclusion and Effectiveness of Monetary Policy in\n  Developing Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study analyzed the impact of financial inclusion on the effectiveness of\nmonetary policy in developing countries. By using a panel data set of 10\ndeveloping countries during 2004-2020, the study revealed that the financial\ninclusion measured by the number of ATM per 100,000 adults had a significant\nnegative effect on monetary policy, whereas the other measure of financial\ninclusion i.e. the number of bank accounts per 100,000 adults had a positive\nimpact on monetary policy, which is not statistically significant. The study\nalso revealed that foreign direct investment (FDI), lending rate and exchange\nrate had a positive impact on inflation, but only the effect of lending rate is\nstatistically significant. Therefore, the governments of these countries should\nmake necessary drives to increase the level of financial inclusion as it\nstabilizes the price level by reducing the inflation in the economy.\n"
    },
    {
        "paper_id": 2308.12856,
        "authors": "Marlon Moresco, M\\'elina Mailhot, Silvana M. Pesenti",
        "title": "Uncertainty Propagation and Dynamic Robust Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a framework for quantifying propagation of uncertainty arising\nin a dynamic setting. Specifically, we define dynamic uncertainty sets designed\nexplicitly for discrete stochastic processes over a finite time horizon. These\ndynamic uncertainty sets capture the uncertainty surrounding stochastic\nprocesses and models, accounting for factors such as distributional ambiguity.\nExamples of uncertainty sets include those induced by the Wasserstein distance\nand f-divergences.\n  We further define dynamic robust risk measures as the supremum of all\ncandidates' risks within the uncertainty set. In an axiomatic way, we discuss\nconditions on the uncertainty sets that lead to well-known properties of\ndynamic robust risk measures, such as convexity and coherence. Furthermore, we\ndiscuss the necessary and sufficient properties of dynamic uncertainty sets\nthat lead to time-consistencies of dynamic robust risk measures. We find that\nuncertainty sets stemming from f-divergences lead to strong time-consistency\nwhile the Wasserstein distance results in a new time-consistent notion of weak\nrecursiveness. Moreover, we show that a dynamic robust risk measure is strong\ntime-consistent or weak recursive if and only if it admits a recursive\nrepresentation of one-step conditional robust risk measures arising from static\nuncertainty sets.\n"
    },
    {
        "paper_id": 2308.13061,
        "authors": "Philipp Otto, Osman Do\\u{g}an, S\\\"uleyman Ta\\c{s}p{\\i}nar, Wolfgang\n  Schmid and Anil K. Bera",
        "title": "Spatial and Spatiotemporal Volatility Models: A Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Spatial and spatiotemporal volatility models are a class of models designed\nto capture spatial dependence in the volatility of spatial and spatiotemporal\ndata. Spatial dependence in the volatility may arise due to spatial spillovers\namong locations; that is, if two locations are in close proximity, they can\nexhibit similar volatilities. In this paper, we aim to provide a comprehensive\nreview of the recent literature on spatial and spatiotemporal volatility\nmodels. We first briefly review time series volatility models and their\nmultivariate extensions to motivate their spatial and spatiotemporal\ncounterparts. We then review various spatial and spatiotemporal volatility\nspecifications proposed in the literature along with their underlying\nmotivations and estimation strategies. Through this analysis, we effectively\ncompare all models and provide practical recommendations for their appropriate\nusage. We highlight possible extensions and conclude by outlining directions\nfor future research.\n"
    },
    {
        "paper_id": 2308.13063,
        "authors": "A. Ege Yilmaz, Stefan Stettler, Thomas Ankenbrand, Urs Rhyner",
        "title": "Grover Search for Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present explicit oracles designed to be used in Grover's algorithm to\nmatch investor preferences. Specifically, the oracles select portfolios with\nreturns and standard deviations exceeding and falling below certain thresholds,\nrespectively. One potential use case for the oracles is selecting portfolios\nwith the best Sharpe ratios. We have implemented these algorithms using quantum\nsimulators.\n"
    },
    {
        "paper_id": 2308.13153,
        "authors": "Jiayi Wen",
        "title": "Occupational Retirement and Pension Reform: The Roles of Physical and\n  Cognitive Health",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Despite increasing cognitive demands of jobs, knowledge about the role of\nhealth in retirement has centered on its physical dimensions. This paper\nestimates a dynamic programming model of retirement that incorporates multiple\nhealth dimensions, allowing differential effects on labor supply across\noccupations. Results show that the effect of cognitive health surges\nexponentially after age 65, and it explains a notable share of employment\ndeclines in cognitively demanding occupations. Under pension reforms, physical\nconstraint mainly impedes manual workers from delaying retirement, whereas\ncognitive constraint dampens the response of clerical and professional workers.\nMultidimensional health thus unevenly exacerbate welfare losses across\noccupations.\n"
    },
    {
        "paper_id": 2308.13156,
        "authors": "Jiayi Wen, Haili Huang",
        "title": "Parental Health Penalty on Adult Children's Employment: Gender\n  Difference and Long-Term Consequence",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jhealeco.2024.102886",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper examines the long-term gender-specific impacts of parental health\nshocks on adult children's employment in China. We build up an inter-temporal\ncooperative framework to analyze household work decisions in response to\nparental health deterioration. Then employing an event-study approach, we\nestablish a causal link between parental health shocks and a notable decline in\nfemale employment rates. Male employment, however, remains largely unaffected.\nThis negative impact shows no abatement up to eight years that are observable\nby the sample. These findings indicate the consequence of \"growing old before\ngetting rich\" for developing countries.\n"
    },
    {
        "paper_id": 2308.13289,
        "authors": "Sascha Frey, Kang Li, Peer Nagy, Silvia Sapora, Chris Lu, Stefan\n  Zohren, Jakob Foerster and Anisoara Calinescu",
        "title": "JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large\n  scale reinforcement learning for trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial exchanges across the world use limit order books (LOBs) to process\norders and match trades. For research purposes it is important to have large\nscale efficient simulators of LOB dynamics. LOB simulators have previously been\nimplemented in the context of agent-based models (ABMs), reinforcement learning\n(RL) environments, and generative models, processing order flows from\nhistorical data sets and hand-crafted agents alike. For many applications,\nthere is a requirement for processing multiple books, either for the\ncalibration of ABMs or for the training of RL agents. We showcase the first\nGPU-enabled LOB simulator designed to process thousands of books in parallel,\nwith a notably reduced per-message processing time. The implementation of our\nsimulator - JAX-LOB - is based on design choices that aim to best exploit the\npowers of JAX without compromising on the realism of LOB-related mechanisms. We\nintegrate JAX-LOB with other JAX packages, to provide an example of how one may\naddress an optimal execution problem with reinforcement learning, and to share\nsome preliminary results from end-to-end RL training on GPUs.\n"
    },
    {
        "paper_id": 2308.13496,
        "authors": "Tesary Lin and Avner Strulov-Shlain",
        "title": "Choice Architecture, Privacy Valuations, and Selection Bias in Consumer\n  Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how choice architecture that companies deploy during data collection\ninfluences consumers' privacy valuations. Further, we explore how this\ninfluence affects the quality of data collected, including both volume and\nrepresentativeness. To this end, we run a large-scale choice experiment to\nelicit consumers' valuation for their Facebook data while randomizing two\ncommon choice frames: default and price anchor. An opt-out default decreases\nvaluations by 14-22% compared to opt-in, while a \\$0-50 price anchor decreases\nvaluations by 37-53% compared to a \\$50-100 anchor. Moreover, in some consumer\nsegments, the susceptibility to frame influence negatively correlates with\nconsumers' average valuation. We find that conventional frame optimization\npractices that maximize the volume of data collected can have opposite effects\non its representativeness. A bias-exacerbating effect emerges when consumers'\nprivacy valuations and frame effects are negatively correlated. On the other\nhand, a volume-maximizing frame may also mitigate the bias by getting a high\npercentage of consumers into the sample data, thereby improving its coverage.\nWe demonstrate the magnitude of the volume-bias trade-off in our data and argue\nthat it should be a decision-making factor in choice architecture design.\n"
    },
    {
        "paper_id": 2308.13642,
        "authors": "Naman S, Gaurang B, Neel S, and Aswath Babu H",
        "title": "The Potential of Quantum Techniques for Stock Price Prediction",
        "comments": "RASSE2023 IEEE",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explored the potential applications of various Quantum Algorithms for\nstock price prediction by conducting a series of experimental simulations using\nboth Classical as well as Quantum Hardware. Firstly, we extracted various stock\nprice indicators, such as Moving Averages (MA), Average True Range (ATR), and\nAroon, to gain insights into market trends and stock price movements. Next, we\nemployed Quantum Annealing (QA) for feature selection and Principal Component\nAnalysis (PCA) for dimensionality reduction. Further, we transformed the stock\nprice prediction task essentially into a classification problem. We trained the\nQuantum Support Vector Machine (QSVM) to predict price movements (whether up or\ndown) contrasted their performance with classical models and analyzed their\naccuracy on a dataset formulated using Quantum Annealing and PCA individually.\nWe focused on the stock price prediction and binary classification of stock\nprices for four different companies, namely Apple, Visa, Johnson and Jonson,\nand Honeywell. We primarily used the real-time stock data of the raw stock\nprices of these companies. We compared various Quantum Computing techniques\nwith their classical counterparts in terms of accuracy and F-score of the\nprediction model. Through these experimental simulations, we shed light on the\npotential advantages and limitations of Quantum Algorithms in stock price\nprediction and contribute to the growing body of knowledge at the intersection\nof Quantum Computing and Finance.\n"
    },
    {
        "paper_id": 2308.13717,
        "authors": "Ricardo T. Fernholz and Robert Fernholz",
        "title": "Portfolios Generated by Contingent Claim Functions, with Applications to\n  Option Pricing",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is a synthesis of the theories of portfolio generating functions\nand rational option pricing. For a family of n >= 2 assets with prices\nrepresented by strictly positive continuous semimartingales, a contingent claim\nfunction is a scalable positive C^{2,1} function of the asset prices and time.\nWe extend the theory of portfolio generation to measure the value of portfolios\ngenerated by contingent claim functions directly, with no numeraire portfolio.\nWe show that if a contingent claim function satisfies a particular parabolic\ndifferential equation, then the value of the portfolio generated by that\ncontingent claim function will replicate the value of the function. This\ndifferential equation is a general form of the Black-Scholes equation.\n"
    },
    {
        "paper_id": 2308.1385,
        "authors": "Yunfei Peng and Wei Wei",
        "title": "Solutions to Equilibrium HJB Equations for Time-Inconsistent\n  Deterministic Linear Quadratic Control: Characterization and Uniqueness",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we study a class of HJB equations which solve for equilibria\nfor general time-inconsistent deterministic linear quadratic control problems\nwithin the intra-personal game theoretic framework, where the inconsistency\narises from non-exponential discount functions. We characterize the solutions\nto the HJB equations using a class of Riccati equations with integral terms. By\nstudying the uniqueness of solutions to the integro-differential Riccati\nequations, we prove the uniqueness of solutions to the equilibrium HJB\nequations.\n"
    },
    {
        "paper_id": 2308.13881,
        "authors": "Wenpin Tang and David D. Yao",
        "title": "Transaction fee mechanism for Proof-of-Stake protocol",
        "comments": "18 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a mechanism design problem in the blockchain proof-of-stake (PoS)\nprotocol. Our main objective is to extend the transaction fee mechanism (TFM)\nrecently proposed in Chung and Shi (SODA, p.3856-3899, 2023), so as to\nincorporate a long-run utility model for the miner into the burning\nsecond-price auction mechanism $\\texttt{BSP}(\\gamma)$ proposed in Chung and Shi\n(where $\\gamma$ is a key parameter in the strict $\\gamma$-utility model that is\napplied to both miners and users). First, we derive an explicit functional form\nfor the long-run utility of the miner using a martingale approach, and reveal a\ncritical discontinuity of the utility function, namely a small deviation from\nbeing truthful will yield a discrete jump (up or down) in the miner's utility.\nWe show that because of this discontinuity the $\\texttt{BSP}(\\gamma)$ mechanism\nwill fail a key desired property in TFM, $c$-side contract proofness ($c$-SCP).\nAs a remedy, we introduce another parameter $\\theta$, and propose a new\n$\\texttt{BSP}(\\theta)$ mechanism, and prove that it satisfies all three desired\nproperties of TFM: user- and miner-incentive compatibility (UIC and MIC) as\nwell as $c$-SCP, provided the parameter $\\theta$ falls into a specific range,\nalong with a proper tick size imposed on user bids.\n"
    },
    {
        "paper_id": 2308.14215,
        "authors": "Sushrut Ghimire",
        "title": "TimeTrail: Unveiling Financial Fraud Patterns through Temporal\n  Correlation Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the field of financial fraud detection, understanding the underlying\npatterns and dynamics is important to ensure effective and reliable systems.\nThis research introduces a new technique, \"TimeTrail,\" which employs advanced\ntemporal correlation analysis to explain complex financial fraud patterns. The\ntechnique leverages time-related insights to provide transparent and\ninterpretable explanations for fraud detection decisions, enhancing\naccountability and trust.\n  The \"TimeTrail\" methodology consists of three key phases: temporal data\nenrichment, dynamic correlation analysis, and interpretable pattern\nvisualization. Initially, raw financial transaction data is enriched with\ntemporal attributes. Dynamic correlations between these attributes are then\nquantified using innovative statistical measures. Finally, a unified\nvisualization framework presents these correlations in an interpretable manner.\nTo validate the effectiveness of \"TimeTrail,\" a study is conducted on a diverse\nfinancial dataset, surrounding various fraud scenarios. Results demonstrate the\ntechnique's capability to uncover hidden temporal correlations and patterns,\nperforming better than conventional methods in both accuracy and\ninterpretability. Moreover, a case study showcasing the application of\n\"TimeTrail\" in real-world scenarios highlights its utility for fraud detection.\n"
    },
    {
        "paper_id": 2308.14235,
        "authors": "Haochen Li, Yi Cao, Maria Polukarov, Carmine Ventre",
        "title": "An Empirical Analysis on Financial Markets: Insights from the\n  Application of Statistical Physics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we introduce a physical model inspired by statistical physics\nfor predicting price volatility and expected returns by leveraging Level 3\norder book data. By drawing parallels between orders in the limit order book\nand particles in a physical system, we establish unique measures for the\nsystem's kinetic energy and momentum as a way to comprehend and evaluate the\nstate of limit order book. Our model goes beyond examining merely the top\nlayers of the order book by introducing the concept of 'active depth', a\ncomputationally-efficient approach for identifying order book levels that have\nimpact on price dynamics. We empirically demonstrate that our model outperforms\nthe benchmarks of traditional approaches and machine learning algorithm. Our\nmodel provides a nuanced comprehension of market microstructure and produces\nmore accurate forecasts on volatility and expected returns. By incorporating\nprinciples of statistical physics, this research offers valuable insights on\nunderstanding the behaviours of market participants and order book dynamics.\n"
    },
    {
        "paper_id": 2308.14473,
        "authors": "Benjamin Joseph, Gregoire Loeper, Jan Obloj",
        "title": "Joint Calibration of Local Volatility Models with Stochastic Interest\n  Rates using Semimartingale Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop and implement a non-parametric method for joint exact calibration\nof a local volatility model and a correlated stochastic short rate model using\nsemimartingale optimal transport. The method relies on the duality results\nestablished in Joseph, Loeper, and Obloj, 2023 and jointly calibrates the whole\nequity-rate dynamics. It uses an iterative approach which starts with a\nparametric model and tries to stay close to it, until a perfect calibration is\nobtained. We demonstrate the performance of our approach on market data using\nEuropean SPX options and European cap interest rate options. Finally, we\ncompare the joint calibration approach with the sequential calibration, in\nwhich the short rate model is calibrated first and frozen.\n"
    },
    {
        "paper_id": 2308.14487,
        "authors": "Daniel Bussell, Camilo Andr\\'es Garc\\'ia-Trillos",
        "title": "Deep multi-step mixed algorithm for high dimensional non-linear PDEs and\n  associated BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new multistep deep learning-based algorithm for the resolution\nof moderate to high dimensional nonlinear backward stochastic differential\nequations (BSDEs) and their corresponding parabolic partial differential\nequations (PDE). Our algorithm relies on the iterated time discretisation of\nthe BSDE and approximates its solution and gradient using deep neural networks\nand automatic differentiation at each time step. The approximations are\nobtained by sequential minimisation of local quadratic loss functions at each\ntime step through stochastic gradient descent. We provide an analysis of\napproximation error in the case of a network architecture with weight\nconstraints requiring only low regularity conditions on the generator of the\nBSDE. The algorithm increases accuracy from its single step parent model and\nhas reduced complexity when compared to similar models in the literature.\n"
    },
    {
        "paper_id": 2308.14634,
        "authors": "Lefteris Loukas, Ilias Stogiannidis, Prodromos Malakasiotis, Stavros\n  Vassos",
        "title": "Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance",
        "comments": "Early pre-print; Accepted at the 5th FinNLP workshop @ IJCAI-2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose the use of conversational GPT models for easy and quick few-shot\ntext classification in the financial domain using the Banking77 dataset. Our\napproach involves in-context learning with GPT-3.5 and GPT-4, which minimizes\nthe technical expertise required and eliminates the need for expensive GPU\ncomputing while yielding quick and accurate results. Additionally, we fine-tune\nother pre-trained, masked language models with SetFit, a recent contrastive\nlearning technique, to achieve state-of-the-art results both in full-data and\nfew-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can\noutperform fine-tuned, non-generative models even with fewer examples. However,\nsubscription fees associated with these solutions may be considered costly for\nsmall organizations. Lastly, we find that generative models perform better on\nthe given task when shown representative samples selected by a human expert\nrather than when shown random ones. We conclude that a) our proposed methods\noffer a practical solution for few-shot tasks in datasets with limited label\navailability, and b) our state-of-the-art results can inspire future work in\nthe area.\n"
    },
    {
        "paper_id": 2308.14703,
        "authors": "Caterina Calsamiglia, Laura Doval, Alejandro Robinson-Cort\\'es,\n  Matthew Shum",
        "title": "Managing Congestion in Two-Sided Platforms: The Case of Online Rentals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thick two-sided matching platforms, such as the room-rental market, face the\nchallenge of showing relevant objects to users to reduce search costs. Many\nplatforms use ranking algorithms to determine the order in which alternatives\nare shown to users. Ranking algorithms may depend on simple criteria, such as\nhow long a listing has been on the platform, or incorporate more sophisticated\naspects, such as personalized inferences about users' preferences. Using rich\ndata on a room rental platform, we show how ranking algorithms can be a source\nof unnecessary congestion, especially when the ranking is invariant across\nusers. Invariant rankings induce users to view, click, and request the same\nrooms in the platform we study, greatly limiting the number of matches it\ncreates. We estimate preferences and simulate counterfactuals under different\nranking algorithms varying the degree of user personalization and variation\nacross users. In our case, increased personalization raises both user match\nutility and congestion, which leads to a trade-off. We find that the current\noutcome is inefficient as it lies below the possibility frontier, and propose\nalternatives that improve upon it.\n"
    },
    {
        "paper_id": 2308.14734,
        "authors": "Yifeng Philip Chen, Edward J. Oughton, Jakub Zagdanski, Maggie Mo Jia,\n  Peter Tyler",
        "title": "Crowdsourced data indicates broadband has a positive impact on local\n  business creation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.tele.2023.102035",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Broadband connectivity is regarded as generally having a positive\nmacroeconomic effect, but we lack evidence as to how it affects key economic\nactivity metrics, such as firm creation, at a very local level. This analysis\nmodels the impact of broadband Next Generation Access (NGA) on new business\ncreation at the local level over the 2011-2015 period in England, United\nKingdom, using high-resolution panel data. After controlling for a range of\nfactors, we find that faster broadband speeds brought by NGA technologies have\na positive effect on the rate of business growth. We find that in England\nbetween 2011-2015, on average a one percentage increase in download speeds is\nassociated with a 0.0574 percentage point increase in the annual growth rate of\nbusiness establishments. The primary hypothesised mechanism behind the\nestimated relationship is the enabling effect that faster broadband speeds have\non innovative business models based on new digital technologies and services.\nEntrepreneurs either sought appropriate locations that offer high quality\nbroadband infrastructure (contributing to new business establishment growth),\nor potentially enjoyed a competitive advantage (resulting in a higher survival\nrate). The findings of this study suggest that aspiring to reach universal high\ncapacity broadband connectivity is economically desirable, especially as the\ncosts of delivering such service decline.\n"
    },
    {
        "paper_id": 2308.14982,
        "authors": "B. N. Kausik",
        "title": "Cognitive Aging and Labor Share",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Labor share, the fraction of economic output accrued as wages, is\ninexplicably declining in industrialized countries. Whilst numerous prior works\nattempt to explain the decline via economic factors, our novel approach links\nthe decline to biological factors. Specifically, we propose a theoretical\nmacroeconomic model where labor share reflects a dynamic equilibrium between\nthe workforce automating existing outputs, and consumers demanding new output\nvariants that require human labor. Industrialization leads to an aging\npopulation, and while cognitive performance is stable in the working years it\ndrops sharply thereafter. Consequently, the declining cognitive performance of\naging consumers reduces the demand for new output variants, leading to a\ndecline in labor share. Our model expresses labor share as an algebraic\nfunction of median age, and is validated with surprising accuracy on historical\ndata across industrialized economies via non-linear stochastic regression.\n"
    },
    {
        "paper_id": 2308.15048,
        "authors": "Chonghu Guan, Zuo Quan Xu",
        "title": "Optimal ratcheting of dividend payout under Brownian motion surplus",
        "comments": "To appear in SICON",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with a long standing optimal dividend payout problem\nsubject to the so-called ratcheting constraint, that is, the dividend payout\nrate shall be non-decreasing over time and is thus self-path-dependent. The\nsurplus process is modeled by a drifted Brownian motion process and the aim is\nto find the optimal dividend ratcheting strategy to maximize the expectation of\nthe total discounted dividend payouts until the ruin time. Due to the\nself-path-dependent control constraint, the standard control theory cannot be\ndirectly applied to tackle the problem. The related Hamilton-Jacobi-Bellman\n(HJB) equation is a new type of variational inequality. In the literature, it\nis only shown to have a viscosity solution, which is not strong enough to\nguarantee the existence of an optimal dividend ratcheting strategy. This paper\nproposes a novel partial differential equation method to study the HJB\nequation. We not only prove the the existence and uniqueness of the solution in\nsome stronger functional space, but also prove the strict monotonicity,\nboundedness, and $C^\\infty$-smoothness of the dividend ratcheting free\nboundary. Based on these results, we eventually derive an optimal dividend\nratcheting strategy, and thus solve the open problem completely. Economically\nspeaking, we find that if the surplus volatility is above an explicit\nthreshold, then one should pay dividends at the maximum rate, regardless the\nsurplus level. Otherwise, by contrast, the optimal dividend ratcheting strategy\nrelays on the surplus level and one should only ratchet up the dividend payout\nrate when the surplus level touches the dividend ratcheting free boundary.\nMoreover, our numerical results suggest that one should invest into those\ncompanies with stable dividend payout strategies since their income rates\nshould be higher and volatility rates smaller.\n"
    },
    {
        "paper_id": 2308.15135,
        "authors": "Owen Futter, Blanka Horvath, Magnus Wiese",
        "title": "Signature Trading: A Path-Dependent Extension of the Mean-Variance\n  Framework with Exogenous Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we introduce a portfolio optimisation framework, in which the\nuse of rough path signatures (Lyons, 1998) provides a novel method of\nincorporating path-dependencies in the joint signal-asset dynamics, naturally\nextending traditional factor models, while keeping the resulting formulas\nlightweight and easily interpretable. We achieve this by representing a trading\nstrategy as a linear functional applied to the signature of a path (which we\nrefer to as \"Signature Trading\" or \"Sig-Trading\"). This allows the modeller to\nefficiently encode the evolution of past time-series observations into the\noptimisation problem. In particular, we derive a concise formulation of the\ndynamic mean-variance criterion alongside an explicit solution in our setting,\nwhich naturally incorporates a drawdown control in the optimal strategy over a\nfinite time horizon. Secondly, we draw parallels between classical portfolio\nstategies and Sig-Trading strategies and explain how the latter leads to a\npathwise extension of the classical setting via the \"Signature Efficient\nFrontier\". Finally, we give examples when trading under an exogenous signal as\nwell as examples for momentum and pair-trading strategies, demonstrated both on\nsynthetic and market data. Our framework combines the best of both worlds\nbetween classical theory (whose appeal lies in clear and concise formulae) and\nbetween modern, flexible data-driven methods that can handle more realistic\ndatasets. The advantage of the added flexibility of the latter is that one can\nbypass common issues such as the accumulation of heteroskedastic and asymmetric\nresiduals during the optimisation phase. Overall, Sig-Trading combines the\nflexibility of data-driven methods without compromising on the clarity of the\nclassical theory and our presented results provide a compelling toolbox that\nyields superior results for a large class of trading strategies.\n"
    },
    {
        "paper_id": 2308.15341,
        "authors": "Elisa Al\\`os, Eulalia Nualart, Makar Pravosud",
        "title": "On the implied volatility of European and Asian call options under the\n  stochastic volatility Bachelier model",
        "comments": "arXiv admin note: text overlap with arXiv:2208.01353",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study the short-time behavior of the at-the-money implied\nvolatility for European and arithmetic Asian call options with fixed strike\nprice. The asset price is assumed to follow the Bachelier model with a general\nstochastic volatility process. Using techniques of the Malliavin calculus such\nas the anticipating It\\^o's formula we first compute the level of the implied\nvolatility when the maturity converges to zero. Then, we find a short maturity\nasymptotic formula for the skew of the implied volatility that depends on the\nroughness of the volatility model. We apply our general results to the SABR and\nfractional Bergomi models, and provide some numerical simulations that confirm\nthe accurateness of the asymptotic formula for the skew.\n"
    },
    {
        "paper_id": 2308.15384,
        "authors": "Elliot Beck, Damian Kozbur, Michael Wolf",
        "title": "Hedging Forecast Combinations With an Application to the Random Forest",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This papers proposes a generic, high-level methodology for generating\nforecast combinations that would deliver the optimal linearly combined forecast\nin terms of the mean-squared forecast error if one had access to two population\nquantities: the mean vector and the covariance matrix of the vector of\nindividual forecast errors. We point out that this problem is identical to a\nmean-variance portfolio construction problem, in which portfolio weights\ncorrespond to forecast combination weights. We allow negative forecast weights\nand interpret such weights as hedging over and under estimation risks across\nestimators. This interpretation follows directly as an implication of the\nportfolio analogy. We demonstrate our method's improved out-of-sample\nperformance relative to standard methods in combining tree forecasts to form\nweighted random forests in 14 data sets.\n"
    },
    {
        "paper_id": 2308.15443,
        "authors": "Weronika Nitka and Rafa{\\l} Weron",
        "title": "Combining predictive distributions of electricity prices: Does\n  minimizing the CRPS lead to optimal decisions in day-ahead bidding?",
        "comments": "12 pages, 7 figures, 2 tables. Submitted to Operations Research and\n  Decisions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Probabilistic price forecasting has recently gained attention in power\ntrading because decisions based on such predictions can yield significantly\nhigher profits than those made with point forecasts alone. At the same time,\nmethods are being developed to combine predictive distributions, since no model\nis perfect and averaging generally improves forecasting performance. In this\narticle we address the question of whether using CRPS learning, a novel\nweighting technique minimizing the continuous ranked probability score (CRPS),\nleads to optimal decisions in day-ahead bidding. To this end, we conduct an\nempirical study using hourly day-ahead electricity prices from the German EPEX\nmarket. We find that increasing the diversity of an ensemble can have a\npositive impact on accuracy. At the same time, the higher computational cost of\nusing CRPS learning compared to an equal-weighted aggregation of distributions\nis not offset by higher profits, despite significantly more accurate\npredictions.\n"
    },
    {
        "paper_id": 2308.15451,
        "authors": "Jon Atwell and Marlon Twyman II",
        "title": "Metawisdom of the Crowd: How Choice Within Aided Decision Making Can\n  Make Crowd Wisdom Robust",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quality information can improve individual judgments but nonetheless fail to\nmake group decisions more accurate; if individuals choose to attend to the same\ninformation in the same way, the predictive diversity that enables crowd wisdom\nmay be lost. Decision support systems, from business intelligence software to\npublic search engines, present individuals with decision aids -- discrete\npresentations of relevant information, interpretative frames, or heuristics --\nto enhance the quality and speed of decision making, but have the potential to\nbias judgments through the selective presentation of information and\ninterpretative frames. We redescribe the wisdom of the crowd as often having\ntwo decisions, the choice of decision aids and then the primary decision. We\nthen define \\emph{metawisdom of the crowd} as any pattern by which the\ncollective choice of aids leads to higher crowd accuracy than randomized\nassignment to the same aids, a comparison that accounts for the information\ncontent of the aids. While choice is ultimately constrained by the setting, in\ntwo experiments -- the prediction of inflation (N=947, pre-registered) and a\ntightly controlled estimation game (N=1198) -- we find strong evidence of\nmetawisdom. It comes about through diverse errors arising through the use of\ndiverse aids, not through widespread use of the aids that induce the most\naccurate estimates. Thus the microfoundations of crowd wisdom appear in the\nfirst choice, suggesting crowd wisdom can be robust in information choice\nproblems. Given the implications for collective decision making, more research\non the nature and use of decision aids is needed.\n"
    },
    {
        "paper_id": 2308.15661,
        "authors": "Thisari K. Mahanama, Abootaleb Shirvani, Svetlozar Rachev, Frank J.\n  Fabozzi",
        "title": "The Financial Market of Environmental Indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces the concept of a global financial market for\nenvironmental indices, addressing sustainability concerns and aiming to attract\ninstitutional investors. Risk mitigation measures are implemented to manage\ninherent risks associated with investments in this new financial market. We\nmonetize the environmental indices using quantitative measures and construct\ncountry-specific environmental indices, enabling them to be viewed as\ndollar-denominated assets. Our primary goal is to encourage the active\nengagement of institutional investors in portfolio analysis and trading within\nthis emerging financial market. To evaluate and manage investment risks, our\napproach incorporates financial econometric theory and dynamic asset pricing\ntools. We provide an econometric analysis that reveals the relationships\nbetween environmental and economic indicators in this market. Additionally, we\nderive financial put options as insurance instruments that can be employed to\nmanage investment risks. Our factor analysis identifies key drivers in the\nglobal financial market for environmental indices. To further evaluate the\nmarket's performance, we employ pricing options, efficient frontier analysis,\nand regression analysis. These tools help us assess the efficiency and\neffectiveness of the market. Overall, our research contributes to the\nunderstanding and development of the global financial market for environmental\nindices.\n"
    },
    {
        "paper_id": 2308.15672,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Asymptotics for Short Maturity Asian Options in Jump-Diffusion models\n  with Local Volatility",
        "comments": "28 pages, 3 figures, 5 tables",
        "journal-ref": "Quantitative Finance 2024, Vol. 24, Nos. 3-4, 433-449",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a study of the short maturity asymptotics for Asian options in a\njump-diffusion model with a local volatility component, where the jumps are\nmodeled as a compound Poisson process. The analysis for out-of-the-money Asian\noptions is extended to models with L\\'evy jumps, including the exponential\nL\\'{e}vy model as a special case. Both fixed and floating strike Asian options\nare considered. Explicit results are obtained for the first-order asymptotics\nof the Asian options prices for a few popular models in the literature: the\nMerton jump-diffusion model, the double-exponential jump model, and the\nVariance Gamma model. We propose an analytical approximation for Asian option\nprices which satisfies the constraints from the short-maturity asymptotics, and\ntest it against Monte Carlo simulations. The asymptotic results are in good\nagreement with numerical simulations for sufficiently small maturity.\n"
    },
    {
        "paper_id": 2308.15769,
        "authors": "Cameron Cornell, Lewis Mitchell and Matthew Roughan",
        "title": "Vector Autoregression in Cryptocurrency Markets: Unraveling Complex\n  Causal Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Methodologies to infer financial networks from the price series of\nspeculative assets vary, however, they generally involve bivariate or\nmultivariate predictive modelling to reveal causal and correlational structures\nwithin the time series data. The required model complexity intimately relates\nto the underlying market efficiency, where one expects a highly developed and\nefficient market to display very few simple relationships in price data. This\nhas spurred research into the applications of complex nonlinear models for\ndeveloped markets. However, it remains unclear if simple models can provide\nmeaningful and insightful descriptions of the dependency and interconnectedness\nof the rapidly developed cryptocurrency market. Here we show that multivariate\nlinear models can create informative cryptocurrency networks that reflect\neconomic intuition, and demonstrate the importance of high-influence nodes. The\nresulting network confirms that node degree, a measure of influence, is\nsignificantly correlated to the market capitalisation of each coin\n($\\rho=0.193$). However, there remains a proportion of nodes whose influence\nextends beyond what their market capitalisation would imply. We demonstrate\nthat simple linear model structure reveals an inherent complexity associated\nwith the interconnected nature of the data, supporting the use of multivariate\nmodelling to prevent surrogate effects and achieve accurate causal\nrepresentation. In a reductive experiment we show that most of the network\nstructure is contained within a small portion of the network, consistent with\nthe Pareto principle, whereby a fraction of the inputs generates a large\nproportion of the effects. Our results demonstrate that simple multivariate\nmodels provide nontrivial information about cryptocurrency market dynamics, and\nthat these dynamics largely depend upon a few key high-influence coins.\n"
    },
    {
        "paper_id": 2308.16054,
        "authors": "Kurada T S S Satyanarayana, Addada Narasimha Rao and Kumpatla jaya\n  surya",
        "title": "Capital Structure Dynamics and Financial Performance in Indian Banks (An\n  Analysis of Mergers and Acquisitions)",
        "comments": "12 pages, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research investigates the multifaceted relationship underlying capital\nstructure dynamics along with financial performance as a result of mergers and\nacquisitions, or M&As, in Indian banks. In the face of increasing competition,\nbanks have deliberately embraced M&A as a strategy of improving commercial\nprospects and maintaining financial stability. The primary goal of this study\nis to examine the changes in the capital framework and financial results of\nbanks before and after M&A transactions. The investigation, which employs a\npaired t-test as a method of statistical analysis, is based on a review of\nannual reports from selected banks over a two-year period before and after M&A\ntransactions. The paired t-test approach allows for a thorough statistical\nanalysis of interconnected datasets, revealing the subtle influence of M&A\nattempts on both bank financial performance as well as capital structure\ndynamics. The study's findings have the potential to add to the current body of\nknowledge on organisational planning, managing finances, and capital structure\noptimisation. The research has practical significance for financial companies,\nlegislators, and scholars interested in understanding the profound effects of\nM&A inside the arena of financial institutions that operate within fiercely\ncompetitive landscapes because it provides comprehensive insights regarding the\ncomplex consequences of banking merger and acquisition (M&A) deals on capital\nstructure as well as financial performance. Finally, the goal of this research\nis to provide the banking sector with educated decision-making capabilities and\nstrategic guidance to businesses facing heightened competition while coping\nwith the complexities of capital structure.\n"
    },
    {
        "paper_id": 2308.16256,
        "authors": "Chester Bella, Danny Boahen, and Sudeep Biswas",
        "title": "A new adaptive pricing framework for perpetual protocols using liquidity\n  curves and on-chain oracles",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This whitepaper introduces an innovative mechanism for pricing perpetual\ncontracts and quoting fees to traders based on current market conditions. The\napproach employs liquidity curves and on-chain oracles to establish a new\nadaptive pricing framework that considers various factors, ensuring pricing\nstability and predictability. The framework utilizes parabolic and sigmoid\nfunctions to quote prices and fees, accounting for liquidity, active long and\nshort positions, and utilization. This whitepaper provides a detailed\nexplanation of how the adaptive pricing framework, in conjunction with\nliquidity curves, operates through mathematical modeling and compares it to\nexisting solutions. Furthermore, we explore additional features that enhance\nthe overall efficiency of the decentralized perpetual protocol.\n"
    },
    {
        "paper_id": 2308.16391,
        "authors": "Phuong Duy Huynh, Son Hoang Dau, Xiaodong Li, Phuc Luong, Emanuele\n  Viterbo",
        "title": "Improving the Accuracy of Transaction-Based Ponzi Detection on Ethereum",
        "comments": "17 pages, 9 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum\nblockchain, causing considerable financial losses to many crypto investors. A\nfew Ponzi detection methods have been proposed in the literature, most of which\ndetect a Ponzi scheme based on its smart contract source code. This\ncontract-code-based approach, while achieving very high accuracy, is not robust\nbecause a Ponzi developer can fool a detection model by obfuscating the opcode\nor inventing a new profit distribution logic that cannot be detected. On the\ncontrary, a transaction-based approach could improve the robustness of\ndetection because transactions, unlike smart contracts, are harder to be\nmanipulated. However, the current transaction-based detection models achieve\nfairly low accuracy. In this paper, we aim to improve the accuracy of the\ntransaction-based models by employing time-series features, which turn out to\nbe crucial in capturing the life-time behaviour a Ponzi application but were\ncompletely overlooked in previous works. We propose a new set of 85 features\n(22 known account-based and 63 new time-series features), which allows\noff-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores\ncompared to existing works.\n"
    },
    {
        "paper_id": 2308.16771,
        "authors": "Rick Steinert, Saskia Altmann",
        "title": "Linking microblogging sentiments to stock price movement: An application\n  of GPT-4",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the potential improvement of the GPT-4 Language\nLearning Model (LLM) in comparison to BERT for modeling same-day daily stock\nprice movements of Apple and Tesla in 2017, based on sentiment analysis of\nmicroblogging messages. We recorded daily adjusted closing prices and\ntranslated them into up-down movements. Sentiment for each day was extracted\nfrom messages on the Stocktwits platform using both LLMs. We develop a novel\nmethod to engineer a comprehensive prompt for contextual sentiment analysis\nwhich unlocks the true capabilities of modern LLM. This enables us to carefully\nretrieve sentiments, perceived advantages or disadvantages, and the relevance\ntowards the analyzed company. Logistic regression is used to evaluate whether\nthe extracted message contents reflect stock price movements. As a result,\nGPT-4 exhibited substantial accuracy, outperforming BERT in five out of six\nmonths and substantially exceeding a naive buy-and-hold strategy, reaching a\npeak accuracy of 71.47 % in May. The study also highlights the importance of\nprompt engineering in obtaining desired outputs from GPT-4's contextual\nabilities. However, the costs of deploying GPT-4 and the need for fine-tuning\nprompts highlight some practical considerations for its use.\n"
    },
    {
        "paper_id": 2309.00025,
        "authors": "Aleksy Leeuwenkamp and Wentao Hu",
        "title": "New general dependence measures: construction, estimation and\n  application to high-frequency stock returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a set of dependence measures that are non-linear, local, invariant\nto a wide range of transformations on the marginals, can show tail and risk\nasymmetries, are always well-defined, are easy to estimate and can be used on\nany dataset. We propose a nonparametric estimator and prove its consistency and\nasymptotic normality. Thereby we significantly improve on existing (extreme)\ndependence measures used in asset pricing and statistics. To show practical\nutility, we use these measures on high-frequency stock return data around\nmarket distress events such as the 2010 Flash Crash and during the GFC.\nContrary to ubiquitously used correlations we find that our measures clearly\nshow tail asymmetry, non-linearity, lack of diversification and endogenous\nbuildup of risks present during these distress events. Additionally, our\nmeasures anticipate large (joint) losses during the Flash Crash while also\nanticipating the bounce back and flagging the subsequent market fragility. Our\nfindings have implications for risk management, portfolio construction and\nhedging at any frequency.\n"
    },
    {
        "paper_id": 2309.00073,
        "authors": "Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng and Tat-Seng Chua",
        "title": "Diffusion Variational Autoencoder for Tackling Stochasticity in\n  Multi-Step Regression Stock Price Prediction",
        "comments": "CIKM 2023",
        "journal-ref": null,
        "doi": "10.1145/3583780.3614844",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Multi-step stock price prediction over a long-term horizon is crucial for\nforecasting its volatility, allowing financial institutions to price and hedge\nderivatives, and banks to quantify the risk in their trading books.\nAdditionally, most financial regulators also require a liquidity horizon of\nseveral days for institutional investors to exit their risky assets, in order\nto not materially affect market prices. However, the task of multi-step stock\nprice prediction is challenging, given the highly stochastic nature of stock\ndata. Current solutions to tackle this problem are mostly designed for\nsingle-step, classification-based predictions, and are limited to low\nrepresentation expressiveness. The problem also gets progressively harder with\nthe introduction of the target price sequence, which also contains stochastic\nnoise and reduces generalizability at test-time. To tackle these issues, we\ncombine a deep hierarchical variational-autoencoder (VAE) and diffusion\nprobabilistic techniques to do seq2seq stock prediction through a stochastic\ngenerative process. The hierarchical VAE allows us to learn the complex and\nlow-level latent variables for stock prediction, while the diffusion\nprobabilistic model trains the predictor to handle stock price stochasticity by\nprogressively adding random noise to the stock data. Our Diffusion-VAE (D-Va)\nmodel is shown to outperform state-of-the-art solutions in terms of its\nprediction accuracy and variance. More importantly, the multi-step outputs can\nalso allow us to form a stock portfolio over the prediction length. We\ndemonstrate the effectiveness of our model outputs in the portfolio investment\ntask through the Sharpe ratio metric and highlight the importance of dealing\nwith different types of prediction uncertainties.\n"
    },
    {
        "paper_id": 2309.00088,
        "authors": "Timothy DeLise",
        "title": "Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures\n  Market",
        "comments": "8 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern financial electronic exchanges are an exciting and fast-paced\nmarketplace where billions of dollars change hands every day. They are also\nrife with manipulation and fraud. Detecting such activity is a major\nundertaking, which has historically been a job reserved exclusively for humans.\nRecently, more research and resources have been focused on automating these\nprocesses via machine learning and artificial intelligence. Fraud detection is\noverwhelmingly associated with the greater field of anomaly detection, which is\nusually performed via unsupervised learning techniques because of the lack of\nlabeled data needed for supervised learning. However, a small quantity of\nlabeled data does often exist. This research article aims to evaluate the\nefficacy of a deep semi-supervised anomaly detection technique, called Deep\nSAD, for detecting fraud in high-frequency financial data. We use exclusive\nproprietary limit order book data from the TMX exchange in Montr\\'eal, with a\nsmall set of true labeled instances of fraud, to evaluate Deep SAD against its\nunsupervised predecessor. We show that incorporating a small amount of labeled\ndata into an unsupervised anomaly detection framework can greatly improve its\naccuracy.\n"
    },
    {
        "paper_id": 2309.00114,
        "authors": "Changkuk Im",
        "title": "Accurate Quality Elicitation in a Multi-Attribute Choice Setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies how to accurately elicit quality for alternatives with\nmultiple attributes. Two multiple price lists (MPLs) are considered: (i) m-MPL\nwhich asks subjects to compare an alternative to money, and (ii) p-MPL where\nsubjects are endowed with money and asked whether they would like to buy an\nalternative or not. Theoretical results show that m-MPL requires fewer\nassumptions for accurate quality elicitation compared to p-MPL. Experimental\nevidence from a within-subject experiment using consumer products shows that\nswitch points between the two MPLs are different, which suggests that quality\nmeasures are sensitive to the elicitation method.\n"
    },
    {
        "paper_id": 2309.00136,
        "authors": "Ali Asgarov",
        "title": "Predicting Financial Market Trends using Time Series Analysis and\n  Natural Language Processing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Forecasting financial market trends through time series analysis and natural\nlanguage processing poses a complex and demanding undertaking, owing to the\nnumerous variables that can influence stock prices. These variables encompass a\nspectrum of economic and political occurrences, as well as prevailing public\nattitudes. Recent research has indicated that the expression of public\nsentiments on social media platforms such as Twitter may have a noteworthy\nimpact on the determination of stock prices. The objective of this study was to\nassess the viability of Twitter sentiments as a tool for predicting stock\nprices of major corporations such as Tesla, Apple. Our study has revealed a\nrobust association between the emotions conveyed in tweets and fluctuations in\nstock prices. Our findings indicate that positivity, negativity, and\nsubjectivity are the primary determinants of fluctuations in stock prices. The\ndata was analyzed utilizing the Long-Short Term Memory neural network (LSTM)\nmodel, which is currently recognized as the leading methodology for predicting\nstock prices by incorporating Twitter sentiments and historical stock prices\ndata. The models utilized in our study demonstrated a high degree of\nreliability and yielded precise outcomes for the designated corporations. In\nsummary, this research emphasizes the significance of incorporating public\nopinions into the prediction of stock prices. The application of Time Series\nAnalysis and Natural Language Processing methodologies can yield significant\nscientific findings regarding financial market patterns, thereby facilitating\ninformed decision-making among investors. The results of our study indicate\nthat the utilization of Twitter sentiments can serve as a potent instrument for\nforecasting stock prices, and ought to be factored in when formulating\ninvestment strategies.\n"
    },
    {
        "paper_id": 2309.0039,
        "authors": "Esther Cabezas-Rivas, Felipe S\\'anchez-Coll and Isaac Tormo-Xaixo",
        "title": "Chance or Chaos? Fractal geometry aimed to inspect the nature of Bitcoin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The aim of this paper is to analyse the Bitcoin in order to shed some light\non its nature and behaviour. We select 9 cryptocurrencies that account for\nalmost 75\\% of total market capitalisation and compare their evolution with\nthat of a wide variety of traditional assets: commodities with spot and futures\ncontracts, treasury bonds, stock indices, growth and value stocks. Fractal\ngeometry will be applied to carry out a careful statistical analysis of the\nperformance of the Bitcoin returns. As a main conclusion, we have detected a\nhigh degree of persistence in its prices, which decreases the efficiency but\nincreases its predictability. Moreover, we observe that the underlying\ntechnology influences price dynamics, with fully decentralised cryptocurrencies\nbeing the only ones to exhibit self-similarity features at any time scale.\n"
    },
    {
        "paper_id": 2309.00536,
        "authors": "Erdem Dogukan Yilmaz, Tim Meyer, Milan Miric",
        "title": "Preventing Others from Commercializing Your Innovation: Evidence from\n  Creative Commons Licenses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Online innovation communities are an important source of innovation for many\norganizations. While contributions to such communities are typically made\nwithout financial compensation, these contributions are often governed by\nlicenses such as Creative Commons that may prevent others from building upon\nand commercializing them. While this can diminish the usefulness of\ncontributions, there is limited work analyzing what leads individuals to impose\nrestrictions on the use of their work. In this paper, we examine innovators\nimposing restrictive licenses within the 3D-printable design community\nThingiverse. Our analyses suggest that innovators are more likely to restrict\ncommercialization of their contributions as their reputation increases and when\nreusing contributions created by others. These findings contribute to\ninnovation communities and the growing literature on property rights in digital\nmarkets.\n"
    },
    {
        "paper_id": 2309.0054,
        "authors": "Fabien Le Floc'h",
        "title": "Instabilities of Super-Time-Stepping Methods on the Heston Stochastic\n  Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This note explores in more details instabilities of explicit\nsuper-time-stepping schemes, such as the Runge-Kutta-Chebyshev or\nRunge-Kutta-Legendre schemes, noticed in the litterature, when applied to the\nHeston stochastic volatility model. The stability remarks are relevant beyond\nthe scope of super-time-stepping schemes.\n"
    },
    {
        "paper_id": 2309.00618,
        "authors": "Albert Wong, Steven Whang, Emilio Sagre, Niha Sachin, Gustavo Dutra,\n  Yew-Wei Lim, Gaetan Hains, Youry Khmelevsky, Frank Zhang",
        "title": "Short-Term Stock Price Forecasting using exogenous variables and Machine\n  Learning Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Creating accurate predictions in the stock market has always been a\nsignificant challenge in finance. With the rise of machine learning as the next\nlevel in the forecasting area, this research paper compares four machine\nlearning models and their accuracy in forecasting three well-known stocks\ntraded in the NYSE in the short term from March 2020 to May 2022. We deploy,\ndevelop, and tune XGBoost, Random Forest, Multi-layer Perceptron, and Support\nVector Regression models. We report the models that produce the highest\naccuracies from our evaluation metrics: RMSE, MAPE, MTT, and MPE. Using a\ntraining data set of 240 trading days, we find that XGBoost gives the highest\naccuracy despite running longer (up to 10 seconds). Results from this study may\nimprove by further tuning the individual parameters or introducing more\nexogenous variables.\n"
    },
    {
        "paper_id": 2309.00626,
        "authors": "Shuyang Wang and Diego Klabjan",
        "title": "An Ensemble Method of Deep Reinforcement Learning for Automated\n  Cryptocurrency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an ensemble method to improve the generalization performance of\ntrading strategies trained by deep reinforcement learning algorithms in a\nhighly stochastic environment of intraday cryptocurrency portfolio trading. We\nadopt a model selection method that evaluates on multiple validation periods,\nand propose a novel mixture distribution policy to effectively ensemble the\nselected models. We provide a distributional view of the out-of-sample\nperformance on granular test periods to demonstrate the robustness of the\nstrategies in evolving market conditions, and retrain the models periodically\nto address non-stationarity of financial data. Our proposed ensemble method\nimproves the out-of-sample performance compared with the benchmarks of a deep\nreinforcement learning strategy and a passive investment strategy.\n"
    },
    {
        "paper_id": 2309.00629,
        "authors": "Arthur Bagourd and Luca Georges Francois",
        "title": "Quantifying MEV On Layer 2 Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper addresses the lack of research on quantifying Maximal Extractable\nValue (MEV) on Ethereum Layer 2 networks (L2s). Our findings reveal a\nsubstantial amount of MEV to be extracted on L2s, particularly on Polygon, with\na lower bound of $213 million surpassing previous estimates. We observe that\nthe majority of detected MEV on L2s consists of arbitrage opportunities, as\nliquidations are rare. These results emphasize the need for continuous\nmonitoring and analysis of MEV on L2s, promoting informed decision-making for\nnetwork selection and highlighting the associated risks.\n"
    },
    {
        "paper_id": 2309.0063,
        "authors": "Jonas Hanetho",
        "title": "Commodities Trading through Deep Policy Gradient Methods",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2308.01910",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Algorithmic trading has gained attention due to its potential for generating\nsuperior returns. This paper investigates the effectiveness of deep\nreinforcement learning (DRL) methods in algorithmic commodities trading. It\nformulates the commodities trading problem as a continuous, discrete-time\nstochastic dynamical system. The proposed system employs a novel\ntime-discretization scheme that adapts to market volatility, enhancing the\nstatistical properties of subsampled financial time series. To optimize\ntransaction-cost- and risk-sensitive trading agents, two policy gradient\nalgorithms, namely actor-based and actor-critic-based approaches, are\nintroduced. These agents utilize CNNs and LSTMs as parametric function\napproximators to map historical price observations to market\npositions.Backtesting on front-month natural gas futures demonstrates that DRL\nmodels increase the Sharpe ratio by $83\\%$ compared to the buy-and-hold\nbaseline. Additionally, the risk profile of the agents can be customized\nthrough a hyperparameter that regulates risk sensitivity in the reward function\nduring the optimization process. The actor-based models outperform the\nactor-critic-based models, while the CNN-based models show a slight performance\nadvantage over the LSTM-based models.\n"
    },
    {
        "paper_id": 2309.00632,
        "authors": "Wayne Chen, Songwei Chen, Preston Rozwood",
        "title": "Improving Capital Efficiency and Impermanent Loss: Multi-Token Proactive\n  Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Current approaches to the cryptocurrency automated market makers result in\npoor impermanent loss and fragmented liquidity. We focus on the development and\nanalysis of a multi-token proactive market maker (MPMM). MPMM is an extension\nof the proactive market maker (PMM) introduced by DODO Exchange, which\ngeneralizes the constant product market maker (CPMM) and allows for adjustments\nto the pricing curve's ``flatness\" and equilibrium points. We analyze these\nmechanics as used in both PMM and MPMM and demonstrate via simulation that MPMM\nsignificantly improves capital efficiency and price impact compared to its\n2-token pool counterparts as well as their multi-token pool generalizations.\nFurthermore, in typical market conditions, MPMM also combats impermanent loss\nmore effectively than other market maker variants. Overall, this research\nhighlights the advantages multi-token market makers have over pairwise-token\nmodels, and poses a novel market making algorithm. The findings provide\nvaluable insights for designing market makers that optimize capital efficiency\nand mitigate risks in decentralized finance ecosystems.\n"
    },
    {
        "paper_id": 2309.00635,
        "authors": "Mikrajuddin Abdullah",
        "title": "Theoretical foundation for the Pareto distribution of international\n  trade strength and introduction of an equation for international trade\n  forecasting",
        "comments": "27 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I propose a new terminology, international trade strength, which is defined\nas the ratio of a country's total international trade to its GDP. This\nparameter represents a country's ability to generate international trade by\nutilizing its GDP. This figure is equivalent to GDP per capita, which\nrepresents a country's ability to use its population to generate GDP. Trade\nstrength varies by country. The intriguing question is, what distribution\nfunction does the trade strength fulfill? In this paper, a theoretical\nfoundation for predicting the distribution of trade strength and the rate of\nchange of trade strength were developed. These two quantities were found to\nsatisfy the Pareto distribution function. The equations were confirmed using\ndata from the World Integrated Trade Solution (WITS) and the World Bank by\ncomparing the Akaike Information Criterion (AIC) and Bayesian Information\nCriterion (BIC) to five types of distribution functions (exponential,\nlognormal, gamma, Pareto, and Weibull). I also discovered that the fitting\nPareto power parameter is fairly close to the theoretical parameter. In\naddition, a formula for forecasting a country's total international trade in\nthe following years was also developed.\n"
    },
    {
        "paper_id": 2309.00638,
        "authors": "Peer Nagy, Sascha Frey, Silvia Sapora, Kang Li, Anisoara Calinescu,\n  Stefan Zohren, Jakob Foerster",
        "title": "Generative AI for End-to-End Limit Order Book Modelling: A Token-Level\n  Autoregressive Generative Model of Message Flow Using a Deep State Space\n  Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Developing a generative model of realistic order flow in financial markets is\na challenging open problem, with numerous applications for market participants.\nAddressing this, we propose the first end-to-end autoregressive generative\nmodel that generates tokenized limit order book (LOB) messages. These messages\nare interpreted by a Jax-LOB simulator, which updates the LOB state. To handle\nlong sequences efficiently, the model employs simplified structured state-space\nlayers to process sequences of order book states and tokenized messages. Using\nLOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message\ndata, converting groups of successive digits to tokens, similar to tokenization\nin large language models. Out-of-sample results show promising performance in\napproximating the data distribution, as evidenced by low model perplexity.\nFurthermore, the mid-price returns calculated from the generated order flow\nexhibit a significant correlation with the data, indicating impressive\nconditional forecast performance. Due to the granularity of generated data, and\nthe accuracy of the model, it offers new application areas for future work\nbeyond forecasting, e.g. acting as a world model in high-frequency financial\nreinforcement learning applications. Overall, our results invite the use and\nextension of the model in the direction of autoregressive large financial\nmodels for the generation of high-frequency financial data and we commit to\nopen-sourcing our code to facilitate future research.\n"
    },
    {
        "paper_id": 2309.00649,
        "authors": "Pawe{\\l} Niszczota, Sami Abbas",
        "title": "GPT has become financially literate: Insights from financial literacy\n  tests of GPT and a preliminary test of how people use it as a source of\n  advice",
        "comments": "43 pages, 2 figures and 2 tables in main text",
        "journal-ref": "Finance Research Letters, 2023, 58, 104333",
        "doi": "10.1016/j.frl.2023.104333",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We assess the ability of GPT -- a large language model -- to serve as a\nfinancial robo-advisor for the masses, by using a financial literacy test.\nDavinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial\nliteracy test, respectively, compared to a baseline of 33%. However, ChatGPT\nbased on GPT-4 achieves a near-perfect 99% score, pointing to financial\nliteracy becoming an emergent ability of state-of-the-art models. We use the\nJudge-Advisor System and a savings dilemma to illustrate how researchers might\nassess advice-utilization from large language models. We also present a number\nof directions for future research.\n"
    },
    {
        "paper_id": 2309.00875,
        "authors": "Viviana Fanelli, Claudio Fontana, Francesco Rotondi",
        "title": "A hidden Markov model for statistical arbitrage in international crude\n  oil futures markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we study statistical arbitrage strategies in international\ncrude oil futures markets. We analyse strategies that extend classical pairs\ntrading strategies, considering the two benchmark crude oil futures (Brent and\nWTI) together with the newly introduced Shanghai crude oil futures. We document\nthat the time series of these three futures prices are cointegrated and we\nmodel the resulting cointegration spread by a mean-reverting regime-switching\nprocess modulated by a hidden Markov chain. By relying on our stochastic model\nand applying online filter-based parameter estimators, we implement and test a\nnumber of statistical arbitrage strategies. Our analysis reveals that\nstatistical arbitrage strategies involving the Shanghai crude oil futures are\nprofitable even under conservative levels of transaction costs and over\ndifferent time periods. On the contrary, statistical arbitrage strategies\ninvolving the three traditional crude oil futures (Brent, WTI, Dubai) do not\nyield profitable investment opportunities. Our findings suggest that the\nShanghai futures, which has already become the benchmark for the Chinese\ndomestic crude oil market, can be a valuable asset for international investors.\n"
    },
    {
        "paper_id": 2309.00909,
        "authors": "Juan Jacobo",
        "title": "There is power in general equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article develops a general equilibrium model where power relations are\ncentral in the determination of unemployment, profitability, and income\ndistribution. The paper contributes to the market forces versus institutions\ndebate by providing a unified model capable of identifying key interrelations\nbetween technical and institutional changes in the economy. Empirically, the\nmodel is used to gauge the relative roles of technology and institutions in the\nbehavior of the labor share, the unemployment rate, the capital-output ratio,\nand business profitability and demonstrates how they complement each other in\nproviding an adequate narrative to the structural changes of the US economy.\n"
    },
    {
        "paper_id": 2309.00943,
        "authors": "Evgenii Vladimirov",
        "title": "iCOS: Option-Implied COS Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes the option-implied Fourier-cosine method, iCOS, for\nnon-parametric estimation of risk-neutral densities, option prices, and option\nsensitivities. The iCOS method leverages the Fourier-based COS technique,\nproposed by Fang and Oosterlee (2008), by utilizing the option-implied cosine\nseries coefficients. Notably, this procedure does not rely on any model\nassumptions about the underlying asset price dynamics, it is fully\nnon-parametric, and it does not involve any numerical optimization. These\nfeatures make it rather general and computationally appealing. Furthermore, we\nderive the asymptotic properties of the proposed non-parametric estimators and\nstudy their finite-sample behavior in Monte Carlo simulations. Our empirical\nanalysis using S&P 500 index options and Amazon equity options illustrates the\neffectiveness of the iCOS method in extracting valuable information from option\nprices under different market conditions. Additionally, we apply our\nmethodology to dissect and quantify observation and discretization errors in\nthe VIX index.\n"
    },
    {
        "paper_id": 2309.01033,
        "authors": "Giulia Di Nunno, K\\k{e}stutis Kubilius, Yuliya Mishura, Anton\n  Yurchenko-Tytarenko",
        "title": "From constant to rough: A survey of continuous volatility modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present a comprehensive survey of continuous stochastic\nvolatility models, discussing their historical development and the key stylized\nfacts that have driven the field. Special attention is dedicated to fractional\nand rough methods: we outline the motivation behind them and characterize some\nlandmark models. In addition, we briefly touch the problem of VIX modeling and\nrecent advances in the SPX-VIX joint calibration puzzle.\n"
    },
    {
        "paper_id": 2309.01139,
        "authors": "Arnab K. Ray",
        "title": "Logistic modelling of economic dynamics",
        "comments": "5 pages, 6 figures, ReVTeX double column format. arXiv admin note:\n  substantial text overlap with arXiv:2109.05262",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate the effectiveness of the logistic function to model the\nevolution of two economic systems. The first is the GDP and trade growth of the\nUSA, and the second is the revenue and human resource growth of IBM. Our\nmodelling is based on the World Bank data in the case of the USA, and on the\ncompany data in the case of IBM. The coupled dynamics of the two relevant\nvariables in both systems - GDP and trade for the USA, and revenue and human\nresource for IBM - follows a power-law behaviour.\n"
    },
    {
        "paper_id": 2309.01363,
        "authors": "Mingyu Lee, Myeongjin Shin, Junseo Lee, Kabgyun Jeong",
        "title": "Mutual Information Maximizing Quantum Generative Adversarial Network and\n  Its Applications in Finance",
        "comments": "15 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the most promising applications in the era of NISQ (Noisy\nIntermediate-Scale Quantum) computing is quantum machine learning. Quantum\nmachine learning offers significant quantum advantages over classical machine\nlearning across various domains. Specifically, generative adversarial networks\nhave been recognized for their potential utility in diverse fields such as\nimage generation, finance, and probability distribution modeling. However,\nthese networks necessitate solutions for inherent challenges like mode\ncollapse. In this study, we capitalize on the concept that the estimation of\nmutual information between high-dimensional continuous random variables can be\nachieved through gradient descent using neural networks. We introduce a novel\napproach named InfoQGAN, which employs the Mutual Information Neural Estimator\n(MINE) within the framework of quantum generative adversarial networks to\ntackle the mode collapse issue. Furthermore, we elaborate on how this approach\ncan be applied to a financial scenario, specifically addressing the problem of\ngenerating portfolio return distributions through dynamic asset allocation.\nThis illustrates the potential practical applicability of InfoQGAN in\nreal-world contexts.\n"
    },
    {
        "paper_id": 2309.01472,
        "authors": "Timur Sattarov, Marco Schreyer, Damian Borth",
        "title": "FinDiff: Diffusion Models for Financial Tabular Data Generation",
        "comments": "9 pages, 5 figures, 3 tables, preprint version, currently under\n  review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The sharing of microdata, such as fund holdings and derivative instruments,\nby regulatory institutions presents a unique challenge due to strict data\nconfidentiality and privacy regulations. These challenges often hinder the\nability of both academics and practitioners to conduct collaborative research\neffectively. The emergence of generative models, particularly diffusion models,\ncapable of synthesizing data mimicking the underlying distributions of\nreal-world data presents a compelling solution. This work introduces 'FinDiff',\na diffusion model designed to generate real-world financial tabular data for a\nvariety of regulatory downstream tasks, for example economic scenario modeling,\nstress tests, and fraud detection. The model uses embedding encodings to model\nmixed modality financial data, comprising both categorical and numeric\nattributes. The performance of FinDiff in generating synthetic tabular\nfinancial data is evaluated against state-of-the-art baseline models using\nthree real-world financial datasets (including two publicly available datasets\nand one proprietary dataset). Empirical results demonstrate that FinDiff excels\nin generating synthetic tabular financial data with high fidelity, privacy, and\nutility.\n"
    },
    {
        "paper_id": 2309.01565,
        "authors": "German Rodikov and Nino Antulov-Fantulin",
        "title": "Introducing the $\\sigma$-Cell: Unifying GARCH, Stochastic Fluctuations\n  and Evolving Mechanisms in RNN-based Volatility Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces the $\\sigma$-Cell, a novel Recurrent Neural Network\n(RNN) architecture for financial volatility modeling. Bridging traditional\neconometric approaches like GARCH with deep learning, the $\\sigma$-Cell\nincorporates stochastic layers and time-varying parameters to capture dynamic\nvolatility patterns. Our model serves as a generative network, approximating\nthe conditional distribution of latent variables. We employ a\nlog-likelihood-based loss function and a specialized activation function to\nenhance performance. Experimental results demonstrate superior forecasting\naccuracy compared to traditional GARCH and Stochastic Volatility models, making\nthe next step in integrating domain knowledge with neural networks.\n"
    },
    {
        "paper_id": 2309.01784,
        "authors": "Song Wei, Andrea Coletta, Svitlana Vyetrenko, Tucker Balch",
        "title": "INTAGS: Interactive Agent-Guided Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In many applications involving multi-agent system (MAS), it is imperative to\ntest an experimental (Exp) autonomous agent in a high-fidelity simulator prior\nto its deployment to production, to avoid unexpected losses in the real-world.\nSuch a simulator acts as the environmental background (BG) agent(s), called\nagent-based simulator (ABS), aiming to replicate the complex real MAS. However,\ndeveloping realistic ABS remains challenging, mainly due to the sequential and\ndynamic nature of such systems. To fill this gap, we propose a metric to\ndistinguish between real and synthetic multi-agent systems, which is evaluated\nthrough the live interaction between the Exp and BG agents to explicitly\naccount for the systems' sequential nature. Specifically, we characterize the\nsystem/environment by studying the effect of a sequence of BG agents' responses\nto the environment state evolution and take such effects' differences as MAS\ndistance metric; The effect estimation is cast as a causal inference problem\nsince the environment evolution is confounded with the previous environment\nstate. Importantly, we propose the Interactive Agent-Guided Simulation (INTAGS)\nframework to build a realistic ABS by optimizing over this novel metric. To\nadapt to any environment with interactive sequential decision making agents,\nINTAGS formulates the simulator as a stochastic policy in reinforcement\nlearning. Moreover, INTAGS utilizes the policy gradient update to bypass\ndifferentiating the proposed metric such that it can support non-differentiable\noperations of multi-agent environments. Through extensive experiments, we\ndemonstrate the effectiveness of INTAGS on an equity stock market simulation\nexample. We show that using INTAGS to calibrate the simulator can generate more\nrealistic market data compared to the state-of-the-art conditional Wasserstein\nGenerative Adversarial Network approach.\n"
    },
    {
        "paper_id": 2309.01936,
        "authors": "Hui Mi, Zuo Quan Xu, Dongfang Yang",
        "title": "Optimal Management of DC Pension Plan with Inflation Risk and Tail VaR\n  Constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates an optimal investment problem under the tail Value at\nRisk (tail VaR, also known as expected shortfall, conditional VaR, average VaR)\nand portfolio insurance constraints confronted by a defined-contribution\npension member. The member's aim is to maximize the expected utility from the\nterminal wealth exceeding the minimum guarantee by investing his wealth in a\ncash bond, an inflation-linked bond and a stock. Due to the presence of the\ntail VaR constraint, the problem cannot be tackled by standard control tools.\nWe apply the Lagrange method along with quantile optimization techniques to\nsolve the problem. Through delicate analysis, the optimal investment output in\nclosed-form and optimal investment strategy are derived. A numerical analysis\nis also provided to show how the constraints impact the optimal investment\noutput and strategy.\n"
    },
    {
        "paper_id": 2309.02072,
        "authors": "Chen Liu, Minh-Ngoc Tran, Chao Wang, Richard Gerlach, Robert Kohn",
        "title": "Data Scaling Effect of Deep Learning in Financial Time Series\n  Forecasting",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  For years, researchers investigated the applications of deep learning in\nforecasting financial time series. However, they continued to rely on the\nconventional econometric approach for model training that optimizes the deep\nlearning models on individual assets. This study highlights the importance of\nglobal training, where the deep learning model is optimized across a wide\nspectrum of stocks. Focusing on stock volatility forecasting as an exemplar, we\nshow that global training is not only beneficial but also necessary for deep\nlearning-based financial time series forecasting. We further demonstrate that,\ngiven a sufficient amount of training data, a globally trained deep learning\nmodel is capable of delivering accurate zero-shot forecasts for any stocks.\n"
    },
    {
        "paper_id": 2309.02205,
        "authors": "Trent Spears, Stefan Zohren, Stephen Roberts",
        "title": "On statistical arbitrage under a conditional factor model of equity\n  returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a conditional factor model for a multivariate portfolio of United\nStates equities in the context of analysing a statistical arbitrage trading\nstrategy. A state space framework underlies the factor model whereby asset\nreturns are assumed to be a noisy observation of a linear combination of factor\nvalues and latent factor risk premia. Filter and state prediction estimates for\nthe risk premia are retrieved in an online way. Such estimates induce filtered\nasset returns that can be compared to measurement observations, with large\ndeviations representing candidate mean reversion trades. Further, in that the\nrisk premia are modelled as time-varying quantities, non-stationarity in\nreturns is de facto captured. We study an empirical trading strategy respectful\nof transaction costs, and demonstrate performance over a long history of 29\nyears, for both a linear and a non-linear state space model. Our results show\nthat the model is competitive relative to the results of other methods,\nincluding simple benchmarks and other cutting-edge approaches as published in\nthe literature. Also of note, while strategy performance degradation is noticed\nthrough time -- especially for the most recent years -- the strategy continues\nto offer compelling economics, and has scope for further advancement.\n"
    },
    {
        "paper_id": 2309.02271,
        "authors": "Wei Luo and Siyuan Kang and Sheng Hu and Lixian Su and Rui Dai",
        "title": "Dual Effects of the US-China Trade War and COVID-19 on United States\n  Imports: Transfer of China's industrial chain?",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The trade tension between the U.S. and China since 2018 has caused a steady\ndecoupling of the world's two largest economies. The pandemic outbreak in 2020\ncomplicated this process and had numerous unanticipated repercussions. This\npaper investigates how U.S. importers reacted to the trade war and worldwide\nlockdowns due to the COVID-19 pandemic. We examine the effects of the two\nincidents on U.S. imports separately and collectively, with various economic\nscopes. Our findings uncover intricate trading dynamics among the U.S., China,\nand Southeast Asia, through which businesses relocated portions of their global\nsupply chain away from China to avoid high tariffs. Our analysis indicates that\nincreased tariffs cause the U.S. to import less from China. Meanwhile,\nSoutheast Asian exporters have integrated more into value chains centered on\nChinese suppliers by participating more in assembling and completing products.\nHowever, the worldwide lockdowns over pandemic have reversed this trend as,\nover this period, the U.S. effectively imported more goods directly from China\nand indirectly through Southeast Asian exporters that imported from China.\n"
    },
    {
        "paper_id": 2309.02323,
        "authors": "Talya ten Brink",
        "title": "Projections of Economic Impacts of Climate Change on Marine Protected\n  Areas: Palau, the Great Barrier Reef, and the Bering Sea",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Climate change substantially impacts ecological systems. Marine species are\nshifting their distribution because of climate change towards colder waters,\npotentially compromising the benefits of currently established Marine Protected\nAreas (MPAs). Therefore, we demonstrate how three case study regions will be\nimpacted by warming ocean waters to prepare stakeholders to understand how the\nfisheries around the MPAs is predicted to change. We chose the case studies to\nfocus on large scale MPAs in i) a cold, polar region, ii) a tropical region\nnear the equator, and iii) a tropical region farther from the equator. We\nquantify the biological impacts of shifts in species distribution due to\nclimate change for fishing communities that depend on the Palau National Marine\nSanctuary, the Great Barrier Reef Marine National Park Zone, and the North\nBering Sea Research Area MPAs. We find that fisheries sectors will be impacted\ndifferently in different regions and show that all three regions can be\nsupported by this methodology for decision making that joins sector income and\nspecies diversity.\n"
    },
    {
        "paper_id": 2309.02338,
        "authors": "Ogutu B. Osoro, Edward J. Oughton, Andrew R. Wilson, Akhil Rao",
        "title": "Sustainability assessment of Low Earth Orbit (LEO) satellite broadband\n  megaconstellations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth of megaconstellations is rapidly increasing the number of rocket\nlaunches. While Low Earth Orbit (LEO) broadband satellites help to connect\nunconnected communities and achieve the Sustainable Development Goals (SDGs),\nthere are also significant environmental emissions impacts from burning rocket\nfuels. We present sustainability analytics for phase 1 of the three main LEO\nconstellations including Amazon Kuiper (3,236 satellites), Eutelsat Group`s\nOneWeb (648 satellites), and SpaceX Starlink (4,425 satellites). We find that\nLEO megaconstellations provide substantially improved broadband speeds for\nrural and remote communities, but are roughly 6-8 times more emissions\nintensive (250 kg CO2eq/subscriber/year) than comparative terrestrial mobile\nbroadband. In the worst-case emissions scenario, this rises to 12-14 times more\n(469 kg CO2eq/subscriber/year). Policy makers must carefully consider the\ntrade-off between connecting unconnected communities to further the SDGs and\nmitigating the growing space sector environmental footprint, particularly\nregarding phase 2 plans to launch an order-of-magnitude more satellites.\n"
    },
    {
        "paper_id": 2309.02447,
        "authors": "Victor Olkhov",
        "title": "Economic Complexity Limits Accuracy of Price Probability Predictions by\n  Gaussian Distributions",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the economic reasons why the predictions of price and return\nstatistical moments in the coming decades, in the best case, will be limited by\ntheir averages and volatilities. That limits the accuracy of the forecasts of\nprice and return probabilities by Gaussian distributions. The economic origin\nof these restrictions lies in the fact that the predictions of the market-based\nn-th statistical moments of price and return for n=1,2,.., require the\ndescription of the economic variables of the n-th order that are determined by\nsums of the n-th degrees of values or volumes of market trades. The lack of\nexisting models that describe the evolution of the economic variables\ndetermined by the sums of the 2nd degrees of market trades results in the fact\nthat even predictions of the volatilities of price and return are very\nuncertain. One can ignore existing economic barriers that we highlight but\ncannot overcome or resolve them. The accuracy of predictions of price and\nreturn probabilities substantially determines the reliability of asset pricing\nmodels and portfolio theories. The restrictions on the accuracy of predictions\nof price and return statistical moments reduce the reliability and veracity of\nmodern asset pricing and portfolio theories.\n"
    },
    {
        "paper_id": 2309.0257,
        "authors": "Tomasz R. Bielecki, Igor Cialenco, Hao Liu",
        "title": "Time consistency of dynamic risk measures and dynamic performance\n  measures generated by distortion functions",
        "comments": "This manuscript is accompanied by a supplement that contains some\n  technical, but important, results and their proofs",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this work is to study risk measures generated by distortion\nfunctions in a dynamic discrete time setup, and to investigate the\ncorresponding dynamic coherent acceptability indices (DCAIs) generated by\nfamilies of such risk measures. First we show that conditional version of\nChoquet integrals indeed are dynamic coherent risk measures (DCRMs), and also\nintroduce the class of dynamic weighted value at risk measures. We prove that\nthese two classes of risk measures coincides. In the spirit of robust\nrepresentations theorem for DCAIs, we establish some relevant properties of\nfamilies of DCRMs generated by distortion functions, and then define and study\nthe corresponding DCAIs. Second, we study the time consistency of DCRMs and\nDCAIs generated by distortion functions. In particular, we prove that such\nDCRMs are sub-martingale time consistent, but they are not super-martingale\ntime consistent. We also show that DCRMs generated by distortion functions are\nnot weakly acceptance time consistent. We also present several widely used\nclasses of distortion functions and derive some new representations of these\ndistortions.\n"
    },
    {
        "paper_id": 2309.02608,
        "authors": "David Robinson, Angel Arcos-Vargas, Micheael Tennican, Fernando\n  N\\'u\\~nez",
        "title": "The Iberian Exception: An overview of its effects over its first 100\n  days",
        "comments": "34 pages, 9 figures and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper offers an independent assessment of certain key economic effects\nof the Iberian Exception (IE). Their stated aim was to reduce the major\ncomponent of electricity prices for most Iberian consumers, a component which\nwas indexed to Iberian wholesale power market spot prices power market prices\nthat were rising alarmingly due to extremely tight international markets for\nnatural gas. The Spanish Government estimates that, during its first 100 days,\nthe IE provided substantial benefits for consumers affected by the IE, which\nincluded over 10 million small consumers as well as many large ones, but the\nauthors of this study question that estimate. The authors of this paper argue\nthat the estimated effect of the IE on retail prices depends critically on the\nassumptions about what would have occurred in the absence of the IE, i.e., in a\ncounterfactual scenario. Although counterfactuals are always difficult to\nconstruct, the government s counterfactual ignores demand elasticity, and this\ninflates their estimate of immediate consumer benefits. Using hourly data on\nthe wholesale electricity market for the first 100 days of the IE, this paper s\nanalysis of alternative counterfactuals that reflect the effects of demand\nelasticity shows substantially lower benefits of the IE for consumers than the\nSpanish government estimates. Indeed, this paper s analysis suggests that\naffected consumers would have paid somewhat less for electricity in the first\n100 days of the IE had it not been introduced. The authors identify several\nother potential short and longterm effects of the IE that deserve further\nstudy. These include increased margins for fossil fired generators, reduced\nmargins for some decarbonized inframarginal plant, heightened investor\nperceptions of regulatory risk, weakened incentives for efficient consumption,\nand higher carbon emissions and gas prices.\n"
    },
    {
        "paper_id": 2309.0297,
        "authors": "Christian Oliver Ewald and Kevin Kamm",
        "title": "On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision\n  Making",
        "comments": null,
        "journal-ref": "Quantitative Finance (2024)",
        "doi": "10.1080/14697688.2024.2308069",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the effect of stochastic feeding costs on animal-based commodities\nwith particular focus on aquaculture. More specifically, we use soybean futures\nto infer on the stochastic behaviour of salmon feed, which we assume to follow\na Schwartz-2-factor model. We compare the decision of harvesting salmon using a\ndecision rule assuming either deterministic or stochastic feeding costs, i.e.\nincluding feeding cost risk. We identify cases, where accounting for stochastic\nfeeding costs leads to significant improvements as well as cases where\ndeterministic feeding costs are a good enough proxy. Nevertheless, in all of\nthese cases, the newly derived rules show superior performance, while the\nadditional computational costs are negligible. From a methodological point of\nview, we demonstrate how to use Deep-Neural-Networks to infer on the decision\nboundary that determines harvesting or continuation, improving on more\nclassical regression-based and curve-fitting methods. To achieve this we use a\ndeep classifier, which not only improves on previous results but also scales\nwell for higher dimensional problems, and in addition mitigates effects due to\nmodel uncertainty, which we identify in this article. effects due to model\nuncertainty, which we identify in this article.\n"
    },
    {
        "paper_id": 2309.02994,
        "authors": "Eyal Neuman, Wolfgang Stockinger, Yufei Zhang",
        "title": "An Offline Learning Approach to Propagator Models",
        "comments": "12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an offline learning problem for an agent who first estimates an\nunknown price impact kernel from a static dataset, and then designs strategies\nto liquidate a risky asset while creating transient price impact. We propose a\nnovel approach for a nonparametric estimation of the propagator from a dataset\ncontaining correlated price trajectories, trading signals and metaorders. We\nquantify the accuracy of the estimated propagator using a metric which depends\nexplicitly on the dataset. We show that a trader who tries to minimise her\nexecution costs by using a greedy strategy purely based on the estimated\npropagator will encounter suboptimality due to so-called spurious correlation\nbetween the trading strategy and the estimator and due to intrinsic uncertainty\nresulting from a biased cost functional. By adopting an offline reinforcement\nlearning approach, we introduce a pessimistic loss functional taking the\nuncertainty of the estimated propagator into account, with an optimiser which\neliminates the spurious correlation, and derive an asymptotically optimal bound\non the execution costs even without precise information on the true propagator.\nNumerical experiments are included to demonstrate the effectiveness of the\nproposed propagator estimator and the pessimistic trading strategy.\n"
    },
    {
        "paper_id": 2309.03003,
        "authors": "Atilla Aras",
        "title": "Proofs for the New Definitions in Financial Markets",
        "comments": "24 pages, 2 new theorems and 2 new propositions are added",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Constructing theorems can help to determine the shape of certain utility\ncurves that make up the new definitions in financial markets. The aim of this\nstudy was to present proofs for these theorems. Basic thoughts of new\nalternative definitions emerge from the decision-making under uncertainty in\neconomics and finance. Shape of the certain utility curve is central to\nstandard definitions in determining risk attitudes of investors. Shape alone\ndetermines risk behavior of investors in standard theory. Although the terms\nrisk-averse, risk-loving, and risk-neutral are equivalent to strict concavity,\nstrict convexity, and linearity, respectively, in standard theory, strict\nconcavity or strict convexity, or linearity are valid for certain new\ndefinitions, not being the same as standard theory. Hence, it can be stated\nthat new alternative definitions are broader than standard definitions from the\nviewpoint of shape. For instance, the certain utility curve of a risk-averse\ninvestor can be strictly concave or strictly convex, or linear in alternative\ndefinitions.\n"
    },
    {
        "paper_id": 2309.03079,
        "authors": "Udit Gupta",
        "title": "GPT-InvestAR: Enhancing Stock Investment Strategies through Annual\n  Report Analysis with Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Annual Reports of publicly listed companies contain vital information about\ntheir financial health which can help assess the potential impact on Stock\nprice of the firm. These reports are comprehensive in nature, going up to, and\nsometimes exceeding, 100 pages. Analysing these reports is cumbersome even for\na single firm, let alone the whole universe of firms that exist. Over the\nyears, financial experts have become proficient in extracting valuable\ninformation from these documents relatively quickly. However, this requires\nyears of practice and experience. This paper aims to simplify the process of\nassessing Annual Reports of all the firms by leveraging the capabilities of\nLarge Language Models (LLMs). The insights generated by the LLM are compiled in\na Quant styled dataset and augmented by historical stock price data. A Machine\nLearning model is then trained with LLM outputs as features. The walkforward\ntest results show promising outperformance wrt S&P500 returns. This paper\nintends to provide a framework for future work in this direction. To facilitate\nthis, the code has been released as open source.\n"
    },
    {
        "paper_id": 2309.03133,
        "authors": "Alexander Gutfraind",
        "title": "Risk-reducing design and operations toolkit: 90 strategies for managing\n  risk and uncertainty in decision problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Uncertainty is a pervasive challenge in decision analysis, and decision\ntheory recognizes two classes of solutions: probabilistic models and cognitive\nheuristics. However, engineers, public planners and other decision-makers\ninstead use a third class of strategies that could be called RDOT\n(Risk-reducing Design and Operations Toolkit). These include incorporating\nrobustness into designs, contingency planning, and others that do not fall into\nthe categories of probabilistic models or cognitive heuristics. Moreover,\nidentical strategies appear in several domains and disciplines, pointing to an\nimportant shared toolkit.\n  The focus of this paper is to develop a catalog of such strategies and\ndevelop a framework for them. The paper finds more than 90 examples of such\nstrategies falling into six broad categories and argues that they provide an\nefficient response to decision problems that are seemingly intractable due to\nhigh uncertainty. It then proposes a framework to incorporate them into\ndecision theory using multi-objective optimization.\n  Overall, RDOT represents an overlooked class of responses to uncertainty.\nBecause RDOT strategies do not depend on accurate forecasting or estimation,\nthey could be applied fruitfully to certain decision problems affected by high\nuncertainty and make them much more tractable.\n"
    },
    {
        "paper_id": 2309.03202,
        "authors": "Ishan S. Khare, Tarun K. Martheswaran, Akshana Dassanaike-Perera",
        "title": "Evaluation of Reinforcement Learning Techniques for Trading on a Diverse\n  Portfolio",
        "comments": "Course project not to be posted online",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work seeks to answer key research questions regarding the viability of\nreinforcement learning over the S&P 500 index. The on-policy techniques of\nValue Iteration (VI) and State-action-reward-state-action (SARSA) are\nimplemented along with the off-policy technique of Q-Learning. The models are\ntrained and tested on a dataset comprising multiple years of stock market data\nfrom 2000-2023. The analysis presents the results and findings from training\nand testing the models using two different time periods: one including the\nCOVID-19 pandemic years and one excluding them. The results indicate that\nincluding market data from the COVID-19 period in the training dataset leads to\nsuperior performance compared to the baseline strategies. During testing, the\non-policy approaches (VI and SARSA) outperform Q-learning, highlighting the\ninfluence of bias-variance tradeoff and the generalization capabilities of\nsimpler policies. However, it is noted that the performance of Q-learning may\nvary depending on the stability of future market conditions. Future work is\nsuggested, including experiments with updated Q-learning policies during\ntesting and trading diverse individual stocks. Additionally, the exploration of\nalternative economic indicators for training the models is proposed.\n"
    },
    {
        "paper_id": 2309.03283,
        "authors": "Fatemeh Nazari, Yellitza Soto, Mohamadhossein Noruzoliaee",
        "title": "Privately-Owned versus Shared Automated Vehicle: The Roles of\n  Utilitarian and Hedonic Beliefs",
        "comments": "21 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transportation systems will be likely transformed by the emergence of\nautomated vehicles (AVs) promising for safe, convenient, and efficient\nmobility, especially if used in shared systems (shared AV or SAV). However, the\npotential tendency is observed towards owning AV as a private asset rather than\nusing SAV. This calls for a research on investigating individuals' attitude\ntowards AV in comparison with SAV to recognize the barriers to the public's\ntendency towards SAV. To do so, the present study proposes a modeling framework\nbased on the theories in behavioral psychology to explain individuals'\npreference for owning AV over using SAV, built as a latent (subjective)\npsychometric construct, by three groups of explanatory latent constructs\nincluding: (i) desire for searching for benefits, i.e., extrinsic motive\nmanifested in utilitarian beliefs; (ii) tendency towards seeking pleasure and\njoy, i.e., intrinsic motive reflected in hedonic beliefs; and (iii) attitude\ntowards three configurations of shared mobility, i.e., experience with car and\nridesharing, bikesharing, and public transit. Estimated on a sample dataset\nfrom the State of California, the findings can shed initial lights on the\npsychological determinants of the public's attitude towards owning AV versus\nusing SAV, which can furthermore provide policy implications intriguing for\npolicy makers and stakeholders. Of note, the findings reveal the strongest\ninfluential factor on preference for AV over SAV as hedonic beliefs reflected\nin perceived enjoyment. This preference is next affected by utilitarian\nbeliefs, particularly perceived benefit and trust of stranger, followed by\nattitude towards car and ride sharing.\n"
    },
    {
        "paper_id": 2309.03311,
        "authors": "David Xiao",
        "title": "Default Process Modeling and Credit Valuation Adjustment",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a convenient framework for modeling default process and\npricing derivative securities involving credit risk. The framework provides an\nintegrated view of credit valuation adjustment by linking distance-to-default,\ndefault probability, survival probability, and default correlation together. We\nshow that risky valuation is Martingale in our model. The framework reduces the\ntechnical issues of performing risky valuation to the same issues faced when\nperforming the ordinary valuation. The numerical results show that the model\nprediction is consistent with the historical observations.\n"
    },
    {
        "paper_id": 2309.03403,
        "authors": "Gordon Getty, Nikita Tkachenko",
        "title": "Sources of capital growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Data from national accounts show no effect of change in net saving or\nconsumption, in ratio to market-value capital, on change in growth rate of\nmarket-value capital (capital acceleration). Thus it appears that capital\ngrowth and acceleration arrive without help from net saving or consumption\nrestraint. We explore ways in which this is possible, and discuss implications\nfor economic teaching and public policy\n"
    },
    {
        "paper_id": 2309.03419,
        "authors": "Mikhail Freer and Daniel Friedman and Simon Weidenholzer",
        "title": "Motives for Delegating Financial Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Why do some investors delegate financial decisions to supposed experts? We\nreport a laboratory experiment designed to disentangle four possible motives.\nAlmost 600 investors drawn from the Prolific subject pool choose whether or not\nto delegate a real-stakes choice among lotteries to a previous investor (an\n``expert'') after seeing information on the performance of several available\nexperts. We find that a surprisingly large fraction of investors delegate even\ntrivial choice tasks, suggesting a major role for the blame shifting motive. A\nlarger fraction of investors delegate our more complex tasks, suggesting that\ndecision costs play a role for some investors. Some investors who delegate\nchoose a low quality expert with high earnings, suggesting a role for chasing\npast performance. We find no evidence for a fourth possible motive, that\ndelegation makes risk more acceptable.\n"
    },
    {
        "paper_id": 2309.03432,
        "authors": "Sabiou Inoua, Vernon Smith",
        "title": "Perishable Goods versus Re-tradable Assets: A Theoretical Reappraisal of\n  a Fundamental Dichotomy",
        "comments": null,
        "journal-ref": "Handbook of Experimental Finance, S. Fullbrunn and E. Haruvy\n  (eds), Edward Elgar Publishing (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Experimental results on market behavior establish a lower stability and\nefficiency of markets for durable re-tradable assets compared to markets for\nnon-durable, or perishable, goods. In this chapter, we revisit this known but\nunderappreciated dichotomy of goods in the light of our theory of competitive\nmarket price formation, and we emphasize the fundamental nature of the concept\nof asset re-tradability in neoclassical finance through a simple reformulation\nof the famous no-trade and no-arbitrage theorems.\n"
    },
    {
        "paper_id": 2309.03541,
        "authors": "David R. Ba\\~nos, Salvador Ortiz-Latorre and Oriol Zamora Font",
        "title": "Thiele's PIDE for unit-linked policies in the Heston-Hawkes stochastic\n  volatility model",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main purpose of the paper is to derive Thiele's differential equation for\nunit-linked policies in the Heston-Hawkes stochastic volatility model\nintroduced in arXiv:2210.15343. This model is an extension of the well-known\nHeston model that incorporates the volatility clustering feature by adding a\ncompound Hawkes process in the volatility. Since the model is arbitrage-free,\npricing unit-linked policies via the equivalence principle under a risk neutral\nprobability measure is possible. Studying the moments of the variance and\ncertain stochastic exponentials, a suitable family of risk neutral probability\nmeasures is found. The established and practical method to compute reserves in\nlife insurance is by solving Thiele's equation, which is crucial to guarantee\nthe solvency of the insurance company.\n"
    },
    {
        "paper_id": 2309.03736,
        "authors": "Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah",
        "title": "TradingGPT: Multi-Agent System with Layered Memory and Distinct\n  Characters for Enhanced Financial Trading Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large Language Models (LLMs), prominently highlighted by the recent evolution\nin the Generative Pre-trained Transformers (GPT) series, have displayed\nsignificant prowess across various domains, such as aiding in healthcare\ndiagnostics and curating analytical business reports. The efficacy of GPTs lies\nin their ability to decode human instructions, achieved through comprehensively\nprocessing historical inputs as an entirety within their memory system. Yet,\nthe memory processing of GPTs does not precisely emulate the hierarchical\nnature of human memory. This can result in LLMs struggling to prioritize\nimmediate and critical tasks efficiently. To bridge this gap, we introduce an\ninnovative LLM multi-agent framework endowed with layered memories. We assert\nthat this framework is well-suited for stock and fund trading, where the\nextraction of highly relevant insights from hierarchical financial data is\nimperative to inform trading decisions. Within this framework, one agent\norganizes memory into three distinct layers, each governed by a custom decay\nmechanism, aligning more closely with human cognitive processes. Agents can\nalso engage in inter-agent debate. In financial trading contexts, LLMs serve as\nthe decision core for trading agents, leveraging their layered memory system to\nintegrate multi-source historical actions and market insights. This equips them\nto navigate financial changes, formulate strategies, and debate with peer\nagents about investment decisions. Another standout feature of our approach is\nto equip agents with individualized trading traits, enhancing memory diversity\nand decision robustness. These sophisticated designs boost the system's\nresponsiveness to historical trades and real-time market signals, ensuring\nsuperior automated trading accuracy.\n"
    },
    {
        "paper_id": 2309.03966,
        "authors": "Rong Du and Duy-Minh Dang",
        "title": "Fourier Neural Network Approximation of Transition Densities in Finance",
        "comments": "35 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces FourNet, a novel single-layer feed-forward neural\nnetwork (FFNN) method designed to approximate transition densities for which\nclosed-form expressions of their Fourier transforms, i.e. characteristic\nfunctions, are available. A unique feature of FourNet lies in its use of a\nGaussian activation function, enabling exact Fourier and inverse Fourier\ntransformations and drawing analogies with the Gaussian mixture model. We\nmathematically establish FourNet's capacity to approximate transition densities\nin the $L_2$-sense arbitrarily well with finite number of neurons. The\nparameters of FourNet are learned by minimizing a loss function derived from\nthe known characteristic function and the Fourier transform of the FFNN,\ncomplemented by a strategic sampling approach to enhance training. We derive\npractical bounds for the $L_2$ estimation error and the potential pointwise\nloss of nonnegativity in FourNet, highlighting its robustness and applicability\nin practical settings. FourNet's accuracy and versatility are demonstrated\nthrough a wide range of dynamics common in quantitative finance, including\nL\\'{e}vy processes and the Heston stochastic volatility models-including those\naugmented with the self-exciting Queue-Hawkes jump process.\n"
    },
    {
        "paper_id": 2309.03968,
        "authors": "Jozef Barunik and Mattia Bevilacqua and Michael Ellington",
        "title": "Common Firm-level Investor Fears: Evidence from Equity Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We identify a new type of risk, common firm-level investor fears, from\ncommonalities within the cross-sectional distribution of individual stock\noptions. We define firm-level fears that link with upward price movements as\ngood fears, and those relating to downward price movements as bad fears. Such\ninformation is different to market fears that we extract from index options.\nStocks with high sensitivities to common firm-level investor fears earn lower\nreturns, with investors demanding a higher compensation for exposure to common\nbad fears relative to common good fears. Risk premium estimates for common bad\nfears range from -5.63% to -4.92% per annum.\n"
    },
    {
        "paper_id": 2309.03984,
        "authors": "Chinonso Nwankwo, Weizhong Dai, Tony Ware",
        "title": "Enhancing accuracy for solving American CEV model with high-order\n  compact scheme and adaptive time stepping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research work, we propose a high-order time adapted scheme for\npricing a coupled system of fixed-free boundary constant elasticity of variance\n(CEV) model on both equidistant and locally refined space-grid. The performance\nof our method is substantially enhanced to improve irregularities in the model\nwhich are both inherent and induced. Furthermore, the system of coupled PDEs is\nstrongly nonlinear and involves several time-dependent coefficients that\ninclude the first-order derivative of the early exercise boundary. These\ncoefficients are approximated from a fourth-order analytical approximation\nwhich is derived using a regularized square-root function. The semi-discrete\nequation for the option value and delta sensitivity is obtained from a\nnon-uniform fourth-order compact finite difference scheme. Fifth-order 5(4)\nDormand-Prince time integration method is used to solve the coupled system of\ndiscrete equations. Enhancing the performance of our proposed method with local\nmesh refinement and adaptive strategies enables us to obtain highly accurate\nsolution with very coarse space grids, hence reducing computational runtime\nsubstantially. We further verify the performance of our methodology as compared\nwith some of the well-known and better-performing existing methods.\n"
    },
    {
        "paper_id": 2309.04116,
        "authors": "Georg Menz and Moritz Vo{\\ss}",
        "title": "Aggregation of financial markets",
        "comments": "49 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a formal framework for the aggregation of financial markets\nmediated by arbitrage. Our main tool is to characterize markets via utility\nfunctions and to employ a one-to-one correspondence to limit order book states.\nInspired by the theory of thermodynamics we argue that the arbitrage-mediated\naggregation mechanism gives rise to a market-dynamical entropy, which\nquantifies the loss of liquidity caused by aggregation. We also discuss future\ndirections of research in this emerging theory of market dynamics.\n"
    },
    {
        "paper_id": 2309.04118,
        "authors": "Md. Toaha, Laboni Mondal",
        "title": "Agriculture Credit and Economic Growth in Bangladesh: A Time Series\n  Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper examined the impact of agricultural credit on economic growth in\nBangladesh. The annual data of agriculture credit were collected from annual\nreports of the Bangladesh Bank and other data were collected from the world\ndevelopment indicator (WDI) of the World Bank. By employing Johansen\ncointegration test and vector error correction model (VECM), the study revealed\nthat there exists a long run relationship between the variables. The results of\nthe study showed that agriculture credit had a positive impact on GDP growth in\nBangladesh. The study also found that gross capital formation had a positive,\nwhile inflation had a negative association with economic growth in Bangladesh.\nTherefore, the government and policymakers should continue their effort to\nincrease the volume of agriculture credit to achieve sustainable economic\ngrowth.\n"
    },
    {
        "paper_id": 2309.04216,
        "authors": "Philippe Bergault, Olivier Gu\\'eant",
        "title": "Liquidity Dynamics in RFQ Markets and Impact on Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To assign a value to a portfolio, it is common to use Mark-to-Market prices.\nHowever, how should one proceed when the securities are illiquid? When\ntransaction prices are scarce, how can one use all the available real-time\ninformation? In this article, we address these questions for over-the-counter\n(OTC) markets based on requests for quotes (RFQs). We extend the concept of\nmicro-price, which was recently introduced for assets exchanged through limit\norder books in the market microstructure literature, and incorporate ideas from\nthe recent literature on OTC market making. To account for liquidity imbalances\nin RFQ markets, we use an approach based on bidimensional Markov-modulated\nPoisson processes. Beyond extending the concept of micro-price to RFQ markets,\nwe introduce the new concept of Fair Transfer Price. Our concepts of price can\nbe used to value securities fairly, even when the market is relatively illiquid\nand/or tends to be one-sided.\n"
    },
    {
        "paper_id": 2309.04259,
        "authors": "Paul Bilokon and Burak Gunduz",
        "title": "C++ Design Patterns for Low-latency Applications Including\n  High-frequency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work aims to bridge the existing knowledge gap in the optimisation of\nlatency-critical code, specifically focusing on high-frequency trading (HFT)\nsystems. The research culminates in three main contributions: the creation of a\nLow-Latency Programming Repository, the optimisation of a market-neutral\nstatistical arbitrage pairs trading strategy, and the implementation of the\nDisruptor pattern in C++. The repository serves as a practical guide and is\nenriched with rigorous statistical benchmarking, while the trading strategy\noptimisation led to substantial improvements in speed and profitability. The\nDisruptor pattern showcased significant performance enhancement over\ntraditional queuing methods. Evaluation metrics include speed, cache\nutilisation, and statistical significance, among others. Techniques like Cache\nWarming and Constexpr showed the most significant gains in latency reduction.\nFuture directions involve expanding the repository, testing the optimised\ntrading algorithm in a live trading environment, and integrating the Disruptor\npattern with the trading algorithm for comprehensive system benchmarking. The\nwork is oriented towards academics and industry practitioners seeking to\nimprove performance in latency-sensitive applications.\n"
    },
    {
        "paper_id": 2309.04483,
        "authors": "Dietmar Pfeifer",
        "title": "Ein neuer Ansatz zur Frequenzmodellierung im Versicherungswesen (A new\n  Approach to frequency modeling in risk theory)",
        "comments": "in German language, 8 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The collective risk model differentiates usually between claims frequencies\n(and their distribution) and claim sizes (and their distribution). For the\nclaims frequencies typically classical discrete distributions are considered,\nsuch as Binomial-, Negative binomial- or Poisson distributions. Since these\ndistributions sometimes do not really fit to the data we propose a different\napproach here for claim frequencies via random proportions of the number of\ninsurance contracts. This approach also allows for a statistical\ngoodness-of-fit test via quantile-quantile-plots and can likewise be applied to\nthe modelling of claim size distributions.\n"
    },
    {
        "paper_id": 2309.04507,
        "authors": "Emiel Lemahieu, Kris Boudt, Maarten Wyns",
        "title": "Generating drawdown-realistic financial price paths using path\n  signatures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A novel generative machine learning approach for the simulation of sequences\nof financial price data with drawdowns quantifiably close to empirical data is\nintroduced. Applications such as pricing drawdown insurance options or\ndeveloping portfolio drawdown control strategies call for a host of\ndrawdown-realistic paths. Historical scenarios may be insufficient to\neffectively train and backtest the strategy, while standard parametric Monte\nCarlo does not adequately preserve drawdowns. We advocate a non-parametric\nMonte Carlo approach combining a variational autoencoder generative model with\na drawdown reconstruction loss function. To overcome issues of numerical\ncomplexity and non-differentiability, we approximate drawdown as a linear\nfunction of the moments of the path, known in the literature as path\nsignatures. We prove the required regularity of drawdown function and\nconsistency of the approximation. Furthermore, we obtain close numerical\napproximations using linear regression for fractional Brownian and empirical\ndata. We argue that linear combinations of the moments of a path yield a\nmathematically non-trivial smoothing of the drawdown function, which gives one\nleeway to simulate drawdown-realistic price paths by including drawdown\nevaluation metrics in the learning objective. We conclude with numerical\nexperiments on mixed equity, bond, real estate and commodity portfolios and\nobtain a host of drawdown-realistic paths.\n"
    },
    {
        "paper_id": 2309.04547,
        "authors": "Alexander Lipton",
        "title": "Kelvin Waves, Klein-Kramers and Kolmogorov Equations, Path-Dependent\n  Financial Instruments: Survey and New Results",
        "comments": "76 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discover several surprising relationships between large classes of\nseemingly unrelated foundational problems of financial engineering and\nfundamental problems of hydrodynamics and molecular physics. Solutions in all\nthese domains can be reduced to solving affine differential equations commonly\nused in various mathematical and scientific disciplines to model dynamic\nsystems. We have identified connections in these seemingly disparate areas as\nwe link together small wave-like perturbations of linear flows in ideal and\nviscous fluids described in hydrodynamics by Kevin waves to motions of free and\nharmonically bound particles described in molecular physics by Klein-Kramers\nand Kolmogorov equations to Gaussian and non-Gaussian affine processes, e.g.,\nOrnstein-Uhlenbeck and Feller, arising in financial engineering. To further\nemphasize the parallels between these diverse fields, we build a coherent\nmathematical framework using Kevin waves to construct transition probability\ndensity functions for problems in hydrodynamics, molecular physics, and\nfinancial engineering. As one of the outcomes of our analysis, we discover that\nthe original solution of the Kolmogorov equation contains an error, which we\nsubsequently correct. We apply our interdisciplinary approach to advance the\nunderstanding of various financial engineering topics, such as pricing of Asian\noptions, volatility and variance swaps, options on stocks with path-dependent\nvolatility, bonds, and bond options. We also discuss further applications to\nother exciting problems of financial engineering.\n"
    },
    {
        "paper_id": 2309.04557,
        "authors": "Xuwei Yang and Anastasis Kratsios and Florian Krach and Matheus\n  Grasselli and Aurelien Lucchi",
        "title": "Regret-Optimal Federated Transfer Learning for Kernel Regression with\n  Applications in American Option Pricing",
        "comments": "54 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose an optimal iterative scheme for federated transfer learning, where\na central planner has access to datasets ${\\cal D}_1,\\dots,{\\cal D}_N$ for the\nsame learning model $f_{\\theta}$. Our objective is to minimize the cumulative\ndeviation of the generated parameters $\\{\\theta_i(t)\\}_{t=0}^T$ across all $T$\niterations from the specialized parameters\n$\\theta^\\star_{1},\\ldots,\\theta^\\star_N$ obtained for each dataset, while\nrespecting the loss function for the model $f_{\\theta(T)}$ produced by the\nalgorithm upon halting. We only allow for continual communication between each\nof the specialized models (nodes/agents) and the central planner (server), at\neach iteration (round). For the case where the model $f_{\\theta}$ is a\nfinite-rank kernel regression, we derive explicit updates for the\nregret-optimal algorithm. By leveraging symmetries within the regret-optimal\nalgorithm, we further develop a nearly regret-optimal heuristic that runs with\n$\\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of\nthe parameter space. Additionally, we investigate the adversarial robustness of\nthe regret-optimal algorithm showing that an adversary which perturbs $q$\ntraining pairs by at-most $\\varepsilon>0$, across all training sets, cannot\nreduce the regret-optimal algorithm's regret by more than\n$\\mathcal{O}(\\varepsilon q \\bar{N}^{1/2})$, where $\\bar{N}$ is the aggregate\nnumber of training pairs. To validate our theoretical findings, we conduct\nnumerical experiments in the context of American option pricing, utilizing a\nrandomly generated finite-rank kernel.\n"
    },
    {
        "paper_id": 2309.04876,
        "authors": "Sabiou Inoua",
        "title": "News-driven Expectations and Volatility Clustering",
        "comments": null,
        "journal-ref": "J. Risk Financial Manag. 2020, 13(1), 17",
        "doi": "10.3390/jrfm13010017",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial volatility obeys two fascinating empirical regularities that apply\nto various assets, on various markets, and on various time scales: it is\nfat-tailed (more precisely power-law distributed) and it tends to be clustered\nin time. Many interesting models have been proposed to account for these\nregularities, notably agent-based models, which mimic the two empirical laws\nthrough a complex mix of nonlinear mechanisms such as traders' switching\nbetween trading strategies in highly nonlinear way. This paper explains the two\nregularities simply in terms of traders' attitudes towards news, an explanation\nthat follows almost by definition of the traditional dichotomy of financial\nmarket participants, investors versus speculators, whose behaviors are reduced\nto their simplest forms. Long-run investors' valuations of an asset are assumed\nto follow a news-driven random walk, thus capturing the investors' persistent,\nlong memory of fundamental news. Short-term speculators' anticipated returns,\non the other hand, are assumed to follow a news-driven autoregressive process,\ncapturing their shorter memory of fundamental news, and, by the same token, the\nfeedback intrinsic to the short-sighted, trend-following (or herding) mindset\nof speculators. These simple, linear, models of traders' expectations, it is\nshown, explain the two financial regularities in a generic and robust way.\nRational expectations, the dominant model of traders' expectations, is not\nassumed here, owing to the famous no-speculation, no-trade results\n"
    },
    {
        "paper_id": 2309.04947,
        "authors": "Joshua Zoen-Git Hiew, Tongseok Lim, Brendan Pass, and Marcelo Cruz de\n  Souza",
        "title": "Geometry of vectorial martingale optimal transport and robust option\n  pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper addresses robust finance, which is concerned with the development\nof models and approaches that account for market uncertainties. Specifically,\nwe investigate the Vectorial Martingale Optimal Transport (VMOT) problem, the\ngeometry of its solutions, and its application with robust option pricing\nproblems in finance. To this end, we consider two-period market models and show\nthat when the spatial dimension $d$ (the number of underlying assets) is 2, the\nextremal model for the cap option with a sub- or super-modular payout reduces\nto a single factor model in the first period, but not in general when $d > 2$.\nThe result demonstrates a subtle relationship between spatial dimension, cost\nfunction supermodularity, and their effect on the geometry of solutions to the\nVMOT problem. We investigate applications of the model to financial problems\nand demonstrate how the dimensional reduction caused by monotonicity can be\nused to improve existing computational methods.\n"
    },
    {
        "paper_id": 2309.05003,
        "authors": "Panpan Zhang, Zuo Quan Xu",
        "title": "Multidimensional indefinite stochastic Riccati equations and zero-sum\n  linear-quadratic stochastic differential games with non-markovian regime\n  switching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with two-player zero-sum linear-quadratic stochastic\ndifferential games in a regime switching model. The controlled inhomogeneous\nsystem coefficients depend on the underlying noises, so it is a non-Markovian\nregime switching model. Based on a new kind of multidimensional indefinite\nstochastic Riccati equation (SRE) and a multidimensional linear backward\nstochastic differential equation (BSDE) with unbounded coefficients, we can\nprovide optimal feedback control-strategy pairs for the two players in a\nclosed-loop form. The main contribution of this paper, which is important in\nits own right from the BSDE theory point of view, is to prove the existence and\nuniqueness of the new kind of multidimensional indefinite SRE. Interestingly,\nthe components of the solution can take positive, zero and negative values\nsimultaneously. We also obtain the corresponding optimal feedback\ncontrol-strategy pairs for homogeneous systems under closed convex cone control\nconstraints. Finally, these results are applied to portfolio selection problems\nwith different short-selling prohibition constraints in a regime switching\nmarket with random coefficients.\n"
    },
    {
        "paper_id": 2309.05054,
        "authors": "John Armstrong and Andrei Ionescu",
        "title": "Gamma Hedging and Rough Paths",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply rough-path theory to study the discrete-time gamma-hedging strategy.\nWe show that if a trader knows that the market price of a set of European\noptions will be given by a diffusive pricing model, then the discrete-time\ngamma-hedging strategy will enable them to replicate other European options so\nlong as the underlying pricing signal is sufficiently regular. This is a sure\nresult and does not require that the underlying pricing signal has a quadratic\nvariation corresponding to a probabilisitic pricing model. We show how to\ngeneralise this result to exotic derivatives when the gamma is defined to be\nthe Gubinelli derivative of the delta by deriving rough-path versions of the\nClark--Ocone formula which hold surely.\n  We illustrate our theory by proving that if the stock price process is\nsufficiently regular, as is the implied volatility process of a European\nderivative with maturity $T$ and smooth payoff $f(S_T)$ satisfying $f^{\\prime\n\\prime}>0$, one can replicate with certainty any European derivative with\nsmooth payoff and maturity $T$.\n"
    },
    {
        "paper_id": 2309.05512,
        "authors": "Tim Leung and Kevin W. Lu",
        "title": "Monte Carlo Simulation for Trading Under a L\\'evy-Driven Mean-Reverting\n  Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a Monte Carlo approach to pairs trading on mean-reverting spreads\nmodeled by L\\'evy-driven Ornstein-Uhlenbeck processes. Specifically, we focus\non using a variance gamma driving process, an infinite activity pure jump\nprocess to allow for more flexible models of the price spread than is available\nin the classical model. However, this generalization comes at the cost of not\nhaving analytic formulas, so we apply Monte Carlo methods to determine optimal\ntrading levels and develop a variance reduction technique using control\nvariates. Within this framework, we numerically examine how the optimal trading\nstrategies are affected by the parameters of the model. In addition, we extend\nour method to bivariate spreads modeled using a weak variance alpha-gamma\ndriving process, and explore the effect of correlation on these trades.\n"
    },
    {
        "paper_id": 2309.0556,
        "authors": "Paul Glasserman, Harry Mamaysky, Jimmy Qin",
        "title": "New News is Bad News",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An increase in the novelty of news predicts negative stock market returns and\nnegative macroeconomic outcomes over the next year. We quantify news novelty -\nchanges in the distribution of news text - through an entropy measure,\ncalculated using a recurrent neural network applied to a large news corpus.\nEntropy is a better out-of-sample predictor of market returns than a collection\nof standard measures. Cross-sectional entropy exposure carries a negative risk\npremium, suggesting that assets that positively covary with entropy hedge the\naggregate risk associated with shifting news language. Entropy risk cannot be\nexplained by existing long-short factors.\n"
    },
    {
        "paper_id": 2309.05682,
        "authors": "Paul Bilokon and Oleksandr Bilokon and Saeed Amen",
        "title": "A compendium of data sources for data science, machine learning, and\n  artificial intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in data science, machine learning, and artificial\nintelligence, such as the emergence of large language models, are leading to an\nincreasing demand for data that can be processed by such models. While data\nsources are application-specific, and it is impossible to produce an exhaustive\nlist of such data sources, it seems that a comprehensive, rather than complete,\nlist would still benefit data scientists and machine learning experts of all\nlevels of seniority. The goal of this publication is to provide just such an\n(inevitably incomplete) list -- or compendium -- of data sources across\nmultiple areas of applications, including finance and economics, legal (laws\nand regulations), life sciences (medicine and drug discovery), news sentiment\nand social media, retail and ecommerce, satellite imagery, and shipping and\nlogistics, and sports.\n"
    },
    {
        "paper_id": 2309.05783,
        "authors": "Jackson P. Lautier",
        "title": "A New Framework to Estimate Return on Investment for Player Salaries in\n  the National Basketball Association",
        "comments": "39 pages, 5 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The National Basketball Association (NBA) imposes a player salary cap. It is\ntherefore useful to develop tools to measure the relative realized return of a\nplayer's salary given their on court performance. Very few such studies exist,\nhowever. We thus present the first known framework to estimate a return on\ninvestment (ROI) for NBA player contracts. The framework operates in five\nparts: (1) decide on a measurement time horizon, such as the standard 82-game\nNBA regular season; (2) calculate the novel game contribution percentage (GCP)\nmeasure we propose, which is a single game summary statistic that sums to unity\nfor each competing team and is comprised of traditional, playtype, hustle, box\nouts, defensive, tracking, and rebounding per game NBA statistics; (3) estimate\nthe single game value (SGV) of each regular season NBA game using a standard\ncurrency conversion calculation; (4) multiply the SGV by the vector of realized\nGCPs to obtain a series of realized per-player single season cash flows; and\n(5) use the player salary as an initial investment to perform the traditional\nROI calculation. We illustrate our framework by compiling a novel, sharable\ndataset of per game GCP statistics and salaries for the 2022-2023 NBA regular\nseason. A scatter plot of ROI by salary for all players is presented, including\nthe top and bottom 50 performers. Notably, missed games are treated as defaults\nbecause GCP is a per game metric. This allows for break-even calculations\nbetween high-performing players with frequent missed games and average\nperformers with few missed games, which we demonstrate with a comparison of the\n2023 NBA regular seasons of Anthony Davis and Brook Lopez. We conclude by\nsuggesting uses of our framework, discussing its flexibility through\ncustomization, and outlining potential future improvements.\n"
    },
    {
        "paper_id": 2309.05866,
        "authors": "Gabriele Torri, Rosella Giacometti, Darinka Dentcheva, Svetlozar T.\n  Rachev, W. Brent Lindquist",
        "title": "ESG-coherent risk measures for sustainable investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growing interest in sustainable investing calls for an axiomatic approach\nto measures of risk and reward that focus not only on financial returns, but\nalso on measures of environmental and social sustainability, i.e.\nenvironmental, social, and governance (ESG) scores. We propose definitions for\nESG-coherent risk measures and ESG reward-risk ratios based on functions of\nbivariate random variables that are applied to financial returns and ESG\nscores, extending the traditional univariate measures to the ESG case. We\nprovide examples and present an empirical analysis in which the ESG-coherent\nrisk measures and ESG reward-risk ratios are used to rank stocks.\n"
    },
    {
        "paper_id": 2309.05926,
        "authors": "Igor Halperin",
        "title": "SCOP: Schrodinger Control Optimal Planning for Goal-Based Wealth\n  Management",
        "comments": "34 pages, 17 figures, 3 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimization of contributions of a financial\nplanner such as a working individual towards a financial goal such as\nretirement. The objective of the planner is to find an optimal and feasible\nschedule of periodic installments to an investment portfolio set up towards the\ngoal. Because portfolio returns are random, the practical version of the\nproblem amounts to finding an optimal contribution scheme such that the goal is\nsatisfied at a given confidence level. This paper suggests a semi-analytical\napproach to a continuous-time version of this problem based on a controlled\nbackward Kolmogorov equation (BKE) which describes the tail probability of the\nterminal wealth given a contribution policy. The controlled BKE is solved\nsemi-analytically by reducing it to a controlled Schrodinger equation and\nsolving the latter using an algebraic method. Numerically, our approach amounts\nto finding semi-analytical solutions simultaneously for all values of control\nparameters on a small grid, and then using the standard two-dimensional spline\ninterpolation to simultaneously represent all satisficing solutions of the\noriginal plan optimization problem. Rather than being a point in the space of\ncontrol variables, satisficing solutions form continuous contour lines\n(efficient frontiers) in this space.\n"
    },
    {
        "paper_id": 2309.05935,
        "authors": "Abhijit Chakraborty, Tetsuo Hatsuda and Yuichi Ikeda",
        "title": "Dynamic relationship between XRP price and correlation tensor spectra of\n  the transaction network",
        "comments": "Main text: 11 pages, 10 figures. Supplementary information: 3 pages,\n  3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The emergence of cryptoassets has sparked a paradigm shift in the world of\nfinance and investment, ushering in a new era of digital assets with profound\nimplications for the future of currency and asset management. A recent study\nshowed that during the bubble period around the year, 2018, the price of\ncryptoasset, XRP has a strong anti correlation with the largest singular values\nof the correlation tensors obtained from the weekly XRP transaction networks.\nIn this study, we provide a detailed analysis of the method of correlation\ntensor spectra for XRP transaction networks. We calculate and compare the\ndistribution of the largest singular values of the correlation tensor using the\nrandom matrix theory with the largest singular values of the empirical\ncorrelation tensor. We investigate the correlation between the XRP price and\nthe largest singular values for a period spanning two years. We also uncover\nthe distinct dependence between XRP price and the singular values for bubble\nand non-bubble periods. The significance of time evolution of singular values\nis shown by comparison with the evolution of singular values of the reshuffled\ncorrelation tensor. Furthermore, we identify a set of driver nodes in the\ntransaction networks that drives the market during the bubble period using the\nsingular vectors.\n"
    },
    {
        "paper_id": 2309.05977,
        "authors": "Hanwen Zhang and Duy-Minh Dang",
        "title": "A monotone numerical integration method for mean-variance portfolio\n  optimization under jump-diffusion models",
        "comments": "38 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a efficient, easy-to-implement, and strictly monotone numerical\nintegration method for Mean-Variance (MV) portfolio optimization in realistic\ncontexts, which involve jump-diffusion dynamics of the underlying controlled\nprocesses, discrete rebalancing, and the application of investment constraints,\nnamely no-bankruptcy and leverage. A crucial element of the MV portfolio\noptimization formulation over each rebalancing interval is a convolution\nintegral, which involves a conditional density of the logarithm of the amount\ninvested in the risky asset. Using a known closed-form expression for the\nFourier transform of this density, we derive an infinite series representation\nfor the conditional density where each term is strictly positive and explicitly\ncomputable. As a result, the convolution integral can be readily approximated\nthrough a monotone integration scheme, such as a composite quadrature rule\ntypically available in most programming languages. The computational complexity\nof our method is an order of magnitude lower than that of existing monotone\nfinite difference methods. To further enhance efficiency, we propose an\nimplementation of the scheme via Fast Fourier Transforms, exploiting the\nToeplitz matrix structure. The proposed monotone scheme is proven to be both\n$\\ell_{\\infty}$-stable and pointwise consistent, and we rigorously establish\nits pointwise convergence to the unique solution of the MV portfolio\noptimization problem. We also intuitively demonstrate that, as the rebalancing\ntime interval approaches zero, the proposed scheme converges to a continuously\nobserved impulse control formulation for MV optimization expressed as a\nHamilton-Jacobi-Bellman Quasi-Variational Inequality. Numerical results show\nremarkable agreement with benchmark solutions obtained through finite\ndifferences and Monte Carlo simulation, underscoring the effectiveness of our\napproach.\n"
    },
    {
        "paper_id": 2309.06353,
        "authors": "Aditya Deeti",
        "title": "The Conundrum of the Pension System in India: A Comprehensive study in\n  the context of India's Growth Story",
        "comments": "5 pages, 3 figures. \"Published with International Research Journal of\n  Economics and Management Studies (IRJEMS)\"",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol. 2, No. 3, pp. 359-363, 2023",
        "doi": "10.56472/25835238/IRJEMS-V2I3P148",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  India is the largest democracy in the world and has recently surpassed China\nto be the highest-populated country, with an estimated 1.425 billion\n(approximately 18% of the world population). Moreover, India's elderly\npopulation is projected to increase to 138 million by 2035. Indian economy is\nalready reeling under the pressure of exorbitant pension liabilities of the\ngovernment for existing pensioners. As such, India has introduced a National\nPension System (NPS), which is a Defined Contribution Scheme for employees\njoining government service on or after 1st January 2004, bidding adieu to the\nage-old, tried and tested Old Pension System (OPS) which is a Direct Benefit\nScheme, in vogue in India since the British Raj. This is an epoch-making move\nby the government as it seeks to inculcate Disciplined Saving among the people\nwhile significantly reducing the government burden by reducing the Pension\nLiabilities of the Central and State Governments. This paper aims to analyse\nvarious features and intricacies of the NPS and address the claims of various\nstakeholders like the Central Government, State Government, Employees,\nPensioners, etc. In light of the above, and taking cognisance of the fact that\nmany states such as Rajasthan, Chattisgarh, Jharkhand, etc, have exited the NPS\nscheme and have sought back their share of NPS employee and employer\ncontribution, this study is relevant to address the current economic and fiscal\nissues of India to propel towards the ambitious goal of becoming a $ 5 trillion\ndollar economy by 2025.\n  Keywords: Old Pension Scheme (OPS), National Pension System (NPS), Direct\nBenefit Scheme, Defined Contribution Scheme, Pension Liabilities.\n"
    },
    {
        "paper_id": 2309.06393,
        "authors": "Yutong Chen, Paul Bilokon, Conan Hales, Laura Kerr",
        "title": "Real-time VaR Calculations for Crypto Derivatives in kdb+/q",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrency market is known for exhibiting significantly higher volatility\nthan traditional asset classes. Efficient and adequate risk calculation is\nvital for managing risk exposures in such market environments where extreme\nprice fluctuations occur in short timeframes. The objective of this thesis is\nto build a real-time computation workflow that provides VaR estimates for\nnon-linear portfolios of cryptocurrency derivatives. Many researchers have\nexamined the predictive capabilities of time-series models within the context\nof cryptocurrencies. In this work, we applied three commonly used models -\nEMWA, GARCH and HAR - to capture and forecast volatility dynamics, in\nconjunction with delta-gamma-theta approach and Cornish-Fisher expansion to\ncrypto derivatives, examining their performance from the perspectives of\ncalculation efficiency and accuracy. We present a calculation workflow which\nharnesses the information embedded in high-frequency market data and the\ncomputation simplicity inherent in analytical estimation procedures. This\nworkflow yields reasonably robust VaR estimates with calculation latencies on\nthe order of milliseconds.\n"
    },
    {
        "paper_id": 2309.06538,
        "authors": "Mario Mitsuo Akita, Everton Josue da Silva",
        "title": "Desenvolvimento de modelo para predi\\c{c}\\~ao de cota\\c{c}\\~oes de\n  a\\c{c}\\~ao baseada em an\\'alise de sentimentos de tweets",
        "comments": "in Portuguese language, Presented at: 1o Semin\\'ario de Ci\\^encia de\n  Dados do IFSP. Campinas: 2023",
        "journal-ref": "Anais do 1o Semin\\'ario de Ci\\^encia de Dados do IFSP. Campinas:\n  2023. p. 51 - 58",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Training machine learning models for predicting stock market share prices is\nan active area of research since the automatization of trading such papers was\navailable in real time. While most of the work in this field of research is\ndone by training Neural networks based on past prices of stock shares, in this\nwork, we use iFeel 2.0 platform to extract 19 sentiment features from posts\nobtained from microblog platform Twitter that mention the company Petrobras.\nThen, we used those features to train XBoot models to predict future stock\nprices for the referred company. Later, we simulated the trading of Petrobras'\nshares based on the model's outputs and determined the gain of R$88,82 (net) in\na 250-day period when compared to a 100 random models' average performance.\n"
    },
    {
        "paper_id": 2309.06559,
        "authors": "Luke Sanborn and Matthew Sahagun",
        "title": "Media Moments and Corporate Connections: A Deep Learning Approach to\n  Stock Movement Classification",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial industry poses great challenges with risk modeling and profit\ngeneration. These entities are intricately tied to the sophisticated prediction\nof stock movements. A stock forecaster must untangle the randomness and\never-changing behaviors of the stock market. Stock movements are influenced by\na myriad of factors, including company history, performance, and\neconomic-industry connections. However, there are other factors that aren't\ntraditionally included, such as social media and correlations between stocks.\nSocial platforms such as Reddit, Facebook, and X (Twitter) create opportunities\nfor niche communities to share their sentiment on financial assets. By\naggregating these opinions from social media in various mediums such as posts,\ninterviews, and news updates, we propose a more holistic approach to include\nthese \"media moments\" within stock market movement prediction. We introduce a\nmethod that combines financial data, social media, and correlated stock\nrelationships via a graph neural network in a hierarchical temporal fashion.\nThrough numerous trials on current S&P 500 index data, with results showing an\nimprovement in cumulative returns by 28%, we provide empirical evidence of our\ntool's applicability for use in investment decisions.\n"
    },
    {
        "paper_id": 2309.06711,
        "authors": "J\\'er\\^ome Busca and L\\'eon Thomir",
        "title": "Epps Effect and the Signature of Short-Term Momentum Traders",
        "comments": "9 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is a well-documented fact that the correlation function of the returns on\ntwo \"related\" assets is generally increasing as a function of the horizon $h$\nof these returns. This phenomenon, termed the Epps Effect, holds true in a wide\nvariety of markets, and there is a large body of literature devoted to its\ntheoretical justification. Our focus here is to describe and understand a\ndeviation to the Epps effect, observed in the context of the foreign exchange\nand cryptocurrency markets. Specifically, we document a sharp local maximum of\nthe cross-correlation function of returns on the Euro EUR/USD and Bitcoin\nBTC/USD pairs as a function of $h$. Our claim is that this anomaly reveals the\nactivity of short-term momentum traders.\n"
    },
    {
        "paper_id": 2309.06875,
        "authors": "Fabian Braesemann, Moritz Marpe",
        "title": "How to foster innovation in the social sciences? Qualitative evidence\n  from focus group workshops at Oxford University",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This report addresses challenges and opportunities for innovation in the\nsocial sciences at the University of Oxford. It summarises findings from two\nfocus group workshops with innovation experts from the University ecosystem.\nExperts included successful social science entrepreneurs and professional\nservice staff from the University. The workshops focused on four different\ndimensions related to innovative activities and commercialisation. The findings\nshow several challenges at the institutional and individual level, together\nwith features of the social scientific discipline that impede more innovation\nin the social sciences. Based on identifying these challenges, we present\npotential solutions and ways forward identified in the focus group discussions\nto foster social science innovation. The report aims to illustrate the\npotential of innovation and commercialisation of social scientific research for\nboth researchers and the university.\n"
    },
    {
        "paper_id": 2309.06885,
        "authors": "Christopher A. Hartwell and Paul M. Vaaler",
        "title": "The Price of Empire: Unrest Location and Sovereign Risk in Tsarist\n  Russia",
        "comments": "13 Tables, 8 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Research on politically motivated unrest and sovereign risk overlooks whether\nand how unrest location matters for sovereign risk in geographically extensive\nstates. Intuitively, political violence in the capital or nearby would seem to\ndirectly threaten the state's ability to pay its debts. However, it is possible\nthat the effect on a government could be more pronounced the farther away the\nviolence is, connected to the longer-term costs of suppressing rebellion. We\nuse Tsarist Russia to assess these differences in risk effects when unrest\noccurs in Russian homeland territories versus more remote imperial territories.\nOur analysis of unrest events across the Russian imperium from 1820 to 1914\nsuggests that unrest increases risk more in imperial territories. Echoing\ncurrent events, we find that unrest in Ukraine increases risk most. The price\nof empire included higher costs in projecting force to repress unrest and\nretain the confidence of the foreign investors financing those costs.\n"
    },
    {
        "paper_id": 2309.06949,
        "authors": "Joao Ricardo Faria, Laudo Ogura, Mauricio Prado, and Christopher J.\n  Boudreaux",
        "title": "Government Investments and Entrepreneurship",
        "comments": "3 figures and 1 table. 27 pages",
        "journal-ref": null,
        "doi": "10.1007/s11187-023-00743-9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How can governments attract entrepreneurs and their businesses? The view that\nnew business creation grows with the optimal level of government investments\nremains appealing to policymakers. In contrast with this active approach, we\nbuild a model where governments may adopt a passive approach to stimulating\nbusiness creation. The insights from this model suggest new business creation\ndepends positively on factors beyond government investments--attracting\nhigh-skilled migrants to the region and lower property prices, taxes, and fines\non firms in the informal sector. These findings suggest whether entrepreneurs\ngenerate business creation in the region does not only depend on government\ninvestments. It also depends on location and skilled migration. Our model also\nprovides methodological implications--the relationship between government\ninvestments and new business creation is endogenously determined, so unless\nadjustments are made, econometric estimates will be biased and inconsistent. We\nconclude with policy and managerial implications.\n"
    },
    {
        "paper_id": 2309.07023,
        "authors": "Christian Bayer, Simon Breneis",
        "title": "Weak Markovian Approximations of Rough Heston",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rough Heston model is a very popular recent model in mathematical\nfinance; however, the lack of Markov and semimartingale properties poses\nsignificant challenges in both theory and practice. A way to resolve this\nproblem is to use Markovian approximations of the model. Several previous works\nhave shown that these approximations can be very accurate even when the number\nof additional factors is very low. Existing error analysis is largely based on\nthe strong error, corresponding to the $L^2$ distance between the kernels.\nExtending earlier results by [Abi Jaber and El Euch, SIAM Journal on Financial\nMathematics 10(2):309--349, 2019], we show that the weak error of the Markovian\napproximations can be bounded using the $L^1$-error in the kernel approximation\nfor general classes of payoff functions for European style options. Moreover,\nwe give specific Markovian approximations which converge super-polynomially in\nthe number of dimensions, and illustrate their numerical superiority in option\npricing compared to previously existing approximations. The new approximations\nalso work for the hyper-rough case $H > -1/2$.\n"
    },
    {
        "paper_id": 2309.0716,
        "authors": "Saadet Yagmur Kumcu",
        "title": "The effect of housewife labor on gdp calculations",
        "comments": "Doctoral Thesis. Manisa Celal Bayar University, Manisa/Turkey",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, the evolutionary development of labor has been tried to be\nrevealed based on theoretical analysis. Using the example of gdp, which is an\nindicator of social welfare, the economic value of the labor of housewives was\ntried to be measured with an empirical modeling. To this end; first of all, the\nconcept of labor was questioned in orthodox (mainstream) economic theories;\nthen, by abstracting from the labor-employment relationship, it was examined\nwhat effect the labor of unpaid housewives who are unemployed in the capitalist\nsystem could have on gdp. In theoretical analysis; It has been determined that\nthe changing human profile moves away from rationality and creates limited\nrationality and, accordingly, a heterogeneous individual profile. Women were\ndefined as the new example of heterogeneous individuals, as those who best fit\nthe definition of limited rational individuals because they prefer to be\nhousewives. In the empirical analysis of the study, housewife labor was taken\ninto account as the main variable. In the empirical analysis of the study; In\nthe case of Turkiye, using turkstat employment data and the atkinson inequality\nscale; the impact of housewife labor on gdp was calculated. The results of the\ntheoretical and empirical analysis were evaluated in the context of\nlabor-employment independence.\n"
    },
    {
        "paper_id": 2309.07371,
        "authors": "Venance Riblier",
        "title": "The Fiscal Cost of Public Debt and Government Spending Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates how the cost of public debt shapes fiscal policy and\nits effect on the economy. Using U.S. historical data, I show that when\nservicing the debt creates a fiscal burden, the government responds to spending\nshocks by limiting debt issuance. As a result, the initial shock triggers only\na limited increase in public spending in the short run, and even leads to\nspending reversal in the long run. Under these conditions, fiscal policy loses\nits ability to stimulate economic activity. This outcome arises as the fiscal\nauthority limits its own ability to borrow to ensure public debt\nsustainability. These findings are robust to several identification and\nestimation strategies.\n"
    },
    {
        "paper_id": 2309.07427,
        "authors": "Wei James Chen, Meng-Jhang Fong, Po-Hsuan Lin",
        "title": "Measuring Higher-Order Rationality with Belief Control",
        "comments": "The experimental design and the analysis plan are pre-registered on\n  Open Science Framework (https://osf.io/gye4u/). The experimental instructions\n  can be found at https://mjfong.github.io/SI_MHOR_final.pdf",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Determining an individual's strategic reasoning capability based solely on\nchoice data is a complex task. This complexity arises because sophisticated\nplayers might have non-equilibrium beliefs about others, leading to\nnon-equilibrium actions. In our study, we pair human participants with computer\nplayers known to be fully rational. This use of robot players allows us to\ndisentangle limited reasoning capacity from belief formation and social biases.\nOur results show that, when paired with robots, subjects consistently\ndemonstrate higher levels of rationality and maintain stable rationality levels\nacross different games compared to when paired with humans. This suggests that\nstrategic reasoning might indeed be a consistent trait in individuals.\nFurthermore, the identified rationality limits could serve as a measure for\nevaluating an individual's strategic capacity when their beliefs about others\nare adequately controlled.\n"
    },
    {
        "paper_id": 2309.07488,
        "authors": "Michael Preisel",
        "title": "Long-Term Mean-Variance Optimization Under Mean-Reverting Equity Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the mean-variance optimal portfolio choice of an investor\npre-committed to a deterministic investment policy in continuous time in a\nmarket with mean-reversion in the risk-free rate and the equity risk-premium.\nIn the tradition of Markowitz, optimal policies are restricted to a subclass of\nfactor exposures in which losses cannot exceed initial capital and it is shown\nthat the optimal policy is characterized by an Euler-Lagrange equation derived\nby the method of Calculus of Variations. It is a main result, that the\nEuler-Lagrange equation can be recast into a matrix differential equation by an\nintegral transformation of the factor exposure and that the solution to the\ncharacteristic equation can be parametrized by the eigenvalues of the\nassociated lambda-matrix, hence, the optimization problem is equivalent to a\nspectral problem. Finally, explicit solutions to the optimal policy are\nprovided by application of suitable boundary conditions and it is demonstrated\nthat - if in fact the equity risk-premium is slowly mean-reverting - then\ninvestors committing to long investment horizons realize better risk-return\ntrade-offs than investors with shorter investment horizons.\n"
    },
    {
        "paper_id": 2309.07664,
        "authors": "Louis Lippens",
        "title": "Computer says 'no': Exploring systemic bias in ChatGPT using an audit\n  approach",
        "comments": "39 pages, 2 tables, 4 figures; for data and supplementary tables, see\n  https://osf.io/vezt7",
        "journal-ref": null,
        "doi": "10.1016/j.chbah.2024.100054",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large language models offer significant potential for increasing labour\nproductivity, such as streamlining personnel selection, but raise concerns\nabout perpetuating systemic biases embedded into their pre-training data. This\nstudy explores the potential ethnic and gender bias of ChatGPT, a chatbot\nproducing human-like responses to language tasks, in assessing job applicants.\nUsing the correspondence audit approach from the social sciences, I simulated a\nCV screening task with 34,560 vacancy-CV combinations where the chatbot had to\nrate fictitious applicant profiles. Comparing ChatGPT's ratings of Arab, Asian,\nBlack American, Central African, Dutch, Eastern European, Hispanic, Turkish,\nand White American male and female applicants, I show that ethnic and gender\nidentity influence the chatbot's evaluations. Ethnic discrimination is more\npronounced than gender discrimination and mainly occurs in jobs with favourable\nlabour conditions or requiring greater language proficiency. In contrast,\ngender discrimination emerges in gender-atypical roles. These findings suggest\nthat ChatGPT's discriminatory output reflects a statistical mechanism echoing\nsocietal stereotypes. Policymakers and developers should address systemic bias\nin language model-driven applications to ensure equitable treatment across\ndemographic groups. Practitioners should practice caution, given the adverse\nimpact these tools can (re)produce, especially in selection decisions involving\nhumans.\n"
    },
    {
        "paper_id": 2309.07667,
        "authors": "Solveig Flaig, Gero Junike",
        "title": "Profit and loss attribution: An empirical study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The profit and loss (p&l) attrition for each business year into different\nrisk or risk factors (e.g., interest rates, credit spreads, foreign exchange\nrate etc.) is a regulatory requirement, e.g., under Solvency 2. Three different\ndecomposition principles are prevalent: one-at-a-time (OAT), sequential\nupdating (SU) and average sequential updating (ASU) decompositions. In this\nresearch, using financial market data from 2003 to 2022, we demonstrate that\nthe OAT decomposition can generate significant unexplained p&l and that the SU\ndecompositions depends significantly on the order or labeling of the risk\nfactors. On the basis of an investment in a foreign stock, we further explain\nthat the SU decomposition is not able to identify all relevant risk factors.\nThis potentially effects the hedging strategy of the portfolio manager. In\nconclusion, we suggest to use the ASU decomposition in practice.\n"
    },
    {
        "paper_id": 2309.07708,
        "authors": "Haochong Xia, Shuo Sun, Xinrun Wang, Bo An",
        "title": "Market-GAN: Adding Control to Financial Market Data Generation with\n  Semantic Context",
        "comments": "Accepted to the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI24), Vancouver, British Columbia, 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial simulators play an important role in enhancing forecasting\naccuracy, managing risks, and fostering strategic financial decision-making.\nDespite the development of financial market simulation methodologies, existing\nframeworks often struggle with adapting to specialized simulation context. We\npinpoint the challenges as i) current financial datasets do not contain context\nlabels; ii) current techniques are not designed to generate financial data with\ncontext as control, which demands greater precision compared to other\nmodalities; iii) the inherent difficulties in generating context-aligned,\nhigh-fidelity data given the non-stationary, noisy nature of financial data. To\naddress these challenges, our contributions are: i) we proposed the Contextual\nMarket Dataset with market dynamics, stock ticker, and history state as\ncontext, leveraging a market dynamics modeling method that combines linear\nregression and Dynamic Time Warping clustering to extract market dynamics; ii)\nwe present Market-GAN, a novel architecture incorporating a Generative\nAdversarial Networks (GAN) for the controllable generation with context, an\nautoencoder for learning low-dimension features, and supervisors for knowledge\ntransfer; iii) we introduce a two-stage training scheme to ensure that\nMarket-GAN captures the intrinsic market distribution with multiple objectives.\nIn the pertaining stage, with the use of the autoencoder and supervisors, we\nprepare the generator with a better initialization for the adversarial training\nstage. We propose a set of holistic evaluation metrics that consider alignment,\nfidelity, data usability on downstream tasks, and market facts. We evaluate\nMarket-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and\nshowcase superior performance in comparison to 4 state-of-the-art time-series\ngenerative models.\n"
    },
    {
        "paper_id": 2309.07843,
        "authors": "Abir Sridi and Paul Bilokon",
        "title": "Applying Deep Learning to Calibrate Stochastic Volatility Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility models, where the volatility is a stochastic process,\ncan capture most of the essential stylized facts of implied volatility surfaces\nand give more realistic dynamics of the volatility smile/skew. However, they\ncome with the significant issue that they take too long to calibrate.\n  Alternative calibration methods based on Deep Learning (DL) techniques have\nbeen recently used to build fast and accurate solutions to the calibration\nproblem. Huge and Savine developed a Differential Machine Learning (DML)\napproach, where Machine Learning models are trained on samples of not only\nfeatures and labels but also differentials of labels to features. The present\nwork aims to apply the DML technique to price vanilla European options (i.e.\nthe calibration instruments), more specifically, puts when the underlying asset\nfollows a Heston model and then calibrate the model on the trained network. DML\nallows for fast training and accurate pricing. The trained neural network\ndramatically reduces Heston calibration's computation time.\n  In this work, we also introduce different regularisation techniques, and we\napply them notably in the case of the DML. We compare their performance in\nreducing overfitting and improving the generalisation error. The DML\nperformance is also compared to the classical DL (without differentiation) one\nin the case of Feed-Forward Neural Networks. We show that the DML outperforms\nthe DL.\n  The complete code for our experiments is provided in the GitHub repository:\nhttps://github.com/asridi/DML-Calibration-Heston-Model\n"
    },
    {
        "paper_id": 2309.08175,
        "authors": "Ying-Li Wang, Cheng-Long Xu, Ping He",
        "title": "A Markovian empirical model for the VIX index and the pricing of the\n  corresponding derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose an empirical model for the VIX index. Our findings\nindicate that the VIX has a long-term empirical distribution. To model its\ndynamics, we utilize a continuous-time Markov process with a uniform\ndistribution as its invariant distribution and a suitable function $h$. We\ndetermined that $h$ is the inverse function of the VIX data's empirical\ndistribution. Additionally, we use the method of variables of separation to get\nthe exact solution to the pricing problem for VIX futures and call options.\n"
    },
    {
        "paper_id": 2309.08287,
        "authors": "Jiefei Yang, Guanglian Li",
        "title": "On Sparse Grid Interpolation for American Option Pricing with Multiple\n  Underlying Assets",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we develop a novel efficient quadrature and sparse grid based\npolynomial interpolation method to price American options with multiple\nunderlying assets. The approach is based on first formulating the pricing of\nAmerican options using dynamic programming, and then employing static sparse\ngrids to interpolate the continuation value function at each time step. To\nachieve high efficiency, we first transform the domain from $\\mathbb{R}^d$ to\n$(-1,1)^d$ via a scaled tanh map, and then remove the boundary singularity of\nthe resulting multivariate function over $(-1,1)^d$ by a bubble function and\nsimultaneously, to significantly reduce the number of interpolation points. We\nrigorously establish that with a proper choice of the bubble function, the\nresulting function has bounded mixed derivatives up to a certain order, which\nprovides theoretical underpinnings for the use of sparse grids. Numerical\nexperiments for American arithmetic and geometric basket put options with the\nnumber of underlying assets up to 16 are presented to validate the\neffectiveness of the approach.\n"
    },
    {
        "paper_id": 2309.08431,
        "authors": "\\'Alvaro Cartea, Fay\\c{c}al Drissi, Marcello Monga",
        "title": "Decentralised Finance and Automated Market Making: Predictable Loss and\n  Optimal Liquidity Provision",
        "comments": "Forthcoming in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Constant product markets with concentrated liquidity (CL) are the most\npopular type of automated market makers. In this paper, we characterise the\ncontinuous-time wealth dynamics of strategic LPs who dynamically adjust their\nrange of liquidity provision in CL pools. Their wealth results from fee income,\nthe value of their holdings in the pool, and rebalancing costs. Next, we derive\na self-financing and closed-form optimal liquidity provision strategy where the\nwidth of the LP's liquidity range is determined by the profitability of the\npool (provision fees minus gas fees), the predictable losses (PL) of the LP's\nposition, and concentration risk. Concentration risk refers to the decrease in\nfee revenue if the marginal exchange rate (akin to the midprice in a limit\norder book) in the pool exits the LP's range of liquidity. When the drift in\nthe marginal rate is stochastic, we show how to optimally skew the range of\nliquidity to increase fee revenue and profit from the expected changes in the\nmarginal rate. Finally, we use Uniswap v3 data to show that, on average, LPs\nhave traded at a significant loss, and to show that the out-of-sample\nperformance of our strategy is superior to the historical performance of LPs in\nthe pool we consider.\n"
    },
    {
        "paper_id": 2309.08652,
        "authors": "Sergio Caprioli, Emanuele Cagliero, Riccardo Crupi",
        "title": "Quantifying Credit Portfolio sensitivity to asset correlations with\n  interpretable generative neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this research, we propose a novel approach for the quantification of\ncredit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the\nuse of synthetic financial correlation matrices generated with deep learning\nmodels. In previous work Generative Adversarial Networks (GANs) were employed\nto demonstrate the generation of plausible correlation matrices, that capture\nthe essential characteristics observed in empirical correlation matrices\nestimated on asset returns. Instead of GANs, we employ Variational Autoencoders\n(VAE) to achieve a more interpretable latent space representation. Through our\nanalysis, we reveal that the VAE latent space can be a useful tool to capture\nthe crucial factors impacting portfolio diversification, particularly in\nrelation to credit portfolio sensitivity to asset correlations changes.\n"
    },
    {
        "paper_id": 2309.088,
        "authors": "Yichi Zhang, Mihai Cucuringu, Alexander Y. Shestopaloff, Stefan Zohren",
        "title": "Dynamic Time Warping for Lead-Lag Relationships in Lagged Multi-Factor\n  Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2305.06704",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In multivariate time series systems, lead-lag relationships reveal\ndependencies between time series when they are shifted in time relative to each\nother. Uncovering such relationships is valuable in downstream tasks, such as\ncontrol, forecasting, and clustering. By understanding the temporal\ndependencies between different time series, one can better comprehend the\ncomplex interactions and patterns within the system. We develop a\ncluster-driven methodology based on dynamic time warping for robust detection\nof lead-lag relationships in lagged multi-factor models. We establish\nconnections to the multireference alignment problem for both the homogeneous\nand heterogeneous settings. Since multivariate time series are ubiquitous in a\nwide range of domains, we demonstrate that our algorithm is able to robustly\ndetect lead-lag relationships in financial markets, which can be subsequently\nleveraged in trading strategies with significant economic benefits.\n"
    },
    {
        "paper_id": 2309.08855,
        "authors": "Sumaiya Binta Islam, Laboni Mondal",
        "title": "An Empirical Analysis on Remittances and Financial Development in Latin\n  American Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Remittances have become one of the driving forces of development for\ncountries all over the world, especially in lower-middle-income nations. This\npaper empirically investigates the association between remittance flows and\nfinancial development in 4 lower-middle-income countries of Latin America. By\nusing a panel data set from 1996 to 2019, the study revealed that remittances\nand financial development are positively associated in these countries. The\nstudy also discovered that foreign direct investment and inflation were\npositively correlated with financial development while trade openness had a\nnegative association with financial development. Therefore, policymakers of\nthese countries should implement and formulate such policies so that migrant\nworkers would have the incentives to send money through formal channels, which\nwill augment the effect of remittances on the recipient country.\n"
    },
    {
        "paper_id": 2309.09094,
        "authors": "S. M. Masrur Ahmed",
        "title": "Sizing Strategies for Algorithmic Trading in Volatile Markets: A Study\n  of Backtesting and Risk Mitigation Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Backtest is a way of financial risk evaluation which helps to analyze how our\ntrading algorithm would work in markets with past time frame. The high\nvolatility situation has always been a critical situation which creates\nchallenges for algorithmic traders. The paper investigates different models of\nsizing in financial trading and backtest to high volatility situations to\nunderstand how sizing models can lower the models of VaR during crisis events.\nHence it tries to show that how crisis events with high volatility can be\ncontrolled using short and long positional size. The paper also investigates\nstocks with AR, ARIMA, LSTM, GARCH with ETF data.\n"
    },
    {
        "paper_id": 2309.09176,
        "authors": "Tomohiro Uchiyama",
        "title": "Odd period cycles and ergodic properties in price dynamics for an\n  exchange economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the first part of this paper (Sections 1-4), we study a standard exchange\neconomy model with Cobb-Douglas type consumers and give a necessary and\nsufficient condition for the existence of an odd period cycle in the\nWalras-Samuelson (tatonnement) price adjustment process. We also give a\nsufficient condition for a price to be eventually attracted to a chaotic\nregion. In the second part (Sections 5 and 6), we investigate ergodic\nproperties of the price dynamics showing that the existence of chaos is not\nnecessarily bad. (The future is still predictable on average.) Moreover,\nsupported by a celebrated work of Avila et al. (Invent. Math., 2003), we\nconduct a sensitivity analysis to investigate a relationship between the\nergodic sum (of prices) and the speed of price adjustment. We believe that our\nmethods in this paper can be used to analyse many other chaotic economic\nmodels.\n"
    },
    {
        "paper_id": 2309.09178,
        "authors": "Suryadeepto Nag",
        "title": "Does Reliable Electricity Mean Lesser Agricultural Labor Wages? Evidence\n  from Indian Villages",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a panel of 1,171 villages in rural India that were surveyed in the\nIndia Human Development Surveys, I perform a difference-in-differences analysis\nto find that improvements in electricity reliability have a negative effect on\nthe increase in casual agricultural labor wage rates. Changes in men's wage\nrates are found to be affected more adversely than women's, resulting in a\nsmaller widening of the gender wage gap. I find that better electricity\nreliability reduces the time spent by women in fuel collection substantially\nwhich could potentially increase labor supply. The demand for labor remains\nunaffected by reliability, which could lead the surplus in labor supply to\ncause wage rates to stunt. However, I show that electrical appliances such as\ngroundwater pumps considerably increase labor demand indicating that\ngovernments could target increasing the adoption of electric pumps along with\nbettering the quality of electricity to absorb the surplus labor into\nagriculture.\n"
    },
    {
        "paper_id": 2309.09202,
        "authors": "Arash Mousavi, Reza Hafezi, Hasan Ahmadi",
        "title": "Examining psychology of science as a potential contributor to science\n  policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The psychology of science is the least developed member of the family of\nscience studies. It is growing, however, increasingly into a promising\ndiscipline. After a very brief review of this emerging sub-field of psychology,\nwe call for it to be invited into the collection of social sciences that\nconstitute the interdisciplinary field of science policy. Discussing the\nclassic issue of resource allocation, this paper tries to indicate how prolific\na new psychological conceptualization of this problem would be. Further, from a\npsychological perspective, this research will argue in favor of a more\nrealistic conception of science which would be a complement to the existing one\nin science policy.\n"
    },
    {
        "paper_id": 2309.09481,
        "authors": "Tae-Hwy Lee and Tao Wang",
        "title": "Estimation and Testing of Forecast Rationality with Many Moments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We in this paper utilize P-GMM (Cheng and Liao, 2015) moment selection\nprocedure to select valid and relevant moments for estimating and testing\nforecast rationality under the flexible loss proposed by Elliott et al. (2005).\nWe motivate the moment selection in a large dimensional setting, explain the\nfundamental mechanism of P-GMM moment selection procedure, and elucidate how to\nimplement it in the context of forecast rationality by allowing the existence\nof potentially invalid moment conditions. A set of Monte Carlo simulations is\nconducted to examine the finite sample performance of P-GMM estimation in\nintegrating the information available in instruments into both the estimation\nand testing, and a real data analysis using data from the Survey of\nProfessional Forecasters issued by the Federal Reserve Bank of Philadelphia is\npresented to further illustrate the practical value of the suggested\nmethodology. The results indicate that the P-GMM post-selection estimator of\nforecaster's attitude is comparable to the oracle estimator by using the\navailable information efficiently. The accompanying power of rationality and\nsymmetry tests utilizing P-GMM estimation would be substantially increased\nthrough reducing the influence of uninformative instruments. When a forecast\nuser estimates and tests for rationality of forecasts that have been produced\nby others such as Greenbook, P-GMM moment selection procedure can assist in\nachieving consistent and more efficient outcomes.\n"
    },
    {
        "paper_id": 2309.0989,
        "authors": "Natasha Latif, Shafqat Ali Shad, Muhammad Usman, Chandan Kumar, Bahman\n  B Motii, MD Mahfuzer Rahman, Khuram Shafi, Zahra Idrees",
        "title": "Pragmatic Comparison Analysis of Alternative Option Pricing Models",
        "comments": null,
        "journal-ref": "Sci.Int.(1013-5316),35(4),525-529,2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we price European Call three different option pricing models,\nwhere the volatility is dynamically changing i.e. non constant. In stochastic\nvolatility (SV) models for option pricing a closed form approximation technique\nis used, indicating that these models are computationally efficient and have\nthe same level of performance as existing ones. We show that the calibration of\nSV models, such as Heston model and the High Order Moment based Stochastic\nVolatility (MSV) is often faster and easier. On 15 different datasets of index\noptions, we show that models which incorporates stochastic volatility achieves\naccuracy comparable with the existing models. Further, we compare the In Sample\nand Out Sample pricing errors of each model on each date. Lastly, the pricing\nof models is compared among three different market to check model performance\nin different markets. Keywords: Option Pricing Model, Simulations, Index\nOptions, Stochastic Volatility Models, Loss Function\nhttp://www.sci-int.com/pdf/638279543859822650.pdf\n"
    },
    {
        "paper_id": 2309.1008,
        "authors": "Alvaro Forteza, Irene Mussio, and Juan S Pereyra",
        "title": "Can political gridlock undermine checks and balances? A lab experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  If checks and balances are aimed at protecting citizens from the government's\nabuse of power, why do they sometimes weaken them? We address this question in\na laboratory experiment in which subjects choose between two decision rules:\nwith and without checks and balances. Voters may prefer an unchecked executive\nif that enables a reform that, otherwise, is blocked by the legislature.\nConsistent with our predictions, we find that subjects are more likely to\nweaken checks and balances when there is political gridlock. However, subjects\nweaken the controls not only when the reform is beneficial but also when it is\nharmful.\n"
    },
    {
        "paper_id": 2309.10152,
        "authors": "Eisuke Yamagata and Shunsuke Ono",
        "title": "Sparse Index Tracking: Simultaneous Asset Selection and Capital\n  Allocation via $\\ell_0$-Constrained Portfolio",
        "comments": "Submitted to IEEE Open Journal of Signal Processing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sparse index tracking is a prominent passive portfolio management strategy\nthat constructs a sparse portfolio to track a financial index. A sparse\nportfolio is preferable to a full portfolio in terms of reducing transaction\ncosts and avoiding illiquid assets. To achieve portfolio sparsity, conventional\nstudies have utilized $\\ell_p$-norm regularizations as a continuous surrogate\nof the $\\ell_0$-norm regularization. Although these formulations can construct\nsparse portfolios, their practical application is challenging due to the\nintricate and time-consuming process of tuning parameters to define the precise\nupper limit of assets in the portfolio. In this paper, we propose a new problem\nformulation of sparse index tracking using an $\\ell_0$-norm constraint that\nenables easy control of the upper bound on the number of assets in the\nportfolio. Moreover, our approach offers a choice between constraints on\nportfolio and turnover sparsity, further reducing transaction costs by limiting\nasset updates at each rebalancing interval. Furthermore, we develop an\nefficient algorithm for solving this problem based on a primal-dual splitting\nmethod. Finally, we illustrate the effectiveness of the proposed method through\nexperiments on the S&P500 and Russell3000 index datasets.\n"
    },
    {
        "paper_id": 2309.1022,
        "authors": "Takanobu Mizuta and Isao Yagi",
        "title": "Comparing effects of price limit and circuit breaker in stock exchanges\n  by an agent-based model",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2202.00831,\n  arXiv:2202.01423",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The prevention of rapidly and steeply falling market prices is vital to avoid\nfinancial crisis. To this end, some stock exchanges implement a price limit or\na circuit breaker, and there has been intensive investigation into which\nregulation best prevents rapid and large variations in price. In this study, we\nexamine this question using an artificial market model that is an agent-based\nmodel for a financial market. Our findings show that the price limit and the\ncircuit breaker basically have the same effect when the parameters, limit price\nrange and limit time range, are the same. However, the price limit is less\neffective when limit the time range is smaller than the cancel time range. With\nthe price limit, many sell orders are accumulated around the lower limit price,\nand when the lower limit price is changed before the accumulated sell orders\nare cancelled, it leads to the accumulation of sell orders of various prices.\nThese accumulated sell orders essentially act as a wall against buy orders,\nthereby preventing price from rising. Caution should be taken in the sense that\nthese results pertain to a limited situation. Specifically, our finding that\nthe circuit breaker is better than the price limit should be adapted only in\ncases where the reason for falling prices is erroneous orders and when\nindividual stocks are regulated.\n"
    },
    {
        "paper_id": 2309.10252,
        "authors": "Akhil Rao, Mark Moretto, Marcus Holzinger, Daniel Kaffine, Brian\n  Weeden",
        "title": "OPUS: An Integrated Assessment Model for Satellites and Orbital Debris",
        "comments": "This work was supported by 2022 grant funding from the NASA ROSES\n  program. Code and simulation data available at\n  https://github.com/akhilrao/OPUS",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  An increasingly salient public policy challenge is how to manage the growing\nnumber of satellites in orbit, including large constellations. Many policy\ninitiatives have been proposed that attempt to address the problem from\ndifferent angles, but there is a paucity of analytical tools to help\npolicymakers evaluate the efficacy of these different proposals and any\npotential counterproductive outcomes. To help address this problem, this paper\nsummarizes work done to develop an experimental integrated assessment model --\nOrbital Debris Propagators Unified with Economic Systems (OPUS) -- that\ncombines both astrodynamics of the orbital population and economic behavior of\nspace actors. For a given set of parameters, the model first utilizes a given\nastrodynamic propagator to assess the state of objects in orbit. It then uses a\nset of user-defined economic and policy parameters -- e.g. launch prices,\ndisposal regulations -- to model how actors will respond to the economic\nincentives created by a given scenario. For the purposes of testing, the MIT\nOrbital Capacity Tool (MOCAT) version 4S was used as the primary astrodynamics\npropagator to simulate the true expected target collision probability ($p_c$)\nfor a given end-of-life (EOL) disposal plan. To demonstrate\npropagator-agnosticism, a Gaussian mixture probability hypothesis density\n(GMPHD) filter was also used to simulate $p_c$. We also explore economic policy\ninstruments to improve both sustainability of and economic welfare from orbit\nuse. In doing so, we demonstrate that this hybrid approach can serve as a\nuseful tool for evaluating policy proposals for managing orbital congestion. We\nalso discuss areas where this work can be made more robust and expanded to\ninclude additional policy considerations.\n"
    },
    {
        "paper_id": 2309.10448,
        "authors": "Francisco Castro, Jian Gao, S\\'ebastien Martin",
        "title": "Human-AI Interactions and Societal Pitfalls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When working with generative artificial intelligence (AI), users may see\nproductivity gains, but the AI-generated content may not match their\npreferences exactly. To study this effect, we introduce a Bayesian framework in\nwhich heterogeneous users choose how much information to share with the AI,\nfacing a trade-off between output fidelity and communication cost. We show that\nthe interplay between these individual-level decisions and AI training may lead\nto societal challenges. Outputs may become more homogenized, especially when\nthe AI is trained on AI-generated content. And any AI bias may become societal\nbias. A solution to the homogenization and bias issues is to improve human-AI\ninteractions, enabling personalized outputs without sacrificing productivity.\n"
    },
    {
        "paper_id": 2309.10477,
        "authors": "Pierre-Antoine Arsaguet and Paul Bilokon",
        "title": "Derivatives Sensitivities Computation under Heston Model on GPU",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report investigates the computation of option Greeks for European and\nAsian options under the Heston stochastic volatility model on GPU. We first\nimplemented the exact simulation method proposed by Broadie and Kaya and used\nit as a baseline for precision and speed. We then proposed a novel method for\ncomputing Greeks using the Milstein discretisation method on GPU. Our results\nshow that the proposed method provides a speed-up up to 200x compared to the\nexact simulation implementation and that it can be used for both European and\nAsian options. However, the accuracy of the GPU method for estimating Rho is\ninferior to the CPU method. Overall, our study demonstrates the potential of\nGPU for computing derivatives sensitivies with numerical methods.\n"
    },
    {
        "paper_id": 2309.10546,
        "authors": "Jakub Micha\\'nk\\'ow, Pawe{\\l} Sakowski, Robert \\'Slepaczuk",
        "title": "Mean Absolute Directional Loss as a New Loss Function for Machine\n  Learning Problems in Algorithmic Investment Strategies",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the issue of an adequate loss function in the\noptimization of machine learning models used in the forecasting of financial\ntime series for the purpose of algorithmic investment strategies (AIS)\nconstruction. We propose the Mean Absolute Directional Loss (MADL) function,\nsolving important problems of classical forecast error functions in extracting\ninformation from forecasts to create efficient buy/sell signals in algorithmic\ninvestment strategies. Finally, based on the data from two different asset\nclasses (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that\nthe new loss function enables us to select better hyperparameters for the LSTM\nmodel and obtain more efficient investment strategies, with regard to\nrisk-adjusted return metrics on the out-of-sample data.\n"
    },
    {
        "paper_id": 2309.10729,
        "authors": "Masanori Hirano, Ryosuke Takata, Kiyoshi Izumi",
        "title": "PAMS: Platform for Artificial Market Simulations",
        "comments": "7pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a new artificial market simulation platform, PAMS:\nPlatform for Artificial Market Simulations. PAMS is developed as a Python-based\nsimulator that is easily integrated with deep learning and enabling various\nsimulation that requires easy users' modification. In this paper, we\ndemonstrate PAMS effectiveness through a study using agents predicting future\nprices by deep learning.\n"
    },
    {
        "paper_id": 2309.10986,
        "authors": "Shanyi Zhou, Ning Yan, Zhijun Li, Mo Geng, Xulong Zhang, Hongbiao Si,\n  Lihua Tang, Wenyuan Sun, Longda Zhang, Yi Cao",
        "title": "Research on the Impact of Executive Shareholding on New Investment in\n  Enterprises Based on Multivariable Linear Regression Model",
        "comments": "Accepted by the 7th APWeb-WAIM International Joint Conference on Web\n  and Big Data. (APWeb 2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on principal-agent theory and optimal contract theory, companies use\nthe method of increasing executives' shareholding to stimulate collaborative\ninnovation. However, from the aspect of agency costs between management and\nshareholders (i.e. the first type) and between major shareholders and minority\nshareholders (i.e. the second type), the interests of management, shareholders\nand creditors will be unbalanced with the change of the marginal utility of\nexecutive equity incentives.In order to establish the correlation between the\nproportion of shares held by executives and investments in corporate\ninnovation, we have chosen a range of publicly listed companies within China's\nA-share market as the focus of our study. Employing a multi-variable linear\nregression model, we aim to analyze this relationship thoroughly.The following\nmodels were developed: (1) the impact model of executive shareholding on\ncorporate innovation investment; (2) the impact model of executive shareholding\non two types of agency costs; (3)The model is employed to examine the mediating\ninfluence of the two categories of agency costs. Following both correlation and\nregression analyses, the findings confirm a meaningful and positive correlation\nbetween executives' shareholding and the augmentation of corporate innovation\ninvestments. Additionally, the results indicate that executive shareholding\ncontributes to the reduction of the first type of agency cost, thereby\nfostering corporate innovation investment. However, simultaneously, it leads to\nan escalation in the second type of agency cost, thus impeding corporate\ninnovation investment.\n"
    },
    {
        "paper_id": 2309.11189,
        "authors": "Boxiang Fu",
        "title": "Increasing Ticketing Allocative Efficiency Using Marginal Price Auction\n  Theory",
        "comments": "12 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most modern ticketing systems rely on a first-come-first-serve or randomized\nallocation system to determine the allocation of tickets. Such systems has\nreceived considerable backlash in recent years due to its inequitable allotment\nand allocative inefficiency. We analyze a ticketing protocol based on a\nvariation of the marginal price auction system. Users submit bids to the\nprotocol based on their own utilities. The protocol awards tickets to the\nhighest bidders and determines the final ticket price paid by all bidders using\nthe lowest winning submitted bid. Game theoretic proof is provided to ensure\nthe protocol more efficiently allocates the tickets to the bidders with the\nhighest utilities. We also prove that the protocol extracts more economic rents\nfor the event organizers and the non-optimality of ticket scalping under\ntime-invariant bidder utilities.\n"
    },
    {
        "paper_id": 2309.11394,
        "authors": "Kenji Saito, Yutaka Soejima, Toshihiko Sugiura, Yukinobu Kitamura,\n  Mitsuru Iwamura",
        "title": "Is Ethereum Proof of Stake Sustainable? $-$ Considering from the\n  Perspective of Competition Among Smart Contract Platforms $-$",
        "comments": "30 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the Merge update upon which Ethereum transitioned to Proof of Stake, it\nhas been touted that it resulted in lower power consumption and increased\nsecurity. However, even if that is the case, can this state be sustained?\n  In this paper, we focus on the potential impact of competition with other\nsmart contract platforms on the price of Ethereum's native currency, Ether\n(ETH), thereby raising questions about the safety and sustainability\npurportedly brought about by the design of Proof of Stake.\n"
    },
    {
        "paper_id": 2309.114,
        "authors": "Paul Bilokon and Yitao Qiu",
        "title": "Transformers versus LSTMs for electronic trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rapid development of artificial intelligence, long short term memory\n(LSTM), one kind of recurrent neural network (RNN), has been widely applied in\ntime series prediction.\n  Like RNN, Transformer is designed to handle the sequential data. As\nTransformer achieved great success in Natural Language Processing (NLP),\nresearchers got interested in Transformer's performance on time series\nprediction, and plenty of Transformer-based solutions on long time series\nforecasting have come out recently. However, when it comes to financial time\nseries prediction, LSTM is still a dominant architecture. Therefore, the\nquestion this study wants to answer is: whether the Transformer-based model can\nbe applied in financial time series prediction and beat LSTM.\n  To answer this question, various LSTM-based and Transformer-based models are\ncompared on multiple financial prediction tasks based on high-frequency limit\norder book data. A new LSTM-based model called DLSTM is built and new\narchitecture for the Transformer-based model is designed to adapt for financial\nprediction. The experiment result reflects that the Transformer-based model\nonly has the limited advantage in absolute price sequence prediction. The\nLSTM-based models show better and more robust performance on difference\nsequence prediction, such as price difference and price movement.\n"
    },
    {
        "paper_id": 2309.11416,
        "authors": "Liang Chen, Eugene Choo, Alfred Galichon, Simon Weber",
        "title": "Existence of a Competitive Equilibrium with Substitutes, with\n  Applications to Matching and Discrete Choice Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose new results for the existence and uniqueness of a general\nnonparametric and nonseparable competitive equilibrium with substitutes. These\nresults ensure the invertibility of a general competitive system. The existing\nliterature has focused on the uniqueness of a competitive equilibrium assuming\nthat existence holds. We introduce three properties that our supply system must\nsatisfy: weak substitutes, pivotal substitutes, and responsiveness. These\nproperties are sufficient to ensure the existence of an equilibrium, thus\nproviding the existence counterpart to Berry, Gandhi, and Haile (2013)'s\nuniqueness results. For two important classes of models, bipartite matching\nmodels with full assignment and discrete choice models, we show that both\nmodels can be reformulated as a competitive system such that our existence and\nuniqueness results can be readily applied. We also provide an algorithm to\ncompute the unique competitive equilibrium. Furthermore, we argue that our\nresults are particularly useful for studying imperfectly transferable utility\nmatching models with full assignment and non-additive random utility models.\n"
    },
    {
        "paper_id": 2309.1169,
        "authors": "Ege Erdil, Tamay Besiroglu",
        "title": "Explosive growth from AI automation: A review of the arguments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine whether substantial AI automation could accelerate global economic\ngrowth by about an order of magnitude, akin to the economic growth effects of\nthe Industrial Revolution. We identify three primary drivers for such growth:\n1) the scalability of an AI \"labor force\" restoring a regime of increasing\nreturns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive\nincrease in output from rapid automation occurring over a brief period of time.\nAgainst this backdrop, we evaluate nine counterarguments, including regulatory\nhurdles, production bottlenecks, alignment issues, and the pace of automation.\nWe tentatively assess these arguments, finding most are unlikely deciders. We\nconclude that explosive growth seems plausible with AI capable of broadly\nsubstituting for human labor, but high confidence in this claim seems currently\nunwarranted. Key questions remain about the intensity of regulatory responses\nto AI, physical bottlenecks in production, the economic value of superhuman\nabilities, and the rate at which AI automation could occur.\n"
    },
    {
        "paper_id": 2309.11693,
        "authors": "Kei Nakagawa, Masaya Abe, Seiichi Kuroki",
        "title": "Doubly Robust Mean-CVaR Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we address the challenge of portfolio optimization, a critical\naspect of managing investment risks and maximizing returns. The mean-CVaR\nportfolio is considered a promising method due to today's unstable financial\nmarket crises like the COVID-19 pandemic. It incorporates expected returns into\nthe CVaR, which considers the expected value of losses exceeding a specified\nprobability level. However, the instability associated with the input parameter\nchanges and estimation errors can deteriorate portfolio performance. Therefore\nin this study, we propose a Doubly Robust mean-CVaR Portfolio refined approach\nto the mean-CVaR portfolio optimization. Our method can solve the instability\nproblem to simultaneously optimize the multiple levels of CVaRs and define\nuncertainty sets for the mean parameter to perform robust optimization.\nTheoretically, the proposed method can be formulated as a second-order cone\nprogramming problem which is the same formulation as traditional mean-variance\nportfolio optimization. In addition, we derive an estimation error bound of the\nproposed method for the finite-sample case. Finally, experiments with benchmark\nand real market data show that our proposed method exhibits better performance\ncompared to existing portfolio optimization strategies.\n"
    },
    {
        "paper_id": 2309.1196,
        "authors": "Wei Jie Yeo, Wihan van der Heever, Rui Mao, Erik Cambria, Ranjan\n  Satapathy, Gianmarco Mengaldo",
        "title": "A Comprehensive Review on Financial Explainable AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The success of artificial intelligence (AI), and deep learning models in\nparticular, has led to their widespread adoption across various industries due\nto their ability to process huge amounts of data and learn complex patterns.\nHowever, due to their lack of explainability, there are significant concerns\nregarding their use in critical sectors, such as finance and healthcare, where\ndecision-making transparency is of paramount importance. In this paper, we\nprovide a comparative survey of methods that aim to improve the explainability\nof deep learning models within the context of finance. We categorize the\ncollection of explainable AI methods according to their corresponding\ncharacteristics, and we review the concerns and challenges of adopting\nexplainable AI methods, together with future directions we deemed appropriate\nand important.\n"
    },
    {
        "paper_id": 2309.11979,
        "authors": "Jiashu Lou",
        "title": "Stock Market Sentiment Classification and Backtesting via Fine-tuned\n  BERT",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rapid development of big data and computing devices, low-latency\nautomatic trading platforms based on real-time information acquisition have\nbecome the main components of the stock trading market, so the topic of\nquantitative trading has received widespread attention. And for non-strongly\nefficient trading markets, human emotions and expectations always dominate\nmarket trends and trading decisions. Therefore, this paper starts from the\ntheory of emotion, taking East Money as an example, crawling user comment\ntitles data from its corresponding stock bar and performing data cleaning.\nSubsequently, a natural language processing model BERT was constructed, and the\nBERT model was fine-tuned using existing annotated data sets. The experimental\nresults show that the fine-tuned model has different degrees of performance\nimprovement compared to the original model and the baseline model.\nSubsequently, based on the above model, the user comment data crawled is\nlabeled with emotional polarity, and the obtained label information is combined\nwith the Alpha191 model to participate in regression, and significant\nregression results are obtained. Subsequently, the regression model is used to\npredict the average price change for the next five days, and use it as a signal\nto guide automatic trading. The experimental results show that the\nincorporation of emotional factors increased the return rate by 73.8\\% compared\nto the baseline during the trading period, and by 32.41\\% compared to the\noriginal alpha191 model. Finally, we discuss the advantages and disadvantages\nof incorporating emotional factors into quantitative trading, and give possible\ndirections for further research in the future.\n"
    },
    {
        "paper_id": 2309.12014,
        "authors": "Arnon Archankul, Giorgio Ferrari, Tobias Hellmann and Jacco J.J.\n  Thijssen",
        "title": "Singular Control in a Cash Management Model with Ambiguity",
        "comments": "32 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a singular control model of cash reserve management, driven by a\ndiffusion under ambiguity. The manager is assumed to have maxmin preferences\nover a set of priors characterized by $\\kappa$-ignorance. A verification\ntheorem is established to determine the firm's cost function and the optimal\ncash policy; the latter taking the form of a control barrier policy. In a model\ndriven by arithmetic Brownian motion, we numerically show that an increase in\nambiguity leads to higher expected costs under the worst-case prior and a\nnarrower inaction region. The latter effect can be used to provide an\nambiguity-driven explanation for observed cash management behavior.\n"
    },
    {
        "paper_id": 2309.12082,
        "authors": "Tobias Wand, Timo Wiedemann, Jan Harren and Oliver Kamps",
        "title": "Estimating Stable Fixed Points and Langevin Potentials for Financial\n  Dynamics",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Geometric Brownian Motion (GBM) is a standard model in quantitative\nfinance, but the potential function of its stochastic differential equation\n(SDE) cannot include stable nonzero prices. This article generalises the GBM to\nan SDE with polynomial drift of order q and shows via model selection that q=2\nis most frequently the optimal model to describe the data. Moreover, Markov\nchain Monte Carlo ensembles of the accompanying potential functions show a\nclear and pronounced potential well, indicating the existence of a stable\nprice.\n"
    },
    {
        "paper_id": 2309.12085,
        "authors": "Marisol Garrouste, Michael T. Craig, Daniel Wendt, Maria Herrera Diaz,\n  William Jenson, Qian Zhang, Brendan Kochunas",
        "title": "Techno-Economic Analysis of Synthetic Fuel Production from Existing\n  Nuclear Power Plants across the United States",
        "comments": "34 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Low carbon synfuel can displace transport fossil fuels such as diesel and jet\nfuel and help achieve the decarbonization of the transportation sector at a\nglobal scale, but large-scale cost-effective production facilities are needed.\nMeanwhile, nuclear power plants are closing due to economic difficulties:\nelectricity prices are too low and variable to cover their operational costs.\nUsing existing nuclear power plants to produce synfuels might prevent loss of\nthese low-carbon assets while producing synfuels at scale, but no\ntechnoeconomic analysis of this Integrated Energy System exist. We quantify the\ntechnoeconomic potential of coupling a synthetic fuel production process with\nfive example nuclear power plants across the U.S. to explore the influence of\ndifferent electricity markets, access to carbon dioxide sources, and fuel\nmarkets. Coupling synfuel production increases nuclear plant profitability by\nup to 792 million USD(2020) in addition to a 10 percent rate of return on\ninvestment over a 20 year period. Our analysis identifies drivers for the\neconomic profitability of the synfuel IES. The hydrogen production tax credit\nfrom the 2022 Inflation Reduction Act is essential to its overall profitability\nrepresenting on average three quarters of its revenues. The carbon feedstock\ntransportation is the highest cost - more than a third on average - closely\nfollowed by the synfuel production process capital costs. Those results show\nthe key role of incentive policies for the decarbonization of the\ntransportation sector and the economic importance of the geographic location of\nIntegrated Energy Systems.\n"
    },
    {
        "paper_id": 2309.12322,
        "authors": "Petar Radanliev",
        "title": "The Rise and Fall of Cryptocurrencies: Defining the Economic and Social\n  Values of Blockchain Technologies, assessing the Opportunities, and defining\n  the Financial and Cybersecurity Risks of the Metaverse",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper contextualises the common queries of \"why is crypto crashing?\" and\n\"why is crypto down?\", the research transcends beyond the frequent market\nfluctuations to unravel how cryptocurrencies fundamentally work and the\nstep-by-step process on how to create a cryptocurrency.\n  The study examines blockchain technologies and their pivotal role in the\nevolving Metaverse, shedding light on topics such as how to invest in\ncryptocurrency, the mechanics behind crypto mining, and strategies to\neffectively buy and trade cryptocurrencies. Through an interdisciplinary\napproach, the research transitions from the fundamental principles of fintech\ninvestment strategies to the overarching implications of blockchain within the\nMetaverse. Alongside exploring machine learning potentials in financial sectors\nand risk assessment methodologies, the study critically assesses whether\ndeveloped or developing nations are poised to reap greater benefits from these\ntechnologies. Moreover, it probes into both enduring and dubious crypto\nprojects, drawing a distinct line between genuine blockchain applications and\nPonzi-like schemes. The conclusion resolutely affirms the continuing dominance\nof blockchain technologies, underlined by a profound exploration of their\nintrinsic value and a reflective commentary by the author on the potential\nrisks confronting individual investors.\n"
    },
    {
        "paper_id": 2309.1233,
        "authors": "Rem Sadykhov, Geoffrey Goodell, Denis de Montigny, Martin Schoernig,\n  Philip Treleaven",
        "title": "Decentralized Token Economy Theory (DeTEcT)",
        "comments": "29 pages, 5 figures, 8 tables",
        "journal-ref": null,
        "doi": "10.3389/fbloc.2023.1298330",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a pioneering approach for simulation of economic\nactivity, policy implementation, and pricing of goods in token economies. The\npaper proposes a formal analysis framework for wealth distribution analysis and\nsimulation of interactions between economic participants in an economy. Using\nthis framework, we define a mechanism for identifying prices that achieve the\ndesired wealth distribution according to some metric, and stability of economic\ndynamics.\n  The motivation to study tokenomics theory is the increasing use of\ntokenization, specifically in financial infrastructures, where designing token\neconomies is in the forefront. Tokenomics theory establishes a quantitative\nframework for wealth distribution amongst economic participants and implements\nthe algorithmic regulatory controls mechanism that reacts to changes in\neconomic conditions.\n  In our framework, we introduce a concept of tokenomic taxonomy where agents\nin the economy are categorized into agent types and interactions between them.\nThis novel approach is motivated by having a generalized model of the\nmacroeconomy with controls being implemented through interactions and policies.\nThe existence of such controls allows us to measure and readjust the wealth\ndynamics in the economy to suit the desired objectives.\n"
    },
    {
        "paper_id": 2309.12384,
        "authors": "Kyriakos Georgiou, Athanasios N. Yannacopoulos",
        "title": "Probability of Default modelling with L\\'evy-driven Ornstein-Uhlenbeck\n  processes and applications in credit risk under the IFRS 9",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we develop a framework for estimating Probability of Default\n(PD) based on stochastic models governing an appropriate asset value processes.\nIn particular, we build upon a L\\'evy-driven Ornstein-Uhlenbeck process and\nconsider a generalized model that incorporates multiple latent variables\naffecting the evolution of the process. We obtain an Integral Equation (IE)\nformulation for the corresponding PD as a function of the initial position of\nthe asset value process and the time until maturity, from which we then prove\nthat the PD function satisfies an appropriate Partial Integro-Differential\nEquation (PIDE). These representations allow us to show that appropriate weak\n(viscosity) as well as strong solutions exist, and develop subsequent numerical\nschemes for the estimation of the PD function. Such a framework is necessary\nunder the newly introduced International Financial Reporting Standards (IFRS) 9\nregulation, which has imposed further requirements on the sophistication and\nrigor underlying credit modelling methodologies. We consider special cases of\nthe generalized model that can be used for applications to credit risk\nmodelling and provide examples specific to provisioning under IFRS 9, and more.\n"
    },
    {
        "paper_id": 2309.12588,
        "authors": "Zhou Yang, Junkee Jeon",
        "title": "A Problem of Finite-Horizon Optimal Switching and Stochastic Control for\n  Utility Maximization",
        "comments": "46 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we undertake an investigation into the utility maximization\nproblem faced by an economic agent who possesses the option to switch jobs,\nwithin a scenario featuring the presence of a mandatory retirement date. The\nagent needs to consider not only optimal consumption and investment but also\nthe decision regarding optimal job-switching. Therefore, the utility\nmaximization encompasses features of both optimal switching and stochastic\ncontrol within a finite horizon. To address this challenge, we employ a\ndual-martingale approach to derive the dual problem defined as a finite-horizon\npure optimal switching problem. By applying a theory of the double obstacle\nproblem with non-standard arguments, we examine the analytical properties of\nthe system of parabolic variational inequalities arising from the optimal\nswitching problem, including those of its two free boundaries. Based on these\nanalytical properties, we establish a duality theorem and characterize the\noptimal job-switching strategy in terms of time-varying wealth boundaries.\nFurthermore, we derive integral equation representations satisfied by the\noptimal strategies and provide numerical results based on these\nrepresentations.\n"
    },
    {
        "paper_id": 2309.12704,
        "authors": "Rasmus Ingemann Tuffveson Jensen, Joras Ferwerda, Christian Remi Wewer",
        "title": "Searching for Smurfs: Testing if Money Launderers Know Alert Thresholds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To combat money laundering, banks raise and review alerts on transactions\nthat exceed confidential thresholds. This paper presents a data-driven approach\nto detect smurfing, i.e., money launderers seeking to evade detection by\nbreaking up large transactions into amounts under the secret thresholds. The\napproach utilizes the notion of a counterfactual distribution and relies on two\nassumptions: (i) smurfing is unfeasible for the very largest financial\ntransactions and (ii) money launderers have incentives to make smurfed\ntransactions close to the thresholds. Simulations suggest that the approach can\ndetect smurfing when as little as 0.1-0.5\\% of all bank transactions are\nsubject to smurfing. An application to real data from a systemically important\nDanish bank finds no evidence of smurfing and, thus, no evidence of leaked\nconfidential thresholds. An implementation of our approach will be available\nonline, providing a free and easy-to-use tool for banks.\n"
    },
    {
        "paper_id": 2309.12818,
        "authors": "Daniel Kirste, Niclas Kannengie{\\ss}er, Ricky Lamberty, Ali Sunyaev",
        "title": "How Automated Market Makers Approach the Thin Market Problem in\n  Cryptoeconomic Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The proper design of automated market makers (AMMs) is crucial to enable the\ncontinuous trading of assets represented as digital tokens on markets of\ncryptoeconomic systems. Improperly designed AMMs can make such markets suffer\nfrom the thin market problem (TMP), which can cause cryptoeconomic systems to\nfail their purposes. We developed an AMM taxonomy that showcases AMM design\ncharacteristics. Based on the AMM taxonomy, we devised AMM archetypes\nimplementing principal solution approaches for the TMP. The main purpose of\nthis article is to support practitioners and researchers in tackling the TMP\nthrough proper AMM designs.\n"
    },
    {
        "paper_id": 2309.12891,
        "authors": "Molei Qin, Shuo Sun, Wentao Zhang, Haochong Xia, Xinrun Wang and Bo An",
        "title": "EarnHFT: Efficient Hierarchical Reinforcement Learning for High\n  Frequency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  High-frequency trading (HFT) uses computer algorithms to make trading\ndecisions in short time scales (e.g., second-level), which is widely used in\nthe Cryptocurrency (Crypto) market (e.g., Bitcoin). Reinforcement learning (RL)\nin financial research has shown stellar performance on many quantitative\ntrading tasks. However, most methods focus on low-frequency trading, e.g.,\nday-level, which cannot be directly applied to HFT because of two challenges.\nFirst, RL for HFT involves dealing with extremely long trajectories (e.g., 2.4\nmillion steps per month), which is hard to optimize and evaluate. Second, the\ndramatic price fluctuations and market trend changes of Crypto make existing\nalgorithms fail to maintain satisfactory performance. To tackle these\nchallenges, we propose an Efficient hieArchical Reinforcement learNing method\nfor High Frequency Trading (EarnHFT), a novel three-stage hierarchical RL\nframework for HFT. In stage I, we compute a Q-teacher, i.e., the optimal action\nvalue based on dynamic programming, for enhancing the performance and training\nefficiency of second-level RL agents. In stage II, we construct a pool of\ndiverse RL agents for different market trends, distinguished by return rates,\nwhere hundreds of RL agents are trained with different preferences of return\nrates and only a tiny fraction of them will be selected into the pool based on\ntheir profitability. In stage III, we train a minute-level router which\ndynamically picks a second-level agent from the pool to achieve stable\nperformance across different markets. Through extensive experiments in various\nmarket trends on Crypto markets in a high-fidelity simulation trading\nenvironment, we demonstrate that EarnHFT significantly outperforms 6\nstate-of-art baselines in 6 popular financial criteria, exceeding the runner-up\nby 30% in profitability.\n"
    },
    {
        "paper_id": 2309.12945,
        "authors": "Thomas Hasenzagl, Luis Perez",
        "title": "The Micro-Aggregated Profit Share",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How much has market power increased in the United States in the last fifty\nyears? And how did the rise in market power affect aggregate profits? Using\nmicro-level data from U.S. Compustat, we find that several indicators of market\npower have steadily increased since 1970. In particular, the aggregate markup\nhas gone up from 10% of price over marginal cost in 1970 to 23% in 2020, and\naggregate returns to scale have risen from 1.00 to 1.13. We connect these\nmarket-power indicators to profitability by showing that the aggregate profit\nshare can be expressed in terms of the aggregate markup, aggregate returns to\nscale, and a sufficient statistic for production networks that captures double\nmarginalization in the economy. We find that despite the rise in market power,\nthe profit share has been constant at 18% of GDP because the increase in\nmonopoly rents has been completely offset by rising fixed costs and changes in\ntechnology. Our empirical results have subtle implications for policymakers:\noverly aggressive enforcement of antitrust law could decrease firm dynamism and\nparadoxically lead to lower competition and higher market power.\n"
    },
    {
        "paper_id": 2309.13064,
        "authors": "Yi Yang, Yixuan Tang, Kar Yan Tam",
        "title": "InvestLM: A Large Language Model for Investment using Financial Domain\n  Instruction Tuning",
        "comments": "Link: https://github.com/AbaciNLP/InvestLM",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a new financial domain large language model, InvestLM, tuned on\nLLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset\nrelated to financial investment. Inspired by less-is-more-for-alignment (Zhou\net al., 2023), we manually curate a small yet diverse instruction dataset,\ncovering a wide range of financial related topics, from Chartered Financial\nAnalyst (CFA) exam questions to SEC filings to Stackexchange quantitative\nfinance discussions. InvestLM shows strong capabilities in understanding\nfinancial text and provides helpful responses to investment related questions.\nFinancial experts, including hedge fund managers and research analysts, rate\nInvestLM's response as comparable to those of state-of-the-art commercial\nmodels (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of\nfinancial NLP benchmarks demonstrates strong generalizability. From a research\nperspective, this work suggests that a high-quality domain specific LLM can be\ntuned using a small set of carefully curated instructions on a well-trained\nfoundation model, which is consistent with the Superficial Alignment Hypothesis\n(Zhou et al., 2023). From a practical perspective, this work develops a\nstate-of-the-art financial domain LLM with superior capability in understanding\nfinancial texts and providing helpful investment advice, potentially enhancing\nthe work efficiency of financial professionals. We release the model parameters\nto the research community.\n"
    },
    {
        "paper_id": 2309.13096,
        "authors": "Sarit Maitra, Vivek Mishra, Sukanya Kundu, Manav Chopra",
        "title": "Econometric Model Using Arbitrage Pricing Theory and Quantile Regression\n  to Estimate the Risk Factors Driving Crude Oil Returns",
        "comments": "13 pages, 8 figures, submitted (JOIV-Scopus)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This work adopts a novel approach to determine the risk and return of crude\noil stocks by employing Arbitrage Pricing Theory (APT) and Quantile Regression\n(QR).The APT identifies the underlying risk factors likely to impact crude oil\nreturns.Subsequently, QR estimates the relationship between the factors and the\nreturns across different quantiles of the distribution. The West Texas\nIntermediate (WTI) crude oil price is used in this study as a benchmark for\ncrude oil prices. WTI price fluctuations can have a significant impact on the\nperformance of crude oil stocks and, subsequently, the global economy.To\ndetermine the proposed models stability, various statistical measures are used\nin this study.The results show that changes in WTI returns can have varying\neffects depending on market conditions and levels of volatility. The study\nhighlights the impact of structural discontinuities on returns, which can be\ncaused by changes in the global economy and the demand for crude oil.The\ninclusion of pandemic, geopolitical, and inflation-related explanatory\nvariables add uniqueness to this study as it considers current global events\nthat can affect crude oil returns.Findings show that the key factors that pose\nmajor risks to returns are industrial production, inflation, the global price\nof energy, the shape of the yield curve, and global economic policy\nuncertainty.This implies that while making investing decisions in WTI futures,\ninvestors should pay particular attention to these elements\n"
    },
    {
        "paper_id": 2309.13219,
        "authors": "Stephenson Strobel",
        "title": "Waiting for Dr. Godot: how much and who responds to predicted health\n  care wait times?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Asymmetric information in healthcare implies that patients could have\ndifficulty trading off non-health and health related information. I document\neffects on patient demand when predicted wait time is disclosed to patients in\nan emergency department (ED) system. I use a regression discontinuity where EDs\nwith similar predicted wait times display different online wait times to\npatients. I use impulse response functions estimated by local projections to\ndemonstrate effects of the higher wait time. I find that an additional thirty\nminutes of wait time results in 15% fewer waiting patients at urgent cares and\n2% fewer waiting patients at EDs within 3 hours of display. I find that the\ntype of patient that stops using emergency care is triaged as having lower\nacuity and would have used an urgent care. However, I find that at very high\nwait times there are declines in all acuity patients including sick patients.\n"
    },
    {
        "paper_id": 2309.13246,
        "authors": "Dangxing Chen",
        "title": "Can I Trust the Explanations? Investigating Explainable Machine Learning\n  Methods for Monotonic Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, explainable machine learning methods have been very\nsuccessful. Despite their success, most explainable machine learning methods\nare applied to black-box models without any domain knowledge. By incorporating\ndomain knowledge, science-informed machine learning models have demonstrated\nbetter generalization and interpretation. But do we obtain consistent\nscientific explanations if we apply explainable machine learning methods to\nscience-informed machine learning models? This question is addressed in the\ncontext of monotonic models that exhibit three different types of monotonicity.\nTo demonstrate monotonicity, we propose three axioms. Accordingly, this study\nshows that when only individual monotonicity is involved, the baseline Shapley\nvalue provides good explanations; however, when strong pairwise monotonicity is\ninvolved, the Integrated gradients method provides reasonable explanations on\naverage.\n"
    },
    {
        "paper_id": 2309.13449,
        "authors": "Alex A.T. Rathke",
        "title": "Profit shifting under the arm's length principle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study analyses the tax-induced profit shifting behaviour of firms and\nthe impact of governments' anti-shifting rules. We derive a model of a firm\nthat combines internal sales and internal debt in a full profit shifting\nstrategy, and which is required to apply the arm's length principle and a\ngeneral thin capitalisation rule. We find several cases where the firm may\nshift profits to low-tax countries while satisfying the usual arm's length\nconditions in all countries. Internal sales and internal debt may be regarded\neither as complementary or as substitute shifting channels, depending on how\nthe implicit concealment costs vary after changes in all transactions. We show\nthat the cross-effect between the shifting channels facilitates profit shifting\nby means of accepted transfer prices and interest rates.\n"
    },
    {
        "paper_id": 2309.13648,
        "authors": "Austin Adams, Benjamin Y Chan, Sarit Markovich, Xin Wan",
        "title": "Don't Let MEV Slip: The Costs of Swapping on the Uniswap Protocol",
        "comments": "31 pages, 7 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present the first in-depth empirical characterization of the costs of\ntrading on a decentralized exchange (DEX). Using quoted prices from the Uniswap\nLabs interface for two pools -- USDC-ETH (5bps) and PEPE-ETH (30bps) -- we\nevaluate the efficiency of trading on DEXs. Our main tool is slippage -- the\ndifference between the realized execution price of a trade, and its quoted\nprice -- which we breakdown into its benign and adversarial components. We also\npresent an alternative way to quantify and identify slippage due to adversarial\nreordering of transactions, which we call reordering slippage, that does not\nrequire quoted prices or mempool data to calculate. We find that the\ncomposition of transaction costs varies tremendously with the trade's\ncharacteristics. Specifically, while for small swaps, gas costs dominate costs,\nfor large swaps price-impact and slippage account for the majority of it.\nMoreover, when trading PEPE, a popular 'memecoin', the probability of\nadversarial slippage is about 80% higher than when trading a mature asset like\nUSDC.\n  Overall, our results provide preliminary evidence that DEXs offer a\ncompelling trust-less alternative to centralized exchanges for trading digital\nassets.\n"
    },
    {
        "paper_id": 2309.13662,
        "authors": "Haseeb Tariq, Marwan Hassani",
        "title": "Topology-Agnostic Detection of Temporal Money Laundering Flows in\n  Billion-Scale Transactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Money launderers exploit the weaknesses in detection systems by purposefully\nplacing their ill-gotten money into multiple accounts, at different banks. That\nmoney is then layered and moved around among mule accounts to obscure the\norigin and the flow of transactions. Consequently, the money is integrated into\nthe financial system without raising suspicion. Path finding algorithms that\naim at tracking suspicious flows of money usually struggle with scale and\ncomplexity. Existing community detection techniques also fail to properly\ncapture the time-dependent relationships. This is particularly evident when\nperforming analytics over massive transaction graphs. We propose a framework\n(called FaSTMAN), adapted for domain-specific constraints, to efficiently\nconstruct a temporal graph of sequential transactions. The framework includes a\nweighting method, using 2nd order graph representation, to quantify the\nsignificance of the edges. This method enables us to distribute complex queries\non smaller and densely connected networks of flows. Finally, based on those\nqueries, we can effectively identify networks of suspicious flows. We\nextensively evaluate the scalability and the effectiveness of our framework\nagainst two state-of-the-art solutions for detecting suspicious flows of\ntransactions. For a dataset of over 1 Billion transactions from multiple large\nEuropean banks, the results show a clear superiority of our framework both in\nefficiency and usefulness.\n"
    },
    {
        "paper_id": 2309.13696,
        "authors": "Abhiraj Sen and Jaydip Sen",
        "title": "Performance Evaluation of Equal-Weight Portfolio and Optimum Risk\n  Portfolio on Indian Stocks",
        "comments": "This is the preprint of our paper that has been accepted for\n  publication in the Inderscience journal \"International Journal of Business\n  Forecasting and Marketing Intelligence\". The preprint consist of 63 pages and\n  contains 26 figures and 66 tables. This is not the final published version of\n  the paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing an optimum portfolio for allocating suitable weights to its\nconstituent assets so that the return and risk associated with the portfolio\nare optimized is a computationally hard problem. The seminal work of Markowitz\nthat attempted to solve the problem by estimating the future returns of the\nstocks is found to perform sub-optimally on real-world stock market data. This\nis because the estimation task becomes extremely challenging due to the\nstochastic and volatile nature of stock prices. This work illustrates three\napproaches to portfolio design minimizing the risk, optimizing the risk, and\nassigning equal weights to the stocks of a portfolio. Thirteen critical sectors\nlisted on the National Stock Exchange (NSE) of India are first chosen. Three\nportfolios are designed following the above approaches choosing the top ten\nstocks from each sector based on their free-float market capitalization. The\nportfolios are designed using the historical prices of the stocks from Jan 1,\n2017, to Dec 31, 2022. The portfolios are evaluated on the stock price data\nfrom Jan 1, 2022, to Dec 31, 2022. The performances of the portfolios are\ncompared, and the portfolio yielding the higher return for each sector is\nidentified.\n"
    },
    {
        "paper_id": 2309.14009,
        "authors": "Salvatore Greco and Diego Rago",
        "title": "Discounting and Impatience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding how people actually trade off time for money is perhaps the\nmajor question in the field of time discounting. There is indeed a vast body of\nwork devoted to explore the underlying mechanisms of the individual decision\nmaking process in an intertemporal context. This paper presents a family of new\ndiscount functions whereof we derive a formal axiomatization. Applying the\nframework proposed by Bleichrodt, Rohde and Wakker, we further extend their\nformulation of CADI and CRDI functions, making discounting a function not only\nof time delay but, simultaneously, also of time distortion. Our main purpose\nis, in practice, to provide a tractable setting within which individual\nintertemporal preferences can be outlined. Furthermore, we apply our models to\nstudy the relation between individual time preferences and personality traits.\nFor the CADI-CADI, results show that the habit of smoking is heavily related\nwith both impatience and time perception. Within the Big-Five framework,\nconscientiousness, agreeableness and openness are positively related with\npatience (low r, initial discount rate).\n"
    },
    {
        "paper_id": 2309.14044,
        "authors": "Marco Caliendo, Robert Mahlstedt, Aiko Schmei{\\ss}er, Sophie Wagner",
        "title": "The Accuracy of Job Seekers' Wage Expectations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Job seekers' misperceptions about the labor market can distort their\ndecision-making and increase the risk of long-term unemployment. Our study\nestablishes objective benchmarks for the subjective wage expectations of\nunemployed workers. This enables us to provide novel insights into the accuracy\nof job seekers' wage expectations. First, especially workers with low objective\nearnings potential tend to display excessively optimistic beliefs about their\nfuture wages and anchor their wage expectations too strongly to their\npre-unemployment wages. Second, among long-term unemployed workers,\noveroptimism remains persistent throughout the unemployment spell. Third,\nhigher extrinsic incentives to search more intensively lead job seekers to hold\nmore optimistic wage expectations, yet this does not translate into higher\nrealized wages for them. Lastly, we document a connection between\noveroptimistic wage expectations and job seekers' tendency to overestimate\ntheir reemployment chances. We discuss the role of information frictions and\nmotivated beliefs as potential sources of job seekers' optimism and the\nheterogeneity in their beliefs.\n"
    },
    {
        "paper_id": 2309.14186,
        "authors": "S. El Geneidy, S. Baumeister, M. Peura and J.S. Kotiaho",
        "title": "Value-transforming financial, carbon and biodiversity footprint\n  accounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Transformative changes in our production and consumption habits are needed to\nenable the sustainability transition towards carbon neutrality, no net loss of\nbiodiversity, and planetary well-being. Organizations are the way we humans\nhave organized our everyday life, and much of our negative environmental\nimpacts, also called carbon and biodiversity footprints, are caused by\norganizations. Here we show how the financial accounts of any organization can\nbe exploited to develop an integrated carbon and biodiversity footprint\naccount. As a metric we utilize spatially explicit potential global loss of\nspecies which, we argue, can be understood as the biodiversity equivalent, the\nutility of which for biodiversity is similar to what carbon dioxide equivalent\nis for climate. We provide a global Biodiversity Footprint Database that\norganizations, experts and researchers can use to assess consumption-based\nbiodiversity footprints. We also argue that the current integration of\nfinancial and environmental accounting is superficial, and provide a framework\nfor a more robust financial value-transforming accounting model. To test the\nmethodologies, we utilized a Finnish university as a living lab. Assigning an\noffsetting cost to the footprints significantly altered the financial value of\nthe organization. We believe such value-transforming accounting is needed in\norder to draw the attention of senior executives and investors to the negative\nenvironmental impacts of their organizations.\n"
    },
    {
        "paper_id": 2309.14201,
        "authors": "Tarun Chitra",
        "title": "Towards a Theory of Maximal Extractable Value II: Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Maximal Extractable Value (MEV) is value extractable by temporary monopoly\npower commonly found in decentralized systems. This extraction stems from a\nlack of user privacy upon transaction submission and the ability of a\nmonopolist validator to reorder, add, and/or censor transactions. There are two\nmain directions to reduce MEV: reduce the flexibility of the miner to reorder\ntransactions by enforcing ordering rules and/or introduce a competitive market\nfor the right to reorder, add, and/or censor transactions. In this work, we\nunify these approaches via \\emph{uncertainty principles}, akin to those found\nin harmonic analysis and physics. This provides a quantitative trade-off\nbetween the freedom to reorder transactions and the complexity of an economic\npayoff to a user in a decentralized network. This trade off is analogous to the\nNyquist-Shannon sampling theorem and demonstrates that sequencing rules in\nblockchains need to be application specific. Our results suggest that neither\nso-called fair ordering techniques nor economic mechanisms can individually\nmitigate MEV for arbitrary payoff functions.\n"
    },
    {
        "paper_id": 2309.14297,
        "authors": "Yeon-Koo Che, Dong Woo Hahm, YingHua He",
        "title": "Leveraging Uncertainties to Infer Preferences: Robust Analysis of School\n  Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Inferring applicant preferences is fundamental in many analyses of\nschool-choice data. Application mistakes make this task challenging. We propose\na novel approach to deal with the mistakes in a deferred-acceptance matching\nenvironment. The key insight is that the uncertainties faced by applicants,\ne.g., due to tie-breaking lotteries, render some mistakes costly, allowing us\nto reliably infer relevant preferences. Our approach extracts all information\non preferences robustly to payoff-insignificant mistakes. We apply it to\nschool-choice data from Staten Island, NYC. Counterfactual analysis suggests\nthat we underestimate the effects of proposed desegregation reforms when\napplicants' mistakes are not accounted for in preference inference and\nestimation.\n"
    },
    {
        "paper_id": 2309.14334,
        "authors": "Gianluca Fabiani, Nikolaos Evangelou, Tianqi Cui, Juan M. Bello-Rivas,\n  Cristina P. Martin-Linares, Constantinos Siettos, Ioannis G. Kevrekidis",
        "title": "Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping\n  Points",
        "comments": "29 pages, 8 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a machine learning (ML)-assisted framework bridging manifold\nlearning, neural networks, Gaussian processes, and Equation-Free multiscale\nmodeling, for (a) detecting tipping points in the emergent behavior of complex\nsystems, and (b) characterizing probabilities of rare events (here,\ncatastrophic shifts) near them. Our illustrative example is an event-driven,\nstochastic agent-based model (ABM) describing the mimetic behavior of traders\nin a simple financial market. Given high-dimensional spatiotemporal data --\ngenerated by the stochastic ABM -- we construct reduced-order models for the\nemergent dynamics at different scales: (a) mesoscopic Integro-Partial\nDifferential Equations (IPDEs); and (b) mean-field-type Stochastic Differential\nEquations (SDEs) embedded in a low-dimensional latent space, targeted to the\nneighborhood of the tipping point. We contrast the uses of the different models\nand the effort involved in learning them.\n"
    },
    {
        "paper_id": 2309.14475,
        "authors": "Emaad Manzoor, Nikhil Malik",
        "title": "Designing Effective Music Excerpts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Excerpts are widely used to preview and promote musical works. Effective\nexcerpts induce consumption of the source musical work and thus generate\nrevenue. Yet, what makes an excerpt effective remains unexplored. We leverage a\npolicy change by Apple that generates quasi-exogenous variation in the excerpts\nof songs in the iTunes Music Store to estimate that having a 60 second longer\nexcerpt increases songs' unique monthly listeners by 5.4% on average, by 9.7%\nfor lesser known songs, and by 11.1% for lesser known artists. This is\ncomparable to the impact of being featured on the Spotify Global Top 50\nplaylist. We develop measures of musical repetition and unpredictability to\nexamine information provision as a mechanism, and find that the\ndemand-enhancing effect of longer excerpts is suppressed when they are\nrepetitive, too predictable, or too unpredictable. Our findings support\nplatforms' adoption of longer excerpts to improve content discovery and our\nmeasures can help inform excerpt selection in practice.\n"
    },
    {
        "paper_id": 2309.14548,
        "authors": "Xingchen Xu, Stephanie Lee, Yong Tan",
        "title": "Algorithmic Collusion or Competition: the Role of Platforms' Recommender\n  Systems",
        "comments": "33 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent academic research has extensively examined algorithmic collusion\nresulting from the utilization of artificial intelligence (AI)-based dynamic\npricing algorithms. Nevertheless, e-commerce platforms employ recommendation\nalgorithms to allocate exposure to various products, and this important aspect\nhas been largely overlooked in previous studies on algorithmic collusion. Our\nstudy bridges this important gap in the literature and examines how\nrecommendation algorithms can determine the competitive or collusive dynamics\nof AI-based pricing algorithms. Specifically, two commonly deployed\nrecommendation algorithms are examined: (i) a recommender system that aims to\nmaximize the sellers' total profit (profit-based recommender system) and (ii) a\nrecommender system that aims to maximize the demand for products sold on the\nplatform (demand-based recommender system). We construct a repeated game\nframework that incorporates both pricing algorithms adopted by sellers and the\nplatform's recommender system. Subsequently, we conduct experiments to observe\nprice dynamics and ascertain the final equilibrium. Experimental results reveal\nthat a profit-based recommender system intensifies algorithmic collusion among\nsellers due to its congruence with sellers' profit-maximizing objectives.\nConversely, a demand-based recommender system fosters price competition among\nsellers and results in a lower price, owing to its misalignment with sellers'\ngoals. Extended analyses suggest the robustness of our findings in various\nmarket scenarios. Overall, we highlight the importance of platforms'\nrecommender systems in delineating the competitive structure of the digital\nmarketplace, providing important insights for market participants and\ncorresponding policymakers.\n"
    },
    {
        "paper_id": 2309.14615,
        "authors": "Foozhan Ataiefard, Hadi Hemmati",
        "title": "Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading\n  Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, deep reinforcement learning (Deep RL) has been successfully\nimplemented as a smart agent in many systems such as complex games,\nself-driving cars, and chat-bots. One of the interesting use cases of Deep RL\nis its application as an automated stock trading agent. In general, any\nautomated trading agent is prone to manipulations by adversaries in the trading\nenvironment. Thus studying their robustness is vital for their success in\npractice. However, typical mechanism to study RL robustness, which is based on\nwhite-box gradient-based adversarial sample generation techniques (like FGSM),\nis obsolete for this use case, since the models are protected behind secure\ninternational exchange APIs, such as NASDAQ. In this research, we demonstrate\nthat a \"gray-box\" approach for attacking a Deep RL-based trading agent is\npossible by trading in the same stock market, with no extra access to the\ntrading agent. In our proposed approach, an adversary agent uses a hybrid Deep\nNeural Network as its policy consisting of Convolutional layers and\nfully-connected layers. On average, over three simulated trading market\nconfigurations, the adversary policy proposed in this research is able to\nreduce the reward values by 214.17%, which results in reducing the potential\nprofits of the baseline by 139.4%, ensemble method by 93.7%, and an automated\ntrading software developed by our industrial partner by 85.5%, while consuming\nsignificantly less budget than the victims (427.77%, 187.16%, and 66.97%,\nrespectively).\n"
    },
    {
        "paper_id": 2309.14784,
        "authors": "Francesca Biagini, Lukas Gonon, Niklas Walter",
        "title": "Approximation Rates for Deep Calibration of (Rough) Stochastic\n  Volatility Models",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive quantitative error bounds for deep neural networks (DNNs)\napproximating option prices on a $d$-dimensional risky asset as functions of\nthe underlying model parameters, payoff parameters and initial conditions. We\ncover a general class of stochastic volatility models of Markovian nature as\nwell as the rough Bergomi model. In particular, under suitable assumptions we\nshow that option prices can be learned by DNNs up to an arbitrary small error\n$\\varepsilon \\in (0,1/2)$ while the network size grows only sub-polynomially in\nthe asset vector dimension $d$ and the reciprocal $\\varepsilon^{-1}$ of the\naccuracy. Hence, the approximation does not suffer from the curse of\ndimensionality. As quantitative approximation results for DNNs applicable in\nour setting are formulated for functions on compact domains, we first consider\nthe case of the asset price restricted to a compact set, then we extend these\nresults to the general case by using convergence arguments for the option\nprices.\n"
    },
    {
        "paper_id": 2309.14869,
        "authors": "Pawel Robert Smolinski, Joseph Januszewicz, Barbara Pawlowska, Jacek\n  Winiarski",
        "title": "Nuclear Energy Acceptance in Poland: From Societal Attitudes to\n  Effective Policy Strategies -- Network Modeling Approach",
        "comments": "preprint Energy Policy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Poland is currently undergoing substantial transformation in its energy\nsector, and gaining public support is pivotal for the success of its energy\npolicies. We conducted a study with 338 Polish participants to investigate\nsocietal attitudes towards various energy sources, including nuclear energy and\nrenewables. Applying a novel network approach, we identified a multitude of\nfactors influencing energy acceptance. Political ideology is the central factor\nin shaping public acceptance, however we also found that environmental\nattitudes, risk perception, safety concerns, and economic variables play\nsubstantial roles. Considering the long-term commitment associated with nuclear\nenergy and its role in Poland's energy transformation, our findings provide a\nfoundation for improving energy policy in Poland. Our research underscores the\nimportance of policies that resonate with the diverse values, beliefs, and\npreferences of the population. While the risk-risk trade-off and\ntechnology-focused strategies are effective to a degree, we advocate for a more\ncomprehensive approach. The framing strategy, which tailors messages to\ndistinct societal values, shows particular promise.\n"
    },
    {
        "paper_id": 2309.14964,
        "authors": "Sibel Eker, Charlie Wilson, Niklas H\\\"ohne, Mark S. McCaffrey, Irene\n  Monasterolo, Leila Niamir, Caroline Zimm",
        "title": "A dynamic systems approach to harness the potential of social tipping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social tipping points are promising levers to achieve net-zero greenhouse gas\nemission targets. They describe how social, political, economic or\ntechnological systems can move rapidly into a new state if cascading positive\nfeedback mechanisms are triggered. Analysing the potential of social tipping\nfor rapid decarbonization requires considering the inherent complexity of\nsocial systems. Here, we identify that existing scientific literature is\ninclined to a narrative-based account of social tipping, lacks a broad\nempirical framework and a multi-systems view. We subsequently outline a dynamic\nsystems approach that entails (i) a systems outlook involving interconnected\nfeedback mechanisms alongside cross-system and cross-scale interactions, and\nincluding a socioeconomic and environmental injustice perspective (ii) directed\ndata collection efforts to provide empirical evidence for and monitor social\ntipping dynamics, (iii) global, integrated, descriptive modelling to project\nfuture dynamics and provide ex-ante evidence for interventions. Research on\nsocial tipping must be accordingly solidified for climate policy relevance.\n"
    },
    {
        "paper_id": 2309.15044,
        "authors": "Andrey Itkin",
        "title": "The ATM implied skew in the ADO-Heston model",
        "comments": "23 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper similar to [P. Carr, A. Itkin, 2019] we construct another\nMarkovian approximation of the rough Heston-like volatility model - the\nADO-Heston model. The characteristic function (CF) of the model is derived\nunder both risk-neutral and real measures which is an unsteady\nthree-dimensional PDE with some coefficients being functions of the time $t$\nand the Hurst exponent $H$. To replicate known behavior of the market implied\nskew we proceed with a wise choice of the market price of risk, and then find a\nclosed form expression for the CF of the log-price and the ATM implied skew.\nBased on the provided example, we claim that the ADO-Heston model (which is a\npure diffusion model but with a stochastic mean-reversion speed of the variance\nprocess, or a Markovian approximation of the rough Heston model) is able\n(approximately) to reproduce the known behavior of the vanilla implied skew at\nsmall $T$. We conclude that the behavior of our implied volatility skew curve\n${\\cal S}(T) \\propto a(H) T^{b\\cdot (H-1/2)}, \\, b = const$, is not exactly\nsame as in rough volatility models since $b \\ne 1$, but seems to be close\nenough for all practical values of $T$. Thus, the proposed Markovian model is\nable to replicate some properties of the corresponding rough volatility model.\nSimilar analysis is provided for the forward starting options where we found\nthat the ATM implied skew for the forward starting options can blow-up for any\n$s > t$ when $T \\to s$. This result, however, contradicts to the observation of\n[E. Alos, D.G. Lorite, 2021] that Markovian approximation is not able to catch\nthis behavior, so remains the question on which one is closer to reality.\n"
    },
    {
        "paper_id": 2309.15309,
        "authors": "Ye Sun, Athen Ma, Georg von Graevenitz, Vito Latora",
        "title": "The importance of quality in austere times: University competitiveness\n  and grant income",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  After 2009 many governments implemented austerity measures, often restricting\nscience funding. Did such restrictions further skew grant income towards elite\nscientists and universities? And did increased competition for funding\nundermine participation? UK science funding agencies significantly reduced\nnumbers of grants and total grant funding in response to austerity, but\nsurprisingly restrictions of science funding were relaxed after the 2015\ngeneral election. Exploiting this natural experiment, we show that conventional\nmeasures of university competitiveness are poor proxies for competitiveness. An\nalternative measure of university competitiveness, drawn from complexity\nscience, captures the highly dynamical way in which universities engage in\nscientific subjects. Building on a data set of 43,430 UK funded grants between\n2006 and 2020, we analyse rankings of UK universities and investigate the\neffect of research competitiveness on grant income. When austerity was relaxed\nin 2015 the elasticity of grant income w.r.t. research competitiveness fell,\nreflecting increased effort by researchers at less competitive universities.\nThese scientists increased number and size of grant applications, increasing\ngrant income. The study reveals how funding agencies, facing heterogeneous\ncompetitiveness in the population of scientists, affect research effort across\nthe distribution of competitiveness.\n"
    },
    {
        "paper_id": 2309.15368,
        "authors": "Lucas Woodley, Chung Yi See, Peter Cook, Megan Yeo, Daniel S. Palmer,\n  Laurena Huh, Seaver Wang, and Ashley Nunes",
        "title": "Enumerating the climate impact of disequilibrium in critical mineral\n  supply",
        "comments": "14 pages, 3 figures, 2 tables for main paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently proposed tailpipe emissions standards aim to significant increases\nin electric vehicle (EV) sales in the United States. Our work examines whether\nthis increase is achievable given potential constraints in EV mineral supply\nchains. We estimate a model that reflects international sourcing rules,\nheterogeneity in the mineral intensity of predominant battery chemistries, and\nlong-run grid decarbonization efforts. Our efforts yield five key findings.\nFirst, compliance with the proposed standard necessitates replacing at least\n10.21 million new ICEVs with EVs between 2027 and 2032. Second, based on\neconomically viable and geologically available mineral reserves, manufacturing\nsufficient EVs is plausible across most battery chemistries and could, subject\nto the chemistry leveraged, reduce up to 457.3 million total tons of CO2e.\nThird, mineral production capacities of the US and its allies constrain battery\nproduction to a total of 5.09 million EV batteries between 2027 and 2032, well\nshort of deployment requirements to meet EPA standards even if battery\nmanufacturing is optimized to exclusively manufacture materials efficient NMC\n811 batteries. Fourth, disequilibrium between mineral supply and demand results\nin at least 59.54 million tons of CO2e in total lost lifecycle emissions\nbenefits. Fifth, limited present-day production of battery-grade graphite and\nto a lesser extent, cobalt, constrain US electric vehicle battery pack\nmanufacturing under strict sourcing rules. We demonstrate that should mineral\nsupply bottlenecks persist, hybrid electric vehicles may offer equivalent\nlifecycle emissions benefits as EVs while relaxing mineral production demands,\nthough this represents a tradeoff of long-term momentum in electric vehicle\ndeployment in favor of near-term carbon dioxide emissions reductions.\n"
    },
    {
        "paper_id": 2309.15511,
        "authors": "Bikramjit Das and Vicky Fasen-Hartmann",
        "title": "Measuring risk contagion in financial networks with CoVaR",
        "comments": "39 pages, 2 figures. Modifications: (1) Section 2 from previous\n  version with discussion on pairwise and mutual asymptotic independence is\n  removed (a separate note on this will appear later), (2) Title of the paper\n  is changed, (3) Minor modifications made to previous results and proofs, (4)\n  New references are added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stability of a complex financial system may be assessed by measuring risk\ncontagion between various financial institutions with relatively high exposure.\nWe consider a financial network model using a bipartite graph of financial\ninstitutions (e.g., banks, investment companies, insurance firms) on one side\nand financial assets on the other. Following empirical evidence, returns from\nsuch risky assets are modeled by heavy-tailed distributions, whereas their\njoint dependence is characterized by copula models exhibiting a variety of tail\ndependence behavior. We consider CoVaR, a popular measure of risk contagion and\nstudy its asymptotic behavior under broad model assumptions. We further propose\nthe Extreme CoVaR Index (ECI) for capturing the strength of risk contagion\nbetween risk entities in such networks, which is particularly useful for models\nexhibiting asymptotic independence. The results are illustrated by providing\nprecise expressions of CoVaR and ECI when the dependence of the assets is\nmodeled using two well-known multivariate dependence structures: the Gaussian\ncopula and the Marshall-Olkin copula.\n"
    },
    {
        "paper_id": 2309.15552,
        "authors": "Mark Potanin, Andrey Chertok, Konstantin Zorin, Cyril Shtabtsovsky",
        "title": "Startup success prediction and VC portfolio simulation using CrunchBase\n  data",
        "comments": "13 pages, preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting startup success presents a formidable challenge due to the\ninherently volatile landscape of the entrepreneurial ecosystem. The advent of\nextensive databases like Crunchbase jointly with available open data enables\nthe application of machine learning and artificial intelligence for more\naccurate predictive analytics. This paper focuses on startups at their Series B\nand Series C investment stages, aiming to predict key success milestones such\nas achieving an Initial Public Offering (IPO), attaining unicorn status, or\nexecuting a successful Merger and Acquisition (M\\&A). We introduce novel deep\nlearning model for predicting startup success, integrating a variety of factors\nsuch as funding metrics, founder features, industry category. A distinctive\nfeature of our research is the use of a comprehensive backtesting algorithm\ndesigned to simulate the venture capital investment process. This simulation\nallows for a robust evaluation of our model's performance against historical\ndata, providing actionable insights into its practical utility in real-world\ninvestment contexts. Evaluating our model on Crunchbase's, we achieved a 14\ntimes capital growth and successfully identified on B round high-potential\nstartups including Revolut, DigitalOcean, Klarna, Github and others. Our\nempirical findings illuminate the importance of incorporating diverse feature\nsets in enhancing the model's predictive accuracy. In summary, our work\ndemonstrates the considerable promise of deep learning models and alternative\nunstructured data in predicting startup success and sets the stage for future\nadvancements in this research area.\n"
    },
    {
        "paper_id": 2309.15574,
        "authors": "Shuyao Wu, Kai-Di Liu, Wentao Zhang, Yuehan Dou, Yuqing Chen, Delong\n  Li",
        "title": "To better understand realized ecosystem services: An integrated analysis\n  framework of supply, demand, flow and use",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Realized ecosystem services (ES) are the actual use of ES by societies, which\nis more directly linked to human well-being than potential ES. However, there\nis a lack of a general analysis framework to understand how much ES was\nrealized. In this study, we first proposed a Supply-Demand-Flow-Use (SDFU)\nframework that integrates the supply, demand, flow, and use of ES and\ndifferentiates these concepts into different aspects (e.g., potential vs.\nactual ES demand, export and import flows of supply, etc.). Then, we applied\nthe framework to three examples of ES that can be found in typical urban green\nparks (i.e., wild berry supply, pollination, and recreation). We showed how the\nframework could assess the actual use of ES and identify the supply-limited,\ndemand-limited, and supply-demand-balanced types of realized ES. We also\ndiscussed the scaling features, temporal dynamics, and spatial characteristics\nof realized ES, as well as some critical questions for future studies. Although\nfacing challenges, we believe that the applications of the SDFU framework can\nprovide a systematic way to accurately assess the actual use of ES and better\ninform management and policy-making for sustainable use of nature's benefits.\nTherefore, we hope that our study will stimulate more research on realized ES\nand contribute to a deeper understanding of their roles in enhancing human\nwell-being.\n"
    },
    {
        "paper_id": 2309.1564,
        "authors": "Jakub Micha\\'nk\\'ow, Pawe{\\l} Sakowski, Robert \\'Slepaczuk",
        "title": "Hedging Properties of Algorithmic Investment Strategies using Long\n  Short-Term Memory and Time Series models for Equity Indices",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel approach to hedging portfolios of risky assets\nwhen financial markets are affected by financial turmoils. We introduce a\ncompletely novel approach to diversification activity not on the level of\nsingle assets but on the level of ensemble algorithmic investment strategies\n(AIS) built based on the prices of these assets. We employ four types of\ndiverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH -\nAutoregressive Integrated Moving Average - Generalized Autoregressive\nConditional Heteroskedasticity, momentum, and contrarian) to generate price\nforecasts, which are then used to produce investment signals in single and\ncomplex AIS. In such a way, we are able to verify the diversification potential\nof different types of investment strategies consisting of various assets\n(energy commodities, precious metals, cryptocurrencies, or soft commodities) in\nhedging ensemble AIS built for equity indices (S&P 500 index). Empirical data\nused in this study cover the period between 2004 and 2022. Our main conclusion\nis that LSTM-based strategies outperform the other models and that the best\ndiversifier for the AIS built for the S&P 500 index is the AIS built for\nBitcoin. Finally, we test the LSTM model for a higher frequency of data (1\nhour). We conclude that it outperforms the results obtained using daily data.\n"
    },
    {
        "paper_id": 2309.15767,
        "authors": "Paul Alexander Bilokon",
        "title": "Implementing portfolio risk management and hedging in practice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In academic literature portfolio risk management and hedging are often versed\nin the language of stochastic control and Hamilton--Jacobi--Bellman~(HJB)\nequations in continuous time. In practice the continuous-time framework of\nstochastic control may be undesirable for various business reasons. In this\nwork we present a straightforward approach for thinking of cross-asset\nportfolio risk management and hedging, providing some implementation details,\nwhile rarely venturing outside the convex optimisation setting of (approximate)\nquadratic programming~(QP). We pay particular attention to the correspondence\nbetween the economic concepts and their mathematical representations; the\nabstractions enabling us to handle multiple asset classes and risk models at\nonce; the dimensional analysis of the resulting equations; and the assumptions\ninherent in our derivations. We demonstrate how to solve the resulting QPs with\nCVXOPT.\n"
    },
    {
        "paper_id": 2309.1589,
        "authors": "Alexander P. Kartun-Giles and Nadia Ameli",
        "title": "An Introduction to Complex Networks in Climate Finance",
        "comments": "12 pages, 5 figures",
        "journal-ref": "Entropy 25(10), Special Issue on Complexity in Economics and\n  Finance: New Directions and Challenges, 2023",
        "doi": "10.3390/e25101371",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this perspective, we introduce recent research into the structure and\nfunction of complex investor networks supporting sustainability efforts. Using\nthe case of solar, wind and hydro energy technologies, this perspective\nexplores the complexity in low-carbon finance markets, defined as markets that\ndirect capital flows towards low-carbon technologies, using network approaches\nto study their structure and dynamics. Investors are modeled as nodes which\nform a network or higher-order network connected by edges representing projects\nin which joint funding or security-related insurance was provided or other\ninvestment-related interaction occurred. We review the literature on investor\nnetworks generally, particularly in the case of complex networks, and address\nareas where these ideas were applied in this emerging field. The complex\ninvestor dynamics which emerge from the extant funding scenarios are not well\nunderstood. These dynamics have the potential to result in interesting\nnon-linear behaviour, growth, and decline, which can be studied, explained and\ncontrolled using the tools of network science.\n"
    },
    {
        "paper_id": 2309.16008,
        "authors": "Boming Ning, Prakash Chakraborty, Kiseop Lee",
        "title": "Optimal Entry and Exit with Signature in Statistical Arbitrage",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore an optimal timing strategy for the trading of price\nspreads exhibiting mean-reverting characteristics. A sequential optimal\nstopping framework is formulated to analyze the optimal timings for both\nentering and subsequently liquidating positions, all while considering the\nimpact of transaction costs. Then we leverages a refined signature optimal\nstopping method to resolve this sequential optimal stopping problem, thereby\nunveiling the precise entry and exit timings that maximize gains. Our framework\noperates without any predefined assumptions regarding the dynamics of the\nunderlying mean-reverting spreads, offering adaptability to diverse scenarios.\nNumerical results are provided to demonstrate its superior performance when\ncomparing with conventional mean reversion trading rules.\n"
    },
    {
        "paper_id": 2309.16047,
        "authors": "Puru Gupta, Saul D. Jacka",
        "title": "Portfolio Choice In Dynamic Thin Markets: Merton Meets Cournot",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an augmented version of Merton's portfolio choice problem, where\ntrading by large investors influences the price of underlying financial asset\nleading to strategic interaction among investors, with investors deciding their\ntrading rates independently and simultaneously at each instant, in the spirit\nof dynamic Cournot competition, modelled here as a non-zero sum singular\nstochastic differential game. We establish an equivalence result for the value\nfunctions of an investor's best-response problem, which is a singular\nstochastic optimal control problem, and an auxiliary classical stochastic\noptimal control problem by exploiting the invariance of the value functions\nwith respect to a diffeomorphic integral flow associated with the drift\ncoefficient of the best-response problem. Under certain regularity conditions,\nwe show that the optimal trajectories of the two control problems coincide,\nwhich permits analytical characterization of Markov-Nash equilibrium\nportfolios. For the special case when asset price volatility is constant, we\nshow that the unique Nash equilibrium is deterministic, and provide a\nclosed-form solution which illuminates the role of imperfect competition in\nexplaining the excessive trade puzzle.\n"
    },
    {
        "paper_id": 2309.16186,
        "authors": "Christian Fries, Lennart Quante",
        "title": "Intergenerational Equity in Models of Climate Change Mitigation:\n  Stochastic Interest Rates introduce Adverse Effects, but (Non-linear) Funding\n  Costs can Improve Intergenerational Equity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Assessing the costs of climate change is essential to finding efficient\npathways for the transition to a net-zero emissions economy, which is necessary\nto stabilise global temperatures at any level. In evaluating the benefits and\ncosts of climate change mitigation, the discount rate converting future damages\nand costs into net-present values influences the timing of mitigation.\n  Here, we amend the DICE model with a stochastic interest rate model to\nconsider the uncertainty of discount rates in the future. Since abatement\nreduces future damages, changing interest rates renders abatement investments\nmore or less beneficial. Stochastic interest rates will hence lead to a\nstochastic abatement strategy.\n  We introduce a simple stochastic abatement model and show that this can\nincrease intergenerational inequality concerning cost and risk.\n  Analysing the sensitivities of the model calibration analytically and\nnumerically exhibits that intergenerational inequality is a consequence of the\nDICE model calibration (and maybe that of IAMs in general).\n  We then show that introducing funding of abatement costs reduces the\nvariation of future cash-flows, which occur at different times but are\noff-setting in their net-present value. This effect can be interpreted as\nimproving intergenerational effort sharing, which might be neglected in\nclassical optimisation. This mechanism is amplified, including dependence of\nthe interest rate risk on the amount of debt to be financed, i.e. considering\nthe limited capacity of funding sources. As an alternative policy optimisation\nmethod, we propose limiting the total cost of damages and abatement below a\nfixed level relative to GDP - this modification induces equality between\ngenerations compared to their respective economic welfare, inducing early and\nfast mitigation of climate change to keep the total cost of climate change\nbelow 3% of global GDP.\n"
    },
    {
        "paper_id": 2309.16196,
        "authors": "Wenting Liu and Zhaozhong Gui and Guilin Jiang and Lihua Tang and\n  Lichun Zhou and Wan Leng and Xulong Zhang and Yujiang Liu",
        "title": "Stock Volatility Prediction Based on Transformer Model Using\n  Mixed-Frequency Data",
        "comments": "Accepted by the 7th APWeb-WAIM International Joint Conference on Web\n  and Big Data. (APWeb 2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the increasing volume of high-frequency data in the information age,\nboth challenges and opportunities arise in the prediction of stock volatility.\nOn one hand, the outcome of prediction using tradition method combining stock\ntechnical and macroeconomic indicators still leaves room for improvement; on\nthe other hand, macroeconomic indicators and peoples' search record on those\nsearch engines affecting their interested topics will intuitively have an\nimpact on the stock volatility. For the convenience of assessment of the\ninfluence of these indicators, macroeconomic indicators and stock technical\nindicators are then grouped into objective factors, while Baidu search indices\nimplying people's interested topics are defined as subjective factors. To align\ndifferent frequency data, we introduce GARCH-MIDAS model. After mixing all the\nabove data, we then feed them into Transformer model as part of the training\ndata. Our experiments show that this model outperforms the baselines in terms\nof mean square error. The adaption of both types of data under Transformer\nmodel significantly reduces the mean square error from 1.00 to 0.86.\n"
    },
    {
        "paper_id": 2309.16408,
        "authors": "Pietro Saggese, Esther Segalla, Michael Sigmund, Burkhard Raunig,\n  Felix Zangerl, Bernhard Haslhofer",
        "title": "Assessing the Solvency of Virtual Asset Service Providers: Are Current\n  Standards Sufficient?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Entities like centralized cryptocurrency exchanges fall under the business\ncategory of virtual asset service providers (VASPs). As any other enterprise,\nthey can become insolvent. VASPs enable the exchange, custody, and transfer of\ncryptoassets organized in wallets across distributed ledger technologies\n(DLTs). Despite the public availability of DLT transactions, the cryptoasset\nholdings of VASPs are not yet subject to systematic auditing procedures. In\nthis paper, we propose an approach to assess the solvency of a VASP by\ncross-referencing data from three distinct sources: cryptoasset wallets,\nbalance sheets from the commercial register, and data from supervisory\nentities. We investigate 24 VASPs registered with the Financial Market\nAuthority in Austria and provide regulatory data insights such as who are the\ncustomers and where do they come from. Their yearly incoming and outgoing\ntransaction volume amount to 2 billion EUR for around 1.8 million users. We\ndescribe what financial services they provide and find that they are most\nsimilar to traditional intermediaries such as brokers, money exchanges, and\nfunds, rather than banks. Next, we empirically measure DLT transaction flows of\nfour VASPs and compare their cryptoasset holdings to balance sheet entries.\nData are consistent for two VASPs only. This enables us to identify gaps in the\ndata collection and propose strategies to address them. We remark that any\nentity in charge of auditing requires proof that a VASP actually controls the\nfunds associated with its on-chain wallets. It is also important to report fiat\nand cryptoasset and liability positions broken down by asset types at a\nreasonable frequency.\n"
    },
    {
        "paper_id": 2309.16437,
        "authors": "Sam Arts, Nicola Melluso, Reinhilde Veugelers",
        "title": "Beyond Citations: Measuring Novel Scientific Ideas and their Impact in\n  Publication Text",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  New scientific ideas fuel economic progress, yet their identification and\nmeasurement remains challenging. In this paper, we use natural language\nprocessing to identify the origin and impact of new scientific ideas in the\ntext of scientific publications. To validate the new techniques and their\nimprovement over traditional metrics based on citations, we first leverage\nNobel prize papers that likely pioneered new scientific ideas with a major\nimpact on scientific progress. Second, we use literature review papers that\ntypically summarize existing knowledge rather than pioneer new scientific\nideas. Finally, we demonstrate that papers introducing new scientific ideas are\nmore likely to become highly cited by both publications and patents. We provide\nopen access to code and data for all scientific papers up to December 2020.\n"
    },
    {
        "paper_id": 2309.1644,
        "authors": "Marco Diana, Andrea Chicco",
        "title": "The effect of COVID restriction levels on shared micromobility travel\n  patterns: A comparison between dockless bike sharing and e-scooter services",
        "comments": "Paper presented at the 101st Annual Meeting of the Transportation\n  Research Board, Washington, D.C., January 9-13, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The spread of the coronavirus pandemic had negative repercussions on the\nmajority of transport systems in virtually all countries. After the lockdown\nperiod, travel restriction policies are now frequently adapted almost real-time\naccording to observed trends in the spread of the disease, resulting in a\nrapidly changing transport market situation. Shared micromobility operators,\nwhose revenues entirely come from their customers, need to understand how the\ndemand is affected to adapt their operations. Within this framework, the\npresent paper investigates how different COVID-19 restriction levels have\naffected the usage patterns of shared micromobility.\n  Usage data of two dockless micromobility services (bike and e-scooters)\noperating in Turin (Italy) are analyzed between October 2020 and March 2021, a\nperiod characterized by different travel restriction levels. The average number\nof daily trips, trip distances and trip duration are retrieved for both\nservices, and then compared to identify significant differences in trends as\nrestriction levels change. Additionally, related impacts on the spatial\ndimension of the services are studied through hotspot maps.\n  Results show that both services decreased during restrictions, however\ne-scooters experienced a larger variability in their demand and they had a\nquicker recovery when travel restrictions were loosened. Shared bikes, in\ngeneral, suffered less from travel restriction levels, suggesting their larger\nusage for work and study-related trip purposes, which is confirmed also by the\nanalysis of hotspots. E-scooters are both substituting and complementing public\ntransport according to restriction levels, while usage patterns of shared bikes\nare more independent.\n"
    },
    {
        "paper_id": 2309.16678,
        "authors": "Jorge Garcia-Hernandez, Roy Brouwer",
        "title": "Water Markets as a Coping Mechanism for Climate-Induced Water Changes on\n  the Canadian Economy: A Computable General Equilibrium Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Water markets represent a policy tool that aims at finding efficient water\nallocations among competing users by promoting reallocations from low-value to\nhigh-value uses. In Canada, water markets have been discussed and implemented\nat the provincial level; however, at the national level a study about the\neconomic benefits of its implementation is still lacking. This paper fills this\nvoid by implementing a water market in Canada and examine how water endowment\nshocks would affect the economy under the assumptions of general equilibrium\ntheory. Our results show a water market would damp the economic loss in case of\nreductions in water endowment, but it also cuts back on the economic expansion\nthat would follow from an increase on it. These results provide new insights on\nthe subject and will provide a novel look and reinvigorate informed discussions\non the use of water markets in Canada as a potential tool to cope with\nclimate-induced water supply changes.\n"
    },
    {
        "paper_id": 2309.16679,
        "authors": "Paraskevi Nousi, Loukia Avramelou, Georgios Rodinos, Maria Tzelepi,\n  Theodoros Manousis, Konstantinos Tsampazis, Kyriakos Stefanidis, Dimitris\n  Spanos, Manos Kirtas, Pavlos Tosidis, Avraam Tsantekidis, Nikolaos Passalis\n  and Anastasios Tefas",
        "title": "Leveraging Deep Learning and Online Source Sentiment for Financial\n  Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial portfolio management describes the task of distributing funds and\nconducting trading operations on a set of financial assets, such as stocks,\nindex funds, foreign exchange or cryptocurrencies, aiming to maximize the\nprofit while minimizing the loss incurred by said operations. Deep Learning\n(DL) methods have been consistently excelling at various tasks and automated\nfinancial trading is one of the most complex one of those. This paper aims to\nprovide insight into various DL methods for financial trading, under both the\nsupervised and reinforcement learning schemes. At the same time, taking into\nconsideration sentiment information regarding the traded assets, we discuss and\ndemonstrate their usefulness through corresponding research studies. Finally,\nwe discuss commonly found problems in training such financial agents and equip\nthe reader with the necessary knowledge to avoid these problems and apply the\ndiscussed methods in practice.\n"
    },
    {
        "paper_id": 2309.16695,
        "authors": "Mohammad Hadavi, Lutong Sun, Djordje Romanic",
        "title": "Normalized insured losses caused by windstorms in Quebec and Ontario,\n  Canada, in the period 2008-2021",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Severe windstorms pose threats to people, human-made structures, and the\nenvironment. An investigation of insured losses caused by windstorms is a\nmultipurpose study that serves to advance the resilience and sustainability of\nmodern communities. The present study proposes a systematic analysis of insured\nlosses imposed by different types of windstorms in two Canadian provinces,\nOntario (ON) and Quebec (QC), during the period 2008-2021. Actual wind damage\ndata from the Canadian insurance market were considered in this study. Our\ncalculations show that ON and QC received half of all wind catastrophes across\nCanada, and nearly three-quarters of all types of catastrophes in ON and QC\nwere wind-related ones. The total windstorm loss of over CA$5.2 billion was not\nevenly distributed between QC and ON, but rather had a QC:ON ratio of 1:3.1. We\nattributed this discrepancy in the inflicted damage between two provinces to\nthe predominantly eastward and northeastward storm trajectories and the higher\ndensity of wealth and population in ON. Convective storms were the most\ndevastating wind type comprising nearly 65% and 67% of the total number of\nevents and associated damage, respectively. Finally, tornadoes had the highest\naverage loss per event in two provinces combined. Future prospects and the\nimplication of this research are also discussed.\n"
    },
    {
        "paper_id": 2309.16888,
        "authors": "Lele Cao, Gustaf Halvardsson, Andrew McCornack, Vilhelm von Ehrenheim\n  and Pawel Herman",
        "title": "Beyond Gut Feel: Using Time Series Transformers to Find Investment Gems",
        "comments": "Published by ICANN (33rd International Conference on Artificial\n  Neural Networks) 2024 as full paper (15 pages and 7 figures)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the growing application of data-driven approaches within\nthe Private Equity (PE) industry, particularly in sourcing investment targets\n(i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present\na comprehensive review of the relevant approaches and propose a novel approach\nleveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for\npredicting the success likelihood of any candidate company. The objective of\nour research is to optimize sourcing performance for VC and GC investments by\nformally defining the sourcing problem as a multivariate time series\nclassification task. We consecutively introduce the key components of our\nimplementation which collectively contribute to the successful application of\nTMTSC in VC/GC sourcing: input features, model architecture, optimization\ntarget, and investor-centric data processing. Our extensive experiments on two\nreal-world investment tasks, benchmarked towards three popular baselines,\ndemonstrate the effectiveness of our approach in improving decision making\nwithin the VC and GC industry.\n"
    },
    {
        "paper_id": 2309.17147,
        "authors": "Julian Ashwin, Aditya Chhabra and Vijayendra Rao",
        "title": "Using Large Language Models for Qualitative Analysis can Introduce\n  Serious Bias",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large Language Models (LLMs) are quickly becoming ubiquitous, but the\nimplications for social science research are not yet well understood. This\npaper asks whether LLMs can help us analyse large-N qualitative data from\nopen-ended interviews, with an application to transcripts of interviews with\nRohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of\ncaution is needed in using LLMs to annotate text as there is a risk of\nintroducing biases that can lead to misleading inferences. We here mean bias in\nthe technical sense, that the errors that LLMs make in annotating interview\ntranscripts are not random with respect to the characteristics of the interview\nsubjects. Training simpler supervised models on high-quality human annotations\nwith flexible coding leads to less measurement error and bias than LLM\nannotations. Therefore, given that some high quality annotations are necessary\nin order to asses whether an LLM introduces bias, we argue that it is probably\npreferable to train a bespoke model on these annotations than it is to use an\nLLM for annotation.\n"
    },
    {
        "paper_id": 2309.17216,
        "authors": "Juan Ponce, Jos\\'e-Ignacio Ant\\'on, Mercedes Onofa, Roberto Castillo",
        "title": "The long-term impact of (un)conditional cash transfers on labour market\n  outcomes in Ecuador",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite the popularity of conditional cash transfers in low- and\nmiddle-income countries, evidence on their long-term effects remains scarce.\nThis paper assesses the impact of the Ecuador's Human Development Grant on the\nformal sector labour market outcomes of children in eligible households. This\ngrant -- one of the first of its kind -- is characterised by weak enforcement\nof its eligibility criteria. By means of a regression discontinuity design, we\nfind that this programme increased formal employment rates and labour income\naround a decade after exposure, thereby curbing the intergenerational\ntransmission of poverty. We discuss possible mediating mechanisms based on\nfindings from previous literature and, in particular, provide evidence on how\nthe programme contributed to persistence in school in the medium run.\n"
    },
    {
        "paper_id": 2309.17219,
        "authors": "Christian Bongiorno and Damien Challet",
        "title": "Covariance matrix filtering and portfolio optimisation: the Average\n  Oracle vs Non-Linear Shrinkage and all the variants of DCC-NLS",
        "comments": "9 pages, 2 figures, 6 tables,",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Average Oracle, a simple and very fast covariance filtering method, is\nshown to yield superior Sharpe ratios than the current state-of-the-art (and\ncomplex) methods, Dynamic Conditional Covariance coupled to Non-Linear\nShrinkage (DCC+NLS). We pit all the known variants of DCC+NLS (quadratic\nshrinkage, gross-leverage or turnover limitations, and factor-augmented NLS)\nagainst the Average Oracle in large-scale randomized experiments. We find\ngenerically that while some variants of DCC+NLS sometimes yield the lowest\naverage realized volatility, albeit with a small improvement, their excessive\ngross leverage and investment concentration, and their 10-time larger turnover\ncontribute to smaller average portfolio returns, which mechanically result in\nsmaller realized Sharpe ratios than the Average Oracle. We also provide simple\nanalytical arguments about the origin of the advantage of the Average Oracle\nover NLS in a changing world.\n"
    },
    {
        "paper_id": 2309.17268,
        "authors": "Viktor Stojkoski, Sonja Mitikj, Marija Trpkova-Nestorovska, Dragan\n  Tevdovski",
        "title": "Income Mobility and Mixing in North Macedonia",
        "comments": "Research in progress to appear at the 4th International Scientific\n  Conference on \"Economic and Business Trends Shaping the Future\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents the inaugural analysis of income mobility in North\nMacedonia from 1995-2021 using the Mixing Time and Mean First Passage Time\n(MFPT) metrics. We document larger mobility (in terms of Mixing Time) during\nthe '90s, with and decreasing trend (in terms of mobility) until 1999. After\nthis year the Mixing time has been consistent with a value of around 4 years.\nUsing the MFPT, we highlight the evolving challenges individuals face when\naspiring to higher income tiers. Namely, we show that there was a noticeable\nupward trend in MFPT from 1995 to 2006, a subsequent decline until 2017, and\nthen an ascent again, peaking in 2021. These findings provide a foundational\nperspective on the income mobility in North Macedonia.\n"
    },
    {
        "paper_id": 2309.17322,
        "authors": "Paul Glasserman, Caden Lin",
        "title": "Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT\n  Sentiment Analysis",
        "comments": "17 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large language models (LLMs), including ChatGPT, can extract profitable\ntrading signals from the sentiment in news text. However, backtesting such\nstrategies poses a challenge because LLMs are trained on many years of data,\nand backtesting produces biased results if the training and backtesting periods\noverlap. This bias can take two forms: a look-ahead bias, in which the LLM may\nhave specific knowledge of the stock returns that followed a news article, and\na distraction effect, in which general knowledge of the companies named\ninterferes with the measurement of a text's sentiment. We investigate these\nsources of bias through trading strategies driven by the sentiment of financial\nnews headlines. We compare trading performance based on the original headlines\nwith de-biased strategies in which we remove the relevant company's identifiers\nfrom the text. In-sample (within the LLM training window), we find,\nsurprisingly, that the anonymized headlines outperform, indicating that the\ndistraction effect has a greater impact than look-ahead bias. This tendency is\nparticularly strong for larger companies--companies about which we expect an\nLLM to have greater general knowledge. Out-of-sample, look-ahead bias is not a\nconcern but distraction remains possible. Our proposed anonymization procedure\nis therefore potentially useful in out-of-sample implementation, as well as for\nde-biased backtesting.\n"
    },
    {
        "paper_id": 2309.17346,
        "authors": "Alessandro Mutti and Patrizia Semeraro",
        "title": "Symmetric Bernoulli distributions and minimal dependence copulas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The key result of this paper is to find all the joint distributions of random\nvectors whose sums $S=X_1+\\ldots+X_d$ are minimal in convex order in the class\nof symmetric Bernoulli distributions. The minimal convex sums distributions are\nknown to be strongly negatively dependent. Beyond their interest per se, these\nresults enable us to explore negative dependence within the class of copulas.\nIn fact, there are two classes of copulas that can be built from multivariate\nsymmetric Bernoulli distributions: the extremal mixture copulas, and the FGM\ncopulas. We study the extremal negative dependence structure of the copulas\ncorresponding to symmetric Bernoulli vectors with minimal convex sums and we\nexplicitly find a class of minimal dependence copulas. Our main results stem\nfrom the geometric and algebraic representations of multivariate symmetric\nBernoulli distributions, which effectively encode several of their statistical\nproperties.\n"
    },
    {
        "paper_id": 2309.17379,
        "authors": "Ir\\`ene Irakoze, R\\'edempteur Ntawiratsa, David Niyukuri",
        "title": "Handling missing data in Burundian sovereign bond market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constructing an accurate yield curve is essential for evaluating financial\ninstruments and analyzing market trends in the bond market. However, in the\ncase of the Burundian sovereign bond market, the presence of missing data poses\na significant challenge to accurately constructing the yield curve. In this\npaper, we explore the limitations and data availability constraints specific to\nthe Burundian sovereign market and propose robust methodologies to effectively\nhandle missing data. The results indicate that the Linear Regression method,\nand the Previous value method perform consistently well across variables,\napproximating a normal distribution for the error values. The non parametric\nMissing Value Imputation using Random Forest (miss-Forest) method performs well\nfor coupon rates but poorly for bond prices, and the Next value method shows\nmixed results. Ultimately, the Linear Regression (LR) method is recommended for\nimputing missing data due to its ability to approximate normality and\npredictive capabilities. However, filling missing values with previous values\nhas high accuracy, thus, it will be the best choice when we have less\ninformation to be able to increase accuracy for LR. This research contributes\nto the development of financial products, trading strategies, and overall\nmarket development in Burundi by improving our understanding of the yield curve\ndynamics.\n"
    },
    {
        "paper_id": 2310.00231,
        "authors": "Cathal ODonoghue, Beenish Amjad, Jules Linden, Nora Lustig, Denisa\n  Sologon, Yang Wang",
        "title": "The Distributional Impact of Inflation in Pakistan: A Case Study of a\n  New Price Focused Microsimulation Framework, PRICES",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a microsimulation model to simulate the distributional\nimpact of price changes using Household Budget Survey data, income survey data\nand an Input Output Model. The primary purpose is to describe the model\ncomponents. The secondary purpose is to demonstrate one component of the model\nby assessing the distributional and welfare impact of recent price changes in\nPakistan. Over the period of November 2020 to November 2022, headline inflation\n41.5 percent, with food and transportation prices increasing most. The analysis\nshows that despite large increases in energy prices, the importance of energy\nprices for the welfare losses due to inflation is limited because energy budget\nshares are small and inflation is relatively low. The overall distributional\nimpact of recent price changes is mildly progressive, but household welfare is\nimpacted significantly irrespective of households position along the income\ndistribution. The biggest driver of welfare losses at the bottom of the income\ndistribution was food price inflation, while inflation in other goods and\nservices was the biggest driver at the top. To compensate households for\nincreased living costs, transfers would need to be on average 40 percent of\npre-inflation expenditure, assuming constant incomes. Behavioural responses to\nprice changes have a negligible impact on the overall welfare cost to\nhouseholds.\n"
    },
    {
        "paper_id": 2310.00321,
        "authors": "R\\'edempteur Ntawiratsa, David Niyukuri, Ir\\`ene Irakoze, Menus\n  Nkurunziza",
        "title": "Modeling the yield curve of Burundian bond market by parametric models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The term structure of interest rates (yield curve) is a critical facet of\nfinancial analytics, impacting various investment and risk management\ndecisions. It is used by the central bank to conduct and monitor its monetary\npolicy. That instrument reflects the anticipation of inflation and the risk by\ninvestors. The rates reported on yield curve are the cornerstone of valuation\nof all assets. To provide such tool for Burundi financial market, we collected\nthe auction reports of treasury securities from the website of the Central Bank\nof Burundi. Then, we computed the zero-coupon rates, and estimated actuarial\nrates of return by applying the Nelson-Siegel and Svensson models. This paper\nconducts a rigorous comparative analysis of these two prominent parametric\nyield curve models and finds that the Nelson-Siegel model is the optimal choice\nfor modeling the Burundian yield curve. The findings contribute to the body of\nknowledge on yield curve modeling, enhancing its precision and applicability in\nfinancial markets. Furthermore, this research holds implications for investment\nstrategies, risk management, second market pricing, financial decision-making,\nand the forthcoming establishment of the Burundian stock market.\n"
    },
    {
        "paper_id": 2310.00446,
        "authors": "Luca Mungo, Alexandra Brintrup, Diego Garlaschelli, Fran\\c{c}ois\n  Lafond",
        "title": "Reconstructing supply networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Network reconstruction is a well-developed sub-field of network science, but\nit has only recently been applied to production networks, where nodes are firms\nand edges represent customer-supplier relationships. We review the literature\nthat has flourished to infer the topology of these networks by partial,\naggregate, or indirect observation of the data. We discuss why this is an\nimportant endeavour, what needs to be reconstructed, what makes it different\nfrom other network reconstruction problems, and how different researchers have\napproached the problem. We conclude with a research agenda.\n"
    },
    {
        "paper_id": 2310.0049,
        "authors": "Ali Namaki, Reza Eyvazloo, Shahin Ramtinnia",
        "title": "A systematic review of early warning systems in finance",
        "comments": "20 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Early warning systems (EWSs) are critical for forecasting and preventing\neconomic and financial crises. EWSs are designed to provide early warning signs\nof financial troubles, allowing policymakers and market participants to\nintervene before a crisis expands. The 2008 financial crisis highlighted the\nimportance of detecting financial distress early and taking preventive measures\nto mitigate its effects. In this bibliometric review, we look at the research\nand literature on EWSs in finance. Our methodology included a comprehensive\nexamination of academic databases and a stringent selection procedure, which\nresulted in the final selection of 616 articles published between 1976 and\n2023. Our findings show that more than 90\\% of the papers were published after\n2006, indicating the growing importance of EWSs in financial research.\nAccording to our findings, recent research has shifted toward machine learning\ntechniques, and EWSs are constantly evolving. We discovered that research in\nthis area could be divided into four categories: bankruptcy prediction, banking\ncrisis, currency crisis and emerging markets, and machine learning forecasting.\nEach cluster offers distinct insights into the approaches and methodologies\nused for EWSs. To improve predictive accuracy, our review emphasizes the\nimportance of incorporating both macroeconomic and microeconomic data into EWS\nmodels. To improve their predictive performance, we recommend more research\ninto incorporating alternative data sources into EWS models, such as social\nmedia data, news sentiment analysis, and network analysis.\n"
    },
    {
        "paper_id": 2310.00553,
        "authors": "Tjeerd de Vries, Alexis Akira Toda",
        "title": "Robust Asset-Liability Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How should financial institutions hedge their balance sheets against interest\nrate risk when they have long-term assets and liabilities? Using the\nperspective of functional and numerical analysis, we propose a model-free bond\nportfolio selection method that generalizes classical immunization and\naccommodates arbitrary liability structure, portfolio constraints, and\nperturbations in interest rates. We prove the generic existence of an\nimmunizing portfolio that maximizes the worst-case equity with a tight error\nestimate and provide a solution algorithm. Numerical evaluations using\nempirical and simulated yield curves from a no-arbitrage term structure model\nsupport the feasibility and accuracy of our approach relative to existing\nmethods.\n"
    },
    {
        "paper_id": 2310.00606,
        "authors": "Yaowen Lu and Duy-Minh Dang",
        "title": "A semi-Lagrangian $\\epsilon$-monotone Fourier method for continuous\n  withdrawal GMWBs under jump-diffusion with stochastic interest rate",
        "comments": "43 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an efficient pricing approach for guaranteed minimum withdrawal\nbenefits (GMWBs) with continuous withdrawals under a realistic modeling setting\nwith jump-diffusions and stochastic interest rate. Utilizing an impulse\nstochastic control framework, we formulate the no-arbitrage GMWB pricing\nproblem as a time-dependent Hamilton-Jacobi-Bellman (HJB) Quasi-Variational\nInequality (QVI) having three spatial dimensions with cross derivative terms.\nThrough a novel numerical approach built upon a combination of a\nsemi-Lagrangian method and the Green's function of an associated linear partial\nintegro-differential equation, we develop an $\\epsilon$-monotone Fourier\npricing method, where $\\epsilon > 0$ is a monotonicity tolerance. Together with\na provable strong comparison result for the HJB-QVI, we mathematically\ndemonstrate convergence of the proposed scheme to the viscosity solution of the\nHJB-QVI as $\\epsilon \\to 0$. We present a comprehensive study of the impact of\nsimultaneously considering jumps in the sub-account process and stochastic\ninterest rate on the no-arbitrage prices and fair insurance fees of GMWBs, as\nwell as on the holder's optimal withdrawal behaviors.\n"
    },
    {
        "paper_id": 2310.00613,
        "authors": "Jiewei Li, Ling Jin, Han Deng, Lin Yang",
        "title": "Review on Decarbonizing the Transportation Sector in China: Overview,\n  Analysis, and Perspectives",
        "comments": "6 pages, submitted to and accepted by International Conference of\n  Energy, Ecology and Environment 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This review identifies challenges and effective strategies to decarbonize\nChina's rapidly growing transportation sector, currently the third largest\ncarbon emitter, considering China's commitment to peak carbon emissions by 2030\nand achieve carbon neutrality by 2060. Key challenges include rising travel\ndemand, unreached peak car ownership, declining bus ridership, gaps between\nenergy technology research and practical application, and limited institutional\ncapacity for decarbonization. This review categorizes current decarbonization\nmeasures, strategies, and policies in China's transportation sector using the\n\"Avoid, Shift, Improve\" framework, complemented by a novel strategic vector of\n\"Institutional Capacity & Technology Development\" to capture broader\ndevelopment perspectives. This comprehensive analysis aims to facilitate\ninformed decision-making and promote collaborative strategies for China's\ntransition to a sustainable transportation future.\n"
    },
    {
        "paper_id": 2310.00747,
        "authors": "Hsiang-Hui Liu, Han-Jay Shu, Wei-Ning Chiu",
        "title": "NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative\n  Trading",
        "comments": "5 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We introduce NoxTrader, a sophisticated system designed for portfolio\nconstruction and trading execution with the primary objective of achieving\nprofitable outcomes in the stock market, specifically aiming to generate\nmoderate to long-term profits. The underlying learning process of NoxTrader is\nrooted in the assimilation of valuable insights derived from historical trading\ndata, particularly focusing on time-series analysis due to the nature of the\ndataset employed. In our approach, we utilize price and volume data of US stock\nmarket for feature engineering to generate effective features, including Return\nMomentum, Week Price Momentum, and Month Price Momentum. We choose the Long\nShort-Term Memory (LSTM)model to capture continuous price trends and implement\ndynamic model updates during the trading execution process, enabling the model\nto continuously adapt to the current market trends. Notably, we have developed\na comprehensive trading backtesting system - NoxTrader, which allows us to\nmanage portfolios based on predictive scores and utilize custom evaluation\nmetrics to conduct a thorough assessment of our trading performance. Our\nrigorous feature engineering and careful selection of prediction targets enable\nus to generate prediction data with an impressive correlation range between\n0.65 and 0.75. Finally, we monitor the dispersion of our prediction data and\nperform a comparative analysis against actual market data. Through the use of\nfiltering techniques, we improved the initial -60% investment return to 325%.\n"
    },
    {
        "paper_id": 2310.00753,
        "authors": "Vaibhav Sherkar, Rituparna Sen",
        "title": "Study of Stylized Facts in Stock Market Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A property of data which is common across a wide range of instruments,\nmarkets and time periods is known as stylized empirical fact in the financial\nstatistics literature. This paper first presents a wide range of stylized facts\nstudied in literature which include some univariate distributional properties,\nmultivariate properties and time series related properties of the financial\ntime series data. In the next part of the paper, price data from several stocks\nlisted on 10 stock exchanges spread across different continents has been\nanalysed and data analysis has been presented.\n"
    },
    {
        "paper_id": 2310.01063,
        "authors": "Jakub Micha\\'nk\\'ow, {\\L}ukasz Kwiatkowski, Janusz Morajda",
        "title": "Combining Deep Learning and GARCH Models for Financial Volatility and\n  Risk Forecasting",
        "comments": "25 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a hybrid approach to forecasting the volatility and\nrisk of financial instruments by combining common econometric GARCH time series\nmodels with deep learning neural networks. For the latter, we employ Gated\nRecurrent Unit (GRU) networks, whereas four different specifications are used\nas the GARCH component: standard GARCH, EGARCH, GJR-GARCH and APARCH. Models\nare tested using daily logarithmic returns on the S&P 500 index as well as gold\nprice Bitcoin prices, with the three assets representing quite distinct\nvolatility dynamics. As the main volatility estimator, also underlying the\ntarget function of our hybrid models, we use the price-range-based Garman-Klass\nestimator, modified to incorporate the opening and closing prices. Volatility\nforecasts resulting from the hybrid models are employed to evaluate the assets'\nrisk using the Value-at-Risk (VaR) and Expected Shortfall (ES) at two different\ntolerance levels of 5% and 1%. Gains from combining the GARCH and GRU\napproaches are discussed in the contexts of both the volatility and risk\nforecasts. In general, it can be concluded that the hybrid solutions produce\nmore accurate point volatility forecasts, although it does not necessarily\ntranslate into superior VaR and ES forecasts.\n"
    },
    {
        "paper_id": 2310.01104,
        "authors": "Purba Banerjee, Srikanth Iyer and Shashi Jain",
        "title": "Multi-period static hedging of European options",
        "comments": "32 pages, 7 figures, 4 sub-figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the hedging of European options when the price of the underlying\nasset follows a single-factor Markovian framework. By working in such a\nsetting, Carr and Wu \\cite{carr2014static} derived a spanning relation between\na given option and a continuum of shorter-term options written on the same\nasset. In this paper, we have extended their approach to simultaneously include\noptions over multiple short maturities. We then show a practical implementation\nof this with a finite set of shorter-term options to determine the hedging\nerror using a Gaussian Quadrature method. We perform a wide range of\nexperiments for both the \\textit{Black-Scholes} and \\textit{Merton Jump\nDiffusion} models, illustrating the comparative performance of the two methods.\n"
    },
    {
        "paper_id": 2310.01285,
        "authors": "Qinmeng Luan and James Hamp",
        "title": "Automated regime detection in multidimensional time series data using\n  sliced Wasserstein k-means clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent work has proposed Wasserstein k-means (Wk-means) clustering as a\npowerful method to identify regimes in time series data, and one-dimensional\nasset returns in particular. In this paper, we begin by studying in detail the\nbehaviour of the Wasserstein k-means clustering algorithm applied to synthetic\none-dimensional time series data. We study the dynamics of the algorithm and\ninvestigate how varying different hyperparameters impacts the performance of\nthe clustering algorithm for different random initialisations. We compute\nsimple metrics that we find are useful in identifying high-quality clusterings.\nThen, we extend the technique of Wasserstein k-means clustering to\nmultidimensional time series data by approximating the multidimensional\nWasserstein distance as a sliced Wasserstein distance, resulting in a method we\ncall `sliced Wasserstein k-means (sWk-means) clustering'. We apply the\nsWk-means clustering method to the problem of automated regime detection in\nmultidimensional time series data, using synthetic data to demonstrate the\nvalidity of the approach. Finally, we show that the sWk-means method is\neffective in identifying distinct market regimes in real multidimensional\nfinancial time series, using publicly available foreign exchange spot rate data\nas a case study. We conclude with remarks about some limitations of our\napproach and potential complementary or alternative approaches.\n"
    },
    {
        "paper_id": 2310.01319,
        "authors": "Zhengyong Jiang, Jeyan Thiayagalingam, Jionglong Su and Jinjun Liang",
        "title": "CAD: Clustering And Deep Reinforcement Learning Based Multi-Period\n  Portfolio Management Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a novel trading strategy that integrates\nreinforcement learning methods with clustering techniques for portfolio\nmanagement in multi-period trading. Specifically, we leverage the clustering\nmethod to categorize stocks into various clusters based on their financial\nindices. Subsequently, we utilize the algorithm Asynchronous Advantage\nActor-Critic to determine the trading actions for stocks within each cluster.\nFinally, we employ the algorithm DDPG to generate the portfolio weight vector,\nwhich decides the amount of stocks to buy, sell, or hold according to the\ntrading actions of different clusters. To the best of our knowledge, our\napproach is the first to combine clustering methods and reinforcement learning\nmethods for portfolio management in the context of multi-period trading.\n  Our proposed strategy is evaluated using a series of back-tests on four\ndatasets, comprising a of 800 stocks, obtained from the Shanghai Stock Exchange\nand National Association of Securities Deal Automated Quotations sources. Our\nresults demonstrate that our approach outperforms conventional portfolio\nmanagement techniques, such as the Robust Median Reversion strategy, Passive\nAggressive Median Reversion Strategy, and several machine learning methods,\nacross various metrics. In our back-test experiments, our proposed strategy\nyields an average return of 151% over 360 trading periods with 800 stocks,\ncompared to the highest return of 124% achieved by other techniques over\nidentical trading periods and stocks.\n"
    },
    {
        "paper_id": 2310.01869,
        "authors": "Martial Ph\\'elipp\\'e-Guinvarc (GAINS), Jean Cordier",
        "title": "Actuarial Implications and Modeling of Yellow Virus on Sugar Beet After\n  the EU's Ban on Neonicotinoids and Climate Change",
        "comments": null,
        "journal-ref": "XVII Congress of EAAE, INRAE; EAAE, Aug 2023, Rennes, France",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the EU's decision to ban neonicotinoids, this article investigates\nthe impacts of yellow virus on sugar beet yields under the ban and under\ncurrent and future climates. Using a model that factors in key variables such\nas sowing dates, phenological stages, first aphid flight and aphid abundance,\nsimulations are performed using long-period climate datasets as inputs. Coupled\nwith incidence and sugar yield loss assumptions, this model allows to\nreconstruct the impact of yellow virus on sugar beet yields using a so called\n'as if' approach. By simulating the effects of viruses over a longer period of\ntime, as if neonicotinoids weren't used in the past, this methodology allows an\naccurate assessment of risks associated with yellow viruses, as well as impact\nof future agroecological mesures. The study eventually provides an actuarial\nrating for an insurance policy that compensates the losses triggered by those\nviruses.\n"
    },
    {
        "paper_id": 2310.02014,
        "authors": "Marcin Pitera, Mikl\\'os R\\'asonyi",
        "title": "Utility-based acceptability indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short paper we introduce a new class of performance measures based on\ncertainty equivalents defined via scaled utility functions. We analyse their\nproperties, show that the corresponding portfolio optimization problem is\nwell-posed under generic conditions, and analyse the link between portfolio\ndynamics, benchmark process, and utility function choice in the long-run\nsetting.\n"
    },
    {
        "paper_id": 2310.02081,
        "authors": "Praful Raj",
        "title": "Student debt and behavioral bias: a trillion dollar problem",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This literature review elucidates the implications of behavioral biases,\nparticularly those stemming from overconfidence and framing, on the\nintertemporal choices made by students on their underline demand preferences\nfor student loans. A secondary objective is to understand the potential utility\nof social media to assist students and young borrowers with the debt repayment\nprocess and management of their loan tenures. A close examination of the\nliterature reveals a substantial influence of these behavioral and cognitive\nprinciples on the intertemporal choices made by students towards debt\nrepayments. This affects not only the magnitude of loans they acquire but also\nthe anchoring of the terms of loan conditions associated with repayment.\nFurthermore, I establish that harnessing social media as the potential to\ncultivate financial literacy and enhanced understanding of loan terms to\nexpedite the process of debt redemption. This review could serve as a valuable\nrepository for students, scholars, and policymakers alike, in order to expound\non the cognitive biases that students and consumers often face when applying\nand entering into loan contract.\n"
    },
    {
        "paper_id": 2310.02084,
        "authors": "Tim Leung, Hyungbin Park, Heejun Yeo",
        "title": "Robust Long-Term Growth Rate of Expected Utility for Leveraged ETFs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the robust long-term growth rate of expected utility and\nexpected return from holding a leveraged exchange-traded fund (LETF). When the\nMarkovian model parameters in the reference asset are uncertain, the robust\nlong-term growth rate is derived by analyzing the worst-case parameters among\nan uncertainty set. We compute the growth rate and describe the optimal\nleverage ratio maximizing the robust long-term growth rate. To achieve this,\nthe worst-case parameters are analyzed by the comparison principle, and the\ngrowth rate of the worst-case is computed using the martingale extraction\nmethod. The robust long-term growth rates are obtained explicitly under a\nnumber of models for the reference asset, including the geometric Brownian\nmotion (GBM), Cox--Ingersoll--Ross (CIR), 3/2, and Heston and 3/2 stochastic\nvolatility models. Additionally, we demonstrate the impact of stochastic\ninterest rates, such as the Vasicek and inverse GARCH short rate models. This\npaper is an extended work of \\citet{Leung2017}.\n"
    },
    {
        "paper_id": 2310.02163,
        "authors": "Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth",
        "title": "Navigating Uncertainty in ESG Investing",
        "comments": "28 pages, 2 figures, presented at Fields - Institute's Mathematics\n  for Climate Change (MfCC) Network & Waterloo Institute for Complexity and\n  Innovation (WICI): Math for Complex Climate Challenges Workshop, Waterloo,\n  Canada; 26th International Congress on Insurance: Mathematics and Economics,\n  Edinburgh, UK; and the 58th Actuarial Research Conference (ARC), Des Moines,\n  Iowa, USA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The widespread confusion among investors regarding Environmental, Social, and\nGovernance (ESG) rankings assigned by rating agencies has underscored a\ncritical issue in sustainable investing. To address this uncertainty, our\nresearch has devised methods that not only recognize this ambiguity but also\noffer tailored investment strategies for different investor profiles. By\ndeveloping ESG ensemble strategies and integrating ESG scores into a\nReinforcement Learning (RL) model, we aim to optimize portfolios that cater to\nboth financial returns and ESG-focused outcomes. Additionally, by proposing the\nDouble-Mean-Variance model, we classify three types of investors based on their\nrisk preferences. We also introduce ESG-adjusted Capital Asset Pricing Models\n(CAPMs) to assess the performance of these optimized portfolios. Ultimately,\nour comprehensive approach provides investors with tools to navigate the\ninherent ambiguities of ESG ratings, facilitating more informed investment\ndecisions.\n"
    },
    {
        "paper_id": 2310.02253,
        "authors": "Viktor Stojkoski, Philipp Koch, Eva Coll, Cesar A. Hidalgo",
        "title": "Estimating Digital Product Trade through Corporate Revenue Data",
        "comments": null,
        "journal-ref": "Nature Communications volume 15, Article number: 5262 (2024)",
        "doi": "10.1038/s41467-024-49141-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite global efforts to harmonize international trade statistics, our\nunderstanding of digital trade and its implications remains limited. Here, we\nintroduce a method to estimate bilateral exports and imports for dozens of\nsectors starting from the corporate revenue data of large digital firms. This\nmethod allows us to provide estimates for digitally ordered and delivered trade\ninvolving digital goods (e.g. video games), productized services (e.g. digital\nadvertising), and digital intermediation fees (e.g. hotel rental), which\ntogether we call digital products. We use these estimates to study five key\naspects of digital trade. We find that, compared to trade in physical goods,\ndigital product exports are more spatially concentrated, have been growing\nfaster, and can offset trade balance estimates, like the United States trade\ndeficit on physical goods. We also find that countries that have decoupled\neconomic growth from greenhouse gas emissions tend to have larger digital\nexports and that digital products exports contribute positively to the\ncomplexity of economies. This method, dataset, and findings provide a new lens\nto understand the impact of international trade in digital products.\n"
    },
    {
        "paper_id": 2310.02322,
        "authors": "Christa Cuchiero and Janka M\\\"oller",
        "title": "Signature Methods in Stochastic Portfolio Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the context of stochastic portfolio theory we introduce a novel class of\nportfolios which we call linear path-functional portfolios. These are\nportfolios which are determined by certain transformations of linear functions\nof a collections of feature maps that are non-anticipative path functionals of\nan underlying semimartingale. As main example for such feature maps we consider\nthe signature of the (ranked) market weights. We prove that these portfolios\nare universal in the sense that every continuous, possibly path-dependent,\nportfolio function of the market weights can be uniformly approximated by\nsignature portfolios. We also show that signature portfolios can approximate\nthe growth-optimal portfolio in several classes of non-Markovian market models\narbitrarily well and illustrate numerically that the trained signature\nportfolios are remarkably close to the theoretical growth-optimal portfolios.\nBesides these universality features, the main numerical advantage lies in the\nfact that several optimization tasks like maximizing (expected) logarithmic\nwealth or mean-variance optimization within the class of linear path-functional\nportfolios reduce to a convex quadratic optimization problem, thus making it\ncomputationally highly tractable. We apply our method also to real market data\nbased on several indices. Our results point towards out-performance on the\nconsidered out-of-sample data, also in the presence of transaction costs.\n"
    },
    {
        "paper_id": 2310.02436,
        "authors": "A.H. Nzokem",
        "title": "Bitcoin versus S&P 500 Index: Return and Risk Analysis",
        "comments": "26 page, 16 figures",
        "journal-ref": "Math. Comput. Appl. 2024, 29(3), 44;",
        "doi": "10.3390/mca29030044",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The S&P 500 index is considered the most popular trading instrument in\nfinancial markets. With the rise of cryptocurrencies over the past years,\nBitcoin has also grown in popularity and adoption. The paper aims to analyze\nthe daily return distribution of the Bitcoin and S&P 500 index and assess their\ntail probabilities through two financial risk measures. As a methodology, We\nuse Bitcoin and S&P 500 Index daily return data to fit The seven-parameter\nGeneral Tempered Stable (GTS) distribution using the advanced Fast Fractional\nFourier transform (FRFT) scheme developed by combining the Fast Fractional\nFourier (FRFT) algorithm and the 12-point rule Composite Newton-Cotes\nQuadrature. The findings show that peakedness is the main characteristic of the\nS&P 500 return distribution, whereas heavy-tailedness is the main\ncharacteristic of the Bitcoin return distribution. The GTS distribution shows\nthat $80.05\\%$ of S&P 500 returns are within $-1.06\\%$ and $1.23\\%$ against\nonly $40.32\\%$ of Bitcoin returns. At a risk level ($\\alpha$), the severity of\nthe loss ($AVaR_{\\alpha}(X)$) on the left side of the distribution is larger\nthan the severity of the profit ($AVaR_{1-\\alpha}(X)$) on the right side of the\ndistribution. Compared to the S&P 500 index, Bitcoin has $39.73\\%$ more\nprevalence to produce high daily returns (more than $1.23\\%$ or less than\n$-1.06\\%$). The severity analysis shows that at a risk level ($\\alpha$) the\naverage value-at-risk ($AVaR(X)$) of the bitcoin returns at one significant\nfigure is four times larger than that of the S&P 500 index returns at the same\nrisk.\n"
    },
    {
        "paper_id": 2310.02608,
        "authors": "Dorinel Bastide (LaMME), St\\'ephane Cr\\'epey (LPSM), Samuel Drapeau,\n  Mekonnen Tadese (LPSM, CMAP)",
        "title": "Resolving a Clearing Member's Default, A Radner Equilibrium Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For vanilla derivatives that constitute the bulk of investment banks' hedging\nportfolios, central clearing through central counterparties (CCPs) has become\nhegemonic. A key mandate of a CCP is to provide an efficient and proper\nclearing member default resolution procedure. When a clearing member defaults,\nthe CCP can hedge and auction or liquidate its positions. The counterparty\ncredit risk cost of auctioning has been analyzed in terms of XVA metrics in\nBastide, Cr{\\'e}pey, Drapeau, and Tadese (2023). In this work we assess the\ncosts of hedging or liquidating. This is done by comparing pre- and\npost-default market equilibria, using a Radner equilibrium approach for\nportfolio allocation and price discovery in each case. We show that the Radner\nequilibria uniquely exist and we provide both analytical and numerical\nsolutions for the latter in elliptically distributed markets. Using such tools,\na CCP could decide rationally on which market to hedge and auction or liquidate\ndefaulted portfolios.\n"
    },
    {
        "paper_id": 2310.02867,
        "authors": "Jozef Barunik and Lubos Hanus",
        "title": "Learning Probability Distributions of Day-Ahead Electricity Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a novel machine learning approach to probabilistic forecasting of\nhourly day-ahead electricity prices. In contrast to recent advances in\ndata-rich probabilistic forecasting that approximate the distributions with\nsome features such as moments, our method is non-parametric and selects the\nbest distribution from all possible empirical distributions learned from the\ndata. The model we propose is a multiple output neural network with a\nmonotonicity adjusting penalty. Such a distributional neural network can learn\ncomplex patterns in electricity prices from data-rich environments and it\noutperforms state-of-the-art benchmarks.\n"
    },
    {
        "paper_id": 2310.03501,
        "authors": "Joshua C. Yang, Carina I. Hausladen, Dominik Peters, Evangelos\n  Pournaras, Regula H\\\"anggli Fricker, Dirk Helbing",
        "title": "Designing Digital Voting Systems for Citizens: Achieving Fairness and\n  Legitimacy in Participatory Budgeting",
        "comments": "Under review in ACM Digital Government: Research and Practice",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Participatory Budgeting (PB) has evolved into a key democratic instrument for\nresource allocation in cities. Enabled by digital platforms, cities now have\nthe opportunity to let citizens directly propose and vote on urban projects,\nusing different voting input and aggregation rules. However, the choices cities\nmake in terms of the rules of their PB have often not been informed by academic\nstudies on voter behaviour and preferences. Therefore, this work presents the\nresults of behavioural experiments where participants were asked to vote in a\nfictional PB setting. We identified approaches to designing PB voting that\nminimise cognitive load and enhance the perceived fairness and legitimacy of\nthe digital process from the citizens' perspective. In our study, participants\npreferred voting input formats that are more expressive (like rankings and\ndistributing points) over simpler formats (like approval voting). Participants\nalso indicated a desire for the budget to be fairly distributed across city\ndistricts and project categories. Participants found the Method of Equal Shares\nvoting rule to be fairer than the conventional Greedy voting rule. These\nfindings offer actionable insights for digital governance, contributing to the\ndevelopment of fairer and more transparent digital systems and collective\ndecision-making processes for citizens.\n"
    },
    {
        "paper_id": 2310.03689,
        "authors": "Alessandro Del Ponte, Audrey De Dominicis, and Paolo Canofari",
        "title": "Initiatives Based on the Psychology of Scarcity Can Increase Covid-19\n  Vaccinations",
        "comments": "21 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Background: Here we investigate whether releasing COVID-19 vaccines in\nlimited quantities and at limited times boosted Italy's vaccination campaign in\n2021. This strategy exploits insights from psychology and consumer marketing.\nMethods: We built an original dataset covering 200 days of vaccination data in\nItaly, including 'open day' events. Open-day events (in short: open days) are\ninstances where COVID-19 vaccines were released in limited quantities and only\nfor a specific day at a specified location (usually, a large pavilion or a\npublic building). Our dependent variables are the number of total and first\ndoses administered in proportion to the eligible population. Our key\nindependent variable is the presence of open-day events in a given region on a\nspecific day. We analyzed the data using regression with fixed effects for time\nand region. The analysis was robust to alternative model specifications.\nFindings: We find that when an open day event was organized, in proportion to\nthe eligible population, there was an average 0.39-0.44 percentage point\nincrease in total doses administered and a 0.30-0.33 percentage point increase\nin first doses administered. These figures correspond to an average increase of\n10,455-11,796 in total doses administered and 8,043-8,847 in the first doses\nadministered. Interpretation: Releasing vaccines in limited quantities and at\nlimited times by organizing open-day events was associated with an increase in\nCOVID-19 vaccinations in most Italian regions. These results call for wider\nadoption of vaccination strategies based on the limited release of vaccines for\nother infectious diseases or future pandemics.\n"
    },
    {
        "paper_id": 2310.03694,
        "authors": "Edward J. Oughton, David Amaglobeli, Marian Moszoro",
        "title": "What would it cost to connect the unconnected? Estimating global\n  universal broadband infrastructure investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Roughly 3 billion citizens remain offline, equating to approximately 40\npercent of the global population. Therefore, providing Internet connectivity is\nan essential part of the Sustainable Development Goals (SDGs) (Goal 9). In this\npaper a high-resolution global model is developed to evaluate the necessary\ninvestment requirements to achieve affordable universal broadband. The results\nindicate that approximately $418 billion needs to be mobilized to connect all\nunconnected citizens globally (targeting 40-50 GB/Month per user with 95\npercent reliability). The bulk of additional investment is for emerging market\neconomies (73 percent) and low-income developing countries (24 percent). To our\nknowledge, the paper contributes the first high-resolution global assessment\nwhich quantifies universal broadband investment at the sub-national level to\nachieve SDG Goal 9.\n"
    },
    {
        "paper_id": 2310.04027,
        "authors": "Boyu Zhang, Hongyang Yang, Tianyu Zhou, Ali Babar, Xiao-Yang Liu",
        "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large\n  Language Models",
        "comments": "ACM International Conference on AI in Finance (ICAIF) 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial sentiment analysis is critical for valuation and investment\ndecision-making. Traditional NLP models, however, are limited by their\nparameter size and the scope of their training datasets, which hampers their\ngeneralization capabilities and effectiveness in this field. Recently, Large\nLanguage Models (LLMs) pre-trained on extensive corpora have demonstrated\nsuperior performance across various NLP tasks due to their commendable\nzero-shot abilities. Yet, directly applying LLMs to financial sentiment\nanalysis presents challenges: The discrepancy between the pre-training\nobjective of LLMs and predicting the sentiment label can compromise their\npredictive performance. Furthermore, the succinct nature of financial news,\noften devoid of sufficient context, can significantly diminish the reliability\nof LLMs' sentiment analysis. To address these challenges, we introduce a\nretrieval-augmented LLMs framework for financial sentiment analysis. This\nframework includes an instruction-tuned LLMs module, which ensures LLMs behave\nas predictors of sentiment labels, and a retrieval-augmentation module which\nretrieves additional context from reliable external sources. Benchmarked\nagainst traditional models and LLMs like ChatGPT and LLaMA, our approach\nachieves 15\\% to 48\\% performance gain in accuracy and F1 score.\n"
    },
    {
        "paper_id": 2310.04125,
        "authors": "Maria Kulikova and Gennady Kulikov",
        "title": "Estimation of market efficiency process within time-varying\n  autoregressive models by extended Kalman filtering approach",
        "comments": null,
        "journal-ref": "Digital Signal Processing, 128: Paper ID 103619, 2022",
        "doi": "10.1016/j.dsp.2022.103619",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores a time-varying version of weak-form market efficiency\nthat is a key component of the so-called Adaptive Market Hypothesis (AMH). One\nof the most common methodologies used for modeling and estimating a degree of\nmarket efficiency lies in an analysis of the serial autocorrelation in observed\nreturn series. Under the AMH, a time-varying market efficiency level is modeled\nby time-varying autoregressive (AR) process and traditionally estimated by the\nKalman filter (KF). Being a linear estimator, the KF is hardly capable to track\nthe hidden nonlinear dynamics that is an essential feature of the models under\ninvestigation. The contribution of this paper is threefold. We first provide a\nbrief overview of time-varying AR models and estimation methods utilized for\ntesting a weak-form market efficiency in econometrics literature. Secondly, we\npropose novel accurate estimation approach for recovering the hidden process of\nevolving market efficiency level by the extended Kalman filter (EKF). Thirdly,\nour empirical study concerns an examination of the Standard and Poor's 500\nComposite stock index and the Dow Jones Industrial Average index. Monthly data\ncovers the period from November 1927 to June 2020, which includes the U.S.\nGreat Depression, the 2008-2009 global financial crisis and the first wave of\nrecent COVID-19 recession. The results reveal that the U.S. market was affected\nduring all these periods, but generally remained weak-form efficient since the\nmid of 1946 as detected by the estimator.\n"
    },
    {
        "paper_id": 2310.04146,
        "authors": "Christian Bayer, Simon Breneis",
        "title": "Efficient option pricing in the rough Heston model using weak simulation\n  schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an efficient and accurate simulation scheme for the rough Heston\nmodel in the standard ($H>0$) as well as the hyper-rough regime ($H > -1/2$).\nThe scheme is based on low-dimensional Markovian approximations of the rough\nHeston process derived in [Bayer and Breneis, arXiv:2309.07023], and provides\nweak approximation to the rough Heston process. Numerical experiments show that\nthe new scheme exhibits second order weak convergence, while the computational\ncost increases linear with respect to the number of time steps. In comparison,\nexisting schemes based on discretization of the underlying stochastic Volterra\nintegrals such as Gatheral's HQE scheme show a quadratic dependence of the\ncomputational cost. Extensive numerical tests for standard and path-dependent\nEuropean options and Bermudan options show the method's accuracy and\nefficiency.\n"
    },
    {
        "paper_id": 2310.04176,
        "authors": "Zachary Feinstein, Niklas Hey, Birgit Rudloff",
        "title": "Approximating the set of Nash equilibria for convex games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Feinstein and Rudloff (2023), it was shown that the set of Nash equilibria\nfor any non-cooperative $N$ player game coincides with the set of Pareto\noptimal points of a certain vector optimization problem with non-convex\nordering cone. To avoid dealing with a non-convex ordering cone, an equivalent\ncharacterization of the set of Nash equilibria as the intersection of the\nPareto optimal points of $N$ multi-objective problems (i.e.\\ with the natural\nordering cone) is proven. So far, algorithms to compute the exact set of Pareto\noptimal points of a multi-objective problem exist only for the class of linear\nproblems, which reduces the possibility of finding the true set of Nash\nequilibria by those algorithms to linear games only.\n  In this paper, we will consider the larger class of convex games. As,\ntypically, only approximate solutions can be computed for convex vector\noptimization problems, we first show, in total analogy to the result above,\nthat the set of $\\epsilon$-approximate Nash equilibria can be characterized by\nthe intersection of $\\epsilon$-approximate Pareto optimal points for $N$ convex\nmulti-objective problems. Then, we propose an algorithm based on results from\nvector optimization and convex projections that allows for the computation of a\nset that, on one hand, contains the set of all true Nash equilibria, and is, on\nthe other hand, contained in the set of $\\epsilon$-approximate Nash equilibria.\nIn addition to the joint convexity of the cost function for each player, this\nalgorithm works provided the players are restricted by either shared polyhedral\nconstraints or independent convex constraints.\n"
    },
    {
        "paper_id": 2310.04242,
        "authors": "Zhenkun Zhou, Zikun Song, Tao Ren",
        "title": "A New Weighted Food CPI from Scanner Big Data in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scanner big data has potential to construct Consumer Price Index (CPI). The\nstudy introduces a new weighted price index called S-FCPIw, which is\nconstructed using scanner big data from retail sales in China. We address the\nlimitations of China's CPI especially for its high cost and untimely release,\nand demonstrate the reliability of S-FCPIw by comparing it with existing price\nindices. S-FCPIw can not only reflect the changes of goods prices in higher\nfrequency and richer dimension, and the analysis results show that S-FCPIw has\na significant and strong relationship with CPI and Food CPI. The findings\nsuggest that scanner big data can supplement traditional CPI calculations in\nChina and provide new insights into macroeconomic trends and inflation\nprediction. We have made S-FCPIw publicly available and update it on a weekly\nbasis to facilitate further study in this field.\n"
    },
    {
        "paper_id": 2310.0428,
        "authors": "Maksim Papenkov and Chris Meredith and Claire Noel and Jai Padalkar\n  and Temple Hendrickson and Daniel Nitiutomo and Thomas Farrell",
        "title": "Multi-Industry Simplex : A Probabilistic Extension of GICS",
        "comments": "17 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurate industry classification is a critical tool for many asset management\napplications. While the current industry gold-standard GICS (Global Industry\nClassification Standard) has proven to be reliable and robust in many settings,\nit has limitations that cannot be ignored. Fundamentally, GICS is a\nsingle-industry model, in which every firm is assigned to exactly one group -\nregardless of how diversified that firm may be. This approach breaks down for\nlarge conglomerates like Amazon, which have risk exposure spread out across\nmultiple sectors. We attempt to overcome these limitations by developing MIS\n(Multi-Industry Simplex), a probabilistic model that can flexibly assign a firm\nto as many industries as can be supported by the data. In particular, we\nutilize topic modeling, an natural language processing approach that utilizes\nbusiness descriptions to extract and identify corresponding industries. Each\nidentified industry comes with a relevance probability, allowing for high\ninterpretability and easy auditing, circumventing the black-box nature of\nalternative machine learning approaches. We describe this model in detail and\nprovide two use-cases that are relevant to asset management - thematic\nportfolios and nearest neighbor identification. While our approach has\nlimitations of its own, we demonstrate the viability of probabilistic industry\nclassification and hope to inspire future research in this field.\n"
    },
    {
        "paper_id": 2310.04312,
        "authors": "Roger Koppl, Kira Pronin, Nick Cowen, Marta Podemska-Mikluch, Pablo\n  Paniagua Prieto",
        "title": "Bespoke scapegoats: Scientific advisory bodies and blame avoidance in\n  the Covid-19 pandemic and beyond",
        "comments": "2 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scholars have not asked why so many governments created ad hoc scientific\nadvisory bodies (ahSABs) to address the Covid-19 pandemic instead of relying on\nexisting public health infrastructure. We address this neglected question with\nan exploratory study of the US, UK, Sweden, Italy, Poland, and Uganda. Drawing\non our case studies and the blame-avoidance literature, we find that ahSABs are\ncreated to excuse unpopular policies and take the blame should things go wrong.\nThus, membership typically represents a narrow range of perspectives. An ahSAB\nis a good scapegoat because it does little to reduce government discretion and\nhas limited ability to deflect blame back to government. Our explanation of our\ndeviant case of Sweden, that did not create and ahSAB, reinforces our general\nprinciples. We draw the policy inference that ahSAB membership should be vetted\nby the legislature to ensure broad membership.\n"
    },
    {
        "paper_id": 2310.04336,
        "authors": "Zoran Stoiljkovic",
        "title": "Applying Reinforcement Learning to Option Pricing and Hedging",
        "comments": "57 pages, 14 figures, 8 tables, 3 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis provides an overview of the recent advances in reinforcement\nlearning in pricing and hedging financial instruments, with a primary focus on\na detailed explanation of the Q-Learning Black Scholes approach, introduced by\nHalperin (2017). This reinforcement learning approach bridges the traditional\nBlack and Scholes (1973) model with novel artificial intelligence algorithms,\nenabling option pricing and hedging in a completely model-free and data-driven\nway. This paper also explores the algorithm's performance under different state\nvariables and scenarios for a European put option. The results reveal that the\nmodel is an accurate estimator under different levels of volatility and hedging\nfrequency. Moreover, this method exhibits robust performance across various\nlevels of option's moneyness. Lastly, the algorithm incorporates proportional\ntransaction costs, indicating diverse impacts on profit and loss, affected by\ndifferent statistical properties of the state variables.\n"
    },
    {
        "paper_id": 2310.04464,
        "authors": "Sarit Maitra, Vivek Mishra, Goutam Kr. Kundu, Kapil Arora",
        "title": "Integration of Fractional Order Black-Scholes Merton with Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/IIT59782.2023.10366496",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This study enhances option pricing by presenting unique pricing model\nfractional order Black-Scholes-Merton (FOBSM) which is based on the\nBlack-Scholes-Merton (BSM) model. The main goal is to improve the precision and\nauthenticity of option pricing, matching them more closely with the financial\nlandscape. The approach integrates the strengths of both the BSM and neural\nnetwork (NN) with complex diffusion dynamics. This study emphasizes the need to\ntake fractional derivatives into account when analyzing financial market\ndynamics. Since FOBSM captures memory characteristics in sequential data, it is\nbetter at simulating real-world systems than integer-order models. Findings\nreveals that in complex diffusion dynamics, this hybridization approach in\noption pricing improves the accuracy of price predictions. the key contribution\nof this work lies in the development of a novel option pricing model (FOBSM)\nthat leverages fractional calculus and neural networks to enhance accuracy in\ncapturing complex diffusion dynamics and memory effects in financial data.\n"
    },
    {
        "paper_id": 2310.04511,
        "authors": "Natalie Packham",
        "title": "Risk factor aggregation and stress testing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stress testing refers to the application of adverse financial or\nmacroeconomic scenarios to a portfolio. For this purpose, financial or\nmacroeconomic risk factors are linked with asset returns, typically via a\nfactor model. We expand the range of risk factors by adapting\ndimension-reduction techniques from unsupervised learning, namely PCA and\nautoencoders. This results in aggregated risk factors, encompassing a global\nfactor, factors representing broad geographical regions, and factors specific\nto cyclical and defensive industries. As the adapted PCA and autoencoders\nprovide an interpretation of the latent factors, this methodology is also\nvaluable in other areas where dimension-reduction and explainability are\ncrucial.\n"
    },
    {
        "paper_id": 2310.04536,
        "authors": "Piotr Pomorski, Denise Gorse",
        "title": "Improving Portfolio Performance Using a Novel Method for Predicting\n  Financial Regimes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work extends a previous work in regime detection, which allowed trading\npositions to be profitably adjusted when a new regime was detected, to ex ante\nprediction of regimes, leading to substantial performance improvements over the\nearlier model, over all three asset classes considered (equities, commodities,\nand foreign exchange), over a test period of four years. The proposed new model\nis also benchmarked over this same period against a hidden Markov model, the\nmost popular current model for financial regime prediction, and against an\nappropriate index benchmark for each asset class, in the case of the\ncommodities model having a test period cost-adjusted cumulative return over\nfour times higher than that expected from the index. Notably, the proposed\nmodel makes use of a contrarian trading strategy, not uncommon in the financial\nindustry but relatively unexplored in machine learning models. The model also\nmakes use of frequent short positions, something not always desirable to\ninvestors due to issues of both financial risk and ethics; however, it is\ndiscussed how further work could remove this reliance on shorting and allow the\nconstruction of a long-only version of the model.\n"
    },
    {
        "paper_id": 2310.04786,
        "authors": "Benjamin Avanzi (1), Xingyun Tan (1), Greg Taylor (2), Bernard Wong\n  (2) ((1) University of Melbourne, (2) UNSW Sydney)",
        "title": "On the evolution of data breach reporting patterns and frequency in the\n  United States: a cross-state analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the emergence of data breaches is crucial for cyber insurance.\nHowever, analyses of data breach frequency trends in the current literature\nlead to contradictory conclusions. We put forward that those discrepancies may\nbe (at least partially) due to inconsistent data collection standards, as well\nas reporting patterns, over time and space. We set out to carefully control\nboth. In this paper, we conduct a joint analysis of state Attorneys General's\npublications on data breaches across eight states (namely, California,\nDelaware, Indiana, Maine, Montana, North Dakota, Oregon, and Washington), all\nof which are subject to established data collection standards-namely, state\ndata breach (mandatory) notification laws. Thanks to our explicit recognition\nof these notification laws, we are capable of modelling frequency of breaches\nin a consistent and comparable way over time. Hence, we are able to isolate and\ncapture the complexities of reporting patterns, adequately estimate IBNRs, and\nyield a highly reliable assessment of historical frequency trends in data\nbreaches. Our analysis also provides a comprehensive comparison of data breach\nfrequency across the eight U.S. states, extending knowledge on state-specific\ndifferences in cyber risk, which has not been extensively discussed in the\ncurrent literature. Furthermore, we uncover novel features not previously\ndiscussed in the literature, such as differences in cyber risk frequency trends\nbetween large and small data breaches. Overall, we find that the reporting\ndelays are lengthening. We also elicit commonalities and heterogeneities in\nreporting patterns across states, severity levels, and time periods. After\nadequately estimating IBNRs, we find that frequency is relatively stable before\n2020 and increasing after 2020. This is consistent across states. Implications\nof our findings for cyber insurance are discussed.\n"
    },
    {
        "paper_id": 2310.04793,
        "authors": "Neng Wang, Hongyang Yang, Christina Dan Wang",
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language\n  Models in Financial Datasets",
        "comments": "Workshop on Instruction Tuning and Instruction Following at NeurIPS\n  2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the swiftly expanding domain of Natural Language Processing (NLP), the\npotential of GPT-based models for the financial sector is increasingly evident.\nHowever, the integration of these models with financial datasets presents\nchallenges, notably in determining their adeptness and relevance. This paper\nintroduces a distinctive approach anchored in the Instruction Tuning paradigm\nfor open-source large language models, specifically adapted for financial\ncontexts. Through this methodology, we capitalize on the interoperability of\nopen-source models, ensuring a seamless and transparent integration. We begin\nby explaining the Instruction Tuning paradigm, highlighting its effectiveness\nfor immediate integration. The paper presents a benchmarking scheme designed\nfor end-to-end training and testing, employing a cost-effective progression.\nFirstly, we assess basic competencies and fundamental tasks, such as Named\nEntity Recognition (NER) and sentiment analysis to enhance specialization.\nNext, we delve into a comprehensive model, executing multi-task operations by\namalgamating all instructional tunings to examine versatility. Finally, we\nexplore the zero-shot capabilities by earmarking unseen tasks and incorporating\nnovel datasets to understand adaptability in uncharted terrains. Such a\nparadigm fortifies the principles of openness and reproducibility, laying a\nrobust foundation for future investigations in open-source financial large\nlanguage models (FinLLMs).\n"
    },
    {
        "paper_id": 2310.04904,
        "authors": "Marjan Petreski, Stefan Tanevski",
        "title": "Bargain your share: The role of workers bargaining power for labor\n  share, with reference to transition economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of the paper is to understand the role of workers bargaining\nfor the labor share in transition economies. We rely on a share-capital\nschedule, whereby workers bargaining power is represented as a move off the\nschedule. Quantitative indicators of bargaining power are amended with\nown-constructed qualitative indices from textual information describing the\nlegal enabling environment for bargaining in each country. Multiple data\nconstraints impose reliance on a cross-sectional empirical model estimated with\nIV methods, whereby former unionization rates and the time since the adoption\nof the ILO Collective Bargaining Convention are used as exogenous instruments.\nThe sample is composed of 23 industrial branches in 69 countries, of which 28\ntransition ones. In general, we find the stronger bargaining power to influence\nhigher labor share, when the former is measured either quantitatively or\nqualitatively. On the contrary, higher bargaining power results in lower labor\nshare in transition economies. This is likely a matter of delayed response to\nwage pushes, reconciled with the increasing role of MNCs which did not confront\nthe workers power rise per se, but introduced automation and changed market\nstructure amid labor-market flexibilization, which eventually deferred\nbargaining power-s positive effect on labor share.\n"
    },
    {
        "paper_id": 2310.04906,
        "authors": "Kailai Wang",
        "title": "Are Generation Z Less Car-centric Than Millennials? A Nationwide\n  Analysis Through the Lens of Youth Licensing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The debate on whether young Americans are becoming less reliant on\nautomobiles is still ongoing. This research compares driver's license\nacquisition patterns between Millennials and their succeeding Generation Z\nduring late adolescence. It also examines factors influencing teenagers'\ndecisions to obtain driver's licenses. The findings suggest that the decline in\nlicensing rates may be attributed in part to generational shifts in attitudes\nand cultural changes, such as Generation Z's inclination toward educational\ntrips and their digital upbringing. This research underscores the implications\nfor planners, practitioners, and policymakers in adapting to potential shifts\nin American car culture.\n"
    },
    {
        "paper_id": 2310.04907,
        "authors": "Emanuele Citera, Francesco De Pretis",
        "title": "An Information Theory Approach to the Stock and Cryptocurrency Market: A\n  Statistical Equilibrium Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the stochastic structure of cryptocurrency rates of returns as\ncompared to stock returns by focusing on the associated cross-sectional\ndistributions. We build two datasets. The first comprises forty-six major\ncryptocurrencies, and the second includes all the companies listed in the S&P\n500. We collect individual data from January 2017 until December 2022. We then\napply the Quantal Response Statistical Equilibrium (QRSE) model to recover the\ncross-sectional frequency distribution of the daily returns of cryptocurrencies\nand S&P 500 companies. We study the stochastic structure of these two markets\nand the properties of investors' behavior over bear and bull trends. Finally,\nwe compare the degree of informational efficiency of these two markets.\n"
    },
    {
        "paper_id": 2310.0511,
        "authors": "Marjan Petreski",
        "title": "The impact of the pandemic of Covid-19 on child poverty in North\n  Macedonia: Simulation-based estimates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of this paper is to estimate the expected effects of the\npandemic of Covid-19 for child poverty in North Macedonia. We rely on MK-MOD\nTax & Benefit Microsimulation Model for North Macedonia based on the Survey on\nIncome and Living Conditions 2019. The simulation takes into account the\ndevelopment of income, as per the observed developments in the first three\nquarters of 2020, derived from the Labor Force Survey, which incorporates the\nraw effect of the pandemic and the government response. In North Macedonia,\nalmost no government measure directly aimed the income of children, however,\nthree key and largest measures addressed household income: the wage subsidy of\n14.500 MKD per worker in the hardest hit companies, relaxation of the criteria\nfor obtaining the guaranteed minimum income, and one-off support to vulnerable\ngroups of the population in two occasions. Results suggest that the relative\nchild poverty rate is estimated to increase from 27.8 percent before the\npandemic to 32.4 percent during the pandemic. This increase puts additional\n19,000 children below the relative poverty threshold. Results further suggest\nthat absolute poverty is likely to reduce primarily because of the automatic\nstabilizers in the case of social assistance and because of the one-time cash\nassistance.\n"
    },
    {
        "paper_id": 2310.05114,
        "authors": "Marjan Petreski",
        "title": "Poverty during Covid-19 in North Macedonia: Analysis of the\n  distributional impact of the crisis and government response",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we simulate the poverty effect of the Covid-19 pandemic in\nNorth Macedonia and we analyze the income-saving power of three key government\nmeasures: the employment-retention scheme, the relaxed Guaranteed Minimum\nIncome support, and one-off cash allowances. In this attempt, the\ncounterfactual scenarios are simulated by using MK-MOD, the Macedonian Tax and\nBenefit Microsimulation Model, incorporating actual data on the shock-s\nmagnitude from the second quarter of 2020. The results suggest that without the\ngovernment interventions, of the country-s two million citizens, an additional\n120,000 people would have been pushed into poverty by COVID-19, where 340,000\nwere already poor before the pandemic. Of the 120,000 newly poor about 16,000\nwould have been pushed into destitute poverty. The government-s automatic\nstabilizers worked to shield the poorest people, though these were clearly\npro-feminine. In all, the analyzed government measures recovered more than half\nof the income loss, which curbed the poverty-increasing effect and pulled an\nadditional 34,000 people out of extreme poverty. The employment-retention\nmeasure was regressive and pro-masculine; the Guaranteed Minimum Income\nrelaxation (including automatic stabilizers) was progressive and pro-feminine;\nand the one-off support has been pro-youth.\n"
    },
    {
        "paper_id": 2310.05117,
        "authors": "Marjan Petreski, Jaakko Pehkonen",
        "title": "Minimum wage and manufacturing labor share: Evidence from North\n  Macedonia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of the paper is to understand if the minimum wage plays a role\nfor the labor share of manufacturing workers in North Macedonia. We decompose\nlabor share movements on those along a share-capital curve, shifts of this\nlocus, and deviations from it. We use the capital-output ratio, total factor\nproductivity and prices of inputs to capture these factors, while the minimum\nwage is introduced as an element that moves the curve off. We estimate a panel\nof 20 manufacturing branches over the 2012-2019 period with FE, IV and\nsystem-GMM estimators. We find that the role of the minimum wage for the labor\nshare is industry-specific. For industrial branches which are labor-intensive\nand low-pay, it increases workers' labor share, along a complementarity between\ncapital and labor. For capital-intensive branches, it reduces labor share,\nlikely through the job loss channel and along a substitutability between labor\nand capital. This applies to both branches where foreign investment and heavy\nindustry are nested.\n"
    },
    {
        "paper_id": 2310.05153,
        "authors": "Marcio Santetti",
        "title": "A time-varying finance-led model for U.S. business cycles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper empirically assesses predictions of Goodwin's model of cyclical\ngrowth regarding demand and distributive regimes when integrating the real and\nfinancial sectors. In addition, it evaluates how financial and employment\nshocks affect the labor market and monetary policy variables over six different\nU.S. business-cycle peaks. It identifies a parsimonious Time-Varying Vector\nAutoregressive model with Stochastic Volatility (TVP-VAR-SV) with the labor\nshare of income, the employment rate, residential investment, and the interest\nrate spread as endogenous variables. Using Bayesian inference methods, key\nresults suggest (i) a combination of profit-led demand and profit-squeeze\ndistribution; (ii) weakening of these regimes during the Great Moderation; and\n(iii) significant connections between the standard Goodwinian variables and\nresidential investment as well as term spreads. Findings presented here broadly\nconform to the transition to increasingly deregulated financial and labor\nmarkets initiated in the 1980s.\n"
    },
    {
        "paper_id": 2310.05275,
        "authors": "Apoorva Lal, Daniel M Thompson",
        "title": "Did Private Election Administration Funding Advantage Democrats in 2020?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Private donors contributed more than $350 million to local election officials\nto support the administration of the 2020 election. Supporters argue these\ngrants were neutral and necessary to maintain normal election operations during\nthe pandemic, while critics worry these grants mostly went to Democratic\nstrongholds and tilted election outcomes. These concerns have led twenty-four\nstates to restrict private election grants. How much did these grants shape the\n2020 presidential election? To answer this question, we collect administrative\ndata on private election administration grants and election outcomes. We then\nuse new advances in synthetic control methods to compare presidential election\nresults and turnout in counties that received grants to counties with identical\naverage presidential election results and turnout before 2020. While counties\nthat favor Democrats were much more likely to apply for a grant, we find that\nthe grants did not have a noticeable effect on the presidential election. Our\nestimates of the average effect of receiving a grant on Democratic vote share\nrange from 0.02 percentage points to 0.36 percentage points. Our estimates of\nthe average effect of receiving a grant on turnout range from -0.03 percentage\npoints to 0.13 percentage points. Across specifications, our 95% confidence\nintervals typically include negative effects, and our confidence intervals from\nall specifications fail to include effects on Democratic vote share larger than\n0.58 percentage points and effects on turnout larger than 0.40 percentage\npoints. We characterize the magnitude of our effects by asking how large they\nare compared to the margin by which Biden won the 2020 election. In simple\nbench-marking exercises, we find that the effects of the grants were likely too\nsmall to have changed the outcome of the 2020 presidential election.\n"
    },
    {
        "paper_id": 2310.05322,
        "authors": "Leilei Shi, Bing Han, Yingzi Zhu, Liyan Han, Yiwen Wang, and Yan Piao",
        "title": "Market Crowds' Trading Behaviors, Agreement Prices, and the Implications\n  of Trading Volume",
        "comments": "57 pages, 11 figures, 5 tables",
        "journal-ref": "Proceedings of 2013 China Finance Review International Conference,\n  845-897 (2013)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been long that literature in financial academics focuses mainly on\nprice and return but much less on trading volume. In the past twenty years, it\nhas already linked both price and trading volume to economic fundamentals, and\nexplored the behavioral implications of trading volume such as investor's\nattitude toward risks, overconfidence, disagreement, and attention etc.\nHowever, what is surprising is how little we really know about trading volume.\nHere we show that trading volume probability represents the frequency of market\ncrowd's trading action in terms of behavior analysis, and test two adaptive\nhypotheses relevant to the volume uncertainty associated with price in China\nstock market. The empirical work reveals that market crowd trade a stock in\nefficient adaptation except for simple heuristics, gradually tend to achieve\nagreement on an outcome or an asset price widely on a trading day, and generate\nsuch a stationary equilibrium price very often in interaction and competition\namong themselves no matter whether it is highly overestimated or\nunderestimated. This suggests that asset prices include not only a fundamental\nvalue but also private information, speculative, sentiment, attention, gamble,\nand entertainment values etc. Moreover, market crowd adapt to gain and loss by\ntrading volume increase or decrease significantly in interaction with\nenvironment in any two consecutive trading days. Our results demonstrate how\ninteraction between information and news, the trading action, and return\noutcomes in the three-term feedback loop produces excessive trading volume\nwhich includes various internal and external causes.\n"
    },
    {
        "paper_id": 2310.05627,
        "authors": "Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li\n  and Dongming Han",
        "title": "Integrating Stock Features and Global Information via Large Language\n  Models for Enhanced Stock Return Prediction",
        "comments": "8 pages, International Joint Conferences on Artificial Intelligence",
        "journal-ref": "International Joint Conferences on Artificial Intelligence,2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The remarkable achievements and rapid advancements of Large Language Models\n(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in\nquantitative investment. Traders can effectively leverage these LLMs to analyze\nfinancial news and predict stock returns accurately. However, integrating LLMs\ninto existing quantitative models presents two primary challenges: the\ninsufficient utilization of semantic information embedded within LLMs and the\ndifficulties in aligning the latent information within LLMs with pre-existing\nquantitative stock features. We propose a novel framework consisting of two\ncomponents to surmount these challenges. The first component, the Local-Global\n(LG) model, introduces three distinct strategies for modeling global\ninformation. These approaches are grounded respectively on stock features, the\ncapabilities of LLMs, and a hybrid method combining the two paradigms. The\nsecond component, Self-Correlated Reinforcement Learning (SCRL), focuses on\naligning the embeddings of financial news generated by LLMs with stock features\nwithin the same semantic space. By implementing our framework, we have\ndemonstrated superior performance in Rank Information Coefficient and returns,\nparticularly compared to models relying only on stock features in the China\nA-share market.\n"
    },
    {
        "paper_id": 2310.0575,
        "authors": "Ioannis Gasteratos, Antoine Jacquier",
        "title": "Transportation-cost inequalities for non-linear Gaussian functionals",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study concentration properties for laws of non-linear Gaussian functionals\non metric spaces. Our focus lies on measures with non-Gaussian tail behaviour\nwhich are beyond the reach of Talagrand's classical Transportation-Cost\nInequalities (TCIs). Motivated by solutions of Rough Differential Equations and\nrelying on a suitable contraction principle, we prove generalised TCIs for\nfunctionals that arise in the theory of regularity structures and, in\nparticular, in the cases of rough volatility and the two-dimensional Parabolic\nAnderson Model. In doing so, we also extend existing results on TCIs for\ndiffusions driven by Gaussian processes.\n"
    },
    {
        "paper_id": 2310.0597,
        "authors": "\\.Ibrahim Halil Efend\\.io\\u{g}lu, G\\\"okhan Akel, Bekir\n  De\\u{g}\\.irmenc\\.i, Dilek Aydo\\u{g}du, Kamile Elmaso\\u{g}lu, Hande Beg\\\"um\n  Bum\\.in Doyduk, Arzu \\c{S}eker, Hatice Bah\\c{c}e",
        "title": "The Mediating Effect of Blockchain Technology on the Cryptocurrency\n  Purchase Intention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Cryptocurrencies, enabling secure digital asset transfers without a central\nauthority, are experiencing increasing interest. With the increasing number of\nglobal and Turkish investors, it is evident that interest in digital assets\nwill continue to rise sustainably, even in the face of financial fluctuations.\nHowever, it remains uncertain whether consumers perceive blockchain\ntechnology's ease of use and usefulness when purchasing cryptocurrencies. This\nstudy aims to explain blockchain technology's perceived ease of use and\nusefulness in cryptocurrency purchases by considering factors such as quality\ncustomer service, reduced costs, efficiency, and reliability. To achieve this\ngoal, data were obtained from 463 participants interested in cryptocurrencies\nin different regions of Turkey. The data were analyzed using SPSS Process Macro\nprograms. The analysis results indicate that perceived ease of use and\nusefulness mediate the effects of customer service and reduced costs,\nefficiency, and security on purchase intention.\n"
    },
    {
        "paper_id": 2310.05971,
        "authors": "Victor Olkhov",
        "title": "Theoretical Economics as Successive Approximations of Statistical\n  Moments",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the links between the descriptions of macroeconomic\nvariables and statistical moments of market trade, price, and return. The\nrandomness of market trade values and volumes during the averaging interval\n{\\Delta} results in the random properties of price and return. We describe how\naverages and volatilities of price and return depend on the averages,\nvolatilities, and correlations of market trade values and volumes. The\naverages, volatilities, and correlations of market trade, price, and return can\nbehave randomly during the long interval {\\Delta}2>>{\\Delta}. To describe their\nstatistical properties during the long interval {\\Delta}2, we introduce the\nsecondary averaging procedure of trade, price, and return. We explain why, in\nthe coming years, predictions of market-based probabilities of price and return\nwill be limited by Gaussian distributions. We discuss the roots of the internal\nweakness of the commonly used hedging tool, Value-at-Risk, that cannot be\nsolved and remains the source of additional risks and losses. One should\nconsider theoretical economics as a set of successive approximations, each of\nwhich describes the next array of the n-th statistical moments of market\ntrades, price, return, and macroeconomic variables, which are repeatedly\naveraged during the sequence of increasing time intervals.\n"
    },
    {
        "paper_id": 2310.05985,
        "authors": "Sam Hainsworth",
        "title": "Does Artificial Intelligence benefit UK businesses? An empirical study\n  of the impact of AI on productivity",
        "comments": "25 pages, Supervisor: Konstantinos Theodoridis",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Media hype and technological breakthroughs are fuelling the race to adopt\nArtificial Intelligence amongst the business community, but is there evidence\nto suggest this will increase productivity? This paper uses 2015-2019 microdata\nfrom the UK Office for National Statistics to identify if the adoption of\nArtificial Intelligence techniques increases labour productivity in UK\nbusinesses. Using fixed effects estimation (Within Group) with a log-linear\nregression specification the paper concludes that there is no statistically\nsignificant impact of AI adoption on labour productivity.\n"
    },
    {
        "paper_id": 2310.06,
        "authors": "Thomas Falconer and Jalal Kazempour and Pierre Pinson",
        "title": "Towards Replication-Robust Analytics Markets",
        "comments": "12 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Many industries rely on data-driven analytics, yet useful datasets are often\ndistributed amongst market competitors that are reluctant to collaborate and\nshare information. Recent literature proposes analytics markets to provide\nmonetary incentives for data sharing, however many of these market designs are\nvulnerable to malicious forms of replication -- whereby agents replicate their\ndata and act under multiple identities to increase revenue. We develop a\nreplication-robust analytics market, centering on supervised learning for\nregression. To allocate revenue, we use a Shapley value-based attribution\npolicy, framing the features of agents as players and their interactions as a\ncharacteristic function game. We show that there are different ways to describe\nsuch a game, each with causal nuances that affect robustness to replication.\nOur proposal is validated using a real-world wind power forecasting case study.\n"
    },
    {
        "paper_id": 2310.06079,
        "authors": "Derick Diana, Tim Gebbie",
        "title": "Anomalous diffusion and price impact in the fluid-limit of an order book",
        "comments": "36 pages, 23 figures, 4 tables; Accepted, Journal of Computational\n  and Applied Mathematics. Clarified notation and added additional commentary\n  and interpretation of this model",
        "journal-ref": "Journal of Computational and Applied Mathematics (2024)",
        "doi": "10.1016/j.cam.2024.116202",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We extend a Discrete Time Random Walk (DTRW) numerical scheme to simulate the\nanomalous diffusion of financial market orders in a simulated order book. Here\nusing random walks with Sibuya waiting times to include a time-dependent\nstochastic forcing function with non-uniformly sampled times between order book\nevents in the setting of fractional diffusion. This models the fluid limit of\nan order book by modelling the continuous arrival, cancellation and diffusion\nof orders in the presence of information shocks. We study the impulse response\nand stylised facts of orders undergoing anomalous diffusion for different\nforcing functions and model parameters. Concretely, we demonstrate the price\nimpact for flash limit-orders and market orders and show how the numerical\nmethod generate kinks in the price impact. We use cubic spline interpolation to\ngenerate smoothed price impact curves. The work promotes the use of non-uniform\nsampling in the presence of diffusive dynamics as the preferred simulation\nmethod.\n"
    },
    {
        "paper_id": 2310.06151,
        "authors": "Silvana M. Pesenti, Pietro Millossovich, and Andreas Tsanakas",
        "title": "Differential Sensitivity in Discontinuous Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Differential sensitivity measures provide valuable tools for interpreting\ncomplex computational models used in applications ranging from simulation to\nalgorithmic prediction. Taking the derivative of the model output in direction\nof a model parameter can reveal input-output relations and the relative\nimportance of model parameters and input variables. Nonetheless, it is unclear\nhow such derivatives should be taken when the model function has\ndiscontinuities and/or input variables are discrete. We present a general\nframework for addressing such problems, considering derivatives of\nquantile-based output risk measures, with respect to distortions to random\ninput variables (risk factors), which impact the model output through\nstep-functions. We prove that, subject to weak technical conditions, the\nderivatives are well-defined and derive the corresponding formulas. We apply\nour results to the sensitivity analysis of compound risk models and to a\nnumerical study of reinsurance credit risk in a multi-line insurance portfolio.\n"
    },
    {
        "paper_id": 2310.06274,
        "authors": "Aleksandar Arandjelovi\\'c, Geoffrey Kingston, Pavel V. Shevchenko",
        "title": "Life cycle insurance, bequest motives and annuity loads",
        "comments": "To appear in Journal of Economic Dynamics and Control",
        "journal-ref": null,
        "doi": "10.1016/j.jedc.2023.104759",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate insurance purchases when bequest motives are age-varying and\nlife insurance and life annuities both carry loads. The existing life cycle\nliterature assumes bequests are normal goods without being either necessities\nor luxuries. Much of the literature also assumes implicitly that life annuity\nloads are negative. A key finding of the literature is that the demand for life\ninsurance and the demand for life annuities are symmetrical. It is optimal to\nbuy life-contingent insurance throughout life, even under loads. A life annuity\nphase backs directly onto a life insurance phase. We find that realistic\nexamples with positive loads on both products reveal up to two distinct periods\nof non-participation, one in midlife and the other adjoining the maximum age.\nWe highlight examples with necessity bequests during child-rearing years and\nluxury bequests thereafter. This set of assumptions explains why a substantial\ndemand for life insurance during child-rearing years can co-exist with\nnegligible demand for life annuities later on. A realistic 18% load on both\nproducts generates this outcome.\n"
    },
    {
        "paper_id": 2310.06844,
        "authors": "Priyanka Bose, Dipanjan Das, Fabio Gritti, Nicola Ruaro, Christopher\n  Kruegel, Giovanni Vigna",
        "title": "Exploiting Unfair Advantages: Investigating Opportunistic Trading in the\n  NFT Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As cryptocurrency evolved, new financial instruments, such as lending and\nborrowing protocols, currency exchanges, fungible and non-fungible tokens\n(NFT), staking and mining protocols have emerged. A financial ecosystem built\non top of a blockchain is supposed to be fair and transparent for each\nparticipating actor. Yet, there are sophisticated actors who turn their domain\nknowledge and market inefficiencies to their strategic advantage; thus\nextracting value from trades not accessible to others. This situation is\nfurther exacerbated by the fact that blockchain-based markets and decentralized\nfinance (DeFi) instruments are mostly unregulated. Though a large body of work\nhas already studied the unfairness of different aspects of DeFi and\ncryptocurrency trading, the economic intricacies of non-fungible token (NFT)\ntrades necessitate further analysis and academic scrutiny.\n  The trading volume of NFTs has skyrocketed in recent years. A single NFT\ntrade worth over a million US dollars, or marketplaces making billions in\nrevenue is not uncommon nowadays. While previous research indicated the\npresence of wrongdoings in the NFT market, to our knowledge, we are the first\nto study predatory trading practices, what we call opportunistic trading, in\ndepth. Opportunistic traders are sophisticated actors who employ automated,\nhigh-frequency NFT trading strategies, which, oftentimes, are malicious,\ndeceptive, or, at the very least, unfair. Such attackers weaponize their\nadvanced technical knowledge and superior understanding of DeFi protocols to\ndisrupt trades of unsuspecting users, and collect profits from economic\nsituations that are inaccessible to ordinary users, in a \"supposedly\" fair\nmarket. In this paper, we explore three such broad classes of opportunistic\nstrategies aiming to realize three distinct trading objectives, viz., acquire,\ninstant profit generation, and loss minimization.\n"
    },
    {
        "paper_id": 2310.06999,
        "authors": "Andrea Alcaraz, Federico Rodriguez Cairoli, Carla Colaci, Constanza\n  Silvestrini, Carolina Gabay, Natalia Espinola",
        "title": "Lung Cancer in Argentina: A Modelling Study of Disease and Economic\n  Burden",
        "comments": "35 pages, 2 tables, 3 figures, 1 supplementary material",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Objectives: Lung cancer remains a significant global public health challenge\nand is still one of the leading cause of cancer-related death in Argentina.\nThis study aims to assess the disease and economic burden of lung cancer in the\ncountry.\n  Study design: Burden of disease study\n  Methods. A mathematical model was developed to estimate the disease burden\nand direct medical cost attributable to lung cancer. Epidemiological parameters\nwere obtained from local statistics, the Global Cancer Observatory, the Global\nBurden of Disease databases, and a literature review. Direct medical costs were\nestimated through micro-costing. Costs were expressed in US dollars (US$),\nApril 2023 (1 US$ =216.38 argentine pesos). A second-order Monte Carlo\nsimulation was performed to estimate the uncertainty.\n  Results: Considering approximately 10,000 deaths, 12,000 incident cases, and\n14,000 5-year prevalent cases, the economic burden of lung cancer in Argentina\nin 2023 was estimated to be US$ 556.20 million (396.96 -718.20), approximately\n1.4% of the total healthcare expenditure for the country. The cost increased\nwith a higher stage of the disease and the main driver was the drug acquisition\n(80%). 179,046 Disability-adjusted life years could be attributable to lung\ncancer representing the 10% of the total cancer.\n  Conclusion: The disease and economic burden of lung cancer in Argentina\nimplies a high cost for the health system and would represent 19% of the\npreviously estimated economic burden for 29 cancers in Argentina.\n"
    },
    {
        "paper_id": 2310.07052,
        "authors": "John R. Birge",
        "title": "Uses of Sub-sample Estimates to Reduce Errors in Stochastic Optimization\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Optimization software enables the solution of problems with millions of\nvariables and associated parameters. These parameters are, however, often\nuncertain and represented with an analytical description of the parameter's\ndistribution or with some form of sample. With large numbers of such\nparameters, optimization of the resulting model is often driven by\nmis-specifications or extreme sample characteristics, resulting in solutions\nthat are far from a true optimum. This paper describes how asymptotic\nconvergence results may not be useful in large-scale problems and how the\noptimization of problems based on sub-sample estimates may achieve improved\nresults over models using full-sample solution estimates. A motivating example\nand numerical results from a portfolio optimization problem demonstrate the\npotential improvement. A theoretical analysis also provides insight into the\nstructure of problems where sub-sample optimization may be most beneficial.\n"
    },
    {
        "paper_id": 2310.0711,
        "authors": "Ye Li and Chen Wang",
        "title": "Valuation Duration of the Stock Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  At the peak of the tech bubble, only 0.57% of market valuation comes from\ndividends in the next year. Taking the ratio of total market value to the value\nof one-year dividends, we obtain a valuation-based duration of 175 years. In\ncontrast, at the height of the global financial crisis, more than 2.2% of\nmarket value is from dividends in the next year, implying a duration of 46\nyears. What drives valuation duration? We find that market participants have\nlimited information about cash flow beyond one year. Therefore, an increase in\nvaluation duration is due to a decrease in the discount rate rather than good\nnews about long-term growth. Accordingly, valuation duration negatively\npredicts annual market return with an out-of-sample R2 of 15%, robustly\noutperforming other predictors in the literature. While the price-dividend\nratio reflects the overall valuation level, our valuation-based measure of\nduration captures the slope of the valuation term structure. We show that\nvaluation duration, as a discount rate proxy, is a critical state variable that\naugments the price-dividend ratio in spanning the (latent) state space for\nstock-market dynamics.\n"
    },
    {
        "paper_id": 2310.07132,
        "authors": "Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald,\n  Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, and Jerret\n  Ross",
        "title": "Risk Aware Benchmarking of Large Language Models",
        "comments": "ICML 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a distributional framework for benchmarking socio-technical risks\nof foundation models with quantified statistical significance. Our approach\nhinges on a new statistical relative testing based on first and second order\nstochastic dominance of real random variables. We show that the second order\nstatistics in this test are linked to mean-risk models commonly used in\neconometrics and mathematical finance to balance risk and utility when choosing\nbetween alternatives. Using this framework, we formally develop a risk-aware\napproach for foundation model selection given guardrails quantified by\nspecified metrics. Inspired by portfolio optimization and selection theory in\nmathematical finance, we define a metrics portfolio for each model as a means\nto aggregate a collection of metrics, and perform model selection based on the\nstochastic dominance of these portfolios. The statistical significance of our\ntests is backed theoretically by an asymptotic analysis via central limit\ntheorems instantiated in practice via a bootstrap variance estimate. We use our\nframework to compare various large language models regarding risks related to\ndrifting from instructions and outputting toxic content.\n"
    },
    {
        "paper_id": 2310.07291,
        "authors": "Marco Maggis",
        "title": "The birth of (a robust) Arbitrage Theory in de Finetti's early\n  contributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \\textit{Il significato soggettivo della probabilit\\`a} (1931) by B. de\nFinetti \\cite{deF} is unanimously considered the rise of `subjectivism', a\nnotion which strongly influenced both Probability and Decision Theory. What is\nless acknowledge is that \\cite{deF} posed the foundations of modern arbitrage\ntheory. In this paper we aim at examining how de Finetti's contribution should\nbe considered as the precursor of Asset Pricing Theory and we show how his\nfindings relate to recent developments in Robust Finance.\n"
    },
    {
        "paper_id": 2310.07326,
        "authors": "Ruimin Song, TIntian Zhao, Chunhui Zhou",
        "title": "Empirical Analysis of the Impact of Legal Tender Digital Currency on\n  Monetary Policy -Based on China's Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper takes the development of China's Central bank digital currencies\nas a perspective, theoretically analyses the impact mechanism of the issuance\nand circulation of Central bank digital currencies on China's monetary policy\nand various variables of the money multiplier; at the same time, it selects the\nquarterly data from 2010 to 2022, and examines the impact of the Central bank\ndigital currencies on the money supply multiplier through the establishment of\nthe VECM model. The research results show that: the issuance of China's Central\nbank digital currencies will have an impact on the effectiveness of monetary\npolicy and intermediary indicators; and have a certain positive impact on the\nnarrow money multiplier and broad money multiplier. Based on theoretical\nanalyses and empirical tests, this paper proposes that China should explore a\nmore effective monetary policy in the context of Central bank digital\ncurrencies in the future on the premise of steadily promoting the development\nof Central bank digital currencies.\n"
    },
    {
        "paper_id": 2310.07427,
        "authors": "Zhengmeng Xu, Yujie Wang, Xiaotong Feng, Yilin Wang, Yanli Li, Hai Lin",
        "title": "Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n  and CNNs for Stock Return Predictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a time series forecasting method named Quantum Gramian Angular\nField (QGAF). This approach merges the advantages of quantum computing\ntechnology with deep learning, aiming to enhance the precision of time series\nclassification and forecasting. We successfully transformed stock return time\nseries data into two-dimensional images suitable for Convolutional Neural\nNetwork (CNN) training by designing specific quantum circuits. Distinct from\nthe classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in\neliminating the need for data normalization and inverse cosine calculations,\nsimplifying the transformation process from time series data to two-dimensional\nimages. To validate the effectiveness of this method, we conducted experiments\non datasets from three major stock markets: the China A-share market, the Hong\nKong stock market, and the US stock market. Experimental results revealed that\ncompared to the classical GAF method, the QGAF approach significantly improved\ntime series prediction accuracy, reducing prediction errors by an average of\n25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This\nresearch confirms the potential and promising prospects of integrating quantum\ncomputing with deep learning techniques in financial time series forecasting.\n"
    },
    {
        "paper_id": 2310.07692,
        "authors": "Aur\\'elien Alfonsi, Nerea Vadillo",
        "title": "Risk valuation of quanto derivatives on temperature and electricity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a coupled model for day-ahead electricity prices and\naverage daily temperature which allows to model quanto weather and energy\nderivatives. These products have gained on popularity as they enable to hedge\nagainst both volumetric and price risks. Electricity day-ahead prices and\naverage daily temperatures are modelled through non homogeneous\nOrnstein-Uhlenbeck processes driven by a Brownian motion and a Normal Inverse\nGaussian L\\'evy process, which allows to include dependence between them. A\nConditional Least Square method is developed to estimate the different\nparameters of the model and used on real data. Then, explicit and semi-explicit\nformulas are obtained for derivatives including quanto options and compared\nwith Monte Carlo simulations. Last, we develop explicit formulas to hedge\nstatically single and double sided quanto options by a portfolio of electricity\noptions and temperature options (CDD or HDD).\n"
    },
    {
        "paper_id": 2310.07738,
        "authors": "Diego Vallarino",
        "title": "Incentives for Private Industrial Investment in historical perspective:\n  the case of industrial promotion and investment promotion in Uruguay\n  (1974-2010)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using as a central instrument a new database, resulting from a compilation of\nhistorical administrative records, which covers the period 1974-2010, we can\nhave new evidence on how industrial companies used tax benefits, and claim that\nthese are decisive for the investment decision of the Uruguayan industrial\ncompanies during that period. The aforementioned findings served as a raw\nmaterial to also affirm that the incentives to increase investment are factors\nthat positively influence the level of economic activity and exports, and\nnegatively on the unemployment rate.\n"
    },
    {
        "paper_id": 2310.07789,
        "authors": "Oluwasola E. Omoju, Emily E. Ikhide, Iyabo A. Olanrele, Lucy E. Abeng,\n  Marjan Petreski, Francis O. Adebayo, Itohan Odigie, Amina M. Muhammed",
        "title": "Empirical Review of Youth-Employment Policies in Nigeria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Youth unemployment is a major socioeconomic problem in Nigeria, and several\nyouth-employment programs have been initiated and implemented to address the\nchallenge. While detailed analyses of the impacts of some of these programs\nhave been conducted, empirical analysis of implementation challenges and of the\ninfluence of limited political inclusivity on distribution of program benefits\nis rare. Using mixed research methods and primary data collected through\nfocus-group discussion and key-informant interviews, this paper turns to that\nanalysis. We found that, although there are several youth-employment programs\nin Nigeria, they have not yielded a marked reduction in youth-unemployment\nrates. The programs are challenged by factors such as lack of framework for\nproper governance and coordination, inadequate funding, lack of institutional\nimplementation capacity, inadequate oversight of implementation, limited\npolitical inclusivity, lack of prioritization of vulnerable and marginalized\ngroups, and focus on stand-alone programs that are not tied to long-term\ndevelopment plans. These issues need to be addressed to ensure that\nyouth-employment programs yield better outcomes and that youth unemployment is\nsignificantly reduced.\n"
    },
    {
        "paper_id": 2310.07865,
        "authors": "Guillermo Angeris, Tarun Chitra, Theo Diamandis, Kshitij Kulkarni",
        "title": "The Specter (and Spectra) of Miner Extractable Value",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Miner extractable value (MEV) refers to any excess value that a transaction\nvalidator can realize by manipulating the ordering of transactions. In this\nwork, we introduce a simple theoretical definition of the 'cost of MEV', prove\nsome basic properties, and show that the definition is useful via a number of\nexamples. In a variety of settings, this definition is related to the\n'smoothness' of a function over the symmetric group. From this definition and\nsome basic observations, we recover a number of results from the literature.\n"
    },
    {
        "paper_id": 2310.08193,
        "authors": "Fabio Ashtar Telarico",
        "title": "Are sanctions for losers? A network study of trade sanctions",
        "comments": "8 pages, 1 figure",
        "journal-ref": "The New Economist, 2023, 17(1): 4-11",
        "doi": "10.7251/NOEEN2333004T",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Studies built on dependency and world-system theory using network approaches\nhave shown that international trade is structured into clusters of 'core' and\n'peripheral' countries performing distinct functions. However, few have used\nthese methods to investigate how sanctions affect the position of the countries\ninvolved in the capitalist world-economy. Yet, this topic has acquired pressing\nrelevance due to the emergence of economic warfare as a key geopolitical weapon\nsince the 1950s. And even more so in light of the preeminent role that\nsanctions have played in the US and their allies' response to the\nRussian-Ukrainian war. Applying several clustering techniques designed for\ncomplex and temporal networks, this paper shows that a shift in the pattern of\ncommerce away from sanctioning countries and towards neutral or friendly ones.\nAdditionally, there are suggestions that these shifts may lead to the creation\nof an alternative 'core' that interacts with the world-economy's periphery\nbypassing traditional 'core' countries such as EU member States and the US.\n"
    },
    {
        "paper_id": 2310.08284,
        "authors": "Fredi \\v{S}ari\\'c, Stjepan Begu\\v{s}i\\'c, Andro Mer\\'cep, and Zvonko\n  Kostanj\\v{c}ar",
        "title": "Statistical arbitrage portfolio construction based on preference\n  relations",
        "comments": null,
        "journal-ref": "Expert Systems with Applications, 2023, 121906",
        "doi": "10.1016/j.eswa.2023.121906",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Statistical arbitrage methods identify mispricings in securities with the\ngoal of building portfolios which are weakly correlated with the market. In\npairs trading, an arbitrage opportunity is identified by observing relative\nprice movements between a pair of two securities. By simultaneously observing\nmultiple pairs, one can exploit different arbitrage opportunities and increase\nthe performance of such methods. However, the use of a large number of pairs is\ndifficult due to the increased probability of contradictory trade signals among\ndifferent pairs. In this paper, we propose a novel portfolio construction\nmethod based on preference relation graphs, which can reconcile contradictory\npairs trading signals across multiple security pairs. The proposed approach\nenables joint exploitation of arbitrage opportunities among a large number of\nsecurities. Experimental results using three decades of historical returns of\nroughly 500 stocks from the S\\&P 500 index show that the portfolios based on\npreference relations exhibit robust returns even with high transaction costs,\nand that their performance improves with the number of securities considered.\n"
    },
    {
        "paper_id": 2310.08285,
        "authors": "Rui Yao and Kenan Zhang",
        "title": "How would mobility-as-a-service (MaaS) platform survive as an\n  intermediary? From the viewpoint of stability in many-to-many matching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mobility-as-a-service (MaaS) provides seamless door-to-door trips by\nintegrating different transport modes. Although many MaaS platforms have\nemerged in recent years, most of them remain at a limited integration level.\nThis study investigates the assignment and pricing problem for a MaaS platform\nas an intermediary in a multi-modal transportation network, which purchases\ncapacity from service operators and sells multi-modal trips to travelers. The\nanalysis framework of many-to-many stable matching is adopted to decompose the\njoint design problem and to derive the stability condition such that both\noperators and travelers are willing to participate in the MaaS system. To\nmaximize the flexibility in route choice and remove boundaries between modes,\nwe design an origin-destination pricing scheme for MaaS trips. On the supply\nside, we propose a wholesale purchase price for service capacity. Accordingly,\nthe assignment problem is reformulated and solved as a bi-level program, where\nMaaS travelers make multi-modal trips to minimize their travel costs meanwhile\ninteracting with non-MaaS travelers in the multi-modal transport system. We\nprove that, under the proposed pricing scheme, there always exists a stable\noutcome to the overall many-to-many matching problem. Further, given an optimal\nassignment and under some mild conditions, a unique optimal pricing scheme is\nensured. Numerical experiments conducted on the extended Sioux Falls network\nalso demonstrate that the proposed MaaS system could create a win-win-win\nsituation -- the MaaS platform is profitable and both traveler welfare and\ntransit operator revenues increase from a baseline scenario without MaaS.\n"
    },
    {
        "paper_id": 2310.08353,
        "authors": "Gianluca Biggi, Andrea Mina, and Federico Tamagni",
        "title": "There are different shades of green: heterogeneous environmental\n  innovations and their effects on firm performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a firm-level dataset from the Spanish Technological Innovation Panel\n(2003-2016), this study explores the characteristics of environmentally\ninnovative firms and quantifies the effects of pursuing different types of\nenvironmental innovation strategies (resource-saving, pollution-reducing, and\nregulation-driven innovations) on sales, employment, and productivity dynamics.\n"
    },
    {
        "paper_id": 2310.08415,
        "authors": "Ummya Salma, Md. Fazlul Huq Khan",
        "title": "The Connection Between Political Stability and Inflation: Insights from\n  Four South Asian Nations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study explores the relationship between political stability and\ninflation in four South Asian countries, employing panel data spanning from\n2001 to 2021. To analyze this relationship, the study utilizes the dynamic\nordinary least square (DOLS) and fully modified ordinary least square (FMOLS)\nmethods, which account for cross-sectional dependence and slope homogeneity in\npanel data analysis. The findings consistently reveal that increased political\nstability is associated with lower inflation, while reduced political stability\nis linked to higher inflation.\n"
    },
    {
        "paper_id": 2310.08466,
        "authors": "Olivier Compte",
        "title": "Belief formation and the persistence of biased beliefs",
        "comments": "40 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a belief-formation model where agents attempt to discriminate\nbetween two theories, and where the asymmetry in strength between confirming\nand disconfirming evidence tilts beliefs in favor of theories that generate\nstrong (and possibly rare) confirming evidence and weak (and frequent)\ndisconfirming evidence. In our model, limitations on information processing\nprovide incentives to censor weak evidence, with the consequence that for some\ndiscrimination problems, evidence may become mostly one-sided, independently of\nthe true underlying theory. Sophisticated agents who know the characteristics\nof the censored data-generating process are not lured by this accumulation of\n``evidence'', but less sophisticated ones end up with biased beliefs.\n"
    },
    {
        "paper_id": 2310.08492,
        "authors": "Benjamin Jourdain, Kexin Shao",
        "title": "Maximal Martingale Wasserstein Inequality",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we complete the analysis of the Martingale Wasserstein\nInequality started in arXiv:2011.11599 by checking that this inequality fails\nin dimension $d\\ge 2$ when the integrability parameter $\\rho$ belongs to\n$[1,2)$ while a stronger Maximal Martingale Wasserstein Inequality holds\nwhatever the dimension $d$ when $\\rho\\ge 2$.\n"
    },
    {
        "paper_id": 2310.08678,
        "authors": "Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei,\n  Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, Sameena Shah",
        "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4\n  on mock CFA Exams",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance on a\nwide range of Natural Language Processing (NLP) tasks, often matching or even\nbeating state-of-the-art task-specific models. This study aims at assessing the\nfinancial reasoning capabilities of LLMs. We leverage mock exam questions of\nthe Chartered Financial Analyst (CFA) Program to conduct a comprehensive\nevaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot\n(ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an\nin-depth analysis of the models' performance and limitations, and estimate\nwhether they would have a chance at passing the CFA exams. Finally, we outline\ninsights into potential strategies and improvements to enhance the\napplicability of LLMs in finance. In this perspective, we hope this work paves\nthe way for future studies to continue enhancing LLMs for financial reasoning\nthrough rigorous evaluation.\n"
    },
    {
        "paper_id": 2310.08704,
        "authors": "Sukwoong Choi, Hyo Kang, Namil Kim, Junsik Kim",
        "title": "How Does Artificial Intelligence Improve Human Decision-Making? Evidence\n  from the AI-Powered Go Program",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study how humans learn from AI, exploiting an introduction of an\nAI-powered Go program (APG) that unexpectedly outperformed the best\nprofessional player. We compare the move quality of professional players to\nthat of APG's superior solutions around its public release. Our analysis of\n749,190 moves demonstrates significant improvements in players' move quality,\naccompanied by decreased number and magnitude of errors. The effect is\npronounced in the early stages of the game where uncertainty is highest. In\naddition, younger players and those in AI-exposed countries experience greater\nimprovement, suggesting potential inequality in learning from AI. Further,\nwhile players of all levels learn, less skilled players derive higher marginal\nbenefits. These findings have implications for managers seeking to adopt and\nutilize AI effectively within their organizations.\n"
    },
    {
        "paper_id": 2310.09022,
        "authors": "Florian Gach, Simon Hochgerner, Eva Kienbacher, Gabriel Schachinger",
        "title": "Mean-field Libor market model and valuation of long term guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existence and uniqueness of solutions to the multi-dimensional mean-field\nLibor market model (introduced by [7]) is shown. This is used as the basis for\na numerical asset-liability management (ALM) model capable of calculating\nfuture discretionary benefits in accordance with Solvency~II regulation. This\nALM model is complimented with aggregated life insurance data to perform a\nrealistic numerical study. This yields numerical evidence for heuristic\nassumptions which allow to derive estimators of lower and upper bounds for\nfuture discretionary benefits. These estimators are applied to publicly\navailable life insurance data.\n"
    },
    {
        "paper_id": 2310.09098,
        "authors": "Indrani Bose",
        "title": "Growth, Poverty Trap and Escape",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The well-known Solow growth model is the workhorse model of the theory of\neconomic growth, which studies capital accumulation in a model economy as a\nfunction of time with capital stock, labour and technology efficiency as the\nbasic ingredients. The capital is assumed to be in the form of manufacturing\nequipments and materials. Two important parameters of the model are: the saving\nfraction $s$ of the output of a production function and the technology\nefficiency parameter $A$, appearing in the production function. The saved\nfraction of the output is fully invested in the generation of new capital and\nthe rest is consumed. The capital stock also depreciates as a function of time\ndue to the wearing out of old capital and the increase in the size of the\nlabour population. We propose a stochastic Solow growth model assuming the\nsaving fraction to be a sigmoidal function of the per capita capital $k_p$. We\nderive analytically the steady state probability distribution $P(k_p)$ and\ndemonstrate the existence of a poverty trap, of central concern in development\neconomics. In a parameter regime, $P(k_p)$ is bimodal with the twin peaks\ncorresponding to states of poverty and well-being respectively. The associated\npotential landscape has two valleys with fluctuation-driven transitions between\nthem. The mean exit times from the valleys are computed and one finds that the\nescape from a poverty trap is more favourable at higher values of $A$. We\nidentify a critical value of $A_c$ below (above) which the state of poverty\n(well-being) dominates and propose two early signatures of the regime shift\noccurring at $A_c$. The economic model, with conceptual foundation in nonlinear\ndynamics and statistical mechanics, shares universal features with dynamical\nmodels from diverse disciplines like ecology and cell biology.\n"
    },
    {
        "paper_id": 2310.09181,
        "authors": "Jim Gatheral and Rado\\v{s} Radoi\\v{c}i\\'c",
        "title": "A generalization of the rational rough Heston approximation",
        "comments": "10 pages, 6 figures",
        "journal-ref": "Quantitative Finance, 2024",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Previously, in [GR19], we derived a rational approximation of the solution of\nthe rough Heston fractional ODE in the special case \\lambda = 0, which\ncorresponds to a pure power-law kernel. In this paper we extend this solution\nto the general case of the Mittag-Leffler kernel with \\lambda \\geq 0. We\nprovide numerical evidence of the convergence of the solution.\n"
    },
    {
        "paper_id": 2310.09273,
        "authors": "Etienne Chevalier, Yadh Hafsi, Vathana Ly Vath",
        "title": "Uncovering Market Disorder and Liquidity Trends Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The primary objective of this paper is to conceive and develop a new\nmethodology to detect notable changes in liquidity within an order-driven\nmarket. We study a market liquidity model which allows us to dynamically\nquantify the level of liquidity of a traded asset using its limit order book\ndata. The proposed metric holds potential for enhancing the aggressiveness of\noptimal execution algorithms, minimizing market impact and transaction costs,\nand serving as a reliable indicator of market liquidity for market makers. As\npart of our approach, we employ Marked Hawkes processes to model trades-through\nwhich constitute our liquidity proxy. Subsequently, our focus lies in\naccurately identifying the moment when a significant increase or decrease in\nits intensity takes place. We consider the minimax quickest detection problem\nof unobservable changes in the intensity of a doubly-stochastic Poisson\nprocess. The goal is to develop a stopping rule that minimizes the robust\nLorden criterion, measured in terms of the number of events until detection,\nfor both worst-case delay and false alarm constraint. We prove our procedure's\noptimality in the case of a Cox process with simultaneous jumps, while\nconsidering a finite time horizon. Finally, this novel approach is empirically\nvalidated by means of real market data analyses.\n"
    },
    {
        "paper_id": 2310.09295,
        "authors": "Kira Henshaw, Jorge M. Ramirez, Jos\\'e M. Flores-Contr\\'o, Enrique A.\n  Thomann, Sooie-Hoe Loke, Corina Constantinescu",
        "title": "On the impact of insurance on households susceptible to random\n  proportional losses: An analysis of poverty trapping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we consider a risk process with deterministic growth and\nmultiplicative jumps to model the capital of a low-income household. Reflecting\nthe high-risk nature of the low-income environment, capital losses are assumed\nto be proportional to the level of accumulated capital at the jump time. Our\naim is to derive the probability that a household falls below the poverty line,\ni.e. the trapping probability, where ``trapping\" occurs when the level of\ncapital of a household holds falls below the poverty line, to an area from\nwhich it is difficult to escape without external help. Considering the\nremaining proportion of capital to be distributed as a special case of the beta\ndistribution, closed-form expressions for the trapping probability are obtained\nvia analysis of the Laplace transform of the infinitesimal generator of the\nprocess. To study the impact of insurance on this probability, introduction of\nan insurance product offering proportional coverage is presented. The\ninfinitesimal generator of the insured process gives rise to non-local\ndifferential equations. To overcome this, we propose a recursive method for\nderiving a closed-form solution of the integro-differential equation associated\nwith the infinitesimal generator of the insured process and provide a numerical\nestimation method for obtaining the trapping probability. Constraints on the\nrate parameters of the process that prevent certain trapping are derived in\nboth the uninsured and insured cases using classical results from risk theory.\n"
    },
    {
        "paper_id": 2310.09323,
        "authors": "Tom Nonnenmacher, Jenny Nelson, Benedict Winchester",
        "title": "Maximum Return on Investment for a Domestic Photovoltaic Installation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rising energy prices in Europe and the urgent need to address global\nwarming have sparked a significant increase in the installation of domestic\nphotovoltaic systems to harness solar energy. However, since solar energy is\navailable only during daytime hours and its availability varies daily,\neffectively shifting energy use becomes crucial. Whilst batteries can assist in\nstoring excess energy, their high prices hinder their widespread adoption. In\nthis study, we explore the importance of load to maximise return on investment.\nWe propose an incremental approach to fitting load profiles into the production\nenvelope, allowing for practical implementation. We compare different meter\nresolutions: 1 second, 5 minutes, 15 minutes, and 1 hour. Our analysis reveals\nthat making real-time decisions (per second) leads to significant energy\nsavings of 16\\% compared to hourly decisions. Furthermore, we explore three\ntypes of device management strategies: ON/OFF management independent of PV\nproduction, ON/OFF management based on the current PV production, ON/OFF\nmanagement based on both current and forecasted PV production, utilising an\noptimal fit algorithm. Through our study, we demonstrate that our\nimplementation of the third approach outperforms a standard management\napproach, resulting in more than 17% cost savings. This study provides insights\ninto the optimisation of load-shifting strategies in domestic photovoltaic\ninstallations, highlighting the importance of load control and the potential\nbenefits in maximising the utilisation of solar energy while minimising energy\ncosts and environmental impact.\n"
    },
    {
        "paper_id": 2310.09578,
        "authors": "Anubha Goel, Puneet Pasricha, Juho Kanniainen",
        "title": "Sparse Index Tracking via Topological Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this research, we introduce a novel methodology for the index tracking\nproblem with sparse portfolios by leveraging topological data analysis (TDA).\nUtilizing persistence homology to measure the riskiness of assets, we introduce\na topological method for data-driven learning of the parameters for\nregularization terms. Specifically, the Vietoris-Rips filtration method is\nutilized to capture the intricate topological features of asset movements,\nproviding a robust framework for portfolio tracking. Our approach has the\nadvantage of accommodating both $\\ell_1$ and $\\ell_2$ penalty terms without the\nrequirement for expensive estimation procedures. We empirically validate the\nperformance of our methodology against state-of-the-art sparse index tracking\ntechniques, such as Elastic-Net and SLOPE, using a dataset that covers 23 years\nof S&P500 index and its constituent data. Our out-of-sample results show that\nthis computationally efficient technique surpasses conventional methods across\nrisk metrics, risk-adjusted performance, and trading expenses in varied market\nconditions. Furthermore, in turbulent markets, it not only maintains but also\nenhances tracking performance.\n"
    },
    {
        "paper_id": 2310.09588,
        "authors": "Gerald Schweiger",
        "title": "\\\"Uber wissenschaftliche Exzellenz und Wettbewerb",
        "comments": "in German language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The pursuit of excellence seems to be the True North of academia. What is\nmeant by excellence? Can excellence be measured? This article discusses the\nconcept of excellence in the context of research and competition.\n"
    },
    {
        "paper_id": 2310.09621,
        "authors": "Antigoni Polychroniadou, Gilad Asharov, Benjamin Diamond, Tucker\n  Balch, Hans Buehler, Richard Hua, Suwen Gu, Greg Gimler, Manuela Veloso",
        "title": "Prime Match: A Privacy-Preserving Inventory Matching System",
        "comments": "27 pages, 7 figures, USENIX Security 2023",
        "journal-ref": "Prime match: A privacy-preserving inventory matching system. In\n  Joseph A. Calandrino and Carmela Troncoso, editors, 32nd USENIX Security\n  Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023. USENIX\n  Association, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inventory matching is a standard mechanism/auction for trading financial\nstocks by which buyers and sellers can be paired. In the financial world, banks\noften undertake the task of finding such matches between their clients. The\nrelated stocks can be traded without adversely impacting the market price for\neither client. If matches between clients are found, the bank can offer the\ntrade at advantageous rates. If no match is found, the parties have to buy or\nsell the stock in the public market, which introduces additional costs. A\nproblem with the process as it is presently conducted is that the involved\nparties must share their order to buy or sell a particular stock, along with\nthe intended quantity (number of shares), to the bank. Clients worry that if\nthis information were to leak somehow, then other market participants would\nbecome aware of their intentions and thus cause the price to move adversely\nagainst them before their transaction finalizes. We provide a solution, Prime\nMatch, that enables clients to match their orders efficiently with reduced\nmarket impact while maintaining privacy. In the case where there are no\nmatches, no information is revealed. Our main cryptographic innovation is a\ntwo-round secure linear comparison protocol for computing the minimum between\ntwo quantities without preprocessing and with malicious security, which can be\nof independent interest. We report benchmarks of our Prime Match system, which\nruns in production and is adopted by J.P. Morgan. The system is designed\nutilizing a star topology network, which provides clients with a centralized\nnode (the bank) as an alternative to the idealized assumption of point-to-point\nconnections, which would be impractical and undesired for the clients to\nimplement in reality. Prime Match is the first secure multiparty computation\nsolution running live in the traditional financial world.\n"
    },
    {
        "paper_id": 2310.09622,
        "authors": "Edson Pindza, Jules Clement Mba, Sutene Mwambi and Nneka Umeorah",
        "title": "Neural Network for valuing Bitcoin options under jump-diffusion and\n  market sentiment model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies and Bitcoin, in particular, are prone to wild swings\nresulting in frequent jumps in prices, making them historically popular for\ntraders to speculate. A better understanding of these fluctuations can greatly\nbenefit crypto investors by allowing them to make informed decisions. It is\nclaimed in recent literature that Bitcoin price is influenced by sentiment\nabout the Bitcoin system. Transaction, as well as the popularity, have shown\npositive evidence as potential drivers of Bitcoin price. This study considers a\nbivariate jump-diffusion model to describe Bitcoin price dynamics and the\nnumber of Google searches affecting the price, representing a sentiment\nindicator. We obtain a closed formula for the Bitcoin price and derive the\nBlack-Scholes equation for Bitcoin options. We first solve the corresponding\nBitcoin option partial differential equation for the pricing process by\nintroducing artificial neural networks and incorporating multi-layer perceptron\ntechniques. The prediction performance and the model validation using various\nhigh-volatile stocks were assessed.\n"
    },
    {
        "paper_id": 2310.09745,
        "authors": "Ishmeet Matharoo",
        "title": "Economics unchained: Investigating the role of cryptocurrency,\n  blockchain and intricacies of Bitcoin price fluctuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This research paper presents a thorough economic analysis of Bitcoin and its\nimpact. We delve into fundamental principles, and technological evolution into\na prominent decentralized digital currency. Analysing Bitcoin's economic\ndynamics, we explore aspects such as transaction volume, market capitalization,\nmining activities, and macro trends. Moreover, we investigate Bitcoin's role in\neconomy ecosystem, considering its implications on traditional financial\nsystems, monetary policies, and financial inclusivity. We utilize statistical\nand analytical tools to assess equilibrium , market behaviour, and economic .\nInsights from this analysis provide a comprehensive understanding of Bitcoin's\neconomic significance and its transformative potential in shaping the future of\nglobal finance. This research contributes to informed decision-making for\nindividuals, institutions, and policymakers navigating the evolving landscape\nof decentralized finance.\n"
    },
    {
        "paper_id": 2310.0977,
        "authors": "Jaydip Sen, Arup Dasgupta, Subhasis Dasgupta, and Sayantani\n  Roychoudhury",
        "title": "A Portfolio Rebalancing Approach for the Indian Stock Market",
        "comments": "This is the draft version of the chapter that will appear in the\n  edited volume titled \"Data Science: Theory and Applications\" edited by Jaydip\n  Sen and Sayantani Royc Choudhury. The volume will be published by Cambridge\n  Scholars Publishing, New Castle upon Tyne, UK, in March 2024. The chapter has\n  80 pages, and it consists of 50 figures, and 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter presents a calendar rebalancing approach to portfolios of stocks\nin the Indian stock market. Ten important sectors of the Indian economy are\nfirst selected. For each of these sectors, the top ten stocks are identified\nbased on their free-float market capitalization values. Using the ten stocks in\neach sector, a sector-specific portfolio is designed. In this study, the\nhistorical stock prices are used from January 4, 2021, to September 20, 2023\n(NSE Website). The portfolios are designed based on the training data from\nJanuary 4, 2021 to June 30, 2022. The performances of the portfolios are tested\nover the period from July 1, 2022, to September 20, 2023. The calendar\nrebalancing approach presented in the chapter is based on a yearly rebalancing\nmethod. However, the method presented is perfectly flexible and can be adapted\nfor weekly or monthly rebalancing. The rebalanced portfolios for the ten\nsectors are analyzed in detail for their performances. The performance results\nare not only indicative of the relative performances of the sectors over the\ntraining (i.e., in-sample) data and test (out-of-sample) data, but they also\nreflect the overall effectiveness of the proposed portfolio rebalancing\napproach.\n"
    },
    {
        "paper_id": 2310.09782,
        "authors": "Roger Lee",
        "title": "All AMMs are CFMMs. All DeFi markets have invariants. A DeFi market is\n  arbitrage-free if and only if it has an increasing invariant",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a universal framework that expresses any market system in terms of state\ntransition rules, we prove that every DeFi market system has an invariant\nfunction and is thus by definition a CFMM; indeed, all automated market makers\n(AMMs) are CFMMs.\n  Invariants connect directly to arbitrage and to completeness, according to\ntwo fundamental equivalences. First, a DeFi market system is, we prove,\narbitrage-free if and only if it has a strictly increasing invariant, where\narbitrage-free means that no state can be transformed into a dominated state by\nany sequence of transactions. Second, the invariant is, we prove, unique if and\nonly if the market system is complete, meaning that it allows transitions\nbetween all pairs of states in the state space, in at least one direction.\n  Thus a necessary and sufficient condition for no-arbitrage (respectively, for\ncompleteness) is the existence of the increasing (respectively, the uniqueness\nof the) invariant, which, therefore, fulfills in nonlinear DeFi theory the\nfoundational role parallel to the existence (respectively, uniqueness) of the\npricing measure in the Fundamental Theorem of Asset Pricing for linear markets.\n  Moreover, a market system is recoverable by its invariant if and only if it\nis complete; and in all cases, complete or incomplete, every market system has,\nand is recoverable by, a multi-invariant. A market system is arbitrage-free if\nand only if its multi-invariant is increasing.\n  Our examples illustrate (non)existence of various specific types of arbitrage\nin the context of various specific types of market systems -- with or without\nfees, with or without liquidity operations, and with or without coordination\namong multiple pools -- but the fundamental theorems have full generality,\napplicable to any DeFi market system and to any notion of arbitrage expressible\nas a strict partial order.\n"
    },
    {
        "paper_id": 2310.09802,
        "authors": "Kwangseob Ahn",
        "title": "Exploitation Business: Leveraging Information Asymmetry",
        "comments": "Exploitation Business, Information Asymmetry, Digital Media, Social\n  Media, Fandom Business, Cognitive Bias,Behavioral Economics, Ethical\n  Implications, Cryptocurrency, Generative AI",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper investigates the \"Exploitation Business\" model, which capitalizes\non information asymmetry to exploit vulnerable populations. It focuses on\nbusinesses targeting non-experts or fraudsters who capitalize on information\nasymmetry to sell their products or services to desperate individuals. This\nphenomenon, also described as \"profit-making activities based on informational\nexploitation,\" thrives on individuals' limited access to information, lack of\nexpertise, and Fear of Missing Out (FOMO).\n  The recent advancement of social media and the rising trend of fandom\nbusiness have accelerated the proliferation of such exploitation business\nmodels. Discussions on the empowerment and exploitation of fans in the digital\nmedia era present a restructuring of relationships between fans and media\ncreators, highlighting the necessity of not overlooking the exploitation of\nfans' free labor.\n  This paper analyzes the various facets and impacts of exploitation business\nmodels, enriched by real-world examples from sectors like cryptocurrency and\nGenAI, thereby discussing their social, economic, and ethical implications.\nMoreover, through theoretical backgrounds and research, it explores similar\nthemes like existing exploitation theories, commercial exploitation, and\nfinancial exploitation to gain a deeper understanding of the \"Exploitation\nBusiness\" subject.\n"
    },
    {
        "paper_id": 2310.09903,
        "authors": "Fatemeh Moodi, Amir Jahangard-Rafsanjani, Sajad Zarifzadeh",
        "title": "Feature selection and regression methods for stock price prediction\n  using technical indicators",
        "comments": "18 pages, 9 figures,5 tables,45 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Due to the influence of many factors, including technical indicators on stock\nprice prediction, feature selection is important to choose the best indicators.\nThis study uses technical indicators and features selection and regression\nmethods to solve the problem of closing the stock market price. The aim of this\nresearch is to predict the stock market price with the least error. By the\nproposed method, the data created by the 3-day time window were converted to\nthe appropriate input for regression methods. In this paper, 10 regressor and\n123 technical indicators have been examined on data of the last 13 years of\nApple Company. The results have been investigated by 5 error-based evaluation\ncriteria. Based on results of the proposed method, MLPSF has 56/47% better\nperformance than MLP. Also, SVRSF has 67/42% improved compared to SVR. LRSF was\n76.7 % improved compared to LR. The RISF method also improved 72.82 % of Ridge\nregression. The DTRSB method had 24.23 % improvement over DTR. KNNSB had 15.52\n% improvement over KNN regression. RFSB had a 6 % improvement over RF. GBRSF\nalso improved at 7% over GBR. Finally, ADASF and ADASB also had a 4%\nimprovement over the ADA regression. Also, Ridge and LinearRegression had the\nbest results for stock price prediction. Based on results, the best indicators\nto predict stock price are: the Squeeze_pro, Percentage Price Oscillator,\nThermo, Decay, Archer On-Balance Volume, Bollinger Bands, Squeeze and Ichimoku\nindicator. According to the results, the use of suitable combination of\nsuggested indicators along with regression methods has resulted in high\naccuracy in predicting the closing price.\n"
    },
    {
        "paper_id": 2310.09938,
        "authors": "Suguru Otani, Takuma Matsuda",
        "title": "Unified Merger List in the Container Shipping Industry from 1966: A\n  Structural Estimation of the Transition of Importance of a Firm's Age,\n  Tonnage Capacity, and Geographical Proximity on Merger Decision",
        "comments": "26 pages and appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We construct a novel unified merger list in the global container shipping\nindustry between 1966 (the beginning of the industry) and 2022. Combining the\nlist with proprietary data, we construct a structural matching model to\ndescribe the historical transition of the importance of a firm's age, size, and\ngeographical proximity on merger decisions. We find that, as a positive factor,\na firm's size is more important than a firm's age by 9.974 times as a merger\nincentive between 1991 and 2005. However, between 2006 and 2022, as a negative\nfactor, a firm's size is more important than a firm's age by 0.026-0.630 times,\nthat is, a firm's size works as a disincentive. We also find that the distance\nbetween buyer and seller firms works as a disincentive for the whole period,\nbut the importance has dwindled to economic insignificance in recent years. In\ncounterfactual simulations, we observe that the prohibition of mergers between\nfirms in the same country would affect the merger configuration of not only the\nfirms involved in prohibited mergers but also those involved in permitted\nmergers. Finally, we present interview-based evidence of the consistency\nbetween our merger lists, estimations, and counterfactual simulations with the\nindustry experts' historical experiences.\n"
    },
    {
        "paper_id": 2310.09993,
        "authors": "Khalid Hasan Al Jasimee and Francisco Javier Blanco-Encomienda",
        "title": "A SEM-NCA approach towards the impact of participative budgeting on\n  budgetary slack and managerial performance: The mediating role of leadership\n  style and leader-member exchange",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study re-examines the impact of participative budgeting on managerial\nperformance and budgetary slack, addressing gaps in current research. A revised\nconceptual model is developed, considering the conditioning roles of leadership\nstyle and leader-member exchange. The sample includes 408 employees with\nmanagerial experience in manufacturing companies. Hypotheses are tested using a\ncombination of PLS-SEM and Necessary Condition Analysis (NCA). The results\ndemonstrate that participative budgeting negatively affects budgetary slack and\ndirectly influences managerial performance, leadership style, and leader-member\nexchange. Moreover, leadership style and leader-member exchange moderate these\nrelationships. The integration of NCA in management accounting research\nprovides valuable insights for decision-makers, allowing for more effective\nmeasures. This study contributes by encouraging a complementary PLS-SEM and NCA\napproach to examine conditional effects. It also enhances understanding of\nbudgetary control by highlighting the importance of leadership in influencing\nparticipative budgeting outcomes.\n"
    },
    {
        "paper_id": 2310.1005,
        "authors": "Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell",
        "title": "EfficientOCR: An Extensible, Open-Source Package for Efficiently\n  Digitizing World Knowledge",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Billions of public domain documents remain trapped in hard copy or lack an\naccurate digitization. Modern natural language processing methods cannot be\nused to index, retrieve, and summarize their texts; conduct computational\ntextual analyses; or extract information for statistical analyses, and these\ntexts cannot be incorporated into language model training. Given the diversity\nand sheer quantity of public domain texts, liberating them at scale requires\noptical character recognition (OCR) that is accurate, extremely cheap to\ndeploy, and sample-efficient to customize to novel collections, languages, and\ncharacter sets. Existing OCR engines, largely designed for small-scale\ncommercial applications in high resource languages, often fall short of these\nrequirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets\nboth the computational and sample efficiency requirements for liberating texts\nat scale by abandoning the sequence-to-sequence architecture typically used for\nOCR, which takes representations from a learned vision model as inputs to a\nlearned language model. Instead, EffOCR models OCR as a character or word-level\nimage retrieval problem. EffOCR is cheap and sample efficient to train, as the\nmodel only needs to learn characters' visual appearance and not how they are\nused in sequence to form language. Models in the EffOCR model zoo can be\ndeployed off-the-shelf with only a few lines of code. Importantly, EffOCR also\nallows for easy, sample efficient customization with a simple model training\ninterface and minimal labeling requirements due to its sample efficiency. We\nillustrate the utility of EffOCR by cheaply and accurately digitizing 20\nmillion historical U.S. newspaper scans, evaluating zero-shot performance on\nrandomly selected documents from the U.S. National Archives, and accurately\ndigitizing Japanese documents for which all other OCR solutions failed.\n"
    },
    {
        "paper_id": 2310.10283,
        "authors": "Shijia Song and Handong Li",
        "title": "Unveiling Early Warning Signals of Systemic Risks in Banks: A Recurrence\n  Network-Based Approach",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bank crisis is challenging to define but can be manifested through bank\ncontagion. This study presents a comprehensive framework grounded in nonlinear\ntime series analysis to identify potential early warning signals (EWS) for\nimpending phase transitions in bank systems, with the goal of anticipating\nsevere bank crisis. In contrast to traditional analyses of exposure networks\nusing low-frequency data, we argue that studying the dynamic relationships\namong bank stocks using high-frequency data offers a more insightful\nperspective on changes in the banking system. We construct multiple recurrence\nnetworks (MRNs) based on multidimensional returns of listed banks' stocks in\nChina, aiming to monitor the nonlinear dynamics of the system through the\ncorresponding indicators and topological structures. Empirical findings\nindicate that key indicators of MRNs, specifically the average mutual\ninformation, provide valuable insights into periods of extreme volatility of\nbank system. This paper contributes to the ongoing discourse on early warning\nsignals for bank instability, highlighting the applicability of predicting\nsystemic risks in the context of banking networks.\n"
    },
    {
        "paper_id": 2310.10333,
        "authors": "Carolina Camassa",
        "title": "Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers",
        "comments": "Accepted at NLLP23",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the rapidly evolving field of crypto assets, white papers are essential\ndocuments for investor guidance, and are now subject to unprecedented content\nrequirements under the European Union's Markets in Crypto-Assets Regulation\n(MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for\nboth analyzing these documents and assisting in regulatory compliance. This\npaper delivers two contributions to the topic. First, we survey existing\napplications of textual analysis to unregulated crypto asset white papers,\nuncovering a research gap that could be bridged with interdisciplinary\ncollaboration. We then conduct an analysis of the changes introduced by MiCAR,\nhighlighting the opportunities and challenges of integrating NLP within the new\nregulatory framework. The findings set the stage for further research, with the\npotential to benefit regulators, crypto asset issuers, and investors.\n"
    },
    {
        "paper_id": 2310.105,
        "authors": "Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren",
        "title": "Few-Shot Learning Patterns in Financial Time-Series for Trend-Following\n  Strategies",
        "comments": "minor edits",
        "journal-ref": null,
        "doi": "10.3905/jfds.2024.1.157",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting models for systematic trading strategies do not adapt quickly\nwhen financial market conditions rapidly change, as was seen in the advent of\nthe COVID-19 pandemic in 2020, causing many forecasting models to take\nloss-making positions. To deal with such situations, we propose a novel\ntime-series trend-following forecaster that can quickly adapt to new market\nconditions, referred to as regimes. We leverage recent developments from the\ndeep learning community and use few-shot learning. We propose the Cross\nAttentive Time-Series Trend Network -- X-Trend -- which takes positions\nattending over a context set of financial time-series regimes. X-Trend\ntransfers trends from similar patterns in the context set to make forecasts,\nthen subsequently takes positions for a new distinct target regime. By quickly\nadapting to new financial regimes, X-Trend increases Sharpe ratio by 18.9% over\na neural forecaster and 10-fold over a conventional Time-series Momentum\nstrategy during the turbulent market period from 2018 to 2023. Our strategy\nrecovers twice as quickly from the COVID-19 drawdown compared to the\nneural-forecaster. X-Trend can also take zero-shot positions on novel unseen\nfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neural\ntime-series trend forecaster over the same period. Furthermore, the\ncross-attention mechanism allows us to interpret the relationship between\nforecasts and patterns in the context set.\n"
    },
    {
        "paper_id": 2310.10518,
        "authors": "Nandi Moksnes and William Usher",
        "title": "Quantifying the relative importance of the spatial and temporal\n  resolution in energy systems optimisation model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An increasing number of studies using energy system optimisation models are\nconducted with higher spatial and temporal resolution. This comes with a\ncomputational cost which places a limit on the size, complexity, and detail of\nthe model. In this paper, we explore the relative importance of structural\naspects of energy system models, spatial and temporal resolution, compared to\nuncertainties in input parameters such as final energy demand, discount rate\nand capital costs. We use global sensitivity analysis to uncover these\ninteractions for two developing countries, Kenya, and Benin, which still lack\nuniversal access to electricity. We find that temporal resolution has a high\ninfluence on all assessed results parameters, and spatial resolution has a\nsignificant influence on the expansion of distribution lines to the\nunelectrified population. The larger overall influence of temporal resolution\nindicates that this should be prioritised compared to spatial resolution.\n"
    },
    {
        "paper_id": 2310.1076,
        "authors": "Bhaskarjit Sarmah, Tianjie Zhu, Dhagash Mehta, Stefano Pasquali",
        "title": "Towards reducing hallucination in extracting information from financial\n  reports using Large Language Models",
        "comments": "4 pages + references. Accepted for publication in Workshop on\n  Generative AI at the 3rd International Conference on AI-ML Systems 2023,\n  Bengaluru, India",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a financial analyst, the question and answer (Q\\&A) segment of the\ncompany financial report is a crucial piece of information for various analysis\nand investment decisions. However, extracting valuable insights from the Q\\&A\nsection has posed considerable challenges as the conventional methods such as\ndetailed reading and note-taking lack scalability and are susceptible to human\nerrors, and Optical Character Recognition (OCR) and similar techniques\nencounter difficulties in accurately processing unstructured transcript text,\noften missing subtle linguistic nuances that drive investor decisions. Here, we\ndemonstrate the utilization of Large Language Models (LLMs) to efficiently and\nrapidly extract information from earnings report transcripts while ensuring\nhigh accuracy transforming the extraction process as well as reducing\nhallucination by combining retrieval-augmented generation technique as well as\nmetadata. We evaluate the outcomes of various LLMs with and without using our\nproposed approach based on various objective metrics for evaluating Q\\&A\nsystems, and empirically demonstrate superiority of our method.\n"
    },
    {
        "paper_id": 2310.10797,
        "authors": "Alex Nathan, Dimosthenis Kaponis, Saul Lustgarten",
        "title": "Understanding and managing blockchain protocol risks",
        "comments": null,
        "journal-ref": "Journal of Risk Management in Financial Institutions 16.4 (2023):\n  337-353",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper addresses the issue of blockchain protocol risks, a foundational\ncategory of risks affecting Distributed Ledger Technology (DLT) which underpins\ndigital assets, smart contracts, and decentralised applications. It presents a\ncomprehensive risk management framework developed in collaboration with\nfinancial institutions, blockchain development teams and regulators that\napplies a traditional risk management taxonomy to address certain overlooked\nblockchain protocol risks. The approach offers a structured way to identify,\nmeasure, monitor and report blockchain protocol risks. The paper provides\nreal-world use cases to demonstrate the practicality and implementation of the\nproposed framework. The findings of this work contribute to the evolving\nunderstanding of blockchain protocol risks and provide valuable insights on how\nthese risks affect the adoption of DLT by financial institutions.\n"
    },
    {
        "paper_id": 2310.1085,
        "authors": "Keshav Agrawal, Susan Athey, Ayush Kanodia, and Emil Palikot",
        "title": "Digital interventions and habit formation in educational technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As online educational technology products have become increasingly prevalent,\nrich evidence indicates that learners often find it challenging to establish\nregular learning habits and complete their programs. Concurrently, online\nproducts geared towards entertainment and social interactions are sometimes so\neffective in increasing user engagement and creating frequent usage habits that\nthey inadvertently lead to digital addiction, especially among youth. In this\nproject, we carry out a contest-based intervention, common in the entertainment\ncontext, on an educational app for Indian children learning English.\nApproximately ten thousand randomly selected learners entered a 100-day reading\ncontest. They would win a set of physical books if they ranked sufficiently\nhigh on a leaderboard based on the amount of educational content consumed.\nTwelve weeks after the end of the contest, when the treatment group had no\nadditional incentives to use the app, they continued their engagement with it\nat a rate 75\\% higher than the control group, indicating a successful formation\nof a reading habit. In addition, we observed a 6\\% increase in retention within\nthe treatment group. These results underscore the potential of digital\ninterventions in fostering positive engagement habits with educational\ntechnology products, ultimately enhancing users' long-term learning outcomes.\n"
    },
    {
        "paper_id": 2310.11023,
        "authors": "Chung-Han Hsieh and Xin-Yu Wang",
        "title": "Robust Trading in a Generalized Lattice Market",
        "comments": "submitted for possible publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a novel robust trading paradigm, called\n\\textit{multi-double linear policies}, situated within a \\textit{generalized}\nlattice market. Distinctively, our framework departs from most existing robust\ntrading strategies, which are predominantly limited to single or paired assets\nand typically embed asset correlation within the trading strategy itself,\nrather than as an inherent characteristic of the market. Our generalized\nlattice market model incorporates both serially correlated returns and asset\ncorrelation through a conditional probabilistic model. In the nominal case,\nwhere the parameters of the model are known, we demonstrate that the proposed\npolicies ensure survivability and probabilistic positivity. We then derive an\nanalytic expression for the worst-case expected gain-loss and prove sufficient\nconditions that the proposed policies can maintain a \\textit{positive expected\nprofits}, even within a seemingly nonprofitable symmetric lattice market. When\nthe parameters are unknown and require estimation, we show that the parameter\nspace of the lattice model forms a convex polyhedron, and we present an\nefficient estimation method using a constrained least-squares method. These\ntheoretical findings are strengthened by extensive empirical studies using data\nfrom the top 30 companies within the S\\&P 500 index, substantiating the\nefficacy of the generalized model and the robustness of the proposed policies\nin sustaining the positive expected profit and providing downside risk\nprotection.\n"
    },
    {
        "paper_id": 2310.11151,
        "authors": "Volha Lazuka and Annika Elwert",
        "title": "Life-Cycle Effects of Comprehensive Sex Education",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sex education can impact pupils sexual activity and convey the social norms\nregarding family formation and responsibility, which can have significant\nconsequences to their future. To investigate the life-cycle effects of social\nnorm transmission, this study draws on the introduction of comprehensive sex\neducation in the curriculum of Swedish primary schools during the 1940s to the\n1950s. Inspired by social-democratic values, sex education during this period\ntaught students about abstinence, rational family planning choices, and the\nimportance of taking social responsibility for their personal decisions. The\nstudy applies a state-of-the-art estimator of the difference-in-differences\nmethod to various outcomes of men and women throughout the life cycle. The\nresults show that the reform affected most intended outcomes for men and women,\nultimately decreasing gender inequality in earnings. The effects of the reform\nalso extended to the succeeding generation of girls. Both generations created a\ncritical mass that altered social norms in favor of collective engagement and\ndemocracy. The findings suggest that social norms, internalized through\nschool-based sex education, persistently affect peoples outcomes in significant\nways.\n"
    },
    {
        "paper_id": 2310.11249,
        "authors": "Xu Yang, Xiao Yang, Weiqing Liu, Jinhui Li, Peng Yu, Zeqi Ye, Jiang\n  Bian",
        "title": "Leveraging Large Language Model for Automatic Evolving of Industrial\n  Data-Centric R&D Cycle",
        "comments": "29 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the wake of relentless digital transformation, data-driven solutions are\nemerging as powerful tools to address multifarious industrial tasks such as\nforecasting, anomaly detection, planning, and even complex decision-making.\nAlthough data-centric R&D has been pivotal in harnessing these solutions, it\noften comes with significant costs in terms of human, computational, and time\nresources. This paper delves into the potential of large language models (LLMs)\nto expedite the evolution cycle of data-centric R&D. Assessing the foundational\nelements of data-centric R&D, including heterogeneous task-related data,\nmulti-facet domain knowledge, and diverse computing-functional tools, we\nexplore how well LLMs can understand domain-specific requirements, generate\nprofessional ideas, utilize domain-specific tools to conduct experiments,\ninterpret results, and incorporate knowledge from past endeavors to tackle new\nchallenges. We take quantitative investment research as a typical example of\nindustrial data-centric R&D scenario and verified our proposed framework upon\nour full-stack open-sourced quantitative research platform Qlib and obtained\npromising results which shed light on our vision of automatic evolving of\nindustrial data-centric R&D cycle.\n"
    },
    {
        "paper_id": 2310.11293,
        "authors": "Jon Danielsson and Andreas Uthemann",
        "title": "On the use of artificial intelligence in financial regulations and the\n  impact on financial stability",
        "comments": "35 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial intelligence (AI) can undermine financial stability because of\nmalicious use, misinformation, misalignment, and the AI analytics market\nstructure. The low frequency and uniqueness of financial crises, coupled with\nmutable and unclear objectives, frustrate machine learning. Even if the\nauthorities prefer a conservative approach to AI adoption, it will likely\nbecome widely used by stealth, taking over increasingly high-level functions\ndriven by significant cost efficiencies and superior performance. We propose\nsix criteria for judging the suitability of AI.\n"
    },
    {
        "paper_id": 2310.11472,
        "authors": "Dwayne Woods",
        "title": "The Sponge Cake Dilemma over the Nile: Achieving Fairness in Resource\n  Allocation with Cake Cutting Algorithms",
        "comments": "31 pages, 7 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article explores the intricate dynamics of the Nile Basin dispute, a\ncomplex conflict involving Egypt, Ethiopia, and Sudan. Our central argument is\nthat we can gain unique insights into this dispute by employing the principles\nof game theory and the Steinhaus cake-cutting problem - a mathematical model of\nfair division. These theoretical frameworks offer a novel perspective to\nunderstand the motivations and potential actions of the involved nations. More\nimportantly, these concepts can illuminate potential pathways toward a\nresolution that ensures equitable resource allocation and maintains regional\nstability. We will analyze the conflict and explore potential solutions to this\nenduring dispute through this lens.\n"
    },
    {
        "paper_id": 2310.11552,
        "authors": "Lu\\'is A.V. Cat\\~ao, Jan Ditzen, Daniel Marcel te Kaat",
        "title": "Global Factors in Non-core Bank Funding and Exchange Rate Flexibility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that fluctuations in the ratio of non-core to core funding in the\nbanking systems of advanced economies are driven by a handful of global factors\nof both real and financial natures, with country-specific factors playing no\nsignificant roles. Exchange rate flexibility helps insulate the non-core to\ncore ratio from such global factors but only significantly so outside periods\nof major global financial disruptions, as in 2008-2009.\n"
    },
    {
        "paper_id": 2310.11771,
        "authors": "Damien Ackerer, Julien Hugonnier, Urban Jermann",
        "title": "Perpetual Futures Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Perpetual futures are contracts without expiration date in which the\nanchoring of the futures price to the spot price is ensured by periodic funding\npayments from long to short. We derive explicit expressions for the\nno-arbitrage price of various perpetual contracts, including linear, inverse,\nand quantos futures in both discrete and continuous-time. In particular, we\nshow that the futures price is given by the risk-neutral expectation of the\nspot sampled at a random time that reflects the intensity of the price\nanchoring. Furthermore, we identify funding specifications that guarantee the\ncoincidence of futures and spot prices, and show that for such specifications\nperpetual futures contracts can be replicated by dynamic trading in primitive\nsecurities.\n"
    },
    {
        "paper_id": 2310.11987,
        "authors": "Georgios I. Papayiannis",
        "title": "A Framework for Treating Model Uncertainty in the Asset Liability\n  Management Problem",
        "comments": "arXiv admin note: text overlap with arXiv:2207.00862",
        "journal-ref": "Journal of Industrial and Management Optimization, 19(11),\n  7811-7825 (2023)",
        "doi": "10.3934/jimo.2023021",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of asset liability management (ALM) is a classic problem of the\nfinancial mathematics and of great interest for the banking institutions and\ninsurance companies. Several formulations of this problem under various model\nsettings have been studied under the Mean-Variance (MV) principle perspective.\nIn this paper, the ALM problem is revisited under the context of model\nuncertainty in the one-stage framework. In practice, uncertainty issues appear\nto several aspects of the problem, e.g. liability process characteristics,\nmarket conditions, inflation rates, inside information effects, etc. A\nframework relying on the notion of the Wasserstein barycenter is presented\nwhich is able to treat robustly this type of ambiguities by appropriate\nhandling the various information sources (models) and appropriately\nreformulating the relevant decision making problem. The proposed framework can\nbe applied to a number of different model settings leading to the selection of\ninvestment portfolios that remain robust to the various uncertainties appearing\nin the market. The paper is concluded with a numerical experiment for a static\nversion of the ALM problem, employing standard modelling approaches,\nillustrating the capabilities of the proposed method with very satisfactory\nresults in retrieving the true optimal strategy even in high noise cases.\n"
    },
    {
        "paper_id": 2310.12056,
        "authors": "Nils Engler and Filip Lindskog",
        "title": "Mack's estimator motivated by large exposure asymptotics in a compound\n  Poisson setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The distribution-free chain ladder of Mack justified the use of the chain\nladder predictor and enabled Mack to derive an estimator of conditional mean\nsquared error of prediction for the chain ladder predictor. Classical insurance\nloss models, i.e. of compound Poisson type, are not consistent with Mack's\ndistribution-free chain ladder. However, for a sequence of compound Poisson\nloss models indexed by exposure (e.g. number of contracts), we show that the\nchain ladder predictor and Mack's estimator of conditional mean squared error\nof prediction can be derived by considering large exposure asymptotics. Hence,\nquantifying chain ladder prediction uncertainty can be done with Mack's\nestimator without relying on the validity of the model assumptions of the\ndistribution-free chain ladder.\n"
    },
    {
        "paper_id": 2310.12255,
        "authors": "Sergio A. Yuhjtman (Flashbots)",
        "title": "Walraswap: a solution to uniform price batch auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Consider a finite set of trade orders and automated market makers (AMMs) at\nsome state. We propose a solution to the problem of finding an equilibrium\nprice vector to execute all the orders jointly with corresponding optimal AMMs\nswaps. The solution is based on Brouwer's fixed-point theorem. We discuss\ncomputational aspects relevant for realistic situations in public blockchain\nactivity.\n"
    },
    {
        "paper_id": 2310.12272,
        "authors": "Nail Kashaev, Natalia Lazzati, Ruli Xiao",
        "title": "Peer Effects in Consideration and Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general model of discrete choice that incorporates peer effects\nin preferences and consideration sets. We characterize the equilibrium behavior\nand establish conditions under which all parts of the model can be recovered\nfrom a sequence of choices. We allow peers to affect only preferences, only\nconsideration, or both. We show that these peer-effect mechanisms have\ndifferent behavioral implications in the data. This allows us to recover the\nset and the type of connections between the agents in the network. We then use\nthis information to recover the preferences and the consideration mechanisms of\neach agent. These nonparametric identification results allow for general forms\nof heterogeneity across agents and do not rely on the variation of either\nexogenous covariates or the set of available options (menus). We apply our\nresults to model expansion decisions by coffee chains and find evidence of\nlimited consideration. We simulate counterfactual predictions and show how\nlimited consideration slows down competition.\n"
    },
    {
        "paper_id": 2310.12333,
        "authors": "Jungjun Park and Andrew L. Nguyen",
        "title": "Black-Litterman Asset Allocation under Hidden Truncation Distribution",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the Black-Litterman (BL) asset allocation model\n(Black and Litterman, 1990) under the hidden truncation skew-normal\ndistribution (Arnold and Beaver, 2000). In particular, when returns are assumed\nto follow this skew normal distribution, we show that the posterior returns,\nafter incorporating views, are also skew normal. By using Simaan three moments\nrisk model (Simaan, 1993), we could then obtain the optimal portfolio.\nEmpirical data show that the optimal portfolio obtained this way has less risk\ncompared to an optimal portfolio of the classical BL model and that they become\nmore negatively skewed as the expected returns of portfolios increase, which\nsuggests that the investors trade a negative skewness for a higher expected\nreturn. We also observe a negative relation between portfolio volatility and\nportfolio skewness. This observation suggests that investors may be making a\ntrade-off, opting for lower volatility in exchange for higher skewness, or vice\nversa. This trade-off indicates that stocks with significant price declines\ntend to exhibit increased volatility.\n"
    },
    {
        "paper_id": 2310.12341,
        "authors": "Debashrita Mohapatra, Debi Prasad Mohapatra and Ram Sewak Dubey",
        "title": "Price dispersion across online platforms: Evidence from hotel room\n  prices in London (UK)",
        "comments": "Accepted for publication in Applied Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the widespread price dispersion of homogeneous products\nacross different online platforms, even when consumers can easily access price\ninformation from comparison websites. We collect data for the 200 most popular\nhotels in London (UK) and document that prices vary widely across booking sites\nwhile making reservations for a hotel room. Additionally, we find that prices\nlisted across different platforms tend to converge as the booking date gets\ncloser to the date of stay. However, the price dispersion persists until the\ndate of stay, implying that the \"law of one price\" does not hold. We present a\nsimple theoretical model to explain this and show that in the presence of\naggregate demand uncertainty and capacity constraints, price dispersion could\nexist even when products are homogeneous, consumers are homogeneous, all agents\nhave perfect information about the market structure, and consumers face no\nsearch costs to acquire information about the products. Our theoretical\nintuition and robust empirical evidence provide additional insights into price\ndispersion across online platforms in different institutional settings. Our\nstudy complements the existing literature that relies on consumer search costs\nto explain the price dispersion phenomenon.\n"
    },
    {
        "paper_id": 2310.12428,
        "authors": "Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios\n  Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali",
        "title": "Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities",
        "comments": "5 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n"
    },
    {
        "paper_id": 2310.125,
        "authors": "Yanhui Shen",
        "title": "American Option Pricing using Self-Attention GRU and Shapley Value\n  Interpretation",
        "comments": "Working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Options, serving as a crucial financial instrument, are used by investors to\nmanage and mitigate their investment risks within the securities market.\nPrecisely predicting the present price of an option enables investors to make\ninformed and efficient decisions. In this paper, we propose a machine learning\nmethod for forecasting the prices of SPY (ETF) option based on gated recurrent\nunit (GRU) and self-attention mechanism. We first partitioned the raw dataset\ninto 15 subsets according to moneyness and days to maturity criteria. For each\nsubset, we matched the corresponding U.S. government bond rates and Implied\nVolatility Indices. This segmentation allows for a more insightful exploration\nof the impacts of risk-free rates and underlying volatility on option pricing.\nNext, we built four different machine learning models, including multilayer\nperceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and\nself-attention GRU in comparison to the traditional binomial model. The\nempirical result shows that self-attention GRU with historical data outperforms\nother models due to its ability to capture complex temporal dependencies and\nleverage the contextual information embedded in the historical data. Finally,\nin order to unveil the \"black box\" of artificial intelligence, we employed the\nSHapley Additive exPlanations (SHAP) method to interpret and analyze the\nprediction results of the self-attention GRU model with historical data. This\nprovides insights into the significance and contributions of different input\nfeatures on the pricing of American-style options.\n"
    },
    {
        "paper_id": 2310.12671,
        "authors": "Freek Holvoet, Katrien Antonio and Roel Henckaerts",
        "title": "Neural networks for insurance pricing with frequency and severity data:\n  a benchmark study from data preprocessing to technical tariff",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Insurers usually turn to generalized linear models for modelling claim\nfrequency and severity data. Due to their success in other fields, machine\nlearning techniques are gaining popularity within the actuarial toolbox. Our\npaper contributes to the literature on frequency-severity insurance pricing\nwith machine learning via deep learning structures. We present a benchmark\nstudy on four insurance data sets with frequency and severity targets in the\npresence of multiple types of input features. We compare in detail the\nperformance of: a generalized linear model on binned input data, a\ngradient-boosted tree model, a feed-forward neural network (FFNN), and the\ncombined actuarial neural network (CANN). Our CANNs combine a baseline\nprediction established with a GLM and GBM, respectively, with a neural network\ncorrection. We explain the data preprocessing steps with specific focus on the\nmultiple types of input features typically present in tabular insurance data\nsets, such as postal codes, numeric and categorical covariates. Autoencoders\nare used to embed the categorical variables into the neural network and we\nexplore their potential advantages in a frequency-severity setting. Finally, we\nconstruct global surrogate models for the neural nets' frequency and severity\nmodels. These surrogates enable the translation of the essential insights\ncaptured by the FFNNs or CANNs to GLMs. As such, a technical tariff table\nresults that can easily be deployed in practice.\n"
    },
    {
        "paper_id": 2310.1276,
        "authors": "Richard S.J. Tol",
        "title": "The Social Cost of Carbon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The social cost of carbon is the damage avoided by slightly reducing carbon\ndioxide emissions. It is a measure of the desired intensity of climate policy.\nThe social cost of carbon is highly uncertain because of the long and complex\ncause-effect chain, and because it quantifies and aggregates impacts over a\nlong period of time, affecting all people in a wide range of possible futures.\nRecent estimates are around $\\$$80/tCO$_2$.\n"
    },
    {
        "paper_id": 2310.13029,
        "authors": "Ioannis Nasios, Konstantinos Vogklis",
        "title": "Blending gradient boosted trees and neural networks for point and\n  probabilistic forecasting of hierarchical time series",
        "comments": null,
        "journal-ref": "Volume 38, Issue 4, 2022, Pages 1448-1459",
        "doi": "10.1016/j.ijforecast.2022.01.001",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we tackle the problem of point and probabilistic forecasting by\ndescribing a blending methodology of machine learning models that belong to\ngradient boosted trees and neural networks families. These principles were\nsuccessfully applied in the recent M5 Competition on both Accuracy and\nUncertainty tracks. The keypoints of our methodology are: a) transform the task\nto regression on sales for a single day b) information rich feature engineering\nc) create a diverse set of state-of-the-art machine learning models and d)\ncarefully construct validation sets for model tuning. We argue that the\ndiversity of the machine learning models along with the careful selection of\nvalidation examples, where the most important ingredients for the effectiveness\nof our approach. Although forecasting data had an inherent hierarchy structure\n(12 levels), none of our proposed solutions exploited that hierarchical scheme.\nUsing the proposed methodology, our team was ranked within the gold medal range\nin both Accuracy and the Uncertainty track. Inference code along with already\ntrained models are available at\nhttps://github.com/IoannisNasios/M5_Uncertainty_3rd_place\n"
    },
    {
        "paper_id": 2310.13081,
        "authors": "Diego Marcondes and Adilson Simonis",
        "title": "Metastable Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Metastability is a phenomenon observed in stochastic systems which stay in a\nfalse-equilibrium within a region of its state space until the occurrence of a\nsequence of rare events that leads to an abrupt transition to a different\nregion. This paper presents financial markets as metastable systems and shows\nthat, under this assumption, financial time series evolve as hidden Markov\nmodels. In special, we propose a theory that outlines an explicit causal\nrelation between a financial market and the evolution of a financial time\nseries. In the context of financial economics and causal factor investment,\nthis theory introduces a paradigm shift, suggesting that investment performance\nfluctuations are primarily driven by the market state rather than direct\ncausation by other variables. While not incompatible with traditional causal\ninference, our approach addresses the non-stationary evolution of time series\nthrough changes in market states, enhancing risk assessment and enabling\nmitigation strategies.\n"
    },
    {
        "paper_id": 2310.132,
        "authors": "Michael Barnett, William Brock, Lars Peter Hansen, Ruimeng Hu, Joseph\n  Huang",
        "title": "A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the implications of model uncertainty in a climate-economics\nframework with three types of capital: \"dirty\" capital that produces carbon\nemissions when used for production, \"clean\" capital that generates no emissions\nbut is initially less productive than dirty capital, and knowledge capital that\nincreases with R\\&D investment and leads to technological innovation in green\nsector productivity. To solve our high-dimensional, non-linear model framework\nwe implement a neural-network-based global solution method. We show there are\nfirst-order impacts of model uncertainty on optimal decisions and social\nvaluations in our integrated climate-economic-innovation framework. Accounting\nfor interconnected uncertainty over climate dynamics, economic damages from\nclimate change, and the arrival of a green technological change leads to\nsubstantial adjustments to investment in the different capital types in\nanticipation of technological change and the revelation of climate damage\nseverity.\n"
    },
    {
        "paper_id": 2310.133,
        "authors": "Kunal Rajesh Lahoti, Shivani Hanji, Pratik Kamble, Kavita Vemuri",
        "title": "Impact of Loss-Framing and Risk Attitudes on Insurance Purchase:\n  Insights from a Game-like Interface Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the impact of loss-framing and individual risk\nattitude on willingness- to purchase insurance products utilizing a game-like\ninterface as choice architecture. The application presents events as\nexperienced in real life. Both financial and emotional loss-framing events are\nfollowed by choices to purchase insurance. The participant cohorts considered\nwere undergraduate students and older participants; the latter group was\nfurther subdivided by income and education. The within-subject analysis reveals\nthat the loss framing effect on insurance consumption is higher in the younger\npopulation, though contingent on the insurance product type. Health and\naccident insurance shows a negative correlation with risk attitudes for younger\nparticipants and a positive correlation with accident insurance for older\nparticipants. Risk attitude and life insurance products showed no dependency.\nThe findings elucidate the role of age, income, family responsibilities, and\nrisk attitude in purchasing insurance products. Importantly, it confirms the\nheuristics of framing/nudging.\n"
    },
    {
        "paper_id": 2310.13511,
        "authors": "Donggyu Kim and Minseog Oh",
        "title": "Dynamic Realized Minimum Variance Portfolio Models",
        "comments": "35 pages with Appendix 15 pages (total 50 pages)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a dynamic minimum variance portfolio (MVP) model using\nnonlinear volatility dynamic models, based on high-frequency financial data.\nSpecifically, we impose an autoregressive dynamic structure on MVP processes,\nwhich helps capture the MVP dynamics directly. To evaluate the dynamic MVP\nmodel, we estimate the inverse volatility matrix using the constrained\n$\\ell_1$-minimization for inverse matrix estimation (CLIME) and calculate daily\nrealized non-normalized MVP weights. Based on the realized non-normalized MVP\nweight estimator, we propose the dynamic MVP model, which we call the dynamic\nrealized minimum variance portfolio (DR-MVP) model. To estimate a large number\nof parameters, we employ the least absolute shrinkage and selection operator\n(LASSO) and predict the future MVP and establish its asymptotic properties.\nUsing high-frequency trading data, we apply the proposed method to MVP\nprediction.\n"
    },
    {
        "paper_id": 2310.13797,
        "authors": "Benjamin Joseph, Gregoire Loeper, Jan Obloj",
        "title": "The Measure Preserving Martingale Sinkhorn Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We contribute to the recent studies of the so-called Bass martingale.\nBackhoff-Veraguas et al. (2020) showed it is the solution to the martingale\nBenamou-Brenier (mBB) problem, i.e., among all martingales with prescribed\ninitial and terminal distributions it is the one closest to the Brownian\nmotion. We link it with semimartingale optimal transport and deduce an\nalternative way to derive the dual formulation recently obtained in\nBackhoff-Veraguas et al. (2023). We then consider computational methods to\ncompute the Bass martingale. The dual formulation of the transport problem\nleads to an iterative scheme that mirrors to the celebrated Sinkhorn algorithm\nfor entropic optimal transport. We call it the measure preserving martingale\nSinkhorn (MPMS) algorithm. We prove that in any dimension, each step of the\nalgorithm improves the value of the dual problem, which implies its\nconvergence. Our MPMS algorithm is equivalent to the fixed-point method of\nConze and Henry-Labordere (2021), studied in Acciaio et al. (2023), and\nperforms very well on a range of examples, including real market data.\n"
    },
    {
        "paper_id": 2310.14138,
        "authors": "Matthew P Hamilton, Caroline X Gao, Glen Wiesner, Kate M Filia, Jana M\n  Menssink, Petra Plencnerova, David G Baker, Patrick D McGorry, Alexandra\n  Parker, Jonathan Karnon, Sue M Cotton and Cathrine Mihalopoulos",
        "title": "A prototype software framework for transferable computational health\n  economic models and its early application in youth mental health",
        "comments": "16 pages, 3 tables, 1 figure",
        "journal-ref": "PharmacoEconomics (2024)",
        "doi": "10.1007/s40273-024-01378-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We are developing an economic model to explore multiple topics in Australian\nyouth mental health policy. We want that model to be readily transferable to\nother jurisdictions. We developed a software framework for authoring\ntransparent, reusable and updatable Computational Health Economic Models\n(CHEMs) (the software files that implement health economic models). We\nspecified framework user requirements of a template CHEM module that\nfacilitates modular model implementations, a simple programming syntax and\ntools for authoring new CHEM modules, supplying CHEMs with data, reporting\nreproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM\nproject website. We implemented the framework as six development version code\nlibraries in the programming language R that integrate with online services for\nsoftware development and research data archiving. We used the framework to\nauthor five development version R libraries of CHEM modules focused on utility\nmapping in youth mental health. These modules provide tools for variable\nvalidation, dataset description, multi-attribute instrument scoring,\nconstruction of mapping models, reporting of mapping studies and making out of\nsample predictions. We assessed these CHEM module libraries as mostly meeting\ntransparency, reusability and updatability criteria that we have previously\ndeveloped, but requiring more detailed documentation and unit testing of\nindividual modules. Our software framework has potential value as a prototype\nfor future tools to support the development of transferable CHEMs.\n"
    },
    {
        "paper_id": 2310.14144,
        "authors": "Marcel Nutz, Kevin Webster, Long Zhao",
        "title": "Unwinding Stochastic Order Flow: When to Warehouse Trades",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how to unwind stochastic order flow with minimal transaction costs.\nStochastic order flow arises, e.g., in the central risk book (CRB), a\ncentralized trading desk that aggregates order flows within a financial\ninstitution. The desk can warehouse in-flow orders, ideally netting them\nagainst subsequent opposite orders (internalization), or route them to the\nmarket (externalization) and incur costs related to price impact and bid-ask\nspread. We model and solve this problem for a general class of in-flow\nprocesses, enabling us to study in detail how in-flow characteristics affect\noptimal strategy and core trading metrics. Our model allows for an analytic\nsolution in semi-closed form and is readily implementable numerically. Compared\nwith a standard execution problem where the order size is known upfront, the\nunwind strategy exhibits an additive adjustment for projected future in-flows.\nIts sign depends on the autocorrelation of orders; only truth-telling\n(martingale) flow is unwound myopically. In addition to analytic results, we\npresent extensive simulations for different use cases and regimes, and\nintroduce new metrics of practical interest.\n"
    },
    {
        "paper_id": 2310.1432,
        "authors": "Waylon Jepsen, Colin Roberts",
        "title": "Analysis of the RMM-01 Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Constant function market makers(CFMMS) are a popular market design for\ndecentralized exchanges(DEX). Liquidity providers(LPs) supply the CFMMs with\nassets to enable trades. In exchange for providing this liquidity, an LP\nreceives a token that replicates a payoff determined by the trading function\nused by the CFMM. In this paper, we study a time-dependent CFMM called RMM-01.\nThe trading function for RMM-01 is chosen such that LPs recover the payoff of a\nBlack--Scholes priced covered call. First, we introduce the general framework\nfor CFMMs. After, we analyze the pricing properties of RMM-01. This includes\nthe cost of price manipulation and the corresponding implications on arbitrage.\nOur first primary contribution is from examining the time-varying price\nproperties of RMM-01 and determining parameter bounds when RMM-01 has a more\nstable price than Uniswap. Finally, we discuss combining lending protocols with\nRMM-01 to achieve other option payoffs which is our other primary contribution.\n"
    },
    {
        "paper_id": 2310.1438,
        "authors": "Songhua Hu, Kailai Wang, Lingyao Li, Yingrui Zhao, Zhenbing He, and\n  Yunpeng (Jack) Zhang",
        "title": "Modeling Link-level Road Traffic Resilience to Extreme Weather Events\n  Using Crowdsourced Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Climate changes lead to more frequent and intense weather events, posing\nescalating risks to road traffic. Crowdsourced data offer new opportunities to\nmonitor and investigate changes in road traffic flow during extreme weather.\nThis study utilizes diverse crowdsourced data from mobile devices and the\ncommunity-driven navigation app, Waze, to examine the impact of three weather\nevents (i.e., floods, winter storms, and fog) on road traffic. Three metrics,\nspeed change, event duration, and area under the curve (AUC), are employed to\nassess link-level traffic change and recovery. In addition, a user's perceived\nseverity is computed to evaluate link-level weather impact based on\ncrowdsourced reports. This study evaluates a range of new data sources, and\nprovides insights into the resilience of road traffic to extreme weather, which\nare crucial for disaster preparedness, response, and recovery in road\ntransportation systems.\n"
    },
    {
        "paper_id": 2310.14383,
        "authors": "Tanhua Jin, Kailai Wang, Yanan Xin, Jian Shi, Ye Hong, and Frank\n  Witlox",
        "title": "Is A 15-minute City within Reach in the United States? An Investigation\n  of Activity-Based Mobility Flows in the 12 Most Populous US Cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Enhanced efforts in the transportation sector should be implemented to\nmitigate the adverse effects of CO2 emissions resulting from zoning-based\nplanning paradigms. The innovative concept of the 15-minute city, with a focus\non proximity-based planning, holds promise in minimizing unnecessary travel and\nadvancing the progress toward achieving carbon neutrality. However, an\nimportant research question that remains insufficiently explored is: to what\nextent is a 15-minute city concept within reach for US cities? This paper\nestablishes a comprehensive framework to evaluate the 15-minute city concept\nusing SafeGraph Point of Interest (POI) check-in data in the 12 most populous\nUS cities. The results reveal that residents are more likely to rely on cars\ndue to the fact that most of their essential activities are located beyond\nconvenient walking, cycling, and public transit distances. However, there is\nsignificant potential for the implementation of the 15-minute city concept, as\nmost residents' current activities can be accommodated within a 15-minute\nradius by the aforementioned low-emission modes of transportation. Our findings\ncan offer policymakers insight into how far US cities are away from the\n15-minute city and the potential CO2 emission reduction they can expect if the\nconcept is successfully implemented.\n"
    },
    {
        "paper_id": 2310.14406,
        "authors": "Jaume Benseny, Jarno Lahteenmaki, Juuso Toyli, Heikki Hammainen",
        "title": "Urban wireless traffic evolution: the role of new devices and the effect\n  of policy",
        "comments": null,
        "journal-ref": "Telecommunications Policy, 47(7), 102595 (2023)",
        "doi": "10.1016/j.telpol.2023.102595",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The emergence of new wireless technologies, such as the Internet of Things,\nallows digitalizing new and diverse urban activities. Thus, wireless traffic\ngrows in volume and complexity, making prediction, investment planning, and\nregulation increasingly difficult. This article characterizes urban wireless\ntraffic evolution, supporting operators to drive mobile network evolution and\npolicymakers to increase national and local competitiveness. We propose a\nholistic method that widens previous research scope, including new devices and\nthe effect of policy from multiple government levels. We provide an analytical\nformulation that combines existing complementary methods on traffic evolution\nresearch and diverse data sources. Results for a centric area of Helsinki\nduring 2020-2030 indicate that daily volumes increase, albeit a surprisingly\nlarge part of the traffic continues to be generated by smartphones. Machine\ntraffic gains importance, driven by surveillance video cameras and connected\ncars. While camera traffic is sensitive to law enforcement policies and data\nregulation, car traffic is less affected by transport electrification policy.\nHigh-priority traffic remains small, even under encouraging autonomous vehicle\npolicies. We suggest that 5G small cells might be needed around 2025, albeit\nthe utilization of novel radio technology and additional mid-band spectrum\ncould delay this need until 2029. We argue that mobile network operators\ninevitably need to cooperate in constructing a single, shared small cell\nnetwork to mitigate the high deployment costs of massively deploying small\ncells. We also provide guidance to local and national policymakers for\nIoT-enabled competitive gains via the mitigation of five bottlenecks. For\nexample, local monopolies for mmWave connectivity should be facilitated on\nspace-limited urban furniture or risk an eventual capacity crunch, slowing down\ndigitalization.\n"
    },
    {
        "paper_id": 2310.14473,
        "authors": "Tongseok Lim",
        "title": "Optimal exercise decision of American options under model uncertainty",
        "comments": "Version 1 of the paper posted on arXiv had an incorrect Proposition\n  2.1, which was used to erroneously derive the equation $P_c = \\overline P_c$.\n  The proposition was removed in Ver 2, and the main theorem now assumes the\n  equation. I would like to find sufficient conditions for the equation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given the marginal distribution information of the underlying asset price at\ntwo future times $T_1$ and $T_2$, we consider the problem of determining a\nmodel-free upper bound on the price of a class of American options that must be\nexercised at either $T_1$ or $T_2$. The model uncertainty consistent with the\ngiven marginal information is described as the martingale optimal transport\nproblem. We show that any option exercise scheme associated with any market\nmodel that jointly maximizes the expected option payoff must be nonrandomized\nif the American option payoff satisfies a suitable convexity condition and the\nmodel-free price upper bound and its relaxed version coincide. The latter\ncondition is desired to be removed under appropriate conditions on the cost and\nmarginals.\n"
    },
    {
        "paper_id": 2310.14536,
        "authors": "Xin Du, Kai Moriyama, Kumiko Tanaka-Ishii",
        "title": "Co-Training Realized Volatility Prediction Model with Neural\n  Distributional Transformation",
        "comments": "Accepted at ICAIF'23",
        "journal-ref": null,
        "doi": "10.1145/3604237.3626870",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows a novel machine learning model for realized volatility (RV)\nprediction using a normalizing flow, an invertible neural network. Since RV is\nknown to be skewed and have a fat tail, previous methods transform RV into\nvalues that follow a latent distribution with an explicit shape and then apply\na prediction model. However, knowing that shape is non-trivial, and the\ntransformation result influences the prediction model. This paper proposes to\njointly train the transformation and the prediction model. The training process\nfollows a maximum-likelihood objective function that is derived from the\nassumption that the prediction residuals on the transformed RV time series are\nhomogeneously Gaussian. The objective function is further approximated using an\nexpectation-maximum algorithm. On a dataset of 100 stocks, our method\nsignificantly outperforms other methods using analytical or naive\nneural-network transformations.\n"
    },
    {
        "paper_id": 2310.1459,
        "authors": "Kaixin Liu, Jiwei Zhou and Junda Wang",
        "title": "Can the Black Lives Matter Movement Reduce Racial Disparities? Evidence\n  from Medical Crowdfunding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using high-frequency donation records from a major medical crowdfunding site\nand careful difference-in-difference analysis, we demonstrate that the 2020 BLM\nsurge decreased the fundraising gap between Black and non-Black beneficiaries\nby around 50\\%. The reduction is largely attributed to non-Black donors. Those\nbeneficiaries in counties with moderate BLM activities were most impacted. We\nconstruct innovative instrumental variable approaches that utilize weekends and\nrainfall to identify the global and local effects of BLM protests. Results\nsuggest a broad social movement has a greater influence on charitable-giving\nbehavior than a local event. Social media significantly magnifies the impact of\nprotests.\n"
    },
    {
        "paper_id": 2310.14604,
        "authors": "Amit Kumar Jha",
        "title": "Beyond VaR and CVaR: Topological Risk Measures in Financial Markets",
        "comments": "14 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.31895.96168",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper introduces a novel approach to financial risk assessment by\nincorporating topological data analysis (TDA), specifically cohomology groups,\ninto the evaluation of equities portfolios. The study aims to go beyond\ntraditional risk measures like Value at Risk (VaR) and Conditional Value at\nRisk (CVaR), offering a more nuanced understanding of market complexities.\nUsing last one year daily real-world closing price return data for three\nequities Apple, Microsoft and Google , we developed a new topological\nriskmeasure, termed Topological VaR Distance (TVaRD). Preliminary results\nindicate a significant change in the density of the point cloud representing\nthe financial time series during stress conditions, suggesting that TVaRD may\noffer additional insights into portfolio risk and has the potential to\ncomplement existing risk management tools.\n"
    },
    {
        "paper_id": 2310.14697,
        "authors": "Justin Larouze, Etienne Martin (ICMCB), Pierre Calmon (LMC)",
        "title": "Human Reliability Assessment method applied to investigate human factors\n  in NDT -- The case of the interpretation of radiograms in the French nuclear\n  sector",
        "comments": "Internatonal conference Non-Destructive Examination (NDE) in Nuclear,\n  Sustainable Nuclear Energy Technology Platorm (SNETP), Jun 2023, Sheffield -\n  UK, United Kingdom",
        "journal-ref": null,
        "doi": "10.58286/28268",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This communication reports on a study carried out in the context of the\ncollaborative FOEHN project (Human and Organizational Factors in\nNon-Destructive Evaluation) supported by the French National Research Agency.\nThe motivation of this project comes from the observation that human and\nOrganizational factors (HOF) are not sufficiently considered by the NDT\ncommunity. Its goal is to analyse and model the influence of the HOF on\nselected cases of study in the perspective of a better evaluation of the\nperformance of inspections. The communication is focused on a radiographic test\n(RT) case of study in which it appeared that several successive inspections had\nfailed to detect an existing in-service defect. The analysis and modelling of\nHOF related to interpretation of films has been achieved in the framework of\nthe CREAM (Cognitive and Reliability and Error Analysis Method). A survey has\nbeen conducted during the training and the maintaining of the proficiency of\nNDT (Non Destructive Testing) operators. This was followed by a non-participant\nobservation of operators on site and several individual interviews including a\nsample of people covering the main organizational and hierarchical roles (eg.\nproject management, management, operations, invigilation). The exchange with\nthe HOF experts resulted in a hierarchical analysis of ''radiogram\ninterpretation'' tasks (31 sub-tasks) and a list of contextual and\norganizational factors that may affect the performance of interpretation of\nfilms by the operator. From such a description the CREAM method allows to\ndetermine critical tasks and probability of ``errors'' linked to a limited set\nof ``Common Performance Conditions'' (CPC). The first conclusions of this study\nare that the model CREAM seems well-adapted to the estimation of the impact of\nHOF on NDT performances. The next phases should be to apply it to other tasks\n(here only radiograph interpretation) and techniques. The expected benefit of\nthis study is to provide tools for the evaluation and optimisation of NDT\nimplementation.\n"
    },
    {
        "paper_id": 2310.14748,
        "authors": "Jaydip Sen, Arup Dasgupta, Partha Pratim Sengupta, and Sayantani Roy\n  Choudhury",
        "title": "A Comparative Study of Portfolio Optimization Methods for the Indian\n  Stock Market",
        "comments": "This is the draft version of the chapter that has been accepted for\n  publication in the edited volume titled \"Data Science: Theory and Practice\".\n  The volume is edited by Jaydip Sen and Sayantani Roy Choudury and will be\n  published by IntechOpen, London, UK. The chapter is 74 pages long and it\n  contains 32 tables and 62 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter presents a comparative study of the three portfolio optimization\nmethods, MVP, HRP, and HERC, on the Indian stock market, particularly focusing\non the stocks chosen from 15 sectors listed on the National Stock Exchange of\nIndia. The top stocks of each cluster are identified based on their free-float\nmarket capitalization from the report of the NSE published on July 1, 2022 (NSE\nWebsite). For each sector, three portfolios are designed on stock prices from\nJuly 1, 2019, to June 30, 2022, following three portfolio optimization\napproaches. The portfolios are tested over the period from July 1, 2022, to\nJune 30, 2023. For the evaluation of the performances of the portfolios, three\nmetrics are used. These three metrics are cumulative returns, annual\nvolatilities, and Sharpe ratios. For each sector, the portfolios that yield the\nhighest cumulative return, the lowest volatility, and the maximum Sharpe Ratio\nover the training and the test periods are identified.\n"
    },
    {
        "paper_id": 2310.14881,
        "authors": "Yuanrong Wang, Antonio Briola, Tomaso Aste",
        "title": "Topological Portfolio Selection and Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern portfolio optimization is centered around creating a low-risk\nportfolio with extensive asset diversification. Following the seminal work of\nMarkowitz, optimal asset allocation can be computed using a constrained\noptimization model based on empirical covariance. However, covariance is\ntypically estimated from historical lookback observations, and it is prone to\nnoise and may inadequately represent future market behavior. As a remedy,\ninformation filtering networks from network science can be used to mitigate the\nnoise in empirical covariance estimation, and therefore, can bring added value\nto the portfolio construction process. In this paper, we propose the use of the\nStatistically Robust Information Filtering Network (SR-IFN) which leverages the\nbootstrapping techniques to eliminate unnecessary edges during the network\nformation and enhances the network's noise reduction capability further. We\napply SR-IFN to index component stock pools in the US, UK, and China to assess\nits effectiveness. The SR-IFN network is partially disconnected with isolated\nnodes representing lesser-correlated assets, facilitating the selection of\nperipheral, diversified and higher-performing portfolios. Further optimization\nof performance can be achieved by inversely proportioning asset weights to\ntheir centrality based on the resultant network.\n"
    },
    {
        "paper_id": 2310.14973,
        "authors": "Ioannis Giagkiozis and Emilio Said",
        "title": "Reconciling Open Interest with Traded Volume in Perpetual Swaps",
        "comments": "10 pages, 2 figures, 5 tables",
        "journal-ref": "Ledger, Volume 9 (2024), 1-15",
        "doi": "10.5195/ledger.2024.325",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Perpetual swaps are derivative contracts that allow traders to speculate on,\nor hedge, the price movements of cryptocurrencies. Unlike futures contracts,\nperpetual swaps have no settlement or expiration in the traditional sense. The\nfunding rate acts as the mechanism that tethers the perpetual swap to its\nunderlying with the help of arbitrageurs. Open interest, in the context of\nperpetual swaps and derivative contracts in general, refers to the total number\nof outstanding contracts at a given point in time. It is a critical metric in\nderivatives markets as it can provide insight into market activity, sentiment\nand overall liquidity. It also provides a way to estimate a lower bound on the\ncollateral required for every cryptocurrency market on an exchange. This\nnumber, cumulated across all markets on the exchange in combination with proof\nof reserves, can be used to gauge whether the exchange in question operates\nwith unsustainable levels of leverage, which could have solvency implications.\nWe find that open interest in Bitcoin perpetual swaps is systematically\nmisquoted by some of the largest derivatives exchanges; however, the degree\nvaries, with some exchanges reporting open interest that is wholly implausible\nto others that seem to be delaying messages of forced trades, i.e.,\nliquidations. We identify these incongruities by analyzing tick-by-tick data\nfor two time periods in $2023$ by connecting directly to seven of the most\nliquid cryptocurrency derivatives exchanges.\n"
    },
    {
        "paper_id": 2310.15082,
        "authors": "Michele Vodret",
        "title": "Cognitive Energy Cost of Informed Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time irreversibility in neuronal dynamics has recently been demonstrated to\ncorrelate with various indicators of cognitive effort in living systems. Using\nLandauer's principle, which posits that time-irreversible information\nprocessing consumes energy, we establish a thermodynamically consistent measure\nof cognitive energy cost associated with belief dynamics. We utilize this\nconcept to analyze a two-armed bandit game, a standard decision-making\nframework under uncertainty, considering exploitation, finite memory, and\nconcurrent allocation to both game options or arms. Through exploitative,\nprediction-error-based belief dynamics, the decision maker incurs a cognitive\nenergy cost. Initially, we observe the rise of dissipative structures in the\nsteady state of the belief space due to time-reversal symmetry breaking at\nintermediate exploitative levels. To delve deeper into the belief dynamics, we\nliken it to the behavior of an active particle subjected to state-dependent\nnoise. This analogy enables us to relate emergent risk aversion to standard\nthermophoresis, connecting two apparently unrelated concepts. Finally, we\nnumerically compute the time irreversibility of belief dynamics in the steady\nstate, revealing a strong correlation between elevated - yet optimized -\ncognitive energy cost and optimal decision-making outcomes. This correlation\nsuggests a mechanism for the evolution of living systems towards maximally\nout-of-equilibrium structures.\n"
    },
    {
        "paper_id": 2310.15505,
        "authors": "Sukwoong Choi, William S. Moses, Neil Thompson",
        "title": "The Quantum Tortoise and the Classical Hare: A simple framework for\n  understanding which problems quantum computing will accelerate (and which it\n  will not)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum computing promises transformational gains for solving some problems,\nbut little to none for others. For anyone hoping to use quantum computers now\nor in the future, it is important to know which problems will benefit. In this\npaper, we introduce a framework for answering this question both intuitively\nand quantitatively. The underlying structure of the framework is a race between\nquantum and classical computers, where their relative strengths determine when\neach wins. While classical computers operate faster, quantum computers can\nsometimes run more efficient algorithms. Whether the speed advantage or the\nalgorithmic advantage dominates determines whether a problem will benefit from\nquantum computing or not. Our analysis reveals that many problems, particularly\nthose of small to moderate size that can be important for typical businesses,\nwill not benefit from quantum computing. Conversely, larger problems or those\nwith particularly big algorithmic gains will benefit from near-term quantum\ncomputing. Since very large algorithmic gains are rare in practice and\ntheorized to be rare even in principle, our analysis suggests that the benefits\nfrom quantum computing will flow either to users of these rare cases, or\npractitioners processing very large data.\n"
    },
    {
        "paper_id": 2310.15687,
        "authors": "Alkis Blanz and Beatriz Gaitan",
        "title": "Reducing residential emissions: carbon pricing vs. subsidizing retrofits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we compare different mitigation policies when housing\ninvestments are irreversible. We use a general equilibrium model with\nnon-homothetic preferences and an elaborate setup of the residential housing\nand energy production sector. In the first-best transition, the energy demand\nplays only a secondary role. However, this changes when optimal carbon taxes\nare not available. While providing subsidies for retrofits results in the\nlowest direct costs for households, it ultimately leads to the highest\naggregate costs and proves to be an ineffective way to decarbonize the economy.\nIn the second-best context, a phased-in carbon price outperforms the\nsubsidy-based transition.\n"
    },
    {
        "paper_id": 2310.15755,
        "authors": "Tomohiro Uchiyama",
        "title": "A necessary and sufficient condition for the existence of chaotic\n  dynamics in an overlapping generations model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study economic dynamics in a standard overlapping\ngenerations model without production. In particular, using numerical methods,\nwe obtain a necessary and sufficient condition for the existence of a\ntopological chaos. This is a new application of a recent result characterising\nthe existence of a topological chaos for a unimodal interval map by Deng, Khan,\nMitra (2022).\n"
    },
    {
        "paper_id": 2310.15933,
        "authors": "David Kr\\\"oger, Milijana Teodosic, Christian Rehtanz",
        "title": "Modeling and Contribution of Flexible Heating Systems for Transmission\n  Grid Congestion Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The large-scale integration of flexible heating systems in the European\nelectricity market leads to a substantial increase of transportation\nrequirements and consecutively grid congestions in the continental transmission\ngrid. Novel model formulations for the grid-aware operation of both individual\nsmall-scale heat pumps and large-scale power-to-heat (PtH) units located in\ndistrict heating networks are presented. The functionality of the models and\nthe contribution of flexible heating systems for transmission grid congestion\nmanagement is evaluated by running simulations for the target year 2035 for the\nGerman transmission grid. The findings show a decrease in annual conventional\nredispatch volumes and renewable energy sources (RES) curtailment resulting in\ncost savings of approximately 6 % through the integration of flexible heating\nsystems in the grid congestion management scheme. The analysis suggests that\nespecially large-scale PtH units in combination with thermal energy storages\ncan contribute significantly to the alleviation of grid congestion and foster\nRES integration.\n"
    },
    {
        "paper_id": 2310.15964,
        "authors": "Marco Caliendo, Nico Pestel, Rebecca Olthaus",
        "title": "Long-Term Employment Effects of the Minimum Wage in Germany: New Data\n  and Estimators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the long-term effects of the 2015 German minimum wage introduction\nand its subsequent increases on regional employment. Using data from two waves\nof the Structure of Earnings Survey allows us to estimate models that account\nfor changes in the minimum wage bite over time. While the introduction mainly\naffected the labour market in East Germany, the raises are also increasingly\naffecting low-wage regions in West Germany, such that around one third of\nregions have changed their (binary) treatment status over time. We apply\ndifferent specifications and extensions of the classic\ndifference-in-differences approach as well as a set of new estimators that\nenables for unbiased effect estimation with a staggered treatment adoption and\nheterogeneous treatment effects. Our results indicate a small negative effect\non dependent employment of 0.5 percent, no significant effect on employment\nsubject to social security contributions, and a significant negative effect of\nabout 2.4 percent on marginal employment until the first quarter of 2022. The\nextended specifications suggest additional effects of the minimum wage\nincreases, as well as stronger negative effects on total dependent and marginal\nemployment for those regions that were strongly affected by the minimum wage in\n2015 and 2019.\n"
    },
    {
        "paper_id": 2310.16098,
        "authors": "David W. Cohen and Bruce M. Boghosian",
        "title": "Bounding the approach to oligarchy in a variant of the yard-sale model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We present analytical results for the Gini coefficient of economic inequality\nunder the dynamics of a modified Yard-Sale Model of kinetic asset exchange. A\nvariant of the Yard-Sale Model is introduced by modifying the underlying binary\ntransaction of the classical system. It is shown that the Gini coefficient is\nmonotone under the resulting dynamics but the approach to oligarchy, as\nmeasured by the Gini index, can be bounded by a first-order differential\ninequality used in conjunction with the differential Gronwall inequality. This\nresult is in the spirit of entropy -- entropy production inequalities for\ndiffusive PDE. The asymptotics of the modified system, with a redistributive\ntax, are derived and shown to agree with the original, taxed Yard-Sale Model,\nwhich implies the modified system is as suitable for matching real wealth\ndistributions. The Gini -- Gini production inequality is shown to hold for a\nbroader class of models.\n"
    },
    {
        "paper_id": 2310.16341,
        "authors": "Ali Bai, Morteza Vahedian, Rashin Ghahreman, Hasan Piri",
        "title": "Elevating Women in the Workplace: The Dual Influence of Spiritual\n  Intelligence and Ethical Environments on Job Satisfaction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In today's rapidly evolving workplace, the dynamics of job satisfaction and\nits determinants have become a focal point of organizational studies. This\nresearch offers a comprehensive examination of the nexus between spiritual\nintelligence and job satisfaction among female employees, with particular\nemphasis on the moderating role of ethical work environments. Beginning with an\nexploration of the multifaceted nature of human needs, the study delves deep\ninto the psychological underpinnings that drive job satisfaction. It elucidates\nhow various tangible and intangible motivators, such as salary benefits and\nrecognition, play pivotal roles in shaping employee attitudes and behaviors.\nMoreover, the research spotlights the unique challenges and experiences of\nfemale employees, advocating for a more inclusive understanding of their needs.\nAn extensive review of the literature and empirical analysis culminates in the\npivotal finding that integrating spiritual intelligence and ethical\nconsiderations within organizational practices can significantly enhance job\nsatisfaction. Such a holistic approach, the paper posits, not only bolsters the\nwell-being and contentment of female employees but also augments overall\norganizational productivity, retention rates, and morale.\n"
    },
    {
        "paper_id": 2310.16424,
        "authors": "Darko Mitrovic",
        "title": "Pre-electoral coalition agreement from the Black-Scholes point of view",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A political party can be considered as a company whose value depends on the\nvoters support i.e. on the percentage of population supporting the party.\nDynamics of the support is thus as a stochastic process with a deterministic\ngrowth rate perturbed by a white noise modeled through the Wiener process. This\nis in an analogy with the option modeling where the stock price behaves\nsimilarly as the voters' support. While in the option theory we have the\nquestion of fair price of an option, the question that we ask here is what is a\nreasonable level of support that the coalition of a major party (safely above\nthe election threshold) and a minor party (under or around the election\nthreshold) should achieve in order the minor party to get one more\nrepresentative. We shall elaborate some of the conclusions in the case of\nrecent elections in Montenegro (June, 2023) which are particularly interesting\ndue to lots of political subjects entering the race.\n"
    },
    {
        "paper_id": 2310.1649,
        "authors": "Alkis Blanz",
        "title": "Climate-related Agricultural Productivity Losses through a Poverty Lens",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we analyze the long-term distributive impact of climate change\nthrough rising food prices. We use a standard incomplete markets model and\naccount for non-linear Engel curves for food consumption. For the calibration\nof our model, we rely on household data from 92 developing countries,\nrepresenting 4.5 billion people. The results indicate that the short-term and\nlong-term distributive impact of climate change differs. Including general\nequilibrium effects change the welfare outcome especially for the poorest\nquintile. In the presence of idiosyncratic risk, higher food prices increase\nprecautionary savings, which through general equilibrium affect labor income of\nall agents. Furthermore, this paper studies the impact on inequality for\ndifferent allocations of productivity losses across sectors. When climate\nimpacts affects total factor productivity in both sectors of the economy,\nclimate impacts increase also wealth inequality.\n"
    },
    {
        "paper_id": 2310.16703,
        "authors": "Kentaro Hoshisashi, Carolyn E. Phelan, Paolo Barucca",
        "title": "No-Arbitrage Deep Calibration for Volatility Smile and Skewness",
        "comments": "25 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility smile and skewness are two key properties of option prices that\nare represented by the implied volatility (IV) surface. However, IV surface\ncalibration through nonlinear interpolation is a complex problem due to several\nfactors, including limited input data, low liquidity, and noise. Additionally,\nthe calibrated surface must obey the fundamental financial principle of the\nabsence of arbitrage, which can be modeled by various differential inequalities\nover the partial derivatives of the option price with respect to the expiration\ntime and the strike price. To address these challenges, we have introduced a\nDerivative-Constrained Neural Network (DCNN), which is an enhancement of a\nmultilayer perceptron (MLP) that incorporates derivatives in the objective\nfunction. DCNN allows us to generate a smooth surface and incorporate the\nno-arbitrage condition thanks to the derivative terms in the loss function. In\nnumerical experiments, we train the model using prices generated with the SABR\nmodel to produce smile and skewness parameters. We carry out different settings\nto examine the stability of the calibrated model under different conditions.\nThe results show that DCNNs improve the interpolation of the implied volatility\nsurface with smile and skewness by integrating the computation of the\nderivatives, which are necessary and sufficient no-arbitrage conditions. The\ndeveloped algorithm also offers practitioners an effective tool for\nunderstanding expected market dynamics and managing risk associated with\nvolatility smile and skewness.\n"
    },
    {
        "paper_id": 2310.16841,
        "authors": "Yi Jiang, Shohei Shimizu",
        "title": "Linkages among the Foreign Exchange, Stock, and Bond Markets in Japan\n  and the United States",
        "comments": "Causal Analysis Workshop Series (CAWS) 2023, 18 pages, 7 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While economic theory explains the linkages among the financial markets of\ndifferent countries, empirical studies mainly verify the linkages through\nGranger causality, without considering latent variables or instantaneous\neffects. Their findings are inconsistent regarding the existence of causal\nlinkages among financial markets, which might be attributed to differences in\nthe focused markets, data periods, and methods applied. Our study adopts causal\ndiscovery methods including VAR-LiNGAM and LPCMCI with domain knowledge to\nexplore the linkages among financial markets in Japan and the United States\n(US) for the post Covid-19 pandemic period under divergent monetary policy\ndirections. The VAR-LiNGAM results reveal that the previous day's US market\ninfluences the following day's Japanese market for both stocks and bonds, and\nthe bond markets of the previous day impact the following day's foreign\nexchange (FX) market directly and the following day's Japanese stock market\nindirectly. The LPCMCI results indicate the existence of potential latent\nconfounders. Our results demonstrate that VAR-LiNGAM uniquely identifies the\ndirected acyclic graph (DAG), and thus provides informative insight into the\ncausal relationship when the assumptions are considered valid. Our study\ncontributes to a better understanding of the linkages among financial markets\nin the analyzed data period by supporting the existence of linkages between\nJapan and the US for the same financial markets and among FX, stock, and bond\nmarkets, thus highlighting the importance of leveraging causal discovery\nmethods in the financial domain.\n"
    },
    {
        "paper_id": 2310.16845,
        "authors": "Veli Safak",
        "title": "Dual-Class Stocks: Can They Serve as Effective Predictors?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Kardemir Karabuk Iron Steel Industry Trade & Co. Inc., ranked as the 24th\nlargest industrial company in Turkey, offers three distinct stocks listed on\nthe Borsa Istanbul: KRDMA, KRDMB, and KRDMD. These stocks, sharing the sole\ndifference in voting power, have exhibited significant price divergence over an\nextended period. This paper conducts an in-depth analysis of the divergence\npatterns observed in these three stock prices from January 2001 to July 2023.\nAdditionally, it introduces an innovative training set selection rule tailored\nfor LSTM models, incorporating a rolling training set, and demonstrates its\nsignificant predictive superiority over the conventional use of LSTM models\nwith large training sets. Despite their strong correlation, the study found no\ncompelling evidence supporting the efficiency of dual-class stocks as\npredictors of each other's performance.\n"
    },
    {
        "paper_id": 2310.16849,
        "authors": "Yun-Shi Dai, Ngoc Quang Anh Huynh, Qing-Huan Zheng, Wei-Xing Zhou",
        "title": "Correlation structure analysis of the global agricultural futures market",
        "comments": "19 pages, 7 figures",
        "journal-ref": "Research in International Business and Finance 61, 101677 (2022)",
        "doi": "10.1016/j.ribaf.2022.101677",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper adopts the random matrix theory (RMT) to analyze the correlation\nstructure of the global agricultural futures market from 2000 to 2020. It is\nfound that the distribution of correlation coefficients is asymmetric and right\nskewed, and many eigenvalues of the correlation matrix deviate from the RMT\nprediction. The largest eigenvalue reflects a collective market effect common\nto all agricultural futures, the other largest deviating eigenvalues can be\nimplemented to identify futures groups, and there are modular structures based\non regional properties or agricultural commodities among the significant\nparticipants of their corresponding eigenvectors. Except for the smallest\neigenvalue, other smallest deviating eigenvalues represent the agricultural\nfutures pairs with highest correlations. This paper can be of reference and\nsignificance for using agricultural futures to manage risk and optimize asset\nallocation.\n"
    },
    {
        "paper_id": 2310.1685,
        "authors": "Wei-Xing Zhou, Yun-Shi Dai, Kiet Tuan Duong, Peng-Fei Dai",
        "title": "The impact of the Russia-Ukraine conflict on the extreme risk spillovers\n  between agricultural futures and spots",
        "comments": "35 pages, 2 figures",
        "journal-ref": "Journal of Economic Behavior & Organization 217, 91-111 (2024)",
        "doi": "10.1016/j.jebo.2023.11.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ongoing Russia-Ukraine conflict between two major agricultural powers has\nposed significant threats and challenges to the global food system and world\nfood security. Focusing on the impact of the conflict on the global\nagricultural market, we propose a new analytical framework for tail dependence,\nand combine the Copula-CoVaR method with the ARMA-GARCH-skewed Student-t model\nto examine the tail dependence structure and extreme risk spillover between\nagricultural futures and spots over the pre- and post-outbreak periods. Our\nresults indicate that the tail dependence structures in the futures-spot\nmarkets of soybean, maize, wheat, and rice have all reacted to the\nRussia-Ukraine conflict. Furthermore, the outbreak of the conflict has\nintensified risks of the four agricultural markets in varying degrees, with the\nwheat market being affected the most. Additionally, all the agricultural\nfutures markets exhibit significant downside and upside risk spillovers to\ntheir corresponding spot markets before and after the outbreak of the conflict,\nwhereas the strengths of these extreme risk spillover effects demonstrate\nsignificant asymmetries at the directional (downside versus upside) and\ntemporal (pre-outbreak versus post-outbreak) levels.\n"
    },
    {
        "paper_id": 2310.16855,
        "authors": "Ryan Chipwanya",
        "title": "Stock Market Directional Bias Prediction Using ML Algorithms",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market has been established since the 13th century, but in the\ncurrent epoch of time, it is substantially more practicable to anticipate the\nstock market than it was at any other point in time due to the tools and data\nthat are available for both traditional and algorithmic trading. There are many\ndifferent machine learning models that can do time-series forecasting in the\ncontext of machine learning. These models can be used to anticipate the future\nprices of assets and/or the directional bias of assets. In this study, we\nexamine and contrast the effectiveness of three different machine learning\nalgorithms, namely, logistic regression, decision tree, and random forest to\nforecast the movement of the assets traded on the Japanese stock market. In\naddition, the models are compared to a feed forward deep neural network, and it\nis found that all of the models consistently reach above 50% in directional\nbias forecasting for the stock market. The results of our study contribute to a\nbetter understanding of the complexity involved in stock market forecasting and\ngive insight on the possible role that machine learning could play in this\ncontext.\n"
    },
    {
        "paper_id": 2310.16927,
        "authors": "Oytun Ha\\c{c}ar{\\i}z, Torsten Kleinow and Angus S. Macdonald",
        "title": "On Technical Bases and Surplus in Life Insurance",
        "comments": "33 pages, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We revisit surplus on general life insurance contracts, represented by Markov\nmodels. We classify technical bases in terms of boundary conditions in Thiele's\nequation(s), allowing more general regulations than Scandinavian-style\n`first-order/second-order' regimes, and replacing the traditional retrospective\npolicy value. We propose a `canonical' model with three technical bases\n(premium, valuation, accumulation) and show how each pair of bases defines\npremium loadings and surplus. Along with a `true' or `real-world' experience\nbasis, this expands fundamental results of Ramlau-Hansen (1988a). We conclude\nwith two applications: lapse-supported business; and the\nretrospectively-oriented regime proposed by M{\\o}ller & Steffensen (2007).\n"
    },
    {
        "paper_id": 2310.17423,
        "authors": "Max Mendel",
        "title": "The Newtonian Mechanics of Demand",
        "comments": "28 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic engineering is a new field wherein economic systems are modelled in\nthe same manner as traditional mechanical and electrical engineering systems.\nIn this paper, we use Newton's theory of motion as the basis for the theory of\ndemand; thereby establishing a theoretical foundation for economic engineering.\nWe follow Newton's original development, as set forth in the Principia, to\ndetermine economic analogs to his three laws of motion. The pivotal result is\nan operational definition for an economic force, i.e. a want or a desire, in\nterms of a price adjustment. With this, we model the price effects of scarcity\nand trade friction in analogy with the models for the spring and damping force.\nIn turn, we define economic benefits and surplus as analogous to the\ndefinitions of mechanical work and energy. These are then used to interpret the\nvarious types of economic equilibrium considered by economists from a\nmechanical perspective. The effectiveness of the analogy is illustrated by\napplying it to modelling the price and inventory dynamics of various economic\nagents -- including consumers, dealers, holders, spot and futures traders --\nusing linear-time invariant systems theory.\n"
    },
    {
        "paper_id": 2310.17721,
        "authors": "Alex Kim, Maximilian Muhn, Valeri Nikolaev",
        "title": "From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.\n"
    },
    {
        "paper_id": 2310.18033,
        "authors": "Pelle Nelissen",
        "title": "An Empirical Analysis of Participatory Budgeting in Amsterdam",
        "comments": "24 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using data from 35 Participatory Budgeting instances in Amsterdam, we\nempirically compare two different Participatory Budgeting rules: the greedy\ncost welfare rule and the Method of Equal Shares. We quantify how proportional,\nequal and fair the rules are and conclude that, for a small price in total\nvoter satisfaction, the Method of Equal Shares performs better on all notions\nof fairness studied. We further provide a popular and a visual explanation of\nthe Method of Equal Shares.\n"
    },
    {
        "paper_id": 2310.18052,
        "authors": "Shiro Armstrong and Danny Quah",
        "title": "Economics for the Global Economic Order: The Tragedy of Epic Fail\n  Equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper casts within a unified economic framework some key challenges for\nthe global economic order: de-globalization; the rising impracticability of\nglobal cooperation; and the increasingly confrontational nature of Great Power\ncompetition. In these, economics has been weaponised in the service of national\ninterest. This need be no bad thing. History provides examples where greater\nopenness and freer trade emerge from nations seeking only to advance their own\nself-interests. But the cases described in the paper provide mixed signals. We\nfind that some developments do draw on a growing zero-sum perception to\neconomic and political engagement. That zero-sum explanation alone, however, is\ncrucially inadequate. Self-serving nations, even when believing the world\nzero-sum, have under certain circumstances produced outcomes that have\nbenefited all. In other circumstances, perfectly-predicted losses have instead\nresulted on all sides. Such lose-lose outcomes -- epic fail equilibria --\ngeneralize the Prisoner's Dilemma game and are strictly worse than zero-sum. In\nour analysis, Third Nations -- those not frontline in Great Power rivalry --\ncan serve an essential role in averting epic fail outcomes. The policy\nimplication is that Third Nations need to provide platforms that will gently\nand unobtrusively nudge Great Powers away from epic-fail equilibria and towards\ninadvertent cooperation.\n"
    },
    {
        "paper_id": 2310.18638,
        "authors": "Andrea Nocera, M. Hashem Pesaran",
        "title": "Causal effects of the Fed's large-scale asset purchases on firms'\n  capital structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate the short- and long-term impacts of the Federal Reserve's\nlarge-scale asset purchases (LSAPs) on non-financial firms' capital structure\nusing a threshold panel ARDL model. To isolate the effects of LSAPs from other\nmacroeconomic conditions, we interact firm- and industry-specific indicators of\ndebt capacity with measures of LSAPs. We find that LSAPs facilitated firms'\naccess to external financing, with both Treasury and MBS purchases having\npositive effects. Our model also allows us to estimate the time profile of the\neffects of LSAPs on firm leverage providing robust evidence that they are\nlong-lasting. These effects have a half-life of 4-5 quarters and a mean lag\nlength of about six quarters. Nevertheless, the magnitudes are small,\nsuggesting that LSAPs have contributed only marginally to the rise in U.S.\ncorporate debt ratios of the past decade.\n"
    },
    {
        "paper_id": 2310.18658,
        "authors": "Weihuan Huang",
        "title": "Estimating Systemic Risk within Financial Networks: A Two-Step\n  Nonparametric Method",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  CoVaR (conditional value-at-risk) is a crucial measure for assessing\nfinancial systemic risk, which is defined as a conditional quantile of a random\nvariable, conditioned on other random variables reaching specific quantiles. It\nenables the measurement of risk associated with a particular node in financial\nnetworks, taking into account the simultaneous influence of risks from multiple\ncorrelated nodes. However, estimating CoVaR presents challenges due to the\nunobservability of the multivariate-quantiles condition. To address the\nchallenges, we propose a two-step nonparametric estimation approach based on\nMonte-Carlo simulation data. In the first step, we estimate the unobservable\nmultivariate-quantiles using order statistics. In the second step, we employ a\nkernel method to estimate the conditional quantile conditional on the order\nstatistics. We establish the consistency and asymptotic normality of the\ntwo-step estimator, along with a bandwidth selection method. The results\ndemonstrate that, under a mild restriction on the bandwidth, the estimation\nerror arising from the first step can be ignored. Consequently, the asymptotic\nresults depend solely on the estimation error of the second step, as if the\nmultivariate-quantiles in the condition were observable. Numerical experiments\ndemonstrate the favorable performance of the two-step estimator.\n"
    },
    {
        "paper_id": 2310.18755,
        "authors": "Kang Gao, Stephen Weston, Perukrishnen Vytelingum, Namid R. Stillman,\n  Wayne Luk, Ce Guo",
        "title": "Deeper Hedging: A New Agent-based Model for Effective Deep Hedging",
        "comments": "Accepted in the 4th ACM International Conference on AI in Finance\n  (ICAIF'23)",
        "journal-ref": null,
        "doi": "10.1145/3604237.3626913",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose the Chiarella-Heston model, a new agent-based model for improving\nthe effectiveness of deep hedging strategies. This model includes momentum\ntraders, fundamental traders, and volatility traders. The volatility traders\nparticipate in the market by innovatively following a Heston-style volatility\nsignal. The proposed model generalises both the extended Chiarella model and\nthe Heston stochastic volatility model, and is calibrated to reproduce as many\nempirical stylized facts as possible. According to the stylised facts distance\nmetric, the proposed model is able to reproduce more realistic financial time\nseries than three baseline models: the extended Chiarella model, the Heston\nmodel, and the Geometric Brownian Motion. The proposed model is further\nvalidated by the Generalized Subtracted L-divergence metric. With the proposed\nChiarella-Heston model, we generate a training dataset to train a deep hedging\nagent for optimal hedging strategies under various transaction cost levels. The\ndeep hedging agent employs the Deep Deterministic Policy Gradient algorithm and\nis trained to maximize profits and minimize risks. Our testing results reveal\nthat the deep hedging agent, trained with data generated by our proposed model,\noutperforms the baseline in most transaction cost levels. Furthermore, the\ntesting process, which is conducted using empirical data, demonstrates the\neffective performance of the trained deep hedging agent in a realistic trading\nenvironment.\n"
    },
    {
        "paper_id": 2310.18835,
        "authors": "Fulin Guo",
        "title": "Experience-weighted attraction learning in network coordination games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the action dynamics of network coordination games with\nbounded-rational agents. I apply the experience-weighted attraction (EWA) model\nto the analysis as the EWA model has several free parameters that can capture\ndifferent aspects of agents' behavioural features. I show that the set of\npossible long-term action patterns can be largely different when the\nbehavioural parameters vary, ranging from a unique possibility in which all\nagents favour the risk-dominant option to some set of outcomes richer than the\ncollection of Nash equilibria. Monotonicity and non-monotonicity in the\nrelationship between the number of possible long-term action profiles and the\nbehavioural parameters are explored. I also study the question of influential\nagents in terms of whose initial predispositions are important to the actions\nof the whole network. The importance of agents can be represented by a left\neigenvector of a Jacobian matrix provided that agents' initial attractions are\nclose to some neutral level. Numerical calculations examine the predictive\npower of the eigenvector for the long-run action profile and how agents'\ninfluences are impacted by their behavioural features and network positions.\n"
    },
    {
        "paper_id": 2310.18903,
        "authors": "Yan-Hong Yang and Ying-Lin Liu and Ying-Hui Shao",
        "title": "Visibility graph analysis of crude oil futures markets: Insights from\n  the COVID-19 pandemic and Russia-Ukraine conflict",
        "comments": "16 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Drawing inspiration from the significant impact of the ongoing Russia-Ukraine\nconflict and the recent COVID-19 pandemic on global financial markets, this\nstudy conducts a thorough analysis of three key crude oil futures markets: WTI,\nBrent, and Shanghai (SC). Employing the visibility graph (VG) methodology, we\nexamine both static and dynamic characteristics using daily and high-frequency\ndata. We identified a clear power-law decay in most VG degree distributions and\nhighlighted the pronounced clustering tendencies within crude oil futures VGs.\nOur results also confirm an inverse correlation between clustering coefficient\nand node degree and further reveal that all VGs not only adhere to the\nsmall-world property but also exhibit intricate assortative mixing. Through the\ntime-varying characteristics of VGs, we found that WTI and Brent demonstrate\naligned behavior, while the SC market, with its unique trading mechanics,\ndeviates. The 5-minute VGs' assortativity coefficient provides a deeper\nunderstanding of these markets' reactions to the pandemic and geopolitical\nevents. Furthermore, the differential responses during the COVID-19 and\nRussia-Ukraine conflict underline the unique sensitivities of each market to\nglobal disruptions. Overall, this research offers profound insights into the\nstructure, dynamics, and adaptability of these essential commodities markets in\nthe face of worldwide challenges.\n"
    },
    {
        "paper_id": 2310.18989,
        "authors": "Marjan Petreski, Stefan Tanevski, Irena Stojmenovska",
        "title": "Employment, labor productivity and environmental sustainability:\n  Firm-level evidence from transition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines how investment in environmentally sustainable practices\nimpacts employment and labor productivity growth of firms in transition\neconomies. The study considers labor skill composition and geographical\ndifferences, shedding light on sustainability dynamics. The empirical analysis\nrelies on the World Bank-s Enterprise Survey 2019 for 24 transition economies,\nconstructing an environmental sustainability index from various indicators\nthrough a Principal Components Analysis. To address endogeneity, a battery of\nfixed effects and instrumental variables are employed. Results reveal the\nrelevance of environmental sustainability for both employment and labor\nproductivity growth. However, the significance diminishes when addressing\nendogeneity comprehensively, alluding that any relation between environmentally\nsustainable practices and jobs growth is more complex and needs time to work.\nThe decelerating job-creation effect of sustainability investments is however\nconfirmed for the high-skill firms, while low-skill firms benefit from labor\nproductivity gains spurred by such investment. Geographically, Central Europe\nsees more pronounced labor productivity impacts, possibly due to its higher\ndevelopment and sustainability-awareness levels as compared to Southeast Europe\nand the Commonwealth of Independent States.\n"
    },
    {
        "paper_id": 2310.19008,
        "authors": "Luyi Qiu",
        "title": "Coupling Coordinated Development among Digital Economy, Regional\n  Innovation and Talent Employment A case study of Hangzhou Metropolitan\n  Circle, China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Coordination development across various subsystems, particularly economic,\nsocial, cultural, and human resources subsystems, is a key aspect of urban\nsustainability that has a direct impact on the quality of urbanization.\nHangzhou Metropolitan Circle composing Hangzhou, Huzhou, Jiaxing, Shaoxing, was\nthe first metropolitan circle approved by National Development and Reform\nCommission (NDRC) as a demonstration of economic transformation. To evaluate\nthe coupling degree of the four cities and to analyze the coordinative\ndevelopment in the three systems (Digital Economy System, Regional Innovation\nSystem, and Talent Employment System), panel data of these four cities during\nthe period 2015-2022 were collected. The development level of these three\nsystems were evaluated using standard deviation and comprehensive development\nindex evaluation. The results are as follows: (1) the coupling coordination\ndegree of the four cities in Hangzhou Metropolitan Circle has significant\nregional differences, with Hangzhou being a leader while Huzhou, Jiaxing,\nShaoxing have shown steady but slow progress in the coupling development of the\nthree systems; and (2) the development of digital economy and talent employment\nare the breakthrough points for construction in Huzhou, Jiaxing, Shaoxing.\nRelated suggestions are made based on the coupling coordination results of the\nHangzhou Metropolitan Circle.\n"
    },
    {
        "paper_id": 2310.19023,
        "authors": "Marcos Escobar-Anel, Yevhen Havrylenko, Rudi Zagst",
        "title": "Optimal fees in hedge funds with first-loss compensation",
        "comments": "32 pages, 17 figures",
        "journal-ref": "Journal of Banking & Finance, Volume 118, 2020, 105884",
        "doi": "10.1016/j.jbankfin.2020.105884",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Hedge fund managers with the first-loss scheme charge a management fee, a\nperformance fee and guarantee to cover a certain amount of investors' potential\nlosses. We study how parties can choose a mutually preferred first-loss scheme\nin a hedge fund with the manager's first-loss deposit and investors' assets\nsegregated. For that, we solve the manager's non-concave utility maximization\nproblem, calculate Pareto optimal first-loss schemes and maximize a decision\ncriterion on this set. The traditional 2% management and 20% performance fees\nare found to be not Pareto optimal, neither are common first-loss fee\narrangements. The preferred first-loss coverage guarantee is increasing as the\ninvestor's risk-aversion or the interest rate increases. It decreases as the\nmanager's risk-aversion or the market price of risk increases. The more risk\naverse the investor or the higher the interest rate, the larger is the\npreferred performance fee. The preferred fee schemes significantly decrease the\nfund's volatility.\n"
    },
    {
        "paper_id": 2310.19036,
        "authors": "Fanchao Liao, Jaap Vleugel, Gustav B\\\"osehans, Dilum Dissanayake, Neil\n  Thorpe, Margaret Bell, Bart van Arem, Gon\\c{c}alo Homem de Almeida Correia",
        "title": "Mode substitution induced by electric mobility hubs: results from\n  Amsterdam",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Electric mobility hubs (eHUBS) are locations where multiple shared electric\nmodes including electric cars and e-bikes are available. To assess their\npotential to reduce private car use, it is important to investigate to what\nextent people would switch to eHUBS modes after their introduction. Moreover,\npeople may adapt their behaviour differently depending on their current travel\nmode. This study is based on stated preference data collected in Amsterdam. We\nanalysed the data using mixed logit models. We found users of different modes\nnot only have a varied general preference for different shared modes, but also\nhave different sensitivity for attributes such as travel time and cost.\nCompared to car users, public transport users are more likely to switch towards\nthe eHUBS modes. People who bike and walk have strong inertia, but the\npercentage choosing eHUBS modes doubles when the trip distance is longer (5 or\n10 km).\n"
    },
    {
        "paper_id": 2310.191,
        "authors": "L\\'aszl\\'o Csat\\'o, L\\'aszl\\'o Marcell Kiss, Zsombor Sz\\'adoczki",
        "title": "The allocation of FIFA World Cup slots based on the ranking of\n  confederations",
        "comments": "21 pages, 2 figures, 6 tables",
        "journal-ref": "Annals of Operations Research, 2024, forthcoming",
        "doi": "10.1007/s10479-024-06091-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Qualifications for several world championships in sports are organised such\nthat distinct sets of teams play in their own tournament for a predetermined\nnumber of slots. Inspired by a recent work studying the problem with the tools\nfrom the literature on fair allocation, this paper provides an alternative\napproach based on historical matches between these sets of teams. We focus on\nthe FIFA World Cup due to the existence of an official rating system and its\nrecent expansion to 48 teams, as well as to allow for a comparison with the\nalready suggested allocations. Our proposal extends the methodology of the FIFA\nWorld Ranking to compare the strengths of five confederations. Various\nallocations are presented depending on the length of the sample, the set of\nteams considered, as well as the frequency of rating updates. The results show\nthat more European and South American teams should play in the FIFA World Cup.\nThe ranking of continents by the number of deserved slots is different from the\nranking implied by FIFA policy. We recommend allocating at least some slots\ntransparently, based on historical performances, similar to the access list of\nthe UEFA Champions League.\n"
    },
    {
        "paper_id": 2310.19498,
        "authors": "Hanxin Zhao",
        "title": "Green ammonia supply chain and associated market structure: an analysis\n  based on transaction cost economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Green ammonia is poised to be a key part in the hydrogen economy. This paper\ndiscusses green ammonia supply chains from a higher-level industry perspective\nwith a focus on market structures. The architecture of upstream and downstream\nsupply chains are explored. Potential ways to accelerate market emergence are\ndiscussed. Market structure is explored based on transaction cost economics and\nlessons from the oil and gas industry. Three market structure prototypes are\ndeveloped for different phases. In the infancy, a highly vertically integrated\nstructure is proposed to reduce risks and ensure capital recovery. A\nrestructuring towards a disintegrated structure is necessary in the next stage\nto improve the efficiency. In the late stage, a competitive structure\ncharacterized by a separation between asset ownership and production activities\nand further development of short-term and spot markets is proposed towards a\nmarket-driven industry. Finally, a multi-linear regression model is developed\nto evaluate the developed structures using a case in the gas industry. Results\nindicate that high asset specificity and uncertainty and low frequency lead to\na more disintegrated market structure, and vice versa, thus supporting the\nstructures designed. We assume the findings and results contribute to\ndeveloping green ammonia supply chains and the hydrogen economy.\n"
    },
    {
        "paper_id": 2310.19552,
        "authors": "Roger J. A. Laeven, Emanuela Rosazza Gianin, Marco Zullino",
        "title": "Law-Invariant Return and Star-Shaped Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents novel characterization results for classes of\nlaw-invariant star-shaped functionals. We begin by establishing\ncharacterizations for positively homogeneous and star-shaped functionals that\nexhibit second- or convex-order stochastic dominance consistency. Building on\nthese characterizations, we proceed to derive Kusuoka-type representations for\nthese functionals, shedding light on their mathematical structure and intimate\nconnections to Value-at-Risk and Expected Shortfall. Furthermore, we offer\nrepresentations of general law-invariant star-shaped functionals as\nrobustifications of Value-at-Risk. Notably, our results are versatile,\naccommodating settings that may, or may not, involve monotonicity and/or\ncash-additivity. All of these characterizations are developed within a general\nlocally convex topological space of random variables, ensuring the broad\napplicability of our results in various financial, insurance and probabilistic\ncontexts.\n"
    },
    {
        "paper_id": 2310.19747,
        "authors": "Pawe{\\l} Szyd{\\l}o, Marcin W\\k{a}torek, Jaros{\\l}aw Kwapie\\'n,\n  Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Characteristics of price related fluctuations in Non-Fungible Token\n  (NFT) market",
        "comments": null,
        "journal-ref": "Chaos 34, 013108 (2024)",
        "doi": "10.1063/5.0185306",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A non-fungible token (NFT) market is a new trading invention based on the\nblockchain technology which parallels the cryptocurrency market. In the present\nwork we study capitalization, floor price, the number of transactions, the\ninter-transaction times, and the transaction volume value of a few selected\npopular token collections. The results show that the fluctuations of all these\nquantities are characterized by heavy-tailed probability distribution\nfunctions, in most cases well described by the stretched exponentials, with a\ntrace of power-law scaling at times, long-range memory, and in several cases\neven the fractal organization of fluctuations, mostly restricted to the larger\nfluctuations, however. We conclude that the NFT market - even though young and\ngoverned by a somewhat different mechanisms of trading - shares several\nstatistical properties with the regular financial markets. However, some\ndifferences are visible in the specific quantitative indicators.\n"
    },
    {
        "paper_id": 2310.19992,
        "authors": "Peter Reinhard Hansen and Yiyao Luo",
        "title": "Robust Estimation of Realized Correlation: New Insight about Intraday\n  Fluctuations in Market Betas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Time-varying volatility is an inherent feature of most economic time-series,\nwhich causes standard correlation estimators to be inconsistent. The quadrant\ncorrelation estimator is consistent but very inefficient. We propose a novel\nsubsampled quadrant estimator that improves efficiency while preserving\nconsistency and robustness. This estimator is particularly well-suited for\nhigh-frequency financial data and we apply it to a large panel of US stocks.\nOur empirical analysis sheds new light on intra-day fluctuations in market\nbetas by decomposing them into time-varying correlations and relative\nvolatility changes. Our results show that intraday variation in betas is\nprimarily driven by intraday variation in correlations.\n"
    },
    {
        "paper_id": 2310.20028,
        "authors": "Maurizio La Rocca, Tiziana La Rocca, Francesco Fasano and Javier\n  Sanchez-Vidal",
        "title": "From the Top Down: Does Corruption Affect Performance?",
        "comments": "7 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Corruption, fraud, and unethical activities have emerged as significant\nobstacles to global economic, political, and social progress. Although many\nempirical studies have focused on country-level corruption metrics, this study\nis the first to utilize a substantial international dataset to assess the\neffects of illicit and unethical managerial practices on firm performance.\nEmploying cross-sectional data, this research examines the influence of\ncorruption on corporate outcomes. Our definition of corruption evaluates the\ndegree to which managers engage in mismanagement, misconduct, or corrupt\nactivities. The repercussions for corporate governance, especially concerning\nthe process of appointing managers, are both crucial and strategic.\n"
    },
    {
        "paper_id": 2311.00384,
        "authors": "Poompak Kusawat, Nopadol Rompho",
        "title": "Impact of Investing Characteristics on Financial Performance of\n  Individual Investors: An Exploratory Study",
        "comments": null,
        "journal-ref": "IEEE International Conference on Industrial Engineering and\n  Engineering Management (IEEM), 2019, pp. 654-658",
        "doi": "10.1109/IEEM44572.2019.8978725",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This exploratory study examines which investing characteristics determine\nsuccess in an equity market. Based on data from 403 respondents, exploratory\nfactor analysis results in 13 factors: middle/long time horizon, qualitative\nanalyst, open-minded/disciplined, organized, emotional stability, na\\\"ive,\ngrowth stock, concentrated portfolio, contrarian, value stock, globalized,\nintrinsic value, and price-independent. Multiple linear regression of\nindividual investors' excess return on these factors show statistically\nsignificant relationship. These results deepen our knowledge on what sort of\ninvesting characteristics are required to survive in equity markets.\n"
    },
    {
        "paper_id": 2311.00777,
        "authors": "Jamie Fogel, Bernardo Modenesi",
        "title": "What is a Labor Market? Classifying Workers and Jobs Using Network\n  Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a new data-driven approach to characterizing latent\nworker skill and job task heterogeneity by applying an empirical tool from\nnetwork theory to large-scale Brazilian administrative data on worker--job\nmatching. We microfound this tool using a standard equilibrium model of workers\nmatching with jobs according to comparative advantage. Our classifications\nidentify important dimensions of worker and job heterogeneity that standard\nclassifications based on occupations and sectors miss. The equilibrium model\nbased on our classifications more accurately predicts wage changes in response\nto the 2016 Olympics than a model based on occupations and sectors.\nAdditionally, for a large simulated shock to demand for workers, we show that\nreduced form estimates of the effects of labor market shock exposure on\nworkers' earnings are nearly 4 times larger when workers and jobs are\nclassified using our classifications as opposed to occupations and sectors.\n"
    },
    {
        "paper_id": 2311.00825,
        "authors": "Eric Ghysels, Jack Morgan, and Hamed Mohammadbagherpoor",
        "title": "Quantum Computational Algorithms for Derivative Pricing and Credit Risk\n  in a Regime Switching Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum computers are not yet up to the task of providing computational\nadvantages for practical stochastic diffusion models commonly used by financial\nanalysts. In this paper we introduce a class of stochastic processes that are\nboth realistic in terms of mimicking financial market risks as well as more\namenable to potential quantum computational advantages. The type of models we\nstudy are based on a regime switching volatility model driven by a Markov chain\nwith observable states. The basic model features a Geometric Brownian Motion\nwith drift and volatility parameters determined by the finite states of a\nMarkov chain. We study algorithms to estimate credit risk and option pricing on\na gate-based quantum computer. These models bring us closer to realistic market\nsettings, and therefore quantum computing closer the realm of practical\napplications.\n"
    },
    {
        "paper_id": 2311.00832,
        "authors": "Minglian Lin, Indranil SenGupta, William Wilson",
        "title": "Estimation of VaR with jump process: application in corn and soybean\n  markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value at Risk (VaR) is a quantitative measure used to evaluate the risk\nlinked to the potential loss of investment or capital. Estimation of the VaR\nentails the quantification of prospective losses in a portfolio of investments,\nusing a certain likelihood, under normal market conditions within a specific\ntime period. The objective of this paper is to construct a model and estimate\nthe VaR for a diversified portfolio consisting of multiple cash commodity\npositions driven by standard Brownian motions and jump processes. Subsequently,\na thorough analytical estimation of the VaR is conducted for the proposed\nmodel. The results are then applied to two distinct commodities -- corn and\nsoybean -- enabling a comprehensive comparison of the VaR values in the\npresence and absence of jumps.\n"
    },
    {
        "paper_id": 2311.00964,
        "authors": "Chengyao Wen, Yin Lou",
        "title": "On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for\n  Fintech Applications",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3637528.3671521",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rules are widely used in Fintech institutions to make fraud prevention\ndecisions, since rules are highly interpretable thanks to their intuitive\nif-then structure. In practice, a two-stage framework of fraud prevention\ndecision rule set mining is usually employed in large Fintech institutions;\nStage 1 generates a potentially large pool of rules and Stage 2 aims to produce\na refined rule subset according to some criteria (typically based on precision\nand recall). This paper focuses on improving the flexibility and efficacy of\nthis two-stage framework, and is concerned with finding high-quality rule\nsubsets in a bi-objective space (such as precision and recall). To this end, we\nfirst introduce a novel algorithm called SpectralRules that directly generates\na compact pool of rules in Stage 1 with high diversity. We empirically find\nsuch diversity improves the quality of the final rule subset. In addition, we\nintroduce an intermediate stage between Stage 1 and 2 that adopts the concept\nof Pareto optimality and aims to find a set of non-dominated rule subsets,\nwhich constitutes a Pareto front. This intermediate stage greatly simplifies\nthe selection criteria and increases the flexibility of Stage 2. For this\nintermediate stage, we propose a heuristic-based framework called PORS and we\nidentify that the core of PORS is the problem of solution selection on the\nfront (SSF). We provide a systematic categorization of the SSF problem and a\nthorough empirical evaluation of various SSF methods on both public and\nproprietary datasets. On two real application scenarios within Alipay, we\ndemonstrate the advantages of our proposed methodology over existing work.\n"
    },
    {
        "paper_id": 2311.01086,
        "authors": "Miryana Grigorova, Marie-Claire Quenez (LPSM, UPCit\\'e), Yuan Peng",
        "title": "Non-linear non-zero-sum Dynkin games with Bermudan strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a non-zero-sum game with two players, where each of\nthe players plays what we call Bermudan strategies and optimizes a general\nnon-linear assessment functional of the pay-off. By using a recursive\nconstruction, we show that the game has a Nash equilibrium point.\n"
    },
    {
        "paper_id": 2311.01206,
        "authors": "Yong Bian, Xiqian Wang, Qin Zhang",
        "title": "How Does China's Household Portfolio Selection Vary with Financial\n  Inclusion?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Portfolio underdiversification is one of the most costly losses accumulated\nover a household's life cycle. We provide new evidence on the impact of\nfinancial inclusion services on households' portfolio choice and investment\nefficiency using 2015, 2017, and 2019 survey data for Chinese households. We\nhypothesize that higher financial inclusion penetration encourages households\nto participate in the financial market, leading to better portfolio\ndiversification and investment efficiency. The results of the baseline model\nare consistent with our proposed hypothesis that higher accessibility to\nfinancial inclusion encourages households to invest in risky assets and\nincreases investment efficiency. We further estimate a dynamic double machine\nlearning model to quantitatively investigate the non-linear causal effects and\ntrack the dynamic change of those effects over time. We observe that the\nmarginal effect increases over time, and those effects are more pronounced\namong low-asset, less-educated households and those located in non-rural areas,\nexcept for investment efficiency for high-asset households.\n"
    },
    {
        "paper_id": 2311.01228,
        "authors": "Giulia Di Nunno, Anton Yurchenko-Tytarenko",
        "title": "Power law in Sandwiched Volterra Volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present analytical proof demonstrating that the Sandwiched\nVolterra Volatility (SVV) model is able to reproduce the power-law behavior of\nthe at-the-money implied volatility skew, provided the correct choice of the\nVolterra kernel. To obtain this result, we assess the second-order Malliavin\ndifferentiability of the volatility process and investigate the conditions that\nlead to explosive behavior in the Malliavin derivative. As a supplementary\nresult, we also prove a general Malliavin product rule.\n"
    },
    {
        "paper_id": 2311.01268,
        "authors": "Bahman Madadi, Ary P. Silvano, Kevin McPherson, John McCarthy, Risto\n  \\\"O\\\"orni, Gon\\c{c}alo Homem de Almeida Correiaa",
        "title": "A connected and automated vehicle readiness framework to support road\n  authorities for C-ITS services",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Connected and Automated Vehicles (CAVs) can have a profound influence on\ntransport systems. However, most levels of automation and connectivity require\nsupport from the road infrastructure. Additional support such as Cooperative\nIntelligent Transport Systems (C-ITS) services can facilitate safe and\nefficient traffic, and alleviate the environmental impacts of road surface\nvehicles. However, due to the rapidly evolving technology, C-ITS service\ndeployment requirements are not always clear. Furthermore, the costs and\nbenefits of infrastructure investments are subject to tremendous uncertainty.\nThis study articulates the requirements using a structured approach to propose\na CAV-Readiness Framework (CRF). The main purpose of the CRF is allowing road\nauthorities to assess their physical and digital infrastructure readiness,\ndefine requirements for C-ITS services, and identify future development paths\nto reach higher levels of readiness to support CAVs by enabling C-ITS services.\nThe CRF is intended to guide and support road authorities' investment decisions\non infrastructure.\n"
    },
    {
        "paper_id": 2311.01542,
        "authors": "Fabio Bagarello and Biagio Bossone",
        "title": "Bank Deposits as {\\em Money Quanta}",
        "comments": "In press in Quantum Economics and Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  According to the Accounting View of Money (AVM), the money issued by\ncommercial banks in the form of demand deposits features a hybrid nature, since\ndeposits can be shown to consist of a share of deposits bearing the\ncharacteristics of debt (debt-deposits) and a share of deposits bearing the\ncharacteristics of equity (equity-deposits), in a mix that depends on factors\nthat relate to the issuing banks and the environment where they operate and\ninteract, which may change over time. Following this important finding of the\nAVM, it is only consequential to associate the hybrid nature of bank deposits\nwith the dual nature of the objects which is typical in quantum physics, and to\ninvestigate whether and how the application of quantum analytical methods and\nideas to a form of money showing dualistic features could be used to extract\nvaluable economic information.\n"
    },
    {
        "paper_id": 2311.0155,
        "authors": "Jai Vipra, Anton Korinek",
        "title": "Market Concentration Implications of Foundation Models",
        "comments": "Working Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We analyze the structure of the market for foundation models, i.e., large AI\nmodels such as those that power ChatGPT and that are adaptable to downstream\nuses, and we examine the implications for competition policy and regulation. We\nobserve that the most capable models will have a tendency towards natural\nmonopoly and may have potentially vast markets. This calls for a two-pronged\nregulatory response: (i) Antitrust authorities need to ensure the\ncontestability of the market by tackling strategic behavior, in particular by\nensuring that monopolies do not propagate vertically to downstream uses, and\n(ii) given the diminished potential for market discipline, there is a role for\nregulators to ensure that the most capable models meet sufficient quality\nstandards (including safety, privacy, non-discrimination, reliability and\ninteroperability standards) to maximally contribute to social welfare.\nRegulators should also ensure a level regulatory playing field between AI and\nnon-AI applications in all sectors of the economy. For models that are behind\nthe frontier, we expect competition to be quite intense, implying a more\nlimited role for competition policy, although a role for regulation remains.\n"
    },
    {
        "paper_id": 2311.01592,
        "authors": "Matthew J. Baker and Jonathan Conning",
        "title": "A Model of Enclosures: Coordination, Conflict, and Efficiency in the\n  Transformation of Land Property Rights",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historians and political economists have long debated the processes that led\nland in frontier regions, managed commons, and a variety of customary\nlandholding regimes to be enclosed and transformed into more exclusive forms of\nprivate property. Using the framework of aggregative games, we examine\nland-holding regimes where access to land is established via possession and\nuse, and then explore the factors that may initiate decentralized privatization\nprocesses. Factors including population density, potential for technology\nimprovement, enclosure costs, shifts in group cohesion and bargaining power, or\nthe policy and institutional environment determine the equilibrium mix of\nproperty regimes. While decentralized processes yield efficient enclosure and\ntechnological transformation in some circumstances, in others, the outcomes\nfall short of second-best. This stems from the interaction of different\nspillover effects, leading to inefficiently low rates of enclosure and\ntechnological transformation in some cases and excessive enclosure in others.\nImplementing policies to strengthen customary governance, compensate displaced\nstakeholders, or subsidize/tax enclosure can realign incentives. However,\naddressing one market failure while overlooking others can worsen outcomes. Our\nanalysis offers a unified framework for evaluating claimed mechanisms and\nprocesses across Neoclassical, neo-institutional, and Marxian interpretations\nof enclosure processes.\n"
    },
    {
        "paper_id": 2311.01692,
        "authors": "Jianming Xia",
        "title": "Benchmark Beating with the Increasing Convex Order",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this paper we model benchmark beating with the increasing convex order\n(ICX order). The mean constraint in the mean-variance theory of portfolio\nselection can be regarded as beating a constant. We then investigate the\nproblem of minimizing the variance of a portfolio with ICX order constraints,\nbased on which we also study the problem of beating-performance-variance\nefficient portfolios. The optimal and efficient portfolios are all worked out\nin closed form for complete markets.\n"
    },
    {
        "paper_id": 2311.01787,
        "authors": "Tongam Sihol Nababan and Elvis Fresly Purba",
        "title": "Labour Absorption In Manufacturing Industry In Indonesia: Anomalous And\n  Regressive Phenomena",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The manufacturing industry sector was expected to generate new employment\nopportunities and take on labour. Gradually, however, it emerged as a menace to\nthe sustenance of its workers. According to the findings of this study, 24\nmanufacturing subsectors with ISIC 2 digits in Indonesia exhibited regressive\nand abnormal patterns in the period 2012-2020. This suggests that, to a great\nextent, labour absorption has been limited and, in some cases, even shown a\ndecline. Anomalous occurrences were observed in three subsectors: ISIC 12\n(tobacco products), ISIC 26 (computer, electronic and optical products), and\nISIC 31 (furniture). In contrast, regressive phenomena were present in the\nremaining 21 ISIC subsectors. Furthermore, the manufacturing industry displayed\na negative correlation between employment and efficiency index, demonstrating\nthis anomalous and regressive phenomenon. This implies that as the efficiency\nindex of the manufacturing industry increases, the index of labour absorption\ndecreases\n"
    },
    {
        "paper_id": 2311.01901,
        "authors": "Conor B. Hamill, Raad Khraishi, Simona Gherghel, Jerrard Lawrence,\n  Salvatore Mercuri, Ramin Okhrati, Greig A. Cowan",
        "title": "Agent-based Modelling of Credit Card Promotions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interest-free promotions are a prevalent strategy employed by credit card\nlenders to attract new customers, yet the research exploring their effects on\nboth consumers and lenders remains relatively sparse. The process of selecting\nan optimal promotion strategy is intricate, involving the determination of an\ninterest-free period duration and promotion-availability window, all within the\ncontext of competing offers, fluctuating market dynamics, and complex consumer\nbehaviour. In this paper, we introduce an agent-based model that facilitates\nthe exploration of various credit card promotions under diverse market\nscenarios. Our approach, distinct from previous agent-based models,\nconcentrates on optimising promotion strategies and is calibrated using\nbenchmarks from the UK credit card market from 2019 to 2020, with agent\nproperties derived from historical distributions of the UK population from\nroughly the same period. We validate our model against stylised facts and\ntime-series data, thereby demonstrating the value of this technique for\ninvestigating pricing strategies and understanding credit card customer\nbehaviour. Our experiments reveal that, in the absence of competitor\npromotions, lender profit is maximised by an interest-free duration of\napproximately 12 months while market share is maximised by offering the longest\nduration possible. When competitors do not offer promotions, extended promotion\navailability windows yield maximum profit for lenders while also maximising\nmarket share. In the context of concurrent interest-free promotions, we\nidentify that the optimal lender strategy entails offering a more competitive\ninterest-free period and a rapid response to competing promotional offers.\nNotably, a delay of three months in responding to a rival promotion corresponds\nto a 2.4% relative decline in income.\n"
    },
    {
        "paper_id": 2311.01963,
        "authors": "Keisuke Kokubun, Yoshiaki Ino, and Kazuyoshi Ishimura",
        "title": "How to maintain compliance among host country employees who are less\n  anxious after strict government regulations are lifted: An attempt to apply\n  conservation of resources theory to the workplace amid the still-unending\n  COVID-19 pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Design/methodology/approach We compared the awareness of 813 people in Wuhan\ncity from January to March 2023 (Wuhan 2023) and 2,973 people in East and South\nChina from February to May 2020 (China 2020) using responses to questionnaires\nconducted at Japanese local subsidiaries during each period. Purpose As the\ncoronavirus pandemic becomes less terrifying than before, there is a trend in\ncountries around the world to abolish strict behavioral restrictions imposed by\ngovernments. How should overseas subsidiaries change the way they manage human\nresources in response to these system changes? To find an answer to this\nquestion, this paper examines what changes occurred in the mindset of employees\nworking at local subsidiaries after the government's strict behavioral\nrestrictions were introduced and lifted during the COVID-19 pandemic. Findings\nThe results showed that the analytical model based on conservation of resources\n(COR) theory can be applied to both China 2020 and Wuhan 2023. However, the\nrelationship between anxiety, fatigue, compliance, turnover intention, and\npsychological and social resources of employees working at local subsidiaries\nchanged after the initiation and removal of government behavioral restrictions\nduring the pandemic, indicating that managers need to adjust their human\nresource management practices in response to these changes. Originality/value\nThis is the first study that compares data after the start of government\nregulations and data after the regulations were lifted. Therefore, this\nresearch proposes a new analytical framework that companies, especially\nforeign-affiliated companies that lack local information, can refer to respond\nappropriately to disasters, which expand damage while changing its nature and\ninfluence while anticipating changes in employee awareness.\n"
    },
    {
        "paper_id": 2311.01985,
        "authors": "Michael Pinelis, David Ruppert",
        "title": "Maximizing Portfolio Predictability with Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We construct the maximally predictable portfolio (MPP) of stocks using\nmachine learning. Solving for the optimal constrained weights in the\nmulti-asset MPP gives portfolios with a high monthly coefficient of\ndetermination, given the sample covariance matrix of predicted return errors\nfrom a machine learning model. Various models for the covariance matrix are\ntested. The MPPs of S&P 500 index constituents with estimated returns from\nElastic Net, Random Forest, and Support Vector Regression models can outperform\nor underperform the index depending on the time period. Portfolios that take\nadvantage of the high predictability of the MPP's returns and employ a Kelly\ncriterion style strategy consistently outperform the benchmark.\n"
    },
    {
        "paper_id": 2311.02088,
        "authors": "Koti S. Jaddu and Paul A. Bilokon",
        "title": "Combining Deep Learning on Order Books with Reinforcement Learning for\n  Profitable Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High-frequency trading is prevalent, where automated decisions must be made\nquickly to take advantage of price imbalances and patterns in price action that\nforecast near-future movements. While many algorithms have been explored and\ntested, analytical methods fail to harness the whole nature of the market\nenvironment by focusing on a limited domain. With the evergrowing machine\nlearning field, many large-scale end-to-end studies on raw data have been\nsuccessfully employed to increase the domain scope for profitable trading but\nare very difficult to replicate. Combining deep learning on the order books\nwith reinforcement learning is one way of breaking down large-scale end-to-end\nlearning into more manageable and lightweight components for reproducibility,\nsuitable for retail trading.\n  The following work focuses on forecasting returns across multiple horizons\nusing order flow imbalance and training three temporal-difference learning\nmodels for five financial instruments to provide trading signals. The\ninstruments used are two foreign exchange pairs (GBPUSD and EURUSD), two\nindices (DE40 and FTSE100), and one commodity (XAUUSD). The performances of\nthese 15 agents are evaluated through backtesting simulation, and successful\nmodels proceed through to forward testing on a retail trading platform. The\nresults prove potential but require further minimal modifications for\nconsistently profitable trading to fully handle retail trading costs, slippage,\nand spread fluctuation.\n"
    },
    {
        "paper_id": 2311.0209,
        "authors": "Luigi Viola (1), Saeed Nordin (2), Daniel Dotta (1), Mohammad Reza\n  Hesamzadeh (2), Ross Baldick (3), Damian Flynn (4) ((1) University of\n  Campinas, Campinas-SP, Brazil, (2) KTH Royal Institute of Technology,\n  Stockholm, Sweden, (3) University of Texas at Austin, Austin, TX, USA, (4)\n  University College Dublin, Dublin, Ireland)",
        "title": "Ancillary Services in Power System Transition Toward a 100% Non-Fossil\n  Future: Market Design Challenges in the United States and Europe",
        "comments": "64 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The expansion of variable generation has driven a transition toward a 100\\%\nnon-fossil power system. New system needs are challenging system stability and\nsuggesting the need for a redesign of the ancillary service (AS) markets. This\npaper presents a comprehensive and broad review for industrial practitioners\nand academic researchers regarding the challenges and potential solutions to\naccommodate high shares of variable renewable energy (VRE) generation levels.\nWe detail the main drivers enabling the energy transition and facilitating the\nprovision of ASs. A systematic review of the United States and European AS\nmarkets is conducted. We clearly organize the main ASs in a standard taxonomy,\nidentifying current practices and initiatives to support the increasing VRE\nshare. Furthermore, we envision the future of modern AS markets, proposing\npotential solutions for some remaining fundamental technical and market design\nchallenges.\n"
    },
    {
        "paper_id": 2311.02422,
        "authors": "Ali Bai, Morteza Vahedian",
        "title": "Beyond the Screen: Safeguarding Mental Health in the Digital Workplace\n  Through Organizational Commitment and Ethical Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research explores the intricate relationship between organizational\ncommitment and nomophobia, illuminating the mediating influence of the ethical\nenvironment. Utilizing Meyer and Allen's three-component model, the study finds\na significant inverse correlation between organizational commitment and\nnomophobia, highlighting how strong organizational ties can alleviate the\nanxiety of digital disconnection. The ethical environment further emerges as a\nsignificant mediator, indicating its dual role in promoting ethical behavior\nand mitigating nomophobia's psychological effects.\n  The study's theoretical advancement lies in its empirical evidence on the\nseldom-explored nexus between organizational commitment and technology-induced\nstress. By integrating organizational ethics and technological impact, the\nresearch offers a novel perspective on managing digital dependence in the\nworkplace. From a practical standpoint, this study serves as a catalyst for\norganizational leaders to reinforce affective and normative commitment, thereby\nreducing nomophobia. The findings underscore the necessity of ethical\nleadership and comprehensive ethical policies as foundations for employee\nwell-being in the digital age.\n  Conclusively, this study delineates the protective role of organizational\ncommitment and the significance of ethical environments, guiding organizations\nto foster cultures that balance technological efficiency with employee welfare.\nAs a contribution to both academic discourse and practical application, it\nemphasizes the importance of nurturing a supportive and ethically sound\nworkplace in an era of pervasive digital integration.\n"
    },
    {
        "paper_id": 2311.02431,
        "authors": "Matthew Sprintson and Edward Oughton",
        "title": "The contribution of US broadband infrastructure subsidy and investment\n  programs to GDP using input-output modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  More than one-fifth of the US population does not subscribe to a fixed\nbroadband service despite broadband being a recognized merit good. For example,\nless than 4% of citizens earning more than US \\$70k annually do not have\nbroadband, compared to 26% of those earning below US \\$20k annually. To address\nthis, the Biden Administration has undertaken one of the largest broadband\ninvestment programs ever via The Bipartisan Infrastructure Law, with the aim of\naddressing this disparity and expanding broadband connectivity to all citizens.\nWe examine broadband availability, adoption, and need for each US state, and\nthen construct an Input-Output model to explore the potential macroeconomic\nimpacts of broadband spending to Gross Domestic Product (GDP) and supply chain\nlinkages. Our analysis indicates that higher funding allocations do appear to\nbe allocated to areas with poorer broadband. While this may be logical, as it\nillustrates funding going to areas most in need, this could not have been\nassumed a priori given politically-motivated funding is not always rationally\nallocated. In terms of macroeconomic impact, the total direct contribution to\nUS GDP by the program could be as high as US \\$84.8 billion, \\$55.2 billion,\nand \\$5.99 billion for the BEAD program, ACP, and TBCP, respectively. Thus,\noverall, the broadband allocations could expand US GDP by \\$146 billion (0.13%\nof annual US GDP over the next five years). We contribute one of the first\neconomic impact assessments of the US Bipartisan Infrastructure Law to the\nliterature.\n"
    },
    {
        "paper_id": 2311.02434,
        "authors": "Kirill Kolmykov",
        "title": "Analysis of Decentralization in Governance and Financial Efficiency of\n  Companies: Studying the Relationship in the Field of Decentralized Finance",
        "comments": "15 pages, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Currently, the advantages of decentralization through blockchain technology\nin the financial sector are actively discussed. In this article, we investigate\nthe decentralization in the governance of Decentralized Autonomous\nOrganizations (DAO) using the Gini coefficient as an indicator of inequality\namong the token owners. This metric is analyzed in the context of Return on\nInvestment (ROI) for companies in the decentralized finance (DeFi) sector. Our\ngoal is to understand whether the level of \"real\" decentralization in\nblockchain-based governance affects financial efficiency, and to explore the\nbenefits and possible limitations of such an approach. This analysis allows for\na deeper understanding of the significance and impact of decentralization on\nthe functioning and productivity of organizations in the DeFi sector, and to\ndetermine the extent to which this impact is positively or negatively reflected\nin their success and profitability. Additionally, the results of this analysis\nwill provide a fuller understanding of the dynamics and potential of blockchain\nfor organization governance.\n"
    },
    {
        "paper_id": 2311.0269,
        "authors": "Nicole Tianjiao Yang, Tomoyuki Ichiba",
        "title": "Relative Arbitrage Opportunities in an Extended Mean Field System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies relative arbitrage opportunities in a market with\ninfinitely many interacting investors. We establish a conditional McKean-Vlasov\nsystem to study the market dynamics coupled with investors. We then provide a\ntheoretical framework to study a mean-field system, where the mean-field terms\nconsist of a joint distribution of wealth and strategies. The optimal relative\narbitrage is characterized by the equilibrium of extended mean field games. We\nshow the conditions on the existence and the uniqueness of the mean field\nequilibrium, then prove the propagation of chaos results for the finite-player\ngame, and demonstrate that the Nash equilibrium converges to the mean field\nequilibrium when the population grows to infinity.\n"
    },
    {
        "paper_id": 2311.02813,
        "authors": "Ethan Holdahl and Jiabin Wu",
        "title": "Institutional Screening and the Sustainability of Conditional\n  Cooperation",
        "comments": "22 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies a preference evolution model in which a population of\nagents are matched to play a sequential prisoner's dilemma in an incomplete\ninformation environment. An institution can design an incentive-compatible\nscreening scheme, such as a special zone that requires an entry fee, or a\ncostly label for purchase, to segregate the conditional cooperators from the\nnon-cooperators. We show that institutional intervention of this sort can help\nthe conditional cooperators to prevail when the psychological benefit of\ncooperating for them is sufficiently strong and the membership of the special\nzone or the label is inheritable with a sufficiently high probability.\n"
    },
    {
        "paper_id": 2311.03224,
        "authors": "Armin Asaadi, Armita Atrian, Hesam Nik Hoseini, Mohammad Mahdi\n  Movahedi",
        "title": "Risk Analysis in the Selection of Project Managers Based on ANP and FMEA",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Project managers play a crucial role in the success of projects. The\nselection of an appropriate project manager is a primary concern for senior\nmanagers in firms. Typically, this process involves candidate interviews and\nassessments of their abilities. There are various criteria for selecting a\nproject manager, and the importance of each criterion depends on the project\ntype, its conditions, and the risks associated with their absence in the chosen\ncandidate. Often, senior managers in engineering companies lack awareness of\nthe significance of these criteria and the potential risks linked to their\nabsence. This research aims to identify these risks in selecting project\nmanagers for civil engineering projects, utilizing a combined ANP-FMEA\napproach. Through a comprehensive literature review, five risk categories have\nbeen identified: individual skills, power-related issues, knowledge and\nexpertise, experience, and personality traits. Subsequently, these risks, along\nwith their respective sub-criteria and internal relationships, were analysed\nusing the combined ANP-FMEA technique. The results highlighted that the lack of\npolitical influence, absence of construction experience, and deficiency in\nproject management expertise represent the most substantial risks in selecting\na project manager. Moreover, upon comparison with the traditional FMEA\napproach, this study demonstrates the superior ability of the ANP-FMEA model in\ndifferentiating risks and pinpointing factors with elevated risk levels.\n"
    },
    {
        "paper_id": 2311.03283,
        "authors": "Haoyang Cao, Haotian Gu, Xin Guo and Mathieu Rosenbaum",
        "title": "Risk of Transfer Learning and its Applications in Finance",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2307.13546",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transfer learning is an emerging and popular paradigm for utilizing existing\nknowledge from previous learning tasks to improve the performance of new ones.\nIn this paper, we propose a novel concept of transfer risk and and analyze its\nproperties to evaluate transferability of transfer learning. We apply transfer\nlearning techniques and this concept of transfer risk to stock return\nprediction and portfolio optimization problems. Numerical results demonstrate a\nstrong correlation between transfer risk and overall transfer learning\nperformance, where transfer risk provides a computationally efficient way to\nidentify appropriate source tasks in transfer learning, including\ncross-continent, cross-sector, and cross-frequency transfer for portfolio\noptimization.\n"
    },
    {
        "paper_id": 2311.03538,
        "authors": "Anne Mackay and Marie-Claude Vachon",
        "title": "On an Optimal Stopping Problem with a Discontinuous Reward",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.36565.40160",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study an optimal stopping problem with an unbounded, time-dependent and\ndiscontinuous reward function.This problem is motivated by the pricing of a\nvariable annuity contract with guaranteed minimum maturity benefit, under the\nassumption that the policyholder's surrender behaviour maximizes the\nrisk-neutral value of the contract. We consider a general fee and surrender\ncharge function, and give a condition under which optimal stopping always\noccurs at maturity. Using an alternative representation for the value function\nof the optimization problem, we study its analytical properties and the\nresulting surrender (or exercise) region. In particular, we show that the\nnon-emptiness and the shape of the surrender region are fully characterized by\nthe fee and the surrender charge functions, which provides a powerful tool to\nunderstand their interrelation and how it affects early surrenders and the\noptimal surrender boundary. Under certain conditions on these two functions, we\ndevelop three representations for the value function; two are analogous to\ntheir American option counterpart, and one is new to the actuarial and American\noption pricing literature.\n"
    },
    {
        "paper_id": 2311.03546,
        "authors": "Iveena Mukherjee",
        "title": "Optimizing Climate Policy through C-ROADS and En-ROADS Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the onset of climate change and the increasing need for effective\npolicies, a multilateral approach is needed to make an impact on the growing\nthreats facing the environment. Through the use of systematic analysis by way\nof C-ROADS and En-ROADS, numerous scenarios have been simulated to shed light\non the most imperative policy factors to mitigate climate change. Within\nC-ROADS, it was determined that the impacts of the shrinking ice-albedo effect\non global temperatures is significant, however differential sea ice melting\nbetween the poles may not impact human dwellings, as all regions are impacted\nby sea ice melt. Flood risks are also becoming more imminent, specifically in\nhigh population density areas. In terms of afforestation, China is the emerging\nleader, and if other countries follow suit, this can incur substantial\ndividends. Upon conducting a comprehensive analysis of global trends through\nEn-ROADS, intriguing patterns appear between the length of a policy initiative,\nand its effectiveness. Quick policies with gradual increases in taxation proved\nsuccessful. Government intervention was also favorable, however an optimized\nmodel is presented, with moderate subsidization of renewable energy. Through\nthis systematic analysis of assumptions and policy for effective climate change\nmitigation efforts, an optimized, economically-favorable solution arises.\n"
    },
    {
        "paper_id": 2311.03594,
        "authors": "Tomohiro Uchiyama",
        "title": "A necessary and sufficient condition for the existence of chaotic\n  dynamics in a neoclassical growth model with a pollution effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a neoclassical growth model with a (productivity\ninhibiting) pollution effect. In particular, we obtain a necessary and\nsufficient condition for the existence of a topological chaos. We investigate\nhow the condition changes as the strength of the pollution effect changes. This\nis a new application of a recent result characterising the existence of a\ntopological chaos for a unimodal interval map by Deng, Khan, Mitra (2022).\n"
    },
    {
        "paper_id": 2311.03595,
        "authors": "Morgan R. Frank",
        "title": "Brief for the Canada House of Commons Study on the Implications of\n  Artificial Intelligence Technologies for the Canadian Labor Force: Generative\n  Artificial Intelligence Shatters Models of AI and Labor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Exciting advances in generative artificial intelligence (AI) have sparked\nconcern for jobs, education, productivity, and the future of work. As with past\ntechnologies, generative AI may not lead to mass unemployment. But, unlike past\ntechnologies, generative AI is creative, cognitive, and potentially ubiquitous\nwhich makes the usual assumptions of automation predictions ill-suited for\ntoday. Existing projections suggest that generative AI will impact workers in\noccupations that were previously considered immune to automation. As AI's full\nset of capabilities and applications emerge, policy makers should promote\nworkers' career adaptability. This goal requires improved data on job\nseparations and unemployment by locality and job titles in order to identify\nearly-indicators for the workers facing labor disruption. Further, prudent\npolicy should incentivize education programs to accommodate learning with AI as\na tool while preparing students for the demands of the future of work.\n"
    },
    {
        "paper_id": 2311.03638,
        "authors": "Tomohiro Hirano, Alexis Akira Toda",
        "title": "Bubble Economics",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jmateco.2024.102944",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article provides a self-contained overview of the theory of rational\nasset price bubbles. We cover topics from basic definitions, properties, and\nclassical results to frontier research, with an emphasis on bubbles attached to\nreal assets such as stocks, housing, and land. The main message is that bubbles\nattached to real assets are fundamentally nonstationary phenomena related to\nunbalanced growth. We present a bare-bones model and draw three new insights:\n(i) the emergence of asset price bubbles is a necessity, instead of a\npossibility; (ii) asset pricing implications are markedly different between\nbalanced growth of stationary nature and unbalanced growth of nonstationary\nnature; and (iii) asset price bubbles occur within larger historical trends\ninvolving shifts in industrial structure driven by technological innovation,\nincluding the transition from the Malthusian economy to the modern economy.\n"
    },
    {
        "paper_id": 2311.04008,
        "authors": "Victor Medina-Olivares, Finn Lindgren, Raffaella Calabrese, Jonathan\n  Crook",
        "title": "Joint model for longitudinal and spatio-temporal survival data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In credit risk analysis, survival models with fixed and time-varying\ncovariates are widely used to predict a borrower's time-to-event. When the\ntime-varying drivers are endogenous, modelling jointly the evolution of the\nsurvival time and the endogenous covariates is the most appropriate approach,\nalso known as the joint model for longitudinal and survival data. In addition\nto the temporal component, credit risk models can be enhanced when including\nborrowers' geographical information by considering spatial clustering and its\nvariation over time. We propose the Spatio-Temporal Joint Model (STJM) to\ncapture spatial and temporal effects and their interaction. This Bayesian\nhierarchical joint model reckons the survival effect of unobserved\nheterogeneity among borrowers located in the same region at a particular time.\nTo estimate the STJM model for large datasets, we consider the Integrated\nNested Laplace Approximation (INLA) methodology. We apply the STJM to predict\nthe time to full prepayment on a large dataset of 57,258 US mortgage borrowers\nwith more than 2.5 million observations. Empirical results indicate that\nincluding spatial effects consistently improves the performance of the joint\nmodel. However, the gains are less definitive when we additionally include\nspatio-temporal interactions.\n"
    },
    {
        "paper_id": 2311.04162,
        "authors": "Luciano Campi, Federico Cannerozzi, Fanny Cartellier",
        "title": "Coarse correlated equilibria in linear quadratic mean field games and\n  application to an emission abatement game",
        "comments": "32 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Coarse correlated equilibria (CCE) are a good alternative to Nash equilibria\n(NE), as they arise more naturally as outcomes of learning algorithms and they\nmay exhibit higher payoffs than NE. CCEs include a device which allows players'\nstrategies to be correlated without any cooperation, only through information\nsent by a mediator. We develop a methodology to concretely compute mean field\nCCEs in a linear-quadratic mean field game framework. We compare their\nperformance to mean field control solutions and mean field NE (usually named\nMFG solutions). Our approach is implemented in the mean field version of an\nemission abatement game between greenhouse gas emitters. In particular, we\nexhibit a simple and tractable class of mean field CCEs which allows to\noutperform very significantly the mean field NE payoff and abatement levels,\nbridging the gap between the mean field NE and the social optimum obtained by\nmean field control.\n"
    },
    {
        "paper_id": 2311.04392,
        "authors": "Edward J. Oughton, Tom Russell, Jeongjin Oh, Sara Ballan, Jim W. Hall",
        "title": "Global Vulnerability Assessment of Mobile Telecommunications\n  Infrastructure to Climate Hazards using Crowdsourced Open Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The ongoing change in Earth`s climate is causing an increase in the frequency\nand severity of climate-related hazards, for example, from coastal flooding,\nriverine flooding, and tropical cyclones. There is currently an urgent need to\nquantify the potential impacts of these events on infrastructure and users,\nespecially for hitherto neglected infrastructure sectors, such as\ntelecommunications, particularly given our increasing dependence on digital\ntechnologies. In this analysis a global assessment is undertaken, quantifying\nthe number of mobile cells vulnerable to climate hazards using open\ncrowdsourced data equating to 7.6 million 2G, 3G, 4G and 5G assets. For a 0.01%\nannual probability event under a high emissions scenario (RCP8.5), the number\nof affected cells is estimated at 2.26 million for tropical cyclones, equating\nto USD 1.01 billion in direct damage (an increase against the historical\nbaseline of 14% and 44%, respectively). Equally, for coastal flooding the\nnumber of potentially affected cells for an event with a 0.01% annual\nprobability under RCP8.5 is 109.9 thousand, equating to direct damage costs of\nUSD 2.69 billion (an increase against the baseline of 70% and 78%,\nrespectively). The findings demonstrate the need for risk analysts to include\nmobile communications (and telecommunications more broadly) in future critical\nnational infrastructure assessments. Indeed, this paper contributes a proven\nassessment methodology to the literature for use in future research for\nassessing this critical infrastructure sector.\n"
    },
    {
        "paper_id": 2311.04475,
        "authors": "Fanyu Zhao",
        "title": "Portfolio Construction using Black-Litterman Model and Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a portfolio construction process, including mainly two\nparts, Factors Selection and Weight Allocations. For the factors selection\npart, We have chosen 20 factors by considering three aspects, the global\nmarket, different assets class, and stock idiosyncratic characteristics. Each\nfactor is proxied by a corresponding ETF. Then, we would apply several weight\nallocation methods to those factors, including two fixed weight allocation\nmethods, three optimisation methods, and a Black-Litterman model. In addition,\nwe would also fit a Deep Learning model for generating views periodically and\nincorporating views with the prior to achieve dynamically updated weights by\nusing the Black-Litterman model. In the end, the robustness checking shows how\nweights change with respect to time evolving and variance increasing. Results\nusing shrinkage variance are provided to alleviate the impacts of\nrepresentativeness of historical data, but there sadly has little impact.\nOverall, the model by using the Deep Learning plus Black-Litterman model\nresults outperform the portfolio by other weight allocation schemes, even\nthough further improvement and robustness checking should be performed.\n"
    },
    {
        "paper_id": 2311.04599,
        "authors": "Chunyang Huang, Shaoliang Zhang",
        "title": "Explainable artificial intelligence model for identifying Market Value\n  in Professional Soccer Players",
        "comments": "13pages, 6figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study introduces an advanced machine learning method for predicting\nsoccer players' market values, combining ensemble models and the Shapley\nAdditive Explanations (SHAP) for interpretability. Utilizing data from about\n12,000 players from Sofifa, the Boruta algorithm streamlined feature selection.\nThe Gradient Boosting Decision Tree (GBDT) model excelled in predictive\naccuracy, with an R-squared of 0.901 and a Root Mean Squared Error (RMSE) of\n3,221,632.175. Player attributes in skills, fitness, and cognitive areas\nsignificantly influenced market value. These insights aid sports industry\nstakeholders in player valuation. However, the study has limitations, like\nunderestimating superstar players' values and needing larger datasets. Future\nresearch directions include enhancing the model's applicability and exploring\nvalue prediction in various contexts.\n"
    },
    {
        "paper_id": 2311.04727,
        "authors": "Siu Hin Tang, Mathieu Rosenbaum, Chao Zhou",
        "title": "Forecasting Volatility with Machine Learning and Rough Volatility:\n  Example from the Crypto-Winter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the application and test the performance of a recently introduced\nvolatility prediction framework encompassing LSTM and rough volatility. Our\nasset class of interest is cryptocurrencies, at the beginning of the\n\"crypto-winter\" in 2022. We first show that to forecast volatility, a universal\nLSTM approach trained on a pool of assets outperforms traditional models. We\nthen consider a parsimonious parametric model based on rough volatility and\nZumbach effect. We obtain similar prediction performances with only five\nparameters whose values are non-asset-dependent. Our findings provide further\nevidence on the universality of the mechanisms underlying the volatility\nformation process.\n"
    },
    {
        "paper_id": 2311.04841,
        "authors": "Gechun Liang, Moris S. Strub and Yuwei Wang",
        "title": "Predictable Relative Forward Performance Processes: Multi-Agent and Mean\n  Field Games for Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a new framework of predictable relative forward performance\nprocesses (PRFPP) to study portfolio management within a competitive\nenvironment. Each agent trades a distinct stock following a binomial\ndistribution with probabilities for a positive return depending on the market\nregime characterized by a binomial common noise. For both the finite population\nand mean field games, we construct and analyse PRFPPs for initial data of the\nCARA class along with the associated equilibrium strategies. We find that\nrelative performance concerns do not necessarily lead to more investment in the\nrisky asset. Under some parameter constellations, agents short a stock with\npositive expected excess return.\n"
    },
    {
        "paper_id": 2311.04946,
        "authors": "Yasuhiro Nakayama, Tomochika Sawaki",
        "title": "Causal Inference on Investment Constraints and Non-stationarity in\n  Dynamic Portfolio Optimization through Reinforcement Learning",
        "comments": "8 pages, 6 figures, 11 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we have developed a dynamic asset allocation investment\nstrategy using reinforcement learning techniques. To begin with, we have\naddressed the crucial issue of incorporating non-stationarity of financial time\nseries data into reinforcement learning algorithms, which is a significant\nimplementation in the application of reinforcement learning in investment\nstrategies. Our findings highlight the significance of introducing certain\nvariables such as regime change in the environment setting to enhance the\nprediction accuracy. Furthermore, the application of reinforcement learning in\ninvestment strategies provides a remarkable advantage of setting the\noptimization problem flexibly. This enables the integration of practical\nconstraints faced by investors into the algorithm, resulting in efficient\noptimization. Our study has categorized the investment strategy formulation\nconditions into three main categories, including performance measurement\nindicators, portfolio management rules, and other constraints. We have\nevaluated the impact of incorporating these conditions into the environment and\nrewards in a reinforcement learning framework and examined how they influence\ninvestment behavior.\n"
    },
    {
        "paper_id": 2311.05219,
        "authors": "Christian R. {\\O}stergaard, Bram Timmermans",
        "title": "Workplace diversity and innovation performance: current state of affairs\n  and future directions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the last 10 years, there has been a growing interest in diversity in\nhuman capital. Fueled by the business case for diversity, there is an\nincreasing interest in understanding how the combination of people with\ndifferent backgrounds fosters the innovation performance of firms. Studies have\nmeasured diversity on a wide range of personal-level characteristics, at\ndifferent levels of the organization, and in particular kinds of settings.\nInnovation performance has been measured using an arsenal of indicators, often\ndrawing on a large range of databases. This paper takes stock of this research,\nidentifying the current state of affairs and proposing future research\ntrajectories in the field of diversity and innovation\n"
    },
    {
        "paper_id": 2311.05234,
        "authors": "Arman Abgaryan, Utkarsh Sharma",
        "title": "Intermediating DFMM Asset (IDA)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Dynamic Function Market Maker (DFMM) introduced a fully automated\nframework for operating a multi-asset market, wherein an algorithmic accounting\nasset was used to connect different liquidity pools and ensure efficient\nrebalancing of risks, and internal accounting processes. In the DFMM design,\nthis asset was not tradaeble; however, in this work, we explore the\ncharacteristics of this asset, if it were to be made tradeable. Named the\nIntermediating DFMM Asset (IDA), this asset serves as a unit of account in\ncross-chain finance, functioning as an intermediating asset for predictable\nbudgeting, and efficient multichain transfers and settlements. Harnessing its\nrobust liquidity as the key counterpart asset in DFMM, it achieves capital\nefficiency through the strategic repurposing of its asset base, while\nsimultaneously mitigating risk via the dynamic optimisation of its\nmulticollateral foundation. We outline key characteristics of the proposed\nasset, unique risk mitigation aspects enabled by the adopting AMM (DFMM), and\ncontrol levers enabling the protocol's tactical asset and liability management\ntoolkit to harmonise the asset's objectives with its real-world realisation,\nthrough a novel prudential market operation to incentivise productive use of a\nfinite asset and dynamic AMM fee to ensure alignment of behaviours. The\nproposed design has the potential to harmonise the interests of diverse market\nparticipants, leading to synergetic reactions to informational flow, aiding IDA\nprotocol in achieving its objectives.\n"
    },
    {
        "paper_id": 2311.05288,
        "authors": "Jochen Wulf and Juerg Meierhofer",
        "title": "Towards a Taxonomy of Large Language Model based Business Model\n  Transformations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Research on the role of Large Language Models (LLMs) in business models and\nservices is limited. Previous studies have utilized econometric models,\ntechnical showcases, and literature reviews. However, this research is\npioneering in its empirical examination of the influence of LLMs at the firm\nlevel. The study introduces a detailed taxonomy that can guide further research\non the criteria for successful LLM-based business model implementation and\ndeepen understanding of LLM-driven business transformations. Existing knowledge\non this subject is sparse and general. This research offers a more detailed\nbusiness model design framework based on LLM-driven transformations. This\ntaxonomy is not only beneficial for academic research but also has practical\nimplications. It can act as a strategic tool for businesses, offering insights\nand best practices. Businesses can lev-erage this taxonomy to make informed\ndecisions about LLM initiatives, ensuring that technology in-vestments align\nwith strategic goals.\n"
    },
    {
        "paper_id": 2311.05743,
        "authors": "Gang Hu",
        "title": "Advancing Algorithmic Trading: A Multi-Technique Enhancement of Deep\n  Q-Network Models",
        "comments": "16 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study enhances a Deep Q-Network (DQN) trading model by incorporating\nadvanced techniques like Prioritized Experience Replay, Regularized Q-Learning,\nNoisy Networks, Dueling, and Double DQN. Extensive tests on assets like BTC/USD\nand AAPL demonstrate superior performance compared to the original model, with\nmarked increases in returns and Sharpe Ratio, indicating improved risk-adjusted\nrewards. Notably, convolutional neural network (CNN) architectures, both 1D and\n2D, significantly boost returns, suggesting their effectiveness in market trend\nanalysis. Across instruments, these enhancements have yielded stable and high\ngains, eclipsing the baseline and highlighting the potential of CNNs in trading\nsystems. The study suggests that applying sophisticated deep learning within\nreinforcement learning can greatly enhance automated trading, urging further\nexploration into advanced methods for broader financial applicability. The\nfindings advocate for the continued evolution of AI in finance.\n"
    },
    {
        "paper_id": 2311.05781,
        "authors": "Hansjoerg Albrecher, Pablo Azcue, Nora Muler",
        "title": "Optimal dividend strategies for a catastrophe insurer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the problem of optimally paying out dividends from an\ninsurance portfolio, when the criterion is to maximize the expected discounted\ndividends over the lifetime of the company and the portfolio contains claims\ndue to natural catastrophes, modelled by a shot-noise Cox claim number process.\nThe optimal value function of the resulting two-dimensional stochastic control\nproblem is shown to be the smallest viscosity supersolution of a corresponding\nHamilton-Jacobi-Bellman equation, and we prove that it can be uniformly\napproximated through a discretization of the space of the free surplus of the\nportfolio and the current claim intensity level. We implement the resulting\nnumerical scheme to identify optimal dividend strategies for such a natural\ncatastrophe insurer, and it is shown that the nature of the barrier and band\nstrategies known from the classical models with constant Poisson claim\nintensity carry over in a certain way to this more general situation, leading\nto action and non-action regions for the dividend payments as a function of the\ncurrent surplus and intensity level. We also discuss some interpretations in\nterms of upward potential for shareholders when including a catastrophe sector\nin the portfolio.\n"
    },
    {
        "paper_id": 2311.05822,
        "authors": "Brendan K. Beare and Alexis Akira Toda",
        "title": "Optimal taxation and the Domar-Musgrave effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article concerns the optimal choice of flat taxes on labor and capital\nincome, and on consumption, in a tractable economic model. Agents manage a\nportfolio of bonds and physical capital while subject to idiosyncratic\ninvestment risk and random mortality. We identify the tax rates which maximize\nwelfare in stationary equilibrium while preserving tax revenue, finding that a\nvery large increase in welfare can be achieved by only taxing capital income\nand consumption. The optimal rate of capital income taxation is zero if the\nnatural borrowing constraint is strictly binding on entrepreneurs, but may\notherwise be positive and potentially large. The Domar-Musgrave effect, whereby\ncapital income taxation with full offset provisions encourages risky investment\nthrough loss sharing, explains cases where it is optimal to tax capital income.\nIn further analysis we study the dynamic response to the substitution of\nconsumption taxation for labor income taxation. We find that consumption\nimmediately drops before rising rapidly to the new stationary equilibrium,\nwhich is higher on average than initial consumption for workers but lower for\nentrepreneurs.\n"
    },
    {
        "paper_id": 2311.0584,
        "authors": "Ricardo Cuervo",
        "title": "Predictive AI for SME and Large Enterprise Financial Performance\n  Management",
        "comments": "8 pages plus appendix. Thesis for MSc in AI at QMUL",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Financial performance management is at the core of business management and\nhas historically relied on financial ratio analysis using Balance Sheet and\nIncome Statement data to assess company performance as compared with\ncompetitors. Little progress has been made in predicting how a company will\nperform or in assessing the risks (probabilities) of financial\nunderperformance. In this study I introduce a new set of financial and\nmacroeconomic ratios that supplement standard ratios of Balance Sheet and\nIncome Statement. I also provide a set of supervised learning models (ML\nRegressors and Neural Networks) and Bayesian models to predict company\nperformance. I conclude that the new proposed variables improve model accuracy\nwhen used in tandem with standard industry ratios. I also conclude that\nFeedforward Neural Networks (FNN) are simpler to implement and perform best\nacross 6 predictive tasks (ROA, ROE, Net Margin, Op Margin, Cash Ratio and Op\nCash Generation); although Bayesian Networks (BN) can outperform FNN under very\nspecific conditions. BNs have the additional benefit of providing a probability\ndensity function in addition to the predicted (expected) value. The study\nfindings have significant potential helping CFOs and CEOs assess risks of\nfinancial underperformance to steer companies in more profitable directions;\nsupporting lenders in better assessing the condition of a company and providing\ninvestors with tools to dissect financial statements of public companies more\naccurately.\n"
    },
    {
        "paper_id": 2311.05977,
        "authors": "Zhiyu Cao and Zachary Feinstein",
        "title": "Price-mediated contagion with endogenous market liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price-mediated contagion occurs when a positive feedback loop develops\nfollowing a drop in asset prices which forces banks and other financial\ninstitutions to sell their holdings. Prior studies of such events fix the level\nof market liquidity without regards to the level of stress applied to the\nsystem. This paper introduces a framework to understand price-mediated\ncontagion in a system where the capacity of the market to absorb liquidated\nassets is determined endogenously. In doing so, we construct a joint clearing\nsystem in interbank payments, asset prices, and market liquidity. We establish\nmild assumptions which guarantee the existence of greatest and least clearing\nsolutions. We conclude with detailed numerical case studies which demonstrate\nthe, potentially severe, repercussions of endogenizing the market liquidity on\nsystem risk.\n"
    },
    {
        "paper_id": 2311.06048,
        "authors": "Monica Lambon-Quayefio, Thomas Yeboah, Nkechi S.Owoo, Marjan Petreski,\n  Catherine Koranchie, Edward Asiedu, Mohammed Zakaria, Ernest Berko, Yaw Nsiah\n  Agyemang",
        "title": "Empirical Review of Youth-Employment Programs in Ghana",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Ghana-s current youth unemployment rate is 19.7%, and the country faces a\nsignificant youth unemployment problem. While a range of youth-employment\nprograms have been created over the years, no systematic documentation and\nevaluation of the impacts of these public initiatives has been undertaken.\nClarifying which interventions work would guide policy makers in creating\nstrategies and programs to address the youth-employment challenge. By\ncomplementing desk reviews with qualitative data gathered from focus-group\ndiscussions and key informant interviews, we observe that most youth-employment\nprograms implemented in Ghana cover a broad spectrum that includes skills\ntraining, job placement matching, seed capital, and subsidies. Duplication of\ninitiatives, lack of coordination, and few to non-existent impact evaluations\nof programs are the main challenges that plague these programs. For better\ncoordination and effective policy making, a more centralized and coordinated\nsystem is needed for program design and implementation. Along the same lines,\nensuring rigorous evaluation of existing youth-employment programs is necessary\nto provide empirical evidence of the effectiveness and efficiency of these\nprograms.\n"
    },
    {
        "paper_id": 2311.06251,
        "authors": "Mohammad Rasouli, Ravi Chiruvolu, Ali Risheh",
        "title": "AI for Investment: A Platform Disruption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  With the investment landscape becoming more competitive, efficiently scaling\ndeal sourcing and improving deal insights have become a dominant strategy for\nfunds. While funds are already spending significant efforts on these two tasks,\nthey cannot be scaled with traditional approaches; hence, there is a surge in\nautomating them. Many third party software providers have emerged recently to\naddress this need with productivity solutions, but they fail due to a lack of\npersonalization for the fund, privacy constraints, and natural limits of\nsoftware use cases. Therefore, most major funds and many smaller funds have\nstarted developing their in-house AI platforms: a game changer for the\nindustry. These platforms grow smarter by direct interactions with the fund and\ncan be used to provide personalized use cases. Recent developments in large\nlanguage models, e.g. ChatGPT, have provided an opportunity for other funds to\nalso develop their own AI platforms. While not having an AI platform now is not\na competitive disadvantage, it will be in two years. Funds require a practical\nplan and corresponding risk assessments for such AI platforms.\n"
    },
    {
        "paper_id": 2311.06256,
        "authors": "Robert Stok and Paul Bilokon",
        "title": "From Deep Filtering to Deep Econometrics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calculating true volatility is an essential task for option pricing and risk\nmanagement. However, it is made difficult by market microstructure noise.\nParticle filtering has been proposed to solve this problem as it favorable\nstatistical properties, but relies on assumptions about underlying market\ndynamics. Machine learning methods have also been proposed but lack\ninterpretability, and often lag in performance. In this paper we implement the\nSV-PF-RNN: a hybrid neural network and particle filter architecture. Our\nSV-PF-RNN is designed specifically with stochastic volatility estimation in\nmind. We then show that it can improve on the performance of a basic particle\nfilter.\n"
    },
    {
        "paper_id": 2311.06273,
        "authors": "Ummara Mumtaz, Summaya Mumtaz",
        "title": "Potential of ChatGPT in predicting stock market trends based on Twitter\n  Sentiment Analysis",
        "comments": "total 11 pages including references, 4 figures and one table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rise of ChatGPT has brought a notable shift to the AI sector, with its\nexceptional conversational skills and deep grasp of language. Recognizing its\nvalue across different areas, our study investigates ChatGPT's capacity to\npredict stock market movements using only social media tweets and sentiment\nanalysis. We aim to see if ChatGPT can tap into the vast sentiment data on\nplatforms like Twitter to offer insightful predictions about stock trends. We\nfocus on determining if a tweet has a positive, negative, or neutral effect on\ntwo big tech giants Microsoft and Google's stock value. Our findings highlight\na positive link between ChatGPT's evaluations and the following days stock\nresults for both tech companies. This research enriches our view on ChatGPT's\nadaptability and emphasizes the growing importance of AI in shaping financial\nmarket forecasts.\n"
    },
    {
        "paper_id": 2311.06278,
        "authors": "Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah, Duc Minh Cao,\n  Ashiqul Haque Ahmed",
        "title": "Boosting Stock Price Prediction with Anticipated Macro Policy Changes",
        "comments": null,
        "journal-ref": "Journal of Mathematics and Statistics Studies, 4(3), 29-34 (2023)",
        "doi": "10.32996/jmss",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of stock prices plays a significant role in aiding the\ndecision-making of investors. Considering its importance, a growing literature\nhas emerged trying to forecast stock prices with improved accuracy. In this\nstudy, we introduce an innovative approach for forecasting stock prices with\ngreater accuracy. We incorporate external economic environment-related\ninformation along with stock prices. In our novel approach, we improve the\nperformance of stock price prediction by taking into account variations due to\nfuture expected macroeconomic policy changes as investors adjust their current\nbehavior ahead of time based on expected future macroeconomic policy changes.\nFurthermore, we incorporate macroeconomic variables along with historical stock\nprices to make predictions. Results from this strongly support the inclusion of\nfuture economic policy changes along with current macroeconomic information. We\nconfirm the supremacy of our method over the conventional approach using\nseveral tree-based machine-learning algorithms. Results are strongly conclusive\nacross various machine learning models. Our preferred model outperforms the\nconventional approach with an RMSE value of 1.61 compared to an RMSE value of\n1.75 from the conventional approach.\n"
    },
    {
        "paper_id": 2311.0628,
        "authors": "Parth Daxesh Modi, Kamyar Arshi, Pertami J. Kunz, Abdelhak M. Zoubir",
        "title": "A Data-driven Deep Learning Approach for Bitcoin Price Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bitcoin as a cryptocurrency has been one of the most important digital coins\nand the first decentralized digital currency. Deep neural networks, on the\nother hand, has shown promising results recently; however, we require huge\namount of high-quality data to leverage their power. There are some techniques\nsuch as augmentation that can help us with increasing the dataset size, but we\ncannot exploit them on historical bitcoin data. As a result, we propose a\nshallow Bidirectional-LSTM (Bi-LSTM) model, fed with feature engineered data\nusing our proposed method to forecast bitcoin closing prices in a daily time\nframe. We compare the performance with that of other forecasting methods, and\nshow that with the help of the proposed feature engineering method, a shallow\ndeep neural network outperforms other popular price forecasting models.\n"
    },
    {
        "paper_id": 2311.06282,
        "authors": "Jiaer He, Roberto Rivera",
        "title": "A Modeling Approach of Return and Volatility of Structured Investment\n  Products with Caps and Floors",
        "comments": null,
        "journal-ref": "Forum Empresarial, 2024",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Popular investment structured products in Puerto Rico are stock market tied\nIndividual Retirement Accounts (IRA), which offer some stock market growth\nwhile protecting the principal. The performance of these retirement strategies\nhas not been studied. This work examines the expected return and risk of Puerto\nRico stock market IRA (PRIRAs) and compares their statistical properties with\nother investment instruments before and after tax. We propose a parametric\nmodeling approach for structured products and apply it to PRIRAs. Our method\nfirst estimates the conditional expected return (and variance) of PRIRA assets\nfrom which we extract marginal moments through the Law of Iterated Expectation.\nOur results indicate that PRIRAs underperform against investing directly in the\nstock market while still carrying substantial risk. The expected return of the\nstock market IRA from Popular Bank (PRIRA1) after tax is slightly greater than\nthat of investing in U.S. bonds, while PRIRA1 has almost two times the risk.\nThe stock market IRA from Universal (PRIRA2) performs similarly to PRIRA1,\nwhile PRIRA2 has a lower risk than PRIRA1. PRIRAs may be reasonable for some\nrisk-averse investors due to their principal protection and tax deferral.\n"
    },
    {
        "paper_id": 2311.06292,
        "authors": "Abel Sancarlos, Edgar Bahilo, Pablo Mozo, Lukas Norman, Obaid Ur\n  Rehma, Mihails Anufrijevs",
        "title": "Towards a data-driven debt collection strategy based on an advanced\n  machine learning framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European debt purchase market as measured by the total book value of\npurchased debt approached 25bn euros in 2020 and it was growing at double-digit\nrates. This is an example of how big the debt collection and debt purchase\nindustry has grown and the important impact it has in the financial sector.\nHowever, in order to ensure an adequate return during the debt collection\nprocess, a good estimation of the propensity to pay and/or the expected\ncashflow is crucial. These estimations can be employed, for instance, to create\ndifferent strategies during the amicable collection to maximize quality\nstandards and revenues. And not only that, but also to prioritize the cases in\nwhich a legal process is necessary when debtors are unreachable for an amicable\nnegotiation. This work offers a solution for these estimations. Specifically, a\nnew machine learning modelling pipeline is presented showing how outperforms\ncurrent strategies employed in the sector. The solution contains a\npre-processing pipeline and a model selector based on the best model\ncalibration. Performance is validated with real historical data of the debt\nindustry.\n"
    },
    {
        "paper_id": 2311.0633,
        "authors": "Zengqing Wu, Run Peng, Xu Han, Shuyuan Zheng, Yixin Zhang, Chuan Xiao",
        "title": "Smart Agent-Based Modeling: On the Use of Large Language Models in\n  Computer Simulations",
        "comments": "Source codes are available at https://github.com/Roihn/SABM",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Computer simulations offer a robust toolset for exploring complex systems\nacross various disciplines. A particularly impactful approach within this realm\nis Agent-Based Modeling (ABM), which harnesses the interactions of individual\nagents to emulate intricate system dynamics. ABM's strength lies in its\nbottom-up methodology, illuminating emergent phenomena by modeling the\nbehaviors of individual components of a system. Yet, ABM has its own set of\nchallenges, notably its struggle with modeling natural language instructions\nand common sense in mathematical equations or rules. This paper seeks to\ntranscend these boundaries by integrating Large Language Models (LLMs) like GPT\ninto ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based\nModeling (SABM). Building upon the concept of smart agents -- entities\ncharacterized by their intelligence, adaptability, and computation ability --\nwe explore in the direction of utilizing LLM-powered agents to simulate\nreal-world scenarios with increased nuance and realism. In this comprehensive\nexploration, we elucidate the state of the art of ABM, introduce SABM's\npotential and methodology, and present three case studies (source codes\navailable at https://github.com/Roihn/SABM), demonstrating the SABM methodology\nand validating its effectiveness in modeling real-world systems. Furthermore,\nwe cast a vision towards several aspects of the future of SABM, anticipating a\nbroader horizon for its applications. Through this endeavor, we aspire to\nredefine the boundaries of computer simulations, enabling a more profound\nunderstanding of complex systems.\n"
    },
    {
        "paper_id": 2311.06476,
        "authors": "Meng Wang and Tai-Ho Wang",
        "title": "Relative entropy-regularized robust optimal order execution",
        "comments": "32 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem of order execution is cast as a relative entropy-regularized\nrobust optimal control problem in this article. The order execution agent's\ngoal is to maximize an objective functional associated with his profit-and-loss\nof trading and simultaneously minimize the execution risk and the market's\nliquidity and uncertainty. We model the market's liquidity and uncertainty by\nthe principle of least relative entropy associated with the market volume. The\nproblem of order execution is made into a relative entropy-regularized\nstochastic differential game. Standard argument of dynamic programming yields\nthat the value function of the differential game satisfies a relative\nentropy-regularized Hamilton-Jacobi-Isaacs (rHJI) equation. Under the\nassumptions of linear-quadratic model with Gaussian prior, the rHJI equation\nreduces to a system of Riccati and linear differential equations. Further\nimposing constancy of the corresponding coefficients, the system of\ndifferential equations can be solved in closed form, resulting in analytical\nexpressions for optimal strategy and trajectory as well as the posterior\ndistribution of market volume. Numerical examples illustrating the optimal\nstrategies and the comparisons with conventional trading strategies are\nconducted.\n"
    },
    {
        "paper_id": 2311.06519,
        "authors": "Nick James and Max Menzies",
        "title": "Portfolio diversification with varying investor abilities",
        "comments": "Minor edits since v2. Equal contribution",
        "journal-ref": "EPL, 145 (2024) 32002",
        "doi": "10.1209/0295-5075/ad1ef2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce new mathematical methods to study the optimal portfolio size of\ninvestment portfolios over time, considering investors with varying skill\nlevels. First, we explore the benefit of portfolio diversification on an annual\nbasis for poor, average and strong investors defined by the 10th, 50th and 90th\npercentiles of risk-adjusted returns, respectively. Second, we conduct a\nthorough regression experiment examining quantiles of risk-adjusted returns as\na function of portfolio size across investor ability, testing for trends and\ncurvature within these functions. Finally, we study the optimal portfolio size\nfor poor, average and strong investors in a continuously temporal manner using\nmore than 20 years of data. We show that strong investors should hold\nconcentrated portfolios, poor investors should hold diversified portfolios;\naverage investors have a less obvious distribution with the optimal number\nvarying materially over time.\n"
    },
    {
        "paper_id": 2311.0659,
        "authors": "Sheng Dai, Natalia Kuosmanen, Timo Kuosmanen, Juuso Liesi\\\"o",
        "title": "Optimal resource allocation: Convex quantile regression approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Optimal allocation of resources across sub-units in the context of\ncentralized decision-making systems such as bank branches or supermarket chains\nis a classical application of operations research and management science. In\nthis paper, we develop quantile allocation models to examine how much the\noutput and productivity could potentially increase if the resources were\nefficiently allocated between units. We increase robustness to random noise and\nheteroscedasticity by utilizing the local estimation of multiple production\nfunctions using convex quantile regression. The quantile allocation models then\nrely on the estimated shadow prices instead of detailed data of units and allow\nthe entry and exit of units. Our empirical results on Finland's business sector\nreveal a large potential for productivity gains through better allocation,\nkeeping the current technology and resources fixed.\n"
    },
    {
        "paper_id": 2311.06621,
        "authors": "Antoine Jacquier, Oleksiy Kondratyev, Gordon Lee, Mugad Oumgari",
        "title": "Quantum Computing for Financial Mathematics",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum computing has recently appeared in the headlines of many scientific\nand popular publications. In the context of quantitative finance, we provide\nhere an overview of its potential.\n"
    },
    {
        "paper_id": 2311.06665,
        "authors": "Hayden Brown",
        "title": "Withdrawal Success Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For $n$ assets and discrete-time rebalancing, the probability to complete a\ngiven schedule of investments and withdrawals is maximized over progressively\nmeasurable portfolio weight functions. Applications consider two assets, namely\nthe S&P Composite Index and an inflation-protected bond. The maximum\nprobability and optimal portfolio weight functions are computed for annually\nrebalanced schedules involving an arbitrary initial investment and then equal\nannual withdrawals over the remainder of the time period. Applications also\nconsider annually rebalanced schedules that start with dollar cost averaging\n(equal annual investments) and then shift to equal annual withdrawals. Results\nindicate noticeable improvements in the probability to complete a given\nschedule when optimal portfolio weights are used instead of constant portfolio\nweights like the standard of keeping 90% in the S&P Composite Index and 10% in\ninflation-protected bonds.\n"
    },
    {
        "paper_id": 2311.06716,
        "authors": "Qionghua Chu",
        "title": "Sustainable Development Goals (SDGs): New Zealand Outlook with Central\n  Bank Digital Currency and SDG 8 Realization on the Horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Central Bank Digital Currency (CBDC) may assist New Zealand accomplish SDG 8.\nI aim to evaluate if SDGs could be achieved together because of mutual\ninteractions between SDG 8 and other SDGs. The SDGs are categorized by their\nshared qualities to affect and effect SDG 8. Also, additional SDGs may help\neach other achieve. Considering the CBDC as a fundamental stimulus to achieving\ndecent work and economic growth, detailed study and analysis of mutual\ninteractions suggests that SDG 8 and other SDGs can be achieved.\n"
    },
    {
        "paper_id": 2311.06718,
        "authors": "Qionghua Chu",
        "title": "Sustainable Development Goal (SDG) 8: New Zealand Prospects while Yield\n  Curve Inverts in Central Bank Digital Currency (CBDC) Era",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the inverted yield curve environment, I intend to assess the feasibility\nof fulfilling Sustainable Development Goal (SDG) 8, decent work and economic\ngrowth, of the United Nations by 2030 in New Zealand. Central Bank Digital\nCurrency (CBDC) issuance supports SDG 8, based on the Cobb-Douglas production\nfunction, the growth accounting relation, and the Theory of Aggregate Demand.\nBright prospects exist for New Zealand.\n"
    },
    {
        "paper_id": 2311.0674,
        "authors": "Clement E. Bohr, Mart\\'i Mestieri, Emre Enes Yavuz",
        "title": "Aggregation and Closed-Form Results for Nonhomothetic CES Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide four novel results for nonhomothetic Constant Elasticity of\nSubstitution preferences (Hanoch, 1975). First, we derive a closed-form\nrepresentation of the expenditure function of nonhomothetic CES under\nrelatively flexible distributional assumptions of demand and price distribution\nparameters. Second, we characterize aggregate demand from heterogeneous\nhouseholds in closed-form, assuming that household total expenditures follow an\nempirically plausible distribution. Third, we leverage these results to study\nthe Euler equation arising from standard intertemporal consumption-saving\nproblems featuring within-period nonhomothetic CES preferences. Finally, we\nshow that nonhomothetic CES expenditure shares arise as the solution of a\ndiscrete-choice logit problem.\n"
    },
    {
        "paper_id": 2311.06745,
        "authors": "Zongxia Liang, Jianming Xia, Fengyi Yuan",
        "title": "Dynamic portfolio selection for nonlinear law-dependent preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the portfolio selection problem for nonlinear\nlaw-dependent preferences in continuous time, which inherently exhibit time\ninconsistency. Employing the method of stochastic maximum principle, we\nestablish verification theorems for equilibrium strategies, accommodating both\nrandom market coefficients and incomplete markets. We derive the first-order\ncondition (FOC) for the equilibrium strategies, using a notion of functional\nderivatives with respect to probability distributions. Then, with the help of\nthe FOC we obtain the equilibrium strategies in closed form for two classes of\nimplicitly defined preferences: CRRA and CARA betweenness preferences, with\ndeterministic market coefficients. Finally, to show applications of our\ntheoretical results to problems with random market coefficients, we examine the\nweighted utility. We reveal that the equilibrium strategy can be described by a\ncoupled system of Quadratic Backward Stochastic Differential Equations\n(QBSDEs). The well-posedness of this system is generally open but is\nestablished under the special structures of our problem.\n"
    },
    {
        "paper_id": 2311.0679,
        "authors": "Ahmet Umur \\\"Ozsoy and \\\"Om\\\"ur U\\u{g}ur",
        "title": "The QLBS Model within the presence of feedback loops through the impacts\n  of a large trader",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend the QLBS model by reformulating via considering a large trader\nwhose transactions leave a permanent impact on the evolution of the exchange\nrate process and therefore affect the price of contingent claims on such\nprocesses. Through a hypothetical limit order book we quantify the exchange\nrate altered by such transactions. We therefore define the quoted exchange rate\nprocess, for which we assume the existence of a postulated hedging strategy.\nGiven the quoted exchange rate and postulated hedging strategy, we find an\noptimal hedging strategy through batch-mode reinforcement learning given the\ntrader alters the course of the exchange rate process. We assume that the\ntrader has its own concept of fair price and we define our problem as finding\nthe hedging strategy with much lower transaction costs yet delivering a price\nthat well converges to the fair price of the trader. We show our contribution\nresults in an optimal hedging strategy with much lower transaction costs and\nconvergence to the fair price is obtained assuming sensible parameters.\n"
    },
    {
        "paper_id": 2311.06865,
        "authors": "Lucas Amherd, Sheng-Nan Li and Claudio J. Tessone",
        "title": "Centralised or Decentralised? Data Analysis of Transaction Network of\n  Hedera Hashgraph",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An important virtue of distributed ledger technologies is their acclaimed\nhigher level of decentralisation compared to traditional financial systems.\nEmpirical literature, however, suggests that many systems tend towards\ncentralisation as well. This study expands the current literature by offering a\nfirst-time, data-driven analysis of the degree of decentralisation of the\nplatform Hedera Hashgraph, a public permissioned distributed ledger technology,\nemploying data directly fetched from a network node. The results show a\nconsiderably higher amount of released supply compared to the release schedule\nand a growing number of daily active accounts. Also, Hedera Hashgraph exhibits\na high centralisation of wealth and a shrinking core that acts as an\nintermediary in transactions for the rest of the network. However, the Nakamoto\nindex and Theil index point to recent progress towards a more decentralised\nnetwork.\n"
    },
    {
        "paper_id": 2311.06896,
        "authors": "Nicole B\\\"auerle and Anna Ja\\'skiewicz",
        "title": "Markov Decision Processes with Risk-Sensitive Criteria: An Overview",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper provides an overview of the theory and applications of\nrisk-sensitive Markov decision processes. The term 'risk-sensitive' refers here\nto the use of the Optimized Certainty Equivalent as a means to measure\nexpectation and risk. This comprises the well-known entropic risk measure and\nConditional Value-at-Risk. We restrict our considerations to stationary\nproblems with an infinite time horizon. Conditions are given under which\noptimal policies exist and solution procedures are explained. We present both\nthe theory when the Optimized Certainty Equivalent is applied recursively as\nwell as the case where it is applied to the cumulated reward. Discounted as\nwell as non-discounted models are reviewed\n"
    },
    {
        "paper_id": 2311.07071,
        "authors": "Kaichen Zhang, Ohchan Kwon, Hui Xiong",
        "title": "The Impact of Generative Artificial Intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise of generative artificial intelligence (AI) has sparked concerns\nabout its potential influence on unemployment and market depression. This study\naddresses this concern by examining the impact of generative AI on product\nmarkets. To overcome the challenge of causal inference, given the inherent\nlimitations of conducting controlled experiments, this paper identifies an\nunanticipated and sudden leak of a highly proficient image-generative AI as a\nnovel instance of a \"natural experiment\". This AI leak spread rapidly,\nsignificantly reducing the cost of generating anime-style images compared to\nother styles, creating an opportunity for comparative assessment. We collect\nreal-world data from an artwork outsourcing platform. Surprisingly, our results\nshow that while generative AI lowers average prices, it substantially boosts\norder volume and overall revenue. This counterintuitive finding suggests that\ngenerative AI confers benefits upon artists rather than detriments. The study\nfurther offers theoretical economic explanations to elucidate this unexpected\nphenomenon. By furnishing empirical evidence, this paper dispels the notion\nthat generative AI might engender depression, instead underscoring its\npotential to foster market prosperity. These findings carry significant\nimplications for practitioners, policymakers, and the broader AI community.\n"
    },
    {
        "paper_id": 2311.07072,
        "authors": "Armita Atrian, Saleh Ghobbeh",
        "title": "Technostress and Job Performance: Understanding the Negative Impacts and\n  Strategic Responses in the Workplace",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study delves into the increasingly pertinent issue of technostress in\nthe workplace and its multifaceted impact on job performance. Technostress,\nemerging from the rapid integration of technology in professional settings, is\nidentified as a significant stressor affecting employees across various\nindustries. The research primarily focuses on the ways in which technostress\ninfluences job performance, both negatively and positively, depending on the\ncontext and individual coping mechanisms. Through a blend of qualitative and\nquantitative methodologies, including surveys and in-depth interviews, the\nstudy examines the experiences of employees from diverse sectors. It highlights\nhow technostress manifests in different forms: from anxiety and frustration due\nto constant connectivity to the pressure of adapting to new technologies. The\npaper also explores the dual role of technology as both a facilitator and a\nhindrance in the workplace.\n  Significant findings indicate that technostress adversely impacts job\nperformance, leading to decreased productivity, diminished job satisfaction,\nand increased turnover intentions. However, the study also uncovers that\nstrategic interventions, such as training programs, supportive leadership, and\nfostering a positive technological culture, can mitigate these negative\neffects. These interventions not only help in managing technostress but also in\nharnessing the potential of technology for enhanced job performance.\n  Furthermore, the research proposes a model outlining the relationship between\ntechnostress, coping mechanisms, and job performance. This model serves as a\nframework for organizations to understand and address the challenges posed by\ntechnostress. The study concludes with recommendations for future research,\nparticularly in exploring the long-term effects of technostress and the\nefficacy of various coping strategies.\n"
    },
    {
        "paper_id": 2311.07211,
        "authors": "Jirong Zhuang, Deng Ding, Weiguo Lu, Xuan Wu, Gangnan Yuan",
        "title": "A Gaussian Process Based Method with Deep Kernel Learning for Pricing\n  High-dimensional American Options",
        "comments": "21pages,8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we present a novel machine learning approach for pricing\nhigh-dimensional American options based on the modified Gaussian process\nregression (GPR). We incorporate deep kernel learning and sparse variational\nGaussian processes to address the challenges traditionally associated with GPR.\nThese challenges include its diminished reliability in high-dimensional\nscenarios and the excessive computational costs associated with processing\nextensive numbers of simulated paths Our findings indicate that the proposed\nmethod surpasses the performance of the least squares Monte Carlo method in\nhigh-dimensional scenarios, particularly when the underlying assets are modeled\nby Merton's jump diffusion model. Moreover, our approach does not exhibit a\nsignificant increase in computational time as the number of dimensions grows.\nConsequently, this method emerges as a potential tool for alleviating the\nchallenges posed by the curse of dimensionality.\n"
    },
    {
        "paper_id": 2311.07231,
        "authors": "Rawin Assabumrungrat, Kentaro Minami, Masanori Hirano",
        "title": "Error Analysis of Option Pricing via Deep PDE Solvers: Empirical Study",
        "comments": "11 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Option pricing, a fundamental problem in finance, often requires solving\nnon-linear partial differential equations (PDEs). When dealing with multi-asset\noptions, such as rainbow options, these PDEs become high-dimensional, leading\nto challenges posed by the curse of dimensionality. While deep learning-based\nPDE solvers have recently emerged as scalable solutions to this\nhigh-dimensional problem, their empirical and quantitative accuracy remains not\nwell-understood, hindering their real-world applicability. In this study, we\naimed to offer actionable insights into the utility of Deep PDE solvers for\npractical option pricing implementation. Through comparative experiments, we\nassessed the empirical performance of these solvers in high-dimensional\ncontexts. Our investigation identified three primary sources of errors in Deep\nPDE solvers: (i) errors inherent in the specifications of the target option and\nunderlying assets, (ii) errors originating from the asset model simulation\nmethods, and (iii) errors stemming from the neural network training. Through\nablation studies, we evaluated the individual impact of each error source. Our\nresults indicate that the Deep BSDE method (DBSDE) is superior in performance\nand exhibits robustness against variations in option specifications. In\ncontrast, some other methods are overly sensitive to option specifications,\nsuch as time to expiration. We also find that the performance of these methods\nimproves inversely proportional to the square root of batch size and the number\nof time steps. This observation can aid in estimating computational resources\nfor achieving desired accuracies with Deep PDE solvers.\n"
    },
    {
        "paper_id": 2311.07478,
        "authors": "Maxime Markov, Vladimir Markov",
        "title": "Optimal portfolio allocation with uncertain covariance matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we explore the portfolio allocation problem involving an\nuncertain covariance matrix. We calculate the expected value of the Constant\nAbsolute Risk Aversion (CARA) utility function, marginalized over a\ndistribution of covariance matrices. We show that marginalization introduces a\nlogarithmic dependence on risk, as opposed to the linear dependence assumed in\nthe mean-variance approach. Additionally, it leads to a decrease in the\nallocation level for higher uncertainties. Our proposed method extends the\nmean-variance approach by considering the uncertainty associated with future\ncovariance matrices and expected returns, which is important for practical\napplications.\n"
    },
    {
        "paper_id": 2311.07513,
        "authors": "Branka Hadji Misheva and Joerg Osterrieder",
        "title": "A Hypothesis on Good Practices for AI-based Systems for Financial Time\n  Series Forecasting: Towards Domain-Driven XAI Methods",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Machine learning and deep learning have become increasingly prevalent in\nfinancial prediction and forecasting tasks, offering advantages such as\nenhanced customer experience, democratising financial services, improving\nconsumer protection, and enhancing risk management. However, these complex\nmodels often lack transparency and interpretability, making them challenging to\nuse in sensitive domains like finance. This has led to the rise of eXplainable\nArtificial Intelligence (XAI) methods aimed at creating models that are easily\nunderstood by humans. Classical XAI methods, such as LIME and SHAP, have been\ndeveloped to provide explanations for complex models. While these methods have\nmade significant contributions, they also have limitations, including\ncomputational complexity, inherent model bias, sensitivity to data sampling,\nand challenges in dealing with feature dependence. In this context, this paper\nexplores good practices for deploying explainability in AI-based systems for\nfinance, emphasising the importance of data quality, audience-specific methods,\nconsideration of data properties, and the stability of explanations. These\npractices aim to address the unique challenges and requirements of the\nfinancial industry and guide the development of effective XAI tools.\n"
    },
    {
        "paper_id": 2311.07597,
        "authors": "Alexej Brauer",
        "title": "Enhancing Actuarial Non-Life Pricing Models via Transformers",
        "comments": "This preprint manuscript was uploaded to arXiv on November 10, 2023.\n  It has not undergone peer review or any post-submission improvements or\n  corrections. The peer reviewed Version of Record of this article is published\n  in the European Actuarial Journal, and is available online at\n  https://doi.org/10.1007/s13385-024-00388-2",
        "journal-ref": null,
        "doi": "10.1007/s13385-024-00388-2",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Currently, there is a lot of research in the field of neural networks for\nnon-life insurance pricing. The usual goal is to improve the predictive power\nvia neural networks while building upon the generalized linear model, which is\nthe current industry standard. Our paper contributes to this current journey\nvia novel methods to enhance actuarial non-life models with transformer models\nfor tabular data. We build here upon the foundation laid out by the combined\nactuarial neural network as well as the localGLMnet and enhance those models\nvia the feature tokenizer transformer. The manuscript demonstrates the\nperformance of the proposed methods on a real-world claim frequency dataset and\ncompares them with several benchmark models such as generalized linear models,\nfeed-forward neural networks, combined actuarial neural networks, LocalGLMnet,\nand pure feature tokenizer transformer. The paper shows that the new methods\ncan achieve better results than the benchmark models while preserving certain\ngeneralized linear model advantages. The paper also discusses the practical\nimplications and challenges of applying transformer models in actuarial\nsettings.\n"
    },
    {
        "paper_id": 2311.07598,
        "authors": "Moritz Scherrmann",
        "title": "Multi-Label Topic Model for Financial Textual Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a multi-label topic model for financial texts like ad-hoc\nannouncements, 8-K filings, finance related news or annual reports. I train the\nmodel on a new financial multi-label database consisting of 3,044 German ad-hoc\nannouncements that are labeled manually using 20 predefined, economically\nmotivated topics. The best model achieves a macro F1 score of more than 85%.\nTranslating the data results in an English version of the model with similar\nperformance. As application of the model, I investigate differences in stock\nmarket reactions across topics. I find evidence for strong positive or negative\nmarket reactions for some topics, like announcements of new Large Scale\nProjects or Bankruptcy Filings, while I do not observe significant price\neffects for some other topics. Furthermore, in contrast to previous studies,\nthe multi-label structure of the model allows to analyze the effects of\nco-occurring topics on stock market reactions. For many cases, the reaction to\na specific topic depends heavily on the co-occurrence with other topics. For\nexample, if allocated capital from a Seasoned Equity Offering (SEO) is used for\nrestructuring a company in the course of a Bankruptcy Proceeding, the market\nreacts positively on average. However, if that capital is used for covering\nunexpected, additional costs from the development of new drugs, the SEO implies\nnegative reactions on average.\n"
    },
    {
        "paper_id": 2311.07735,
        "authors": "Christopher J. Banks, Katherine Simpson, Nicholas Hanley, Rowland R.\n  Kao",
        "title": "Assessing the potential impact of environmental land management schemes\n  on emergent infection disease risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial incentives are provided by governments to encourage the plantation\nof new woodland to increase habitat, biodiversity, carbon sequestration, and\nother economic benefits for landowners. Whilst these are largely positive\neffects, it is worth considering that greater biodiversity and presence of\nwildlife species in proximity to agricultural holdings may pose a risk of\ndisease transmission between wildlife and livestock. Wildlife transmission and\nthe provision of a reservoir for infectious disease is particularly important\nin the transmission dynamics of bovine tuberculosis.\n  In this paper we develop an economic model for changing land use due to\nforestry subsidies. We use this asses the impact on wild deer populations in\nthe newly created woodland areas and the emergent infectious disease risk\narising from the proximity of new and existing wild deer populations and\nexisting cattle holdings.\n  We consider an area in the South-West of Scotland, having existing woodland,\ndeer populations, and extensive and diverse cattle farm holdings. In this area\nwe find that, with a varying level of subsidy and plausible new woodland\ncreation, the contact risk between areas of wild deer and cattle increases\nbetween 26% and 35% over the contact risk present with zero subsidy.\n  This model provides a foundation for extending to larger regions and for\nexamining potential risk mitigation strategies, for example the targeting of\nsubsidy in low risk areas or provisioning for buffer zones between woodland and\nagricultural holdings.\n"
    },
    {
        "paper_id": 2311.07738,
        "authors": "Ethan Ratliff-Crain, Colin M. Van Oort, James Bagrow, Matthew T. K.\n  Koehler, Brian F. Tivnan",
        "title": "Revisiting Cont's Stylized Facts for Modern Stock Markets",
        "comments": "31 pages, 32 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2001, Rama Cont introduced a now-widely used set of 'stylized facts' to\nsynthesize empirical studies of financial price changes (returns), resulting in\n11 statistical properties common to a large set of assets and markets. These\nproperties are viewed as constraints a model should be able to reproduce in\norder to accurately represent returns in a market. It has not been established\nwhether the characteristics Cont noted in 2001 still hold for modern markets\nfollowing significant regulatory shifts and technological advances. It is also\nnot clear whether a given time series of financial returns for an asset will\nexpress all 11 stylized facts. We test both of these propositions by attempting\nto replicate each of Cont's 11 stylized facts for intraday returns of the\nindividual stocks in the Dow 30, using the same authoritative data as that used\nby the U.S. regulator from October 2018 - March 2019. We find conclusive\nevidence for eight of Cont's original facts and no support for the remaining\nthree. Our study represents the first test of Cont's 11 stylized facts against\na consistent set of stocks, therefore providing insight into how these stylized\nfacts should be viewed in the context of modern stock markets.\n"
    },
    {
        "paper_id": 2311.0792,
        "authors": "Kan Kuno",
        "title": "Dynamic Incentives in Centralized Matching: The Case of Japanese Daycare",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the strategic behavior of applicants in the Japanese\ndaycare market, where waitlisted applicants are granted additional priority\npoints in subsequent application rounds. Utilizing data from Tokyo's Bunkyo\nmunicipality, this paper provides evidence of considerable manipulation, with\nparents strategically choosing to be waitlisted to enhance the likelihood of\ntheir child's admission into more selective daycare centers. I extend the\nstatic framework of school choice posited by Agarwal and Somaini (2018) to\nincorporate dynamic incentives and estimate a structural model that allows for\nreapplication if waitlisted. Empirical findings indicate that approximately 30%\nof applicants forgo listing safer options in their initial application, a\nbehavior significantly pronounced among those who stand to benefit from the\nwaitlist prioritization. Counterfactual simulations, conducted under the\nscenario of no additional waitlist priority, predict a 17.7% decrease in the\nnumber of waitlisted applicants and a 1.2% increase in overall welfare. These\nfindings highlight the profound influence of dynamic incentives on applicant\nbehavior and underscore the necessity for reevaluating current priority\nmechanisms.\n"
    },
    {
        "paper_id": 2311.07936,
        "authors": "Valentin Tissot-Daguette",
        "title": "Occupied Processes: Going with the Flow",
        "comments": "47 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop an It\\^o calculus for functionals of the \"time\" spent by a path at\narbitrary levels. A Markovian setting is recovered by lifting a process $X$\nwith its flow of occupation measures $\\mathcal{O}$ and call the pair\n$(\\mathcal{O},X)$ the occupied process. While the occupation measure erases the\nchronology of the path, we show that our framework still includes many relevant\nproblems in stochastic analysis and financial mathematics. The study of\noccupied processes therefore strikes a middle ground between the\npath-independent case and Dupire's Functional It\\^o Calculus. We extend It\\^o's\nand Feynman-Kac's formula by introducing the occupation derivative, a\nprojection of the functional linear derivative used extensively in mean field\ngames and McKean-Vlasov optimal control. Importantly, we can recast through\nFeynman-Kac's theorem a large class of path-dependent PDEs as parabolic\nproblems where the occupation measure plays the role of time. We apply the\npresent tools to the optimal stopping of spot local time and discuss financial\nexamples including exotic options, corridor variance swaps, and path-dependent\nvolatility.\n"
    },
    {
        "paper_id": 2311.0825,
        "authors": "Sarit Agami",
        "title": "Audit fees in auditor switching",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The auditor work is examining that a company's financial statements\nfaithfully reflect its financial situation. His wage, the audit fees, are not\nfixed among all companies, but can be affected by the financial and structural\ncharacteristics of the company, as well as the characteristics of the firm he\nbelongs to. Another factor that may affect his wage in an auditor switching,\nwhich can be resulted from changes in the company that may influence the fees.\nThis paper examines the effect nature of the auditor switching on his wage, and\nthe factors of the company characteristics and the economy data which determine\nthe wage at switching. A product of the research are tools for predicting and\nevaluating the auditor wage at switching. These tools are important for the\nauditor himself, but also for the company manager to correctly determine the\nwage due to the possibility that the quality of the audit work depends on its\nfees. Two main results are obtained. First, the direction of the wage change in\nthe switching year depends on the economic stability of the economy. Second,\nthe switching effect on the direction and the change size in wage depends on\nthe change size in the company characteristics before and after switching - a\nlarge change versus a stable one. We get that forecasting the change size in\nwage for companies with a larger change is their characteristics is paralleled\nto forecasting a wage increasing. And vice versa, forecasting the change size\nin wage for companies with a stable change in their characteristics is\nparalleled to forecasting a wage decreasing. But, whereas the former can be\nachieved based on the company characteristics and macroeconomics factors, the\npredictably of these characteristics and factors is negligible for the letter.\n"
    },
    {
        "paper_id": 2311.08256,
        "authors": "Abhijit Banerjee and Olivier Compte",
        "title": "Consensus and Disagreement: Information Aggregation under (not so) Naive\n  Learning",
        "comments": "49 pages 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore a model of non-Bayesian information aggregation in networks.\nAgents non-cooperatively choose among Friedkin-Johnsen type aggregation rules\nto maximize payoffs. The DeGroot rule is chosen in equilibrium if and only if\nthere is noiseless information transmission, leading to consensus. With noisy\ntransmission, while some disagreement is inevitable, the optimal choice of rule\namplifies the disagreement: even with little noise, individuals place\nsubstantial weight on their own initial opinion in every period, exacerbating\nthe disagreement. We use this framework to think about equilibrium versus\nsocially efficient choice of rules and its connection to polarization of\nopinions across groups.\n"
    },
    {
        "paper_id": 2311.08289,
        "authors": "Alexandre Pannier",
        "title": "Path-dependent PDEs for volatility derivatives",
        "comments": "28 pages. 1 figure. Section 6, which deals with the Markovian case,\n  was added. Assumption 4.4 was relaxed to cover square-root payoffs",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We regard options on VIX and Realised Variance as solutions to path-dependent\npartial differential equations (PDEs) in a continuous stochastic volatility\nmodel. The modeling assumption specifies that the instantaneous variance is a\n$C^3$ function of a multidimensional Gaussian Volterra process; this includes a\nlarge class of models suggested for the purpose of VIX option pricing, either\nrough, or not, or mixed. We unveil the path-dependence of those volatility\nderivatives and, under a regularity hypothesis on the payoff function, we prove\nthe well-posedness of the associated PDE. The latter is of heat type, because\nof the Gaussian assumption, and the terminal condition is also path-dependent.\nFurthermore, formulae for the greeks are provided, the implied volatility is\nshown to satisfy a quasi-linear path-dependent PDE and, in Markovian models,\nfinite-dimensional pricing PDEs are obtained for VIX options.\n"
    },
    {
        "paper_id": 2311.08533,
        "authors": "Ixandra Achitouv, Dragos Gorduza and Antoine Jacquier",
        "title": "Natural Language Processing for Financial Regulation",
        "comments": "20 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.\n"
    },
    {
        "paper_id": 2311.08617,
        "authors": "Anas Azzabi and Younes Lahrichi",
        "title": "Bank Performance Determinants: State of the Art and Future Research\n  Avenues",
        "comments": null,
        "journal-ref": "New Challenges in Accounting and Finance. Volume 9. Pages 26-41\n  (2023)",
        "doi": "10.32038/NCAF.2023.09.03",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Banks' performance is an important topic for both professionals and\nresearchers. Given the important literature on this subject, this paper aims to\nbring an up-to-date and organized review of literature on the determinants of\nbanks performance. This paper discusses the main approaches that molded the\ndebate on banks performance and their main determinants. An in-depth\nunderstanding of these latter may allow on the one hand, bank managers and\nregulators to improve the sector efficiency and to deal with the new trends\nshaping the future of their industry and on the other hand, academicians to\nenrich research and knowledge on this field. Through the analysis of 54 studies\npublished in 42 peer-reviewed journals, we show that despite the importance of\nthe existent literature, the subject of bank performance factors did not reveal\nall its secrets and still constitute a fertile field for critical debates,\nespecially since the COVID-19 and the increasingly pressing rise in power of\ndigital transformation and artificial intelligence in general and FinTechs in\nparticular. The study concludes by suggesting new promising research avenues.\n"
    },
    {
        "paper_id": 2311.0865,
        "authors": "Takeshi Fukasawa",
        "title": "The Use of Symmetry for Models with Variable-size Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper shows the universal representations of symmetric functions with\nmultidimensional variable-size variables, which help assessing the\njustification of approximation methods aggregating the information of each\nvariable by moments. It then discusses how the results give insights into\neconomic applications, including two-step policy function estimation,\nmoment-based Markov equilibrium, and aggregative games.\n"
    },
    {
        "paper_id": 2311.08671,
        "authors": "Narges Ramezani, Erfan Mohammadi",
        "title": "Managing Biotechnology and Healthcare Innovation Challenges and\n  Opportunities for Startups and Small Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The biotechnology industry poses challenges and possibilities for startups\nand small businesses. It is characterized by high charges and complex policies,\nmaking it difficult for such agencies to set up themselves. However, it\nadditionally offers avenues for innovation and increase. This paper delves into\npowerful techniques that can be a resource in managing biotechnology\ninnovation, which includes partnerships, highbrow assets improvement, virtual\ntechnologies, customer engagement, and government investment. These strategies\nare important for fulfillment in an industry that is constantly evolving. By\nembracing agility and area of interest focus, startups and small companies can\nsuccessfully compete in this dynamic discipline.\n"
    },
    {
        "paper_id": 2311.08678,
        "authors": "Mehrnaz Kouhihabibi, Erfan Mohammadi",
        "title": "The Future of Sustainability in Germany: Areas for Improvement and\n  Innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper reviews the literature on biodegradable waste management in\nGermany, a multifaceted endeavor that reflects its commitment to sustainability\nand environmental responsibility. It examines the processes and benefits of\nseparate collection, recycling, and eco-friendly utilization of biodegradable\nmaterials, which produce valuable byproducts such as compost, digestate, and\nbiogas. These byproducts serve as organic fertilizers, peat substitutes, and\nrenewable energy sources, contributing to ecological preservation and economic\nprudence. The paper also discusses the global implications of biodegradable\nwaste management, such as preventing methane emissions from landfills, a major\nsource of greenhouse gas, and reusing organic matter and essential nutrients.\nMoreover, the paper explores how biodegradable waste management reduces waste\nvolume, facilitates waste sorting and incineration, and sets a global example\nfor addressing climate change and working toward a sustainable future. The\npaper highlights the importance of a comprehensive and holistic approach to\nsustainability that encompasses waste management, renewable energy,\ntransportation, agriculture, waste reduction, and urban development.\n"
    },
    {
        "paper_id": 2311.08826,
        "authors": "Akihiro Kaneko",
        "title": "Multi-stage Euler-Maruyama methods for backward stochastic differential\n  equations driven by continuous-time Markov chains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Numerical methods for computing the solutions of Markov backward stochastic\ndifferential equations (BSDEs) driven by continuous-time Markov chains (CTMCs)\nare explored. The main contributions of this paper are as follows: (1) we\nobserve that Euler-Maruyama temporal discretization methods for solving Markov\nBSDEs driven by CTMCs are equivalent to exponential integrators for solving the\nassociated systems of ordinary differential equations (ODEs); (2) we introduce\nmulti-stage Euler-Maruyama methods for effectively solving \"stiff\" Markov BSDEs\ndriven by CTMCs; these BSDEs typically arise from the spatial discretization of\nMarkov BSDEs driven by Brownian motion; (3) we propose a multilevel spatial\ndiscretization method on sparse grids that efficiently approximates\nhigh-dimensional Markov BSDEs driven by Brownian motion with a combination of\nmultiple Markov BSDEs driven by CTMCs on grids with different resolutions. We\nalso illustrate the effectiveness of the presented methods with a number of\nnumerical experiments in which we treat nonlinear BSDEs arising from option\npricing problems in finance.\n"
    },
    {
        "paper_id": 2311.0883,
        "authors": "Timur Gareev, Irina Peker",
        "title": "Quantity versus quality in publication activity: knowledge production at\n  the regional level",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study contributes to the ongoing debate regarding the balance between\nquality and quantity in research productivity and publication activity. Using\nempirical regional knowledge production functions, we establish a significant\ncorrelation between R&D spending and research output, specifically publication\nproductivity, while controlling for patenting activity and socioeconomic\nfactors. Our focus is on the dilemma of research quantity versus quality, which\nis analysed in the context of regional thematic specialization using spatial\nlags. When designing policies and making forecasts, it is important to consider\nthe quality of research measured by established indicators. In this study, we\nexamine the dual effect of research quality on publication activity. We\nidentify two groups of quality factors: those related to the quality of\njournals and those related to the impact of publications. On average, these\nfactors have different influences on quantitative measures. The quality of\njournals shows a negative relationship with quantity, indicating that as\njournal quality increases, the number of publications decreases. On the other\nhand, the impact of publications can be approximated by an inverse parabolic\nshape, with a positive decreasing slope within a common range of values. This\nduality in the relationship between quality factors and quantitative measures\nmay explain some of the significant variations in conclusions found in the\nliterature. We compare several models that explore factors influencing\npublication activity using a balanced panel dataset of Russian regions from\n2009 to 2021. Additionally, we propose a novel approach using thematic\nscientometric parameters as a special type of proximity measure between regions\nin thematic space. Incorporating spatial spillovers in thematic space allows us\nto account for potential cross-sectional dependence in regional data.\n"
    },
    {
        "paper_id": 2311.08847,
        "authors": "Meriam El Mansour and Emmanuel Lepinette",
        "title": "Robust discrete-time super-hedging strategies under AIP condition and\n  under price uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve the problem of super-hedging European or Asian options for\ndiscrete-time financial market models where executable prices are uncertain.\nThe risky asset prices are not described by single-valued processes but\nmeasurable selections of random sets that allows to consider a large variety of\nmodels including bid-ask models with order books, but also models with a delay\nin the execution of the orders. We provide a numerical procedure to compute the\ninfimum price under a weak no-arbitrage condition, the so-called AIP condition,\nunder which the prices of the non negative European options are non negative.\nThis condition is weaker than the existence of a risk-neutral martingale\nmeasure but it is sufficient to numerically solve the super-hedging problem. We\nillustrate our method by a numerical example.\n"
    },
    {
        "paper_id": 2311.08871,
        "authors": "Dorsaf Cherif, Meriam El Mansour and Emmanuel Lepinette",
        "title": "A short note on super-hedging an arbitrary number of European options\n  with integer-valued strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The usual theory of asset pricing in finance assumes that the financial\nstrategies, i.e. the quantity of risky assets to invest, are real-valued so\nthat they are not integer-valued in general, see the Black and Scholes model\nfor instance. This is clearly contrary to what it is possible to do in the real\nworld. Surprisingly, it seems that there is no many contributions in that\ndirection in the literature, except for a finite number of states. In this\npaper, for arbitrary {\\Omega}, we show that, in discrete-time, it is possible\nto evaluate the minimal super-hedging price when we restrict ourselves to\ninteger-valued strategies. To do so, we only consider terminal claims that are\ncontinuous piecewise affine functions of the underlying asset. We formulate a\ndynamic programming principle that can be directly implemented on an historical\ndata and which also provides the optimal integer-valued strategy. The problem\nwith general payoffs remains open but should be solved with the same approach.\n"
    },
    {
        "paper_id": 2311.08929,
        "authors": "Nkosingizwile Mazwi Mchunu, George Okechukwu Onatu, Trynos Gumbo",
        "title": "The impact of Electricity Blackouts and poor infrastructure on the\n  livelihood of residents and the local economy of City of Johannesburg, South\n  Africa",
        "comments": "13 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper discusses the impact of electricity blackouts and poor\ninfrastructure on the livelihood of residents and the local economy of\nJohannesburg, South Africa. The importance of a stable electricity grid plays a\nvital role in the effective functioning of urban infrastructure and the\neconomy. The importance of electricity in the present-day South Africa has not\nbeen emphasized enough to be prioritized at all levels of government,\nespecially at the local level, as it is where all socio-economic activities\ntake place. The new South Africa needs to redefine the importance of\nelectricity by ensuring that it is accessible, affordable, and produced\nsustainably, and most of all, by ensuring that the energy transition\ninitiatives to green energy take place in a planned manner without causing harm\nto the economy, which might deepen the plight of South Africans. Currently, the\nCity of Johannesburg is a growing spatial entity in both demographic and\nurbanization terms, and growing urban spaces require a stable supply of\nelectricity for the proper functioning of urban systems and the growth of the\nlocal economy. The growth of the city brings about a massive demand for\nelectricity that outstrips the current supply of electricity available on the\nlocal grid. The imbalance in the current supply and growing demand for\nelectricity result in energy blackouts in the city, which have ripple effects\non the economy and livelihoods of the people of Johannesburg. This paper\nexamines the impact of electricity blackouts and poor infrastructure on the\nlivelihood of residents and the local economy of Johannesburg, South Africa.\n"
    },
    {
        "paper_id": 2311.09148,
        "authors": "Reza Yarbakhsh, Mahdieh Soleymani Baghshah, Hamidreza Karimaghaie",
        "title": "Predicting risk/reward ratio in financial markets for asset management\n  using machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Financial market forecasting remains a formidable challenge despite the surge\nin computational capabilities and machine learning advancements. While numerous\nstudies have underscored the precision of computer-generated market\npredictions, many of these forecasts fail to yield profitable trading outcomes.\nThis discrepancy often arises from the unpredictable nature of profit and loss\nratios in the event of successful and unsuccessful predictions. In this study,\nwe introduce a novel algorithm specifically designed for forecasting the profit\nand loss outcomes of trading activities. This is further augmented by an\ninnovative approach for integrating these forecasts with previous predictions\nof market trends. This approach is designed for algorithmic trading, enabling\ntraders to assess the profitability of each trade and calibrate the optimal\ntrade size. Our findings indicate that this method significantly improves the\nperformance of traditional trading strategies as well as algorithmic trading\nsystems, offering a promising avenue for enhancing trading decisions.\n"
    },
    {
        "paper_id": 2311.09429,
        "authors": "Edilson Pereira Junior",
        "title": "New configurations of the interface between innovation and urban spatial\n  agglomeration: the localized industrial systems (lis) of the clothing in\n  Fortaleza/Brazil",
        "comments": "16 pages, 2 figures, 2 tabels",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper seeks to interpret the interface between innovation, industry and\nurban business agglomeration, by trying to understand how local/regional\nentrepreneurs and social agents use the forces of agglomeration to organize\nproductive activities with a view to achieving greater competitiveness and\nstrategic advantages. There are many territorial manifestations that\nmaterialize from this new reality and the text seeks to propose a reading of\nits characteristics based on what we call \"productive spatial configurations\",\nwhich represent the specific functioning of a certain innovative production\nprocess and its territorial impact. To illustrate this approach, we take as an\nexample a case study that illustrates how productive spatial configurations\nmanifest themselves through the revitalization of an industrial economy that\nincorporates innovative efforts, whether technological, process or\norganizational. This is the localized industrial system (LIS) of clothing and\napparel in Fortaleza, Cear\\'a state, Brazil, which reveals an industrial\nexperience of resistance with significant organizational innovation in the\nproduction and distribution processes of clothing. The main objective of the\nproposal is to organize theoretical and empirical tools that will allow us to\nread the combination of economic, social and political variables in spaces\nwhere collaborative networks of companies, service centers and public\ninstitutions flourish, in the context of various industrial production\nprocesses. This could point to the progress we need to overcome the many false\ncontroversies on the subject.\n"
    },
    {
        "paper_id": 2311.09432,
        "authors": "Denise Cristina Bomtempo",
        "title": "Urban economics of migration in the cities of the northeast region of\n  Brazil",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The focus of this text is to discuss how geographical science can contribute\nto an understanding of international migration in the 21st century. To this\nend, in the introduction we present the central ideas, as well as the internal\nstructure of the text. Then, we present the theoretical and methodological\napproach that guides the research and, in section three, we show the results\nthrough texts and cartograms based on secondary data and empirical information.\nFinally, in the final remarks, we summarize the text with a view to\ncontributing to the proposed debate.\n"
    },
    {
        "paper_id": 2311.09885,
        "authors": "Ksenia Panidi, Yaroslava Grebenschikova, Vasily Klucharev",
        "title": "The efficacy of the sugar-free labels is reduced by the health-sweetness\n  tradeoff",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the present study, we use an experimental setting to explore the effects\nof sugar-free labels on the willingness to pay for food products. In our\nexperiment, participants placed bids for sugar-containing and analogous\nsugar-free products in a Becker-deGroot-Marschak auction to determine the\nwillingness to pay. Additionally, they rated each product on the level of\nperceived healthiness, sweetness, tastiness and familiarity with the product.\nWe then used structural equation modelling to estimate the direct, indirect and\ntotal effect of the label on the willingness to pay. The results suggest that\nsugar-free labels significantly increase the willingness to pay due to the\nperception of sugar-free products as healthier than sugar-containing ones.\nHowever, this positive effect is overridden by a significant decrease in\nperceived sweetness (and hence, tastiness) of products labelled as sugar-free\ncompared to sugar-containing products. As in our sample, healthiness and\ntastiness are positively related, while healthiness and sweetness are related\nnegatively, these results suggest that it is health-sweetness rather than\nhealth-tastiness tradeoff that decreases the efficiency of the sugar-free\nlabelling in nudging consumers towards healthier options.\n"
    },
    {
        "paper_id": 2311.10021,
        "authors": "Sascha Desmettre, Sebastian Merkel, Annalena Mickel and Alexander\n  Steinicke",
        "title": "Worst-Case Optimal Investment in Incomplete Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study and solve the worst-case optimal portfolio problem as pioneered by\nKorn and Wilmott (2002) of an investor with logarithmic preferences facing the\npossibility of a market crash with stochastic market coefficients by enhancing\nthe martingale approach developed by Seifried in 2010. With the help of\nbackward stochastic differential equations (BSDEs), we are able to characterize\nthe resulting indifference optimal strategies in a fairly general setting. We\nalso deal with the question of existence of those indifference strategies for\nmarket models with an unbounded market price of risk. We therefore solve the\ncorresponding BSDEs via solving their associated PDEs using a utility\ncrash-exposure transformation. Our approach is subsequently demonstrated for\nHeston's stochastic volatility model, Bates' stochastic volatility model\nincluding jumps, and Kim-Omberg's model for a stochastic excess return.\n"
    },
    {
        "paper_id": 2311.10191,
        "authors": "Jean-Fran\\c{c}ois Renaud, Alexandre Roch, Clarence Simard",
        "title": "An optimization dichotomy for capital injections and absolutely\n  continuous dividend strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an optimal stochastic control problem in which a firm's\ncash/surplus process is controlled by dividend payments and capital injections.\nStockholders aim to maximize their dividend stream minus the cost of injecting\ncapital, if needed. We consider absolutely continuous dividend policies subject\nto a level-dependent upper bound on the dividend rate while we allow for\ngeneral capital injections behavior. We prove that the optimal strategy can\nonly be of two types: dividends are paid according to a \\textit{mean-reverting}\nstrategy with capital injections performed each time the cash process reaches\nzero; or, dividends are paid according to another \\textit{mean-reverting}\nstrategy and no injection of capital is ever made, until ruin is reached. We\ngive a complete solution to this problem and characterize this dichotomy by\ncomparing (the derivatives of) the value functions at zero of two sub-problems.\nThe first sub-problem is concerned solely with the maximization of dividends,\nwhile the second sub-problem is the corresponding bail-out optimal dividend\nproblem for which we provide also a complete solution.\n"
    },
    {
        "paper_id": 2311.1021,
        "authors": "Melvyn Li, Kaili Wang, Yicong Liu, Khandker Nurul Habib",
        "title": "Deriving Weeklong Activity-Travel Dairy from Google Location History:\n  Survey Tool Development and A Field Test in Toronto",
        "comments": "The manuscript has been accepted for presentation at the 103rd\n  Transportation Research Board (TRB) Annual Meeting, January 7-11, 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces an innovative travel survey methodology that utilizes\nGoogle Location History (GLH) data to generate travel diaries for\ntransportation demand analysis. By leveraging the accuracy and omnipresence\namong smartphone users of GLH, the proposed methodology avoids the need for\nproprietary GPS tracking applications to collect smartphone-based GPS data.\nThis research enhanced an existing travel survey designer, Travel Activity\nInternet Survey Interface (TRAISI), to make it capable of deriving travel\ndiaries from the respondents' GLH. The feasibility of this data collection\napproach is showcased through the Google Timeline Travel Survey (GTTS)\nconducted in the Greater Toronto Area, Canada. The resultant dataset from the\nGTTS is demographically representative and offers detailed and accurate travel\nbehavioural insights.\n"
    },
    {
        "paper_id": 2311.1052,
        "authors": "Davide Fiaschi, Angela Parenti, Cristiano Ricci",
        "title": "Unveiling spatial patterns of population in Italian municipalities",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the evolution of population density across Italian municipalities on\nthe based of their trajectories in the Moran space. We find evidence of spatial\ndynamical patterns of concentrated urban growth, urban sprawl, agglomeration,\nand depopulation. Over the long run, three distinct settlement systems emerge:\nurban, suburban, and rural. We discuss how estimating these demographic trends\nat the municipal level can help the design and validation of policies\ncontrasting the socio-economic decline in specific Italian areas, as in the\ncase of the Italian National Strategy for Inner Areas (Strategia Nazionale per\nle Aree Interne, SNAI).\n"
    },
    {
        "paper_id": 2311.10685,
        "authors": "Andrew Y. Chen and Chukwuma Dim",
        "title": "High-Throughput Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use empirical Bayes (EB) to mine data on 140,000 long-short strategies\nconstructed from accounting ratios, past returns, and ticker symbols. This\n\"high-throughput asset pricing\" produces out-of-sample performance comparable\nto strategies in top finance journals. But unlike the published strategies, the\ndata-mined strategies are free of look-ahead bias. EB predicts that high\nreturns are concentrated in accounting strategies, small stocks, and pre-2004\nsamples, consistent with limited attention theories. The intuition is seen in\nthe cross-sectional distribution of t-stats, which is far from the null for\nequal-weighted accounting strategies. High-throughput methods provide a\nrigorous, unbiased method for documenting asset pricing facts.\n"
    },
    {
        "paper_id": 2311.10713,
        "authors": "Johannes Ruf",
        "title": "Diversifying an Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In July 2023, Nasdaq announced a `Special Rebalance' of the Nasdaq-100 index\nto reduce the index weights of its large constituents. A rebalance as suggested\ncurrently by Nasdaq index methodology may have several undesirable effects.\nThese effects can be avoided by a different, but simple rebalancing strategy.\nSuch rebalancing is easily computable and guarantees (a) that the maximum\noverall index weight does not increase through the rebalancing and (b) that the\norder of index weights is preserved.\n"
    },
    {
        "paper_id": 2311.10716,
        "authors": "Ion Pohoata, Delia-Elena Diaconasu and Ioana Negru",
        "title": "The independence of Central Banks, a reductio ad impossibile",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper testifies to the fact that the independence of the Central Banks,\nas stated by its founding fathers, is nothing more than a chimera. We\ndemonstrate that the hypothesis inflation is a purely monetary phenomenon does\nnot support the plea for independence. Moreover, we show that the conservative\ncentral banker, the imaginary Principal-Agent contract, the alleged financial\nautonomy, just like the ban on budgetary financing, are all arguments that lack\nlogic. We equally show that the idea of independence is not convincing because\nits operational toolbox, as well as the system of rules it relies on, lack\nwell-defined outlines.\n"
    },
    {
        "paper_id": 2311.10717,
        "authors": "Ravi Kashyap",
        "title": "Arguably Adequate Aqueduct Algorithm: Crossing A Bridge-Less Block-Chain\n  Chasm",
        "comments": null,
        "journal-ref": "Finance Research Letters, September 2023, 104421",
        "doi": "10.1016/j.frl.2023.104421",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of being a cross-chain wealth management platform\nwith deposits, redemptions and investment assets across multiple networks. We\ndiscuss the need for blockchain bridges to facilitates fund flows across\nplatforms. We point out several issues with existing bridges. We develop an\nalgorithm - tailored to overcome current constraints - that dynamically changes\nthe utilization of bridge capacities and hence the amounts to be transferred\nacross networks. We illustrate several scenarios using numerical simulations.\n"
    },
    {
        "paper_id": 2311.10718,
        "authors": "Soumyadip Sarkar",
        "title": "Harnessing Deep Q-Learning for Enhanced Statistical Arbitrage in\n  High-Frequency Trading: A Comprehensive Exploration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The realm of High-Frequency Trading (HFT) is characterized by rapid\ndecision-making processes that capitalize on fleeting market inefficiencies. As\nthe financial markets become increasingly competitive, there is a pressing need\nfor innovative strategies that can adapt and evolve with changing market\ndynamics. Enter Reinforcement Learning (RL), a branch of machine learning where\nagents learn by interacting with their environment, making it an intriguing\ncandidate for HFT applications. This paper dives deep into the integration of\nRL in statistical arbitrage strategies tailored for HFT scenarios. By\nleveraging the adaptive learning capabilities of RL, we explore its potential\nto unearth patterns and devise trading strategies that traditional methods\nmight overlook. We delve into the intricate exploration-exploitation trade-offs\ninherent in RL and how they manifest in the volatile world of HFT. Furthermore,\nwe confront the challenges of applying RL in non-stationary environments,\ntypical of financial markets, and investigate methodologies to mitigate\nassociated risks. Through extensive simulations and backtests, our research\nreveals that RL not only enhances the adaptability of trading strategies but\nalso shows promise in improving profitability metrics and risk-adjusted\nreturns. This paper, therefore, positions RL as a pivotal tool for the next\ngeneration of HFT-based statistical arbitrage, offering insights for both\nresearchers and practitioners in the field.\n"
    },
    {
        "paper_id": 2311.10719,
        "authors": "Jiahao Chen, Xiaofei Li",
        "title": "Analysis of frequent trading effects of various machine learning models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, high-frequency trading has emerged as a crucial strategy in\nstock trading. This study aims to develop an advanced high-frequency trading\nalgorithm and compare the performance of three different mathematical models:\nthe combination of the cross-entropy loss function and the quasi-Newton\nalgorithm, the FCNN model, and the vector machine. The proposed algorithm\nemploys neural network predictions to generate trading signals and execute buy\nand sell operations based on specific conditions. By harnessing the power of\nneural networks, the algorithm enhances the accuracy and reliability of the\ntrading strategy. To assess the effectiveness of the algorithm, the study\nevaluates the performance of the three mathematical models. The combination of\nthe cross-entropy loss function and the quasi-Newton algorithm is a widely\nutilized logistic regression approach. The FCNN model, on the other hand, is a\ndeep learning algorithm that can extract and classify features from stock data.\nMeanwhile, the vector machine is a supervised learning algorithm recognized for\nachieving improved classification results by mapping data into high-dimensional\nspaces. By comparing the performance of these three models, the study aims to\ndetermine the most effective approach for high-frequency trading. This research\nmakes a valuable contribution by introducing a novel methodology for\nhigh-frequency trading, thereby providing investors with a more accurate and\nreliable stock trading strategy.\n"
    },
    {
        "paper_id": 2311.1072,
        "authors": "Qin Wang, Guangsheng Yu, Shiping Chen",
        "title": "Cryptocurrency in the Aftermath: Unveiling the Impact of the SVB\n  Collapse",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we explore the aftermath of the Silicon Valley Bank (SVB)\ncollapse, with a particular focus on its impact on crypto markets. We conduct a\nmulti-dimensional investigation, which includes a factual summary, analysis of\nuser sentiment, and examination of market performance. Based on such efforts,\nwe uncover a somewhat counterintuitive finding: the SVB collapse did not lead\nto the destruction of cryptocurrencies; instead, they displayed resilience.\n"
    },
    {
        "paper_id": 2311.10723,
        "authors": "Yinheng Li, Shaofei Wang, Han Ding, Hang Chen",
        "title": "Large Language Models in Finance: A Survey",
        "comments": "Accepted by 4th ACM International Conference on AI in Finance\n  (ICAIF-23) https://ai-finance.org",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent advances in large language models (LLMs) have opened new possibilities\nfor artificial intelligence applications in finance. In this paper, we provide\na practical survey focused on two key aspects of utilizing LLMs for financial\ntasks: existing solutions and guidance for adoption.\n  First, we review current approaches employing LLMs in finance, including\nleveraging pretrained models via zero-shot or few-shot learning, fine-tuning on\ndomain-specific data, and training custom LLMs from scratch. We summarize key\nmodels and evaluate their performance improvements on financial natural\nlanguage processing tasks.\n  Second, we propose a decision framework to guide financial professionals in\nselecting the appropriate LLM solution based on their use case constraints\naround data, compute, and performance needs. The framework provides a pathway\nfrom lightweight experimentation to heavy investment in customized LLMs.\n  Lastly, we discuss limitations and challenges around leveraging LLMs in\nfinancial applications. Overall, this survey aims to synthesize the\nstate-of-the-art and provide a roadmap for responsibly applying LLMs to advance\nfinancial AI.\n"
    },
    {
        "paper_id": 2311.10738,
        "authors": "Andres M. Alonso and Zehang Li",
        "title": "Approximation of supply curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this note, we illustrate the computation of the approximation of the\nsupply curves using a one-step basis. We derive the expression for the L2\napproximation and propose a procedure for the selection of nodes of the\napproximation. We illustrate the use of this approach with three large sets of\nbid curves from European electricity markets.\n"
    },
    {
        "paper_id": 2311.10739,
        "authors": "Mohammadreza Mahmoudi",
        "title": "Examining the Effect of Monetary Policy and Monetary Policy Uncertainty\n  on Cryptocurrencies Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the influence of monetary policy and monetary policy\nuncertainties on Bitcoin returns, utilizing monthly data of BTC, and MPU from\nJuly 2010 to August 2023, and employing the Markov Switching Means VAR\n(MSM-VAR) method. The findings reveal that Bitcoin returns can be categorized\ninto two distinct regimes: 1) regime 1 with low volatility, and 2) regime 2\nwith high volatility. In both regimes, an increase in MPU leads to a decline in\nBitcoin returns: -0.028 in regime 1 and -0.44 in regime 2. This indicates that\nmonetary policy uncertainty exerts a negative influence on Bitcoin returns\nduring both downturns and upswings. Furthermore, the study explores Bitcoin's\nsensitivity to Federal Open Market Committee (FOMC) decisions.\n"
    },
    {
        "paper_id": 2311.10742,
        "authors": "Manuel Woersdoerfer",
        "title": "AI Ethics and Ordoliberalism 2.0: Towards A 'Digital Bill of Rights'",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article analyzes AI ethics from a distinct business ethics perspective,\ni.e., 'ordoliberalism 2.0.' It argues that the ongoing discourse on\n(generative) AI relies too much on corporate self-regulation and voluntary\ncodes of conduct and thus lacks adequate governance mechanisms. To address\nthese issues, the paper suggests not only introducing hard-law legislation with\na more effective oversight structure but also merging already existing AI\nguidelines with an ordoliberal-inspired regulatory and competition policy.\nHowever, this link between AI ethics, regulation, and antitrust is not yet\nadequately discussed in the academic literature and beyond. The paper thus\ncloses a significant gap in the academic literature and adds to the\npredominantly legal-political and philosophical discourse on AI governance. The\npaper's research questions and goals are twofold: First, it identifies\nordoliberal-inspired AI ethics principles that could serve as the foundation\nfor a 'digital bill of rights.' Second, it shows how those principles could be\nimplemented at the macro level with the help of ordoliberal competition and\nregulatory policy.\n"
    },
    {
        "paper_id": 2311.10756,
        "authors": "Moritz Scherrmann, Ralf Elsas",
        "title": "Earnings Prediction Using Recurrent Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Firm disclosures about future prospects are crucial for corporate valuation\nand compliance with global regulations, such as the EU's MAR and the US's SEC\nRule 10b-5 and RegFD. To comply with disclosure obligations, issuers must\nidentify nonpublic information with potential material impact on security\nprices as only new, relevant and unexpected information materially affects\nprices in efficient markets. Financial analysts, assumed to represent public\nknowledge on firms' earnings prospects, face limitations in offering\ncomprehensive coverage and unbiased estimates. This study develops a neural\nnetwork to forecast future firm earnings, using four decades of financial data,\naddressing analysts' coverage gaps and potentially revealing hidden insights.\nThe model avoids selectivity and survivorship biases as it allows for missing\ndata. Furthermore, the model is able to produce both fiscal-year-end and\nquarterly earnings predictions. Its performance surpasses benchmark models from\nthe academic literature by a wide margin and outperforms analysts' forecasts\nfor fiscal-year-end earnings predictions.\n"
    },
    {
        "paper_id": 2311.10759,
        "authors": "Xitai Yu",
        "title": "Application Research of Spline Interpolation and ARIMA in the Field of\n  Stock Market Forecasting",
        "comments": "13 pages,12 figures,5 tables,3377 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ARIMA (Autoregressive Integrated Moving Average model) has extensive\napplications in the field of time series forecasting. However, the predictive\nperformance of the ARIMA model is limited when dealing with data gaps or\nsignificant noise. Based on previous research, we have found that cubic spline\ninterpolation performs well in capturing the smooth changes of stock price\ncurves, especially when the market trends are relatively stable. Therefore,\nthis paper integrates the two approaches by taking the time series data in\nstock trading as an example, establishes a time series forecasting model based\non cubic spline interpolation and ARIMA. Through validation, the model has\ndemonstrated certain guidance and reference value for short-term time series\nforecasting.\n"
    },
    {
        "paper_id": 2311.10799,
        "authors": "Minati Rath, Hema Date",
        "title": "Adaptive Modelling Approach for Row-Type Dependent Predictive Analysis\n  (RTDPA): A Framework for Designing Machine Learning Models for Credit Risk\n  Analysis in Banking Sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In many real-world datasets, rows may have distinct characteristics and\nrequire different modeling approaches for accurate predictions. In this paper,\nwe propose an adaptive modeling approach for row-type dependent predictive\nanalysis(RTDPA). Our framework enables the development of models that can\neffectively handle diverse row types within a single dataset. Our dataset from\nXXX bank contains two different risk categories, personal loan and agriculture\nloan. each of them are categorised into four classes standard, sub-standard,\ndoubtful and loss. We performed tailored data pre processing and feature\nengineering to different row types. We selected traditional machine learning\npredictive models and advanced ensemble techniques. Our findings indicate that\nall predictive approaches consistently achieve a precision rate of no less than\n90%. For RTDPA, the algorithms are applied separately for each row type,\nallowing the models to capture the specific patterns and characteristics of\neach row type. This approach enables targeted predictions based on the row\ntype, providing a more accurate and tailored classification for the given\ndataset.Additionally, the suggested model consistently offers decision makers\nvaluable and enduring insights that are strategic in nature in banking sector.\n"
    },
    {
        "paper_id": 2311.10801,
        "authors": "Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao\n  Song, Xinrun Wang, Bo An",
        "title": "Reinforcement Learning with Maskable Stock Representation for Portfolio\n  Management in Customizable Stock Pools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio management (PM) is a fundamental financial trading task, which\nexplores the optimal periodical reallocation of capitals into different stocks\nto pursue long-term profits. Reinforcement learning (RL) has recently shown its\npotential to train profitable agents for PM through interacting with financial\nmarkets. However, existing work mostly focuses on fixed stock pools, which is\ninconsistent with investors' practical demand. Specifically, the target stock\npool of different investors varies dramatically due to their discrepancy on\nmarket states and individual investors may temporally adjust stocks they desire\nto trade (e.g., adding one popular stocks), which lead to customizable stock\npools (CSPs). Existing RL methods require to retrain RL agents even with a tiny\nchange of the stock pool, which leads to high computational cost and unstable\nperformance. To tackle this challenge, we propose EarnMore, a rEinforcement\nleARNing framework with Maskable stOck REpresentation to handle PM with CSPs\nthrough one-shot training in a global stock pool (GSP). Specifically, we first\nintroduce a mechanism to mask out the representation of the stocks outside the\ntarget pool. Second, we learn meaningful stock representations through a\nself-supervised masking and reconstruction process. Third, a re-weighting\nmechanism is designed to make the portfolio concentrate on favorable stocks and\nneglect the stocks outside the target pool. Through extensive experiments on 8\nsubset stock pools of the US stock market, we demonstrate that EarnMore\nsignificantly outperforms 14 state-of-the-art baselines in terms of 6 popular\nfinancial metrics with over 40% improvement on profit.\n"
    },
    {
        "paper_id": 2311.10831,
        "authors": "Hector Galindo-Silva, Guy Tchuente",
        "title": "Religious Competition, Culture and Domestic Violence: Evidence from\n  Colombia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies how religious competition, as measured by the emergence of\nreligious organizations with innovative worship styles and cultural practices,\nimpacts domestic violence. Using data from Colombia, the study estimates a\ntwo-way fixed-effects model and reveals that the establishment of the first\nnon-Catholic church in a predominantly Catholic municipality leads to a\nsignificant decrease in reported cases of domestic violence. This effect\npersists in the long run, indicating that religious competition introduces\nvalues and practices that discourage domestic violence, such as household\nstability and reduced male dominance. Additionally, the effect is more\npronounced in municipalities with less clustered social networks, suggesting\nthe diffusion of these values and practices through social connections. This\nresearch contributes to the understanding of how culture influences domestic\nviolence, emphasizing the role of religious competition as a catalyst for\ncultural change.\n"
    },
    {
        "paper_id": 2311.10861,
        "authors": "Marc J. Pfeiffer",
        "title": "First, Do No Harm: Algorithms, AI, and Digital Product Liability",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The ethical imperative for technology should be first, do no harm. But\ndigital innovations like AI and social media increasingly enable societal\nharms, from bias to misinformation. As these technologies grow ubiquitous, we\nneed solutions to address unintended consequences. This report proposes a model\nto incentivize developers to prevent foreseeable algorithmic harms. It does\nthis by expanding negligence and product liability laws. Digital product\ndevelopers would be incentivized to mitigate potential algorithmic risks before\ndeployment to protect themselves and investors. Standards and penalties would\nbe set proportional to harm. Insurers would require harm mitigation during\ndevelopment in order to obtain coverage. This shifts tech ethics from move fast\nand break things to first, do no harm. Details would need careful refinement\nbetween stakeholders to enact reasonable guardrails without stifling\ninnovation. Policy and harm prevention frameworks would likely evolve over\ntime. Similar accountability schemes have helped address workplace,\nenvironmental, and product safety. Introducing algorithmic harm negligence\nliability would acknowledge the real societal costs of unethical tech. The\ntiming is right for reform. This proposal provides a model to steer the digital\nrevolution toward human rights and dignity. Harm prevention must be prioritized\nover reckless growth. Vigorous liability policies are essential to stop\ntechnologists from breaking things\n"
    },
    {
        "paper_id": 2311.10935,
        "authors": "Leonard Mushunje, Maxwell Mashasha and Edina Chandiwana",
        "title": "Short-term Volatility Estimation for High Frequency Trades using\n  Gaussian processes (GPs)",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fundamental theorem behind financial markets is that stock prices are\nintrinsically complex and stochastic. One of the complexities is the volatility\nassociated with stock prices. Volatility is a tendency for prices to change\nunexpectedly [1]. Price volatility is often detrimental to the return\neconomics, and thus, investors should factor it in whenever making investment\ndecisions, choices, and temporal or permanent moves. It is, therefore, crucial\nto make necessary and regular short and long-term stock price volatility\nforecasts for the safety and economics of investors returns. These forecasts\nshould be accurate and not misleading. Different models and methods, such as\nARCH GARCH models, have been intuitively implemented to make such forecasts.\nHowever, such traditional means fail to capture the short-term volatility\nforecasts effectively. This paper, therefore, investigates and implements a\ncombination of numeric and probabilistic models for short-term volatility and\nreturn forecasting for high-frequency trades. The essence is that one-day-ahead\nvolatility forecasts were made with Gaussian Processes (GPs) applied to the\noutputs of a Numerical market prediction (NMP) model. Firstly, the stock price\ndata from NMP was corrected by a GP. Since it is not easy to set price limits\nin a market due to its free nature and randomness, a Censored GP was used to\nmodel the relationship between the corrected stock prices and returns.\nForecasting errors were evaluated using the implied and estimated data.\n"
    },
    {
        "paper_id": 2311.10987,
        "authors": "Tie Wei, Youqi Chen, Zhicheng Duan",
        "title": "Research on the Dynamic Evolution and Influencing Factors of Energy\n  Resilience in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy security is the guarantee for achieving the goal of carbon peaking and\ncarbon neutrality, and exploring energy resilience is one of the important ways\nto promote energy security transition and adapt to changes in international and\ndomestic energy markets. This paper applies the combined dynamic evaluation\nmethod to measure China's energy resilience level from 2004-2021, analyses the\nspatio-temporal dynamic evolution of China's energy resilience through the\ncenter of gravity-standard deviation ellipse and kernel density estimation, and\nemploys geo-detectors to detect the main influencing factors and interactions\nof China's energy resilience. The study finds that:(1)China's energy resilience\nlevel generally shows a zigzagging forward development trend, and the spatial\nimbalance characteristic of China's energy resilience is more obvious.(2)The\nspatial dynamics of China's energy resilience level evolves in a\nnortheast-southwest direction, and the whole moves towards the southwest, with\nan overall counterclockwise trend of constant offset.(3)When the energy\nresilience level of neighboring provinces is too low or too high, it has little\neffect on the improvement of the energy resilience level of the province; when\nthe energy resilience level of neighboring provinces is 1-1.4, it has a\npositive spatial correlation with the energy resilience level of the province,\nand the synergistic development of the provinces can improve the energy\nresilience level together.(4)GDP, the number of employees, the number of\nemployees enrolled in basic pension and medical insurance, and the number of\npatent applications in high-tech industries have a more significant impact on\nChina's energy resilience, while China's energy resilience is affected by the\ninteraction of multiple factors.\n"
    },
    {
        "paper_id": 2311.1099,
        "authors": "Yunpeng Xiao, Bufan Deng, Siqi Chen, Kyrie Zhixuan Zhou, Ray LC, Luyao\n  Zhang, Xin Tong",
        "title": "\"Centralized or Decentralized?\": Concerns and Value Judgments of\n  Stakeholders in the Non-Fungible Tokens (NFTs) Market",
        "comments": "Accepted by CSCW 2024",
        "journal-ref": null,
        "doi": "10.31219/osf.io/evz4p",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-fungible tokens (NFTs) are decentralized digital tokens to represent the\nunique ownership of items. Recently, NFTs have been gaining popularity and at\nthe same time bringing up issues, such as scams, racism, and sexism.\nDecentralization, a key attribute of NFT, contributes to some of the issues\nthat are easier to regulate under centralized schemes, which are intentionally\nleft out of the NFT marketplace. In this work, we delved into this\ncentralization-decentralization dilemma in the NFT space through mixed\nquantitative and qualitative methods. Centralization-decentralization dilemma\nis the dilemma caused by the conflict between the slogan of decentralization\nand the interests of stakeholders. We first analyzed over 30,000 NFT-related\ntweets to obtain a high-level understanding of stakeholders' concerns in the\nNFT space. We then interviewed 15 NFT stakeholders (both creators and\ncollectors) to obtain their in-depth insights into these concerns and potential\nsolutions. Our findings identify concerning issues among users: financial\nscams, counterfeit NFTs, hacking, and unethical NFTs. We further reflected on\nthe centralization-decentralization dilemma drawing upon the perspectives of\nthe stakeholders in the interviews. Finally, we gave some inferences to solve\nthe centralization-decentralization dilemma in the NFT market and thought about\nthe future of NFT and decentralization.\n"
    },
    {
        "paper_id": 2311.11231,
        "authors": "Lanqing Du, Jinwook Lee",
        "title": "Workforce pDEI: Productivity Coupled with DEI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Ranking pertaining to the human-centered tasks -- underscoring their\nparamount significance in these domains such as evaluation and hiring process\n-- exhibits widespread prevalence across various industries. Consequently,\ndecision-makers are taking proactive measurements to promote diversity,\nunderscore equity, and advance inclusion. Their unwavering commitment to these\nideals emanates from the following convictions: (i) Diversity encompasses a\nbroad spectrum of differences; (ii) Equity involves the assurance of equitable\nopportunities; and (iii) Inclusion revolves around the cultivation of a sense\nof value and impartiality, concurrently empowering individuals. Data-driven AI\ntools have been used for screening and ranking processes. However, there is a\ngrowing concern that the presence of pre-existing biases in databases may be\nexacerbated, particularly in the context of imbalanced datasets or the\nblack-box-schema. In this research, we propose a model-driven recruitment\ndecision support tool that addresses fairness together with equity in the\nscreening phase. We introduce the term ``pDEI\" to represent the output-input\noriented production efficiency adjusted by socioeconomic disparity. Taking into\naccount various aspects of interpreting socioeconomic disparity, our goals are\n(i) maximizing the relative efficiency of underrepresented groups and (ii)\nunderstanding how socioeconomic disparity affects the cultivation of a\nDEI-positive workplace.\n"
    },
    {
        "paper_id": 2311.11248,
        "authors": "Daniel Bartl, Ariel Neufeld, Kyunghyun Park",
        "title": "Sensitivity of robust optimization problems under drift and volatility\n  uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine optimization problems in which an investor has the opportunity to\ntrade in $d$ stocks with the goal of maximizing her worst-case cost of\ncumulative gains and losses. Here, worst-case refers to taking into account all\npossible drift and volatility processes for the stocks that fall within a\n$\\varepsilon$-neighborhood of predefined fixed baseline processes. Although\nsolving the worst-case problem for a fixed $\\varepsilon>0$ is known to be very\nchallenging in general, we show that it can be approximated as $\\varepsilon\\to\n0$ by the baseline problem (computed using the baseline processes) in the\nfollowing sense: Firstly, the value of the worst-case problem is equal to the\nvalue of the baseline problem plus $\\varepsilon$ times a correction term. This\ncorrection term can be computed explicitly and quantifies how sensitive a given\noptimization problem is to model uncertainty. Moreover, approximately optimal\ntrading strategies for the worst-case problem can be obtained using optimal\nstrategies from the corresponding baseline problem.\n"
    },
    {
        "paper_id": 2311.11406,
        "authors": "Amirmohammad Nateghi and Maedeh Mosharraf",
        "title": "Architecting the Future: A Model for Enterprise Integration in the\n  Metaverse",
        "comments": "10 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.57019/jmv.1355500",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although it has a history that goes back about three decades, Metaverse has\ngrown to be one of the most talked-about subjects today. Metaverse gradually\nincreased its influence in the realm of business discourse after initially\nbeing restricted to discussions about entertainment. Before getting deep into\nthe Metaverse, it should be noted that failure and deviating from the business\npath are highly likely for an enterprise that relies heavily on information\ntechnology (IT) because of improper use and thinking about IT. The idea of\nenterprise architecture (EA) emerged as a management strategy to address this\nissue. As the first school of thought of EA, it sought to transform IT from an\nunnecessary burden in an enterprise to a guiding and supporting force. Then an\nextended EA model is suggested as a result of the attempt made in this paper to\nuse the idea of EA to steer virtual enterprises on Metaverse-based platforms.\nFinally, to evaluate the conceptual model and demonstrate that the Metaverse\ncan support businesses, three case studies Decentraland, Battle Infinity, and\nRooom were utilized.\n"
    },
    {
        "paper_id": 2311.11576,
        "authors": "Paul Maximilian R\\\"ohrig, Nils K\\\"orber, Julius Zocher, Andreas Ulbig",
        "title": "Multi-stage optimisation towards transformation pathways for municipal\n  energy systems",
        "comments": "13 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An essential facet of achieving climate neutrality by 2045 is the\ndecarbonization of municipal energy systems. To accomplish this, it is\nnecessary to establish implementation concepts that detail the timing,\nlocation, and specific measures required to achieve decarbonization. This\nrestructuring process involves identifying the measures that offer the most\ncompelling techno-economic and ecological advantages. In particular, measures\nthat contribute to the interconnection of energy vectors and domains, e.g.\nheating, cooling, and electricity supply, in the sense of decentralized\nmulti-energy systems are a promising future development option. Due to the high\ncomplexity resulting from a multitude of decision options as well as a temporal\ncoupling across the transformation path, the use of optimization methods is\nrequired, which enable a bottom-up identification of suitable transformation\nsolutions in a high spatial resolution. For the design of reasonable concepts,\nwe develop a multistage optimization problem for the derivation of\ntransformation pathways in the context of a multi-location structure,\nexpansion, and operation problem. The results show that the heat supply in the\nfuture will mainly be provided by heat pumps with a share of 60%. It can also\nbe shown that an early dismantling of the gas network will lead to the need for\ntransitional technologies such as pellet heating. Overall, the conversion of\nthe municipal energy system can significantly reduce emissions (97%).\n"
    },
    {
        "paper_id": 2311.11672,
        "authors": "Roberto Daluiso",
        "title": "Fast and Stable Credit Gamma of CVA",
        "comments": "22 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Valuation Adjustment is a balance sheet item which is nowadays subject\nto active risk management by specialized traders. However, one of the most\nimportant risk factors, which is the vector of default intensities of the\ncounterparty, affects in a non-differentiable way the most general Monte Carlo\nestimator of the adjustment, through simulation of default times. Thus the\ncomputation of first and second order (pure and mixed) sensitivities involving\nthese inputs cannot rely on direct path-wise differentiation, while any\napproach involving finite differences shows very high statistical noise. We\npresent ad hoc analytical estimators which overcome these issues while offering\nvery low runtime overheads over the baseline computation of the price\nadjustment. We also discuss the conversion of the so-obtained sensitivities to\nmodel parameters (e.g. default intensities) into sensitivities to market quotes\n(e.g. Credit Default Swap spreads).\n"
    },
    {
        "paper_id": 2311.11828,
        "authors": "Eiji Yamamura and Yoshiro Tsutsui and Fumio Ohtake",
        "title": "Would Monetary Incentives to COVID-19 vaccination reduce motivation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some people did not get the COVID-19 vaccine even though it was offered at no\ncost. A monetary incentive might lead people to vaccinate, although existing\nstudies have provided different findings about this effect. We investigate how\nmonetary incentives differ according to individual characteristics. Using panel\ndata with online experiments, we found that (1) subsidies reduced vaccine\nintention but increased it after controlling heterogeneity; (2) the stronger\nthe social image against the vaccination, the lower the monetary incentive; and\n(3) persistently unvaccinated people would intend to be vaccinated only if a\nlarge subsidy was provided.\n"
    },
    {
        "paper_id": 2311.11913,
        "authors": "Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu\n  Zhu, Tao Chen, Perukrishnen Vytelingum",
        "title": "Deep Calibration of Market Simulations using Neural Density Estimators\n  and Embedding Networks",
        "comments": "4th ACM International Conference on AI in Finance (ICAIF 2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The ability to construct a realistic simulator of financial exchanges,\nincluding reproducing the dynamics of the limit order book, can give insight\ninto many counterfactual scenarios, such as a flash crash, a margin call, or\nchanges in macroeconomic outlook. In recent years, agent-based models have been\ndeveloped that reproduce many features of an exchange, as summarised by a set\nof stylised facts and statistics. However, the ability to calibrate simulators\nto a specific period of trading remains an open challenge. In this work, we\ndevelop a novel approach to the calibration of market simulators by leveraging\nrecent advances in deep learning, specifically using neural density estimators\nand embedding networks. We demonstrate that our approach is able to correctly\nidentify high probability parameter sets, both when applied to synthetic and\nhistorical data, and without reliance on manually selected or weighted\nensembles of stylised facts.\n"
    },
    {
        "paper_id": 2311.12055,
        "authors": "Almendra Awerkin, Paolo Falbo, Tiziano Vargiolu",
        "title": "Optimal Investment and Fair Sharing Rules of the Incentives for\n  Renewable Energy Communities",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The focus on Renewable Energy Communities (REC) is fastly growing after the\nEuropean Union (EU) has introduced a dedicated regulation in 2018. The idea of\ncreating local groups of citizens, small- and medium-sized companies, and\npublic institutions, which self-produce and self-consume energy from renewable\nsources is at the same time a way to save money for the participants, increase\nefficiency of the energy system, and reduce CO$_2$ emissions. Member states\ninside the EU are fixing more detailed regulations, which describe, how public\nincentives are measured. A natural objective for the incentive policies is of\ncourse to promote the self-consumption of a REC. A sophisticated incentive\npolicy is that based on the so called 'virtual framework'. Under this framework\nall the energy produced by a REC is sold to the market, and all the energy\nconsumed must be paid to retailers: self-consumption occurs only 'virtually',\nthanks a money compensation (paid by a central authority) for every MWh\nproduced and consumed by the REC in the same hour. In this context, two\nproblems have to be solved: the optimal investment in new technologies and a\nfair division of the incentive among the community members. We address these\nproblems by considering a particular type of REC, composed by a representative\nhousehold and a biogas producer, where the potential demand of the community is\ngiven by the household's demand, while both members produce renewable energy.\nWe set the problem as a leader-follower problem: the leader decide how to share\nthe incentive for the self-consumed energy, while the followers decide their\nown optimal installation strategy. We solve the leader's problem by searching\nfor a Nash bargaining solution for the incentive's fair division, while the\nfollower problem is solved by finding the Nash equilibria of a static\ncompetitive game between the members.\n"
    },
    {
        "paper_id": 2311.12129,
        "authors": "Martin Winist\\\"orfer, Ivan Zhdankin",
        "title": "Measure of Dependence for Financial Time-Series",
        "comments": "9 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1111.6857 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Assessing the predictive power of both data and models holds paramount\nsignificance in time-series machine learning applications. Yet, preparing time\nseries data accurately and employing an appropriate measure for predictive\npower seems to be a non-trivial task. This work involves reviewing and\nestablishing the groundwork for a comprehensive analysis of shaping time-series\ndata and evaluating various measures of dependence. Lastly, we present a\nmethod, framework, and a concrete example for selecting and evaluating a\nsuitable measure of dependence.\n"
    },
    {
        "paper_id": 2311.12169,
        "authors": "Giorgio Ferrari and Shihao Zhu",
        "title": "Optimal Retirement Choice under Age-dependent Force of Mortality",
        "comments": "56 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the retirement decision, optimal investment, and\nconsumption strategies under an age-dependent force of mortality. We formulate\nthe optimization problem as a combined stochastic control and optimal stopping\nproblem with a random time horizon, featuring three state variables: wealth,\nlabor income, and force of mortality. To address this problem, we transform it\ninto its dual form, which is a finite time horizon, three-dimensional\ndegenerate optimal stopping problem with interconnected dynamics. We establish\nthe existence of an optimal retirement boundary that splits the state space\ninto continuation and stopping regions. Regularity of the optimal stopping\nvalue function is derived and the boundary is proved to be Lipschitz\ncontinuous, and it is characterized as the unique solution to a nonlinear\nintegral equation, which we compute numerically. In the original coordinates,\nthe agent thus retires whenever her wealth exceeds an age-, labor income- and\nmortality-dependent transformed version of the optimal stopping boundary. We\nalso provide numerical illustrations of the optimal strategies, including the\nsensitivities of the optimal retirement boundary concerning the relevant\nmodel's parameters.\n"
    },
    {
        "paper_id": 2311.12177,
        "authors": "Mark Craddock, Martino Grasselli, Andrea Mazzoran",
        "title": "Novel exact solutions for PDEs with mixed boundary conditions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop methods for the solution of inhomogeneous Robin type boundary\nvalue problems (BVPs) that arise for certain linear parabolic Partial\nDifferential Equations (PDEs) on a half line, as well as a second order\ngeneralisation. We are able to obtain non-standard solutions to equations\narising in a range of areas, including mathematical finance, stochastic\nanalysis, hyperbolic geometry and mathematical physics. Our approach uses the\nodd and even Hilbert transforms. The solutions we obtain and the method itself\nseem to be new.\n"
    },
    {
        "paper_id": 2311.12183,
        "authors": "Silvana M. Pesenti, Steven Vanduffel",
        "title": "Optimal Transport Divergences induced by Scoring Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We employ scoring functions, used in statistics for eliciting risk\nfunctionals, as cost functions in the Monge-Kantorovich (MK) optimal transport\nproblem. This gives raise to a rich variety of novel asymmetric MK divergences,\nwhich subsume the family of Bregman-Wasserstein divergences. We show that for\ndistributions on the real line, the comonotonic coupling is optimal for the\nmajority of the new divergences. Specifically, we derive the optimal coupling\nof the MK divergences induced by functionals including the mean, generalised\nquantiles, expectiles, and shortfall measures. Furthermore, we show that while\nany elicitable law-invariant coherent risk measure gives raise to infinitely\nmany MK divergences, the comonotonic coupling is simultaneously optimal.\n  The novel MK divergences, which can be efficiently calculated, open an array\nof applications in robust stochastic optimisation. We derive sharp bounds on\ndistortion risk measures under a Bregman-Wasserstein divergence constraint, and\nsolve for cost-efficient payoffs under benchmark constraints.\n"
    },
    {
        "paper_id": 2311.12239,
        "authors": "Chuhao Sun, Asaf Cohen, James Stokes, Shravan Veerapaneni",
        "title": "Quantum-inspired nonlinear Galerkin ansatz for high-dimensional HJB\n  equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural networks are increasingly recognized as a powerful numerical solution\ntechnique for partial differential equations (PDEs) arising in diverse\nscientific computing domains, including quantum many-body physics. In the\ncontext of time-dependent PDEs, the dominant paradigm involves casting the\napproximate solution in terms of stochastic minimization of an objective\nfunction given by the norm of the PDE residual, viewed as a function of the\nneural network parameters. Recently, advancements have been made in the\ndirection of an alternative approach which shares aspects of nonlinearly\nparametrized Galerkin methods and variational quantum Monte Carlo, especially\nfor high-dimensional, time-dependent PDEs that extend beyond the usual scope of\nquantum physics. This paper is inspired by the potential of solving\nHamilton-Jacobi-Bellman (HJB) PDEs using Neural Galerkin methods and commences\nthe exploration of nonlinearly parametrized trial functions for which the\nevolution equations are analytically tractable. As a precursor to the Neural\nGalerkin scheme, we present trial functions with evolution equations that admit\nclosed-form solutions, focusing on time-dependent HJB equations relevant to\nfinance.\n"
    },
    {
        "paper_id": 2311.12247,
        "authors": "Nicolas Cofre, Magdalena Mosionek-Schweda",
        "title": "A simulated electronic market with speculative behaviour and bubble\n  formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents an agent based model of an electronic market with two\ntypes of trading agents. One type follows a mean reverting strategy and the\nother, the speculative trader, tracks the maximum realised return over recent\ntrades. The speculators have a distribution of returns concentrated on negative\nreturns, with a small fraction making profits. The market experiences an\nincreased volatility and prices that greatly depart from the fundamental value\nof the asset. Our research provides synthetic datasets of the order book to\nstudy its dynamics under different levels of speculation\n"
    },
    {
        "paper_id": 2311.1233,
        "authors": "Cheng-Der Fuh, Yanwei Jia, and Steven Kou",
        "title": "A General Framework for Importance Sampling with Latent Markov Processes",
        "comments": "59 pages, 2 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although stochastic models driven by latent Markov processes are widely used,\nthe classical importance sampling method based on the exponential tilting\nmethod for these models suffers from the difficulty of computing the eigenvalue\nand associated eigenfunction and the plausibility of the indirect asymptotic\nlarge deviation regime for the variance of the estimator. We propose a general\nimportance sampling framework that twists the observable and latent processes\nseparately based on a link function that directly minimizes the estimator's\nvariance. An optimal choice of the link function is chosen within the locally\nasymptotically normal family. We show the logarithmic efficiency of the\nproposed estimator under the asymptotic normal regime. As applications, we\nestimate an overflow probability under a pandemic model and the CoVaR, a\nmeasurement of the co-dependent financial systemic risk. Both applications are\nbeyond the scope of traditional importance sampling methods due to their\nnonlinear structures.\n"
    },
    {
        "paper_id": 2311.1245,
        "authors": "Michele Azzone, Maria Chiara Pocelli, Davide Stocco",
        "title": "Hedging carbon risk with a network approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sustainable investing refers to the integration of environmental and social\naspects in investors' decisions. We propose a novel methodology based on the\nTriangulated Maximally Filtered Graph and node2vec algorithms to construct an\nhedging portfolio for climate risk, represented by various risk factors, among\nwhich the CO2 and the ESG ones. The CO2 factor is strongly correlated\nconsistently over time with the Utility sector, which is the most carbon\nintensive in the S&P 500 index. Conversely, identifying a group of sectors\nlinked to the ESG factor proves challenging. As a consequence, while it is\npossible to obtain an efficient hedging portfolio strategy with our methodology\nfor the carbon factor, the same cannot be achieved for the ESG one. The ESG\nscores appears to be an indicator too broadly defined for market applications.\nThese results support the idea that bank capital requirements should take into\naccount carbon risk.\n"
    },
    {
        "paper_id": 2311.12491,
        "authors": "Hugo Schnoering, Michalis Vazirgiannis",
        "title": "Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This research delves into the intricacies of Bitcoin, a decentralized\npeer-to-peer network, and its associated blockchain, which records all\ntransactions since its inception. While this ensures integrity and\ntransparency, the transparent nature of Bitcoin potentially compromises users'\nprivacy rights. To address this concern, users have adopted CoinJoin, a method\nthat amalgamates multiple transaction intents into a single, larger transaction\nto bolster transactional privacy. This process complicates individual\ntransaction tracing and disrupts many established blockchain analysis\nheuristics. Despite its significance, limited research has been conducted on\nidentifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin\nimplementations such as JoinMarket, Wasabi, and Whirlpool, each presenting\ndistinct challenges due to their unique transaction structures. This study\ndelves deeply into the open-source implementations of these protocols, aiming\nto develop refined heuristics for identifying their transactions on the\nblockchain. Our exhaustive analysis covers transactions up to block 760,000,\noffering a comprehensive insight into CoinJoin transactions and their\nimplications for Bitcoin blockchain analysis.\n"
    },
    {
        "paper_id": 2311.12517,
        "authors": "Wenyuan Wang, Kaixin Yan, Xiang Yu",
        "title": "Optimal Portfolio with Ratio Type Periodic Evaluation under\n  Short-Selling Prohibition",
        "comments": "35 pages, 12 figures, 26 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies some unconventional utility maximization problems when the\nratio type relative portfolio performance is periodically evaluated over an\ninfinite horizon. Meanwhile, the agent is prohibited from short-selling stocks.\nOur goal is to understand the impact of the periodic reward structure on the\nlong-run constrained portfolio strategy. For power and logarithmic utilities,\nwe can reformulate the original problem into an auxiliary one-period\noptimization problem. To cope with the auxiliary problem with no short-selling,\nthe dual control problem is introduced and studied, which gives the\ncharacterization of the candidate optimal portfolio within one period. With the\nhelp of the results from the auxiliary problem, the value function and the\noptimal constrained portfolio for the original problem with periodic evaluation\ncan be derived and verified, allowing us to discuss some financial implications\nunder the new performance paradigm.\n"
    },
    {
        "paper_id": 2311.12575,
        "authors": "Gijs Mast, Xiaoyu Shen and Fang Fang",
        "title": "Fast calculation of Counterparty Credit exposures and associated\n  sensitivities using fourier series expansion",
        "comments": "26 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a novel approach for computing netting--set level and\ncounterparty level exposures, such as Potential Future Exposure (PFE) and\nExpected Exposure (EE), along with associated sensitivities. The method is\nessentially an extension of the Fourier-cosine series expansion (COS) method,\noriginally proposed for option pricing. This method can accommodate a broad\nrange of models where the joint distribution of involved risk factors is\nanalytically or semi-analytically tractable. This inclusivity encompasses\nnearly all CCR models commonly employed in practice. A notable advantage of the\nCOS method is its sustained efficiency, particularly when handling large\nportfolios. A theoretical error analysis is also provided to justify the\nmethod's theoretical stability and accuracy. Various numerical tests are\nconducted using real-sized portfolios, and the results underscore its potential\nas a significantly more efficient alternative to the Monte Carlo method for\npractical usage, particularly applicable to portfolios involving a relatively\nmodest number of risk factors. Furthermore, the observed error convergence\nrates align closely with the theoretical error analysis.\n"
    },
    {
        "paper_id": 2311.12743,
        "authors": "Umut \\c{C}etin",
        "title": "Insider trading with penalties, entropy and quadratic BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Kyle model in continuous time where the insider may be subject to legal\npenalties is considered. In equilibrium the insider internalises this legal\nrisk by trading less aggressively. The equilibrium is characterised via the\nsolution of a backward stochastic differential equation (BSDE) whose terminal\ncondition is determined as the fixed point of a non-linear operator in\nequilibrium. The insider's expected penalties in equilibrium is non-monotone in\nthe fee structure and is given by the relative entropy of the law of a\nparticular h-transformation of Brownian motion.\n"
    },
    {
        "paper_id": 2311.13046,
        "authors": "Yuxi Heluo and Kexin Wang and Charles W. Robson",
        "title": "Do we listen to what we are told? An empirical study on human behaviour\n  during the COVID-19 pandemic: neural networks vs. regression analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we contribute the first visual open-source empirical study on\nhuman behaviour during the COVID-19 pandemic, in order to investigate how\ncompliant a general population is to mask-wearing-related public-health policy.\nObject-detection-based convolutional neural networks, regression analysis and\nmultilayer perceptrons are combined to analyse visual data of the Viennese\npublic during 2020. We find that mask-wearing-related government regulations\nand public-transport announcements encouraged correct mask-wearing-behaviours\nduring the COVID-19 pandemic. Importantly, changes in announcement and\nregulation contents led to heterogeneous effects on people's behaviour.\nComparing the predictive power of regression analysis and neural networks, we\ndemonstrate that the latter produces more accurate predictions of population\nreactions during the COVID-19 pandemic. Our use of regression modelling also\nallows us to unearth possible causal pathways underlying societal behaviour.\nSince our findings highlight the importance of appropriate communication\ncontents, our results will facilitate more effective non-pharmaceutical\ninterventions to be developed in future. Adding to the literature, we\ndemonstrate that regression modelling and neural networks are not mutually\nexclusive but instead complement each other.\n"
    },
    {
        "paper_id": 2311.13278,
        "authors": "Daniel Kr\\v{s}ek and Dylan Possama\\\"i",
        "title": "Randomisation with moral hazard: a path to existence of optimal\n  contracts",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a generic principal-agent problem in continuous time on a finite\ntime horizon. We introduce a framework in which the agent is allowed to employ\nmeasure-valued controls and characterise the continuation utility as a solution\nto a specific form of a backward stochastic differential equation driven by a\nmartingale measure. We leverage this characterisation to prove that, under\nappropriate conditions, an optimal solution to the principal's problem exists,\neven when constraints on the contract are imposed. In doing so, we employ\ncompactification techniques and, as a result, circumvent the typical challenge\nof showing well-posedness for a degenerate partial differential equation with\npotential boundary conditions, where regularity problems often arise.\n"
    },
    {
        "paper_id": 2311.13326,
        "authors": "Woosung Koh, Insu Choi, Yuntae Jang, Gimin Kang, Woo Chang Kim",
        "title": "Curriculum Learning and Imitation Learning for Model-free Control on\n  Financial Time-series",
        "comments": "AAAI 2024 AI4TS Workshop Oral",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Curriculum learning and imitation learning have been leveraged extensively in\nthe robotics domain. However, minimal research has been done on leveraging\nthese ideas on control tasks over highly stochastic time-series data. Here, we\ntheoretically and empirically explore these approaches in a representative\ncontrol task over complex time-series data. We implement the fundamental ideas\nof curriculum learning via data augmentation, while imitation learning is\nimplemented via policy distillation from an oracle. Our findings reveal that\ncurriculum learning should be considered a novel direction in improving\ncontrol-task performance over complex time-series. Our ample random-seed\nout-sample empirics and ablation studies are highly encouraging for curriculum\nlearning for time-series control. These findings are especially encouraging as\nwe tune all overlapping hyperparameters on the baseline -- giving an advantage\nto the baseline. On the other hand, we find that imitation learning should be\nused with caution.\n"
    },
    {
        "paper_id": 2311.13564,
        "authors": "Gabriel Turinici",
        "title": "High order universal portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Cover universal portfolio (UP from now on) has many interesting\ntheoretical and numerical properties and was investigated for a long time.\nBuilding on it, we explore what happens when we add this UP to the market as a\nnew synthetic asset and construct by recurrence higher order UPs. We\ninvestigate some important theoretical properties of the high order UPs and\nshow in particular that they are indeed different from the Cover UP and are\ncapable to break the time permutation invariance. We show that under some\nperturbation regime the second high order UP has better Sharp ratio than the\nstandard UP and briefly investigate arbitrage opportunities thus created.\nNumerical experiences on a benchmark from the literature confirm that high\norder UPs improve Cover's UP performances.\n"
    },
    {
        "paper_id": 2311.13743,
        "authors": "Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui\n  Zhang, Rong Liu, Jordan W. Suchow, Khaldoun Khashanah",
        "title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and\n  Character Design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent advancements in Large Language Models (LLMs) have exhibited notable\nefficacy in question-answering (QA) tasks across diverse domains. Their prowess\nin integrating extensive web knowledge has fueled interest in developing\nLLM-based autonomous agents. While LLMs are efficient in decoding human\ninstructions and deriving solutions by holistically processing historical\ninputs, transitioning to purpose-driven agents requires a supplementary\nrational architecture to process multi-source information, establish reasoning\nchains, and prioritize critical tasks. Addressing this, we introduce\n\\textsc{FinMem}, a novel LLM-based agent framework devised for financial\ndecision-making. It encompasses three core modules: Profiling, to customize the\nagent's characteristics; Memory, with layered message processing, to aid the\nagent in assimilating hierarchical financial data; and Decision-making, to\nconvert insights gained from memories into investment decisions. Notably,\n\\textsc{FinMem}'s memory module aligns closely with the cognitive structure of\nhuman traders, offering robust interpretability and real-time tuning. Its\nadjustable cognitive span allows for the retention of critical information\nbeyond human perceptual limits, thereby enhancing trading outcomes. This\nframework enables the agent to self-evolve its professional knowledge, react\nagilely to new investment cues, and continuously refine trading decisions in\nthe volatile financial environment. We first compare \\textsc{FinMem} with\nvarious algorithmic agents on a scalable real-world financial dataset,\nunderscoring its leading trading performance in stocks. We then fine-tuned the\nagent's perceptual span and character setting to achieve a significantly\nenhanced trading performance. Collectively, \\textsc{FinMem} presents a\ncutting-edge LLM agent framework for automated trading, boosting cumulative\ninvestment returns.\n"
    },
    {
        "paper_id": 2311.13802,
        "authors": "Eva L\\\"utkebohmert, Julian Sester, Hongyi Shen",
        "title": "On the Relevance and Appropriateness of Name Concentration Risk\n  Adjustments for Portfolios of Multilateral Development Banks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sovereign loan portfolios of Multilateral Development Banks (MDBs) typically\nconsist of only a small number of borrowers and hence are heavily exposed to\nsingle name concentration risk. Based on realistic MDB portfolios constructed\nfrom publicly available data, this paper quantifies the magnitude of the\nexposure to name concentration risk using exact Monte Carlo simulations. In\ncomparing the exact adjustment for name concentration risk to its analytic\napproximation as currently applied by the major rating agency Standard &\nPoor's, we further investigate whether current capital adequacy frameworks for\nMDBs are overly conservative. Finally, we discuss the choice of appropriate\nmodel parameters and their impact on measures of name concentration risk.\n"
    },
    {
        "paper_id": 2311.14009,
        "authors": "Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Maria Bogren",
        "title": "Organizational support for work-family life balance as an antecedent to\n  the well-being of tourism employees in Spain",
        "comments": "2023 Journal of Hospitality and Tourism Management",
        "journal-ref": null,
        "doi": "10.1016/j.jhtm.2023.08.018",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The study of work-family conflict (WFC) and work-family policies (WFP) and\ntheir impact on the well-being of employees in the tourism sector is\nincreasingly attracting the attention of researchers. To overcome the adverse\neffects of WFC, managers should promote WFP, which contribute to increased\nwell-being at work and employees' commitment. This paper aims to analyze the\nimpact of WFP accessibility and organizational support on well-being directly\nand by mediating the organizational commitment that these policies might\nencourage. In addition, we also study whether these relationships vary\naccording to gender and employee seniority. To test the hypotheses derived from\nthis objective, we collected 530 valid and completed questionnaires from\nworkers in the tourism sector in Spain, which we analyzed using structural\nequation modeling based on the PLS-SEM approach. The results show that human\nresource management must consider the importance of organizational support for\nworkers to make WFP accessible and generate organizational commitment and\nwell-being at work.\n"
    },
    {
        "paper_id": 2311.14219,
        "authors": "Takanori Adachi",
        "title": "Hierarchical Structure of Uncertainty",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The experience of unknown events such as financial crises and infectious\ndisease crises has revealed the limitations of measuring risk under a fixed\nprobability measure. In order to solve this problem, the importance of\nso-called ambiguity, which allows the probability measure itself to change, has\nlong been recognized in the financial world. On the other hand, there have been\nmany studies o n subjective probability measures in the field of economics.But\neven in those cases, the studies are based on the two levels of uncertainty:\nrisk when a conventional probability measure (probability distribution) is\nknown, and ambiguity due to the fact that the subjective probability measure\ncan be taken arbitrarily in a certain space. In this study, we express n-layer\nuncertainty, which we call hierarchical uncertainty by introducing a new\nconcepts called uncertainty spaces which is an extended concept of probability\nspaces and U-sequence that are sequences of uncertainty spaces. We use\nU-sequence for providing examples that illustrate Ellsberg's paradox. We also\ninvestigate categories of U-sequences. Next, we construct an endofunctor S of\nMble, the category formed by measurable spaces and measurable functions between\nthem, in order to embed a given U-sequence into it. The endofunctor S maps a\nmeasurable space to a set of capacities defined on the space, where a capacity\nis a non-additive probability measure introduced by Choquet. After developing\nn-layer uncertainty analysis through U-sequences, we construct the universal\nuncertainty space as a limit of the sequence of measurable spaces representing\nmulti-layer uncertainty. This universal uncertainty space may be able to serve\nas a basis for multi-layer uncertainty theory because it has as its projections\nthe uncertainty spaces of all levels. Lastly, we check a sufficient condition\nfor making the functor S be a probability monad.\n"
    },
    {
        "paper_id": 2311.14318,
        "authors": "Lijun Bo, Yijie Huang, Xiang Yu",
        "title": "On optimal tracking portfolio in incomplete markets: The classical\n  control and the reinforcement learning approaches",
        "comments": "Optimal tracking portfolio, capital injection, incomplete market,\n  stochastic control with reflection, continuous-time reinforcement learning,\n  q-learning",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an infinite horizon optimal tracking portfolio problem\nusing capital injection in incomplete market models. We consider the benchmark\nprocess modelled by a geometric Brownian motion with zero drift driven by some\nunhedgeable risk. The relaxed tracking formulation is adopted where the\nportfolio value compensated by the injected capital needs to outperform the\nbenchmark process at any time, and the goal is to minimize the cost of the\ndiscounted total capital injection. In the first part, we solve the stochastic\ncontrol problem when the market model is known, for which the equivalent\nauxiliary control problem with reflections and the associated HJB equation with\na Neumann boundary condition are studied. In the second part, the market model\nis assumed to be unknown, for which we consider the exploratory formulation of\nthe control problem with entropy regularizer and develop the continuous-time\nq-learning algorithm for the stochastic control problem with state reflections.\nIn an illustrative example, we show the satisfactory performance of the\nq-learning algorithm.\n"
    },
    {
        "paper_id": 2311.1432,
        "authors": "Kota Ogasawara",
        "title": "Consumption Smoothing in Metropolis: Evidence from the Working-class\n  Households in Prewar Tokyo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I analyze the risk-coping behaviors among factory worker households in early\n20th-century Tokyo. I have digitized a unique daily longitudinal household\nbudget survey to analyze how consumption is impacted by idiosyncratic shocks. I\nfind that while the households were so vulnerable that the shocks impacted\ntheir consumption levels, the estimated income elasticity for food consumption\nis relatively low in the short-run perspective. The event-study analysis using\nthe adverse health shock confirms the robustness of the results. The result\nfrom mechanism analysis suggests that credit purchases with local retailers\nsmoothed short-run food consumption. Despite the potential loss of profit,\nretailers in a competitive situation allowed consumers to trade on credit. This\nshows the roles of informal credit institutions in mitigating vulnerability\namong urban worker households.\n"
    },
    {
        "paper_id": 2311.1434,
        "authors": "Alicia Martin-Navarro, Felix Velicia-Martin, Jose Aurelio\n  Medina-Garrido, Pedro R. Palos-Sanchez",
        "title": "Impact of effectual propensity on entrepreneurial intention",
        "comments": null,
        "journal-ref": "Journal of Business Research, 157 (January, 2023)",
        "doi": "10.1016/j.jbusres.2022.113604",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  For decades, entrepreneurship has been promoted in academia and tourism\nsector and has it been seen as an opportunity for new business ventures. In\nterms of entrepreneurial behavior, effectual logic shows how the individual\nuses his or her resources to create new opportunities. In this context, this\npaper aims to determine effectual propensity as an antecedent of\nentrepreneurial intentions. For this purpose, and based on the TPB model, we\nconducted our research with tourism students from Cadiz and Seville (Spain)\nuniversities with Smart PLS 3. The results show that effectual propensity\ninfluences entrepreneurial intentions and that attitude and perceived\nbehavioral control mediate between subjective norms and intentions. Our\nresearch has a great added value since we have studied for the first time\nefficacious propensity as an antecedent of intentions in people who have never\nbeen entrepreneurs.\n"
    },
    {
        "paper_id": 2311.14348,
        "authors": "Alicia Martin-Navarro, Maria Paula Lechuga Sancho, Jose Aurelio\n  Medina-Garrido",
        "title": "Testing an instrument to measure the BPMS-KM Support Model",
        "comments": null,
        "journal-ref": "Expert Systems with Applications, 178:115005 (2021)",
        "doi": "10.1016/j.eswa.2021.115005",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  BPMS (Business Process Management System) represents a type of software that\nautomates the organizational processes looking for efficiency. Since the\nknowledge of organizations lies in their processes, it seems probable that a\nBPMS can be used to manage the knowledge applied in these processes. Through\nthe BPMS-KM Support Model, this study aims to determine the reliability and\nvalidity of a 65-item instrument to measure the utility and the use of a BPMS\nfor knowledge management (KM). A questionnaire was sent to 242 BPMS users and\nto determine its validity, a factorial analysis was conducted. The results\nshowed that the measuring instrument is trustworthy and valid. It represents\nimplications for research, since it provides an instrument validated for\nresearch on the success of a BPMS for KM. There would also be practical\nimplications, since managers can evaluate the use of BPMS, in addition to\nautomating processes to manage knowledge.\n"
    },
    {
        "paper_id": 2311.14358,
        "authors": "Jose Maria Biedma Ferrer, Jose Aurelio Medina Garrido",
        "title": "Impact of family-friendly HRM policies in organizational performance",
        "comments": null,
        "journal-ref": "Intangible Capital, Vol 10, No 3 (448-466) (2014)",
        "doi": "10.3926/ic.506",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Purpose: The objective of this work is to analyze the impact of implementing\nwork-family reconciliation measures on workers' perception and how this can\ninfluence their behavior, especially in their organizational performance.\nDesign/methodology/approach: A literature review and the main research works\nrelated to the work-family conflict and reconciliation measures to overcome\nthis conflict have been conducted to draw conclusions about their impact on\nworker performance. Contributions and results: This work proposes an\nintegrative model that shows the existing relationships between work-family\nreconciliation and perceptual variables on one side, and those related to the\nworker's organizational behavior on the other. Perceptual variables such as\nstress, job satisfaction, and motivation are analyzed. Regarding variables\nrelated to the worker's organizational behavior, absenteeism, turnover, and\nperformance are analyzed. The results of the analysis provide evidence that the\nexistence of work-family reconciliation is perceived favorably by workers and\nimproves their organizational behavior, especially their performance.\nOriginality/Added value: This study integrates different perspectives related\nto the conflict and work-family reconciliation, from an eclectic vision. Thus,\nit contributes to existing literature with a more comprehensive approach to the\ninvestigated topic. Additionally, the proposed integrative model allows for\nuseful conclusions for management from both a purely human resources management\nperspective and organizational productivity improvement. Keywords: Work-family\nconflict, work-family reconciliation, perceptual variables, organizational\nperformance, human resources managements\n"
    },
    {
        "paper_id": 2311.14417,
        "authors": "Lucas Javaudin, Andrea Araldo, Andr\\'e de Palma",
        "title": "Personalised incentives with constrained regulator's budget",
        "comments": null,
        "journal-ref": "Transportmetrica A: Transport Science, 2023",
        "doi": "10.1080/23249935.2023.2284353",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider a regulator driving individual choices towards increasing social\nwelfare by providing personal incentives. We formalise and solve this problem\nby maximising social welfare under a budget constraint. The personalised\nincentives depend on the alternatives available to each individual and on her\npreferences. A polynomial time approximation algorithm computes a policy within\nfew seconds. We analytically prove that it is boundedly close to the optimum.\nWe efficiently calculate the curve of social welfare achievable for each value\nof budget within a given range. This curve can be useful for the regulator to\ndecide the appropriate amount of budget to invest. We extend our formulation to\nenforcement, taxation and non-personalised-incentive policies. We analytically\nshow that our personalised-incentive policy is also optimal within this class\nof policies and construct close-to-optimal enforcement and proportional\ntax-subsidy policies. We then compare analytically and numerically our policy\nwith other state-of-the-art policies. Finally, we simulate a large-scale\napplication to mode choice to reduce CO2 emissions.\n"
    },
    {
        "paper_id": 2311.14419,
        "authors": "Deborah Miori and Constantin Petrov",
        "title": "Narratives from GPT-derived Networks of News, and a link to Financial\n  Markets Dislocations",
        "comments": "Also available at papers.ssrn.com/sol3/papers.cfm?abstract_id=4628533",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Starting from a corpus of economic articles from The Wall Street Journal, we\npresent a novel systematic way to analyse news content that evolves over time.\nWe leverage on state-of-the-art natural language processing techniques (i.e.\nGPT3.5) to extract the most important entities of each article available, and\naggregate co-occurrence of entities in a related graph at the weekly level.\nNetwork analysis techniques and fuzzy community detection are tested on the\nproposed set of graphs, and a framework is introduced that allows systematic\nbut interpretable detection of topics and narratives. In parallel, we propose\nto consider the sentiment around main entities of an article as a more accurate\nproxy for the overall sentiment of such piece of text, and describe a\ncase-study to motivate this choice. Finally, we design features that\ncharacterise the type and structure of news within each week, and map them to\nmoments of financial markets dislocations. The latter are identified as dates\nwith unusually high volatility across asset classes, and we find quantitative\nevidence that they relate to instances of high entropy in the high-dimensional\nspace of interconnected news. This result further motivates the pursued efforts\nto provide a novel framework for the systematic analysis of narratives within\nnews.\n"
    },
    {
        "paper_id": 2311.14567,
        "authors": "Beatrice Acciaio, Antonio Marini, Gudmund Pammer",
        "title": "Calibration of the Bass Local Volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Bass local volatility model introduced by\nBackhoff-Veraguas--Beiglb\\\"ock--Huesmann--K\\\"allblad is a Markov model\nperfectly calibrated to vanilla options at finitely many maturities, that\napproximates the Dupire local volatility model. Conze and Henry-Labord\\`ere\nshow that its calibration can be achieved by solving a fixed-point equation. In\nthis paper we complement the analysis and show existence and uniqueness of the\nsolution to this equation, and that the fixed-point iteration scheme converges\nat a linear rate.\n"
    },
    {
        "paper_id": 2311.14577,
        "authors": "Jen-Yin Yeh, Hsin-Yu Chiu, and Jhih-Huei Huang",
        "title": "Predicting Failure of P2P Lending Platforms through Machine Learning:\n  The Case in China",
        "comments": null,
        "journal-ref": "Finance Research Letters Volume 59, January 2024, 104784",
        "doi": "10.1016/j.frl.2023.104784",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study employs machine learning models to predict the failure of\nPeer-to-Peer (P2P) lending platforms, specifically in China. By employing the\nfilter method and wrapper method with forward selection and backward\nelimination, we establish a rigorous and practical procedure that ensures the\nrobustness and importance of variables in predicting platform failures. The\nresearch identifies a set of robust variables that consistently appear in the\nfeature subsets across different selection methods and models, suggesting their\nreliability and relevance in predicting platform failures. The study highlights\nthat reducing the number of variables in the feature subset leads to an\nincrease in the false acceptance rate while the performance metrics remain\nstable, with an AUC value of approximately 0.96 and an F1 score of around 0.88.\nThe findings of this research provide significant practical implications for\nregulatory authorities and investors operating in the Chinese P2P lending\nindustry.\n"
    },
    {
        "paper_id": 2311.14588,
        "authors": "Jana Hlavinova, Birgit Rudloff, Alexander Smirnow",
        "title": "Set-valued intrinsic measures of systemic risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, it has become apparent that an isolated microprudential\napproach to capital adequacy requirements of individual institutions is\ninsufficient. It can increase the homogeneity of the financial system and\nultimately the cost to society. For this reason, the focus of the financial and\nmathematical literature has shifted towards the macroprudential regulation of\nthe financial network as a whole. In particular, systemic risk measures have\nbeen discussed as a risk measurement and mitigation tool. In this spirit, we\nadopt a general approach of multivariate, set-valued risk measures and combine\nit with the notion of intrinsic risk measures. In order to define the risk of a\nfinancial position, intrinsic risk measures utilise only internal capital,\nwhich is received when part of the currently held assets are sold, instead of\nrelying on external capital. We translate this methodology into the systemic\nframework and show that systemic intrinsic risk measures have desirable\nproperties such as the set-valued equivalents of monotonicity and\nquasi-convexity. Furthermore, for convex acceptance sets we derive a dual\nrepresentation of the systemic intrinsic risk measure. We apply our methodology\nto a modified Eisenberg-Noe network of banks and discuss the appeal of this\napproach from a regulatory perspective, as it does not elevate the financial\nsystem with external capital. We show evidence that this approach allows to\nmitigate systemic risk by moving the network towards more stable assets.\n"
    },
    {
        "paper_id": 2311.14676,
        "authors": "Yutong Quan, Xintong Wu, Wanlin Deng, Luyao Zhang",
        "title": "Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain\n  Governance Communities",
        "comments": null,
        "journal-ref": null,
        "doi": "10.31219/osf.io/bq6tu",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain technology is leading a revolutionary transformation across\ndiverse industries, with effective governance being critical for the success\nand sustainability of blockchain projects. Community forums, pivotal in\nengaging decentralized autonomous organizations (DAOs), significantly impact\nblockchain governance decisions. Concurrently, Natural Language Processing\n(NLP), particularly sentiment analysis, provides powerful insights from textual\ndata. While prior research has explored the potential of NLP tools in social\nmedia sentiment analysis, there is a gap in understanding the sentiment\nlandscape of blockchain governance communities. The evolving discourse and\nsentiment dynamics on the forums of top DAOs remain largely unknown. This paper\ndelves deep into the evolving discourse and sentiment dynamics on the public\nforums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit\nCircle, and Balancer, focusing primarily on discussions related to governance\nissues. Our study shows that participants in decentralized communities\ngenerally express positive sentiments during Discord discussions. Furthermore,\nthere is a potential interaction between discussion intensity and sentiment\ndynamics; higher discussion volume may contribute to a more stable sentiment\nfrom code analysis. The insights gained from this study are valuable for\ndecision-makers in blockchain governance, underscoring the pivotal role of\nsentiment analysis in interpreting community emotions and its evolving impact\non the landscape of blockchain governance. This research significantly\ncontributes to the interdisciplinary exploration of the intersection of\nblockchain and society, specifically emphasizing the decentralized blockchain\ngovernance ecosystem. We provide our data and code for replicability as open\naccess on GitHub.\n"
    },
    {
        "paper_id": 2311.1472,
        "authors": "Nir Chemaya and Daniel Martin",
        "title": "Perceptions and Detection of AI Use in Manuscript Preparation for\n  Academic Journals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The emergent abilities of Large Language Models (LLMs), which power tools\nlike ChatGPT and Bard, have produced both excitement and worry about how AI\nwill impact academic writing. In response to rising concerns about AI use,\nauthors of academic publications may decide to voluntarily disclose any AI\ntools they use to revise their manuscripts, and journals and conferences could\nbegin mandating disclosure and/or turn to using detection services, as many\nteachers have done with student writing in class settings. Given these looming\npossibilities, we investigate whether academics view it as necessary to report\nAI use in manuscript preparation and how detectors react to the use of AI in\nacademic writing.\n"
    },
    {
        "paper_id": 2311.14731,
        "authors": "Shalini Sharma, Angshul Majumdar, Emilie Chouzenoux, Victor Elvira",
        "title": "Deep State-Space Model for Predicting Cryptocurrency Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Our work presents two fundamental contributions. On the application side, we\ntackle the challenging problem of predicting day-ahead crypto-currency prices.\nOn the methodological side, a new dynamical modeling approach is proposed. Our\napproach keeps the probabilistic formulation of the state-space model, which\nprovides uncertainty quantification on the estimates, and the function\napproximation ability of deep neural networks. We call the proposed approach\nthe deep state-space model. The experiments are carried out on established\ncryptocurrencies (obtained from Yahoo Finance). The goal of the work has been\nto predict the price for the next day. Benchmarking has been done with both\nstate-of-the-art and classical dynamical modeling techniques. Results show that\nthe proposed approach yields the best overall results in terms of accuracy.\n"
    },
    {
        "paper_id": 2311.14735,
        "authors": "Ruslan Tepelyan, Achintya Gopal",
        "title": "Generative Machine Learning for Multivariate Equity Returns",
        "comments": "13 pages, 2-column format, presented at ICAIF'23",
        "journal-ref": null,
        "doi": "10.1145/3604237.3626884",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of machine learning to generate synthetic data has grown in\npopularity with the proliferation of text-to-image models and especially large\nlanguage models. The core methodology these models use is to learn the\ndistribution of the underlying data, similar to the classical methods common in\nfinance of fitting statistical models to data. In this work, we explore the\nefficacy of using modern machine learning methods, specifically conditional\nimportance weighted autoencoders (a variant of variational autoencoders) and\nconditional normalizing flows, for the task of modeling the returns of\nequities. The main problem we work to address is modeling the joint\ndistribution of all the members of the S&P 500, or, in other words, learning a\n500-dimensional joint distribution. We show that this generative model has a\nbroad range of applications in finance, including generating realistic\nsynthetic data, volatility and correlation estimation, risk analysis (e.g.,\nvalue at risk, or VaR, of portfolios), and portfolio optimization.\n"
    },
    {
        "paper_id": 2311.14738,
        "authors": "Efendi, Rahmadani Srifitri, Septriza Berliana",
        "title": "The Impact Of Interest Rates On Firms Financial Decisions",
        "comments": "15 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial decisions are the decisions that managers take with regard to the\nfinances of a company. This article aims to examine and explain the effect of\ninterest rates on economic and financial decisions such as investment, funding,\nand dividend in a firm. This research uses the correlation coefficient analysis\nmethods and descriptive methods to illustrate the relationship between interest\nrates and financial decisions. The data used in this research was obtained from\nseveral government reports and leading economic sources. The results of this\nresearch show that interest rates have a negatively insignificant effect on\ninvestment and funding decisions, but positively moderate effect on dividend\ndecisions.\n"
    },
    {
        "paper_id": 2311.14759,
        "authors": "Vincent Gurgul, Stefan Lessmann, Wolfgang Karl H\\\"ardle",
        "title": "Forecasting Cryptocurrency Prices Using Deep Learning: Integrating\n  Financial, Blockchain, and Text Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper explores the application of Machine Learning (ML) and Natural\nLanguage Processing (NLP) techniques in cryptocurrency price forecasting,\nspecifically Bitcoin (BTC) and Ethereum (ETH). Focusing on news and social\nmedia data, primarily from Twitter and Reddit, we analyse the influence of\npublic sentiment on cryptocurrency valuations using advanced deep learning NLP\nmethods. Alongside conventional price regression, we treat cryptocurrency price\nforecasting as a classification problem. This includes both the prediction of\nprice movements (up or down) and the identification of local extrema. We\ncompare the performance of various ML models, both with and without NLP data\nintegration. Our findings reveal that incorporating NLP data significantly\nenhances the forecasting performance of our models. We discover that\npre-trained models, such as Twitter-RoBERTa and BART MNLI, are highly effective\nin capturing market sentiment, and that fine-tuning Large Language Models\n(LLMs) also yields substantial forecasting improvements. Notably, the BART MNLI\nzero-shot classification model shows considerable proficiency in extracting\nbullish and bearish signals from textual data. All of our models consistently\ngenerate profit across different validation scenarios, with no observed decline\nin profits or reduction in the impact of NLP data over time. The study\nhighlights the potential of text analysis in improving financial forecasts and\ndemonstrates the effectiveness of various NLP techniques in capturing nuanced\nmarket sentiment.\n"
    },
    {
        "paper_id": 2311.14985,
        "authors": "Shiva Zamani, Alireza Moslemi Haghighi, Hamid Arian",
        "title": "Temporal Volatility Surface Projection: Parametric Surface Projection\n  Method for Derivatives Portfolio Risk Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study delves into the intricate realm of risk evaluation within the\ndomain of specific financial derivatives, notably options. Unlike other\nfinancial instruments, like bonds, options are susceptible to broader risks. A\ndistinctive trait characterizing this category of instruments is their\nnon-linear price behavior relative to their pricing parameters. Consequently,\nevaluating the risk of these securities is notably more intricate when\njuxtaposed with analogous scenarios involving fixed-income instruments, such as\ndebt securities. A paramount facet in options risk assessment is the inherent\nuncertainty stemming from first-order fluctuations in the underlying asset's\nvolatility. The dynamic patterns of volatility fluctuations manifest striking\nresemblances to the interest rate risk associated with zero-coupon bonds.\nHowever, it is imperative to bestow heightened attention on this risk category\ndue to its dependence on a more extensive array of variables and the temporal\nvariability inherent in these variables. This study scrutinizes the\nmethodological approach to risk assessment by leveraging the implied volatility\nsurface as a foundational component, thereby diverging from the reliance on a\nsingular estimate of the underlying asset's volatility.\n"
    },
    {
        "paper_id": 2311.1518,
        "authors": "Boyang Yu",
        "title": "Benchmarking Large Language Model Volatility",
        "comments": "7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance,\n  ICAIF 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.\n"
    },
    {
        "paper_id": 2311.15222,
        "authors": "Jai Pal",
        "title": "Decision Tree Psychological Risk Assessment in Currency Trading",
        "comments": "8 pages, 3 figures, 7 listings",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research paper focuses on the integration of Artificial Intelligence\n(AI) into the currency trading landscape, positing the development of\npersonalized AI models, essentially functioning as intelligent personal\nassistants tailored to the idiosyncrasies of individual traders. The paper\nposits that AI models are capable of identifying nuanced patterns within the\ntrader's historical data, facilitating a more accurate and insightful\nassessment of psychological risk dynamics in currency trading. The PRI is a\ndynamic metric that experiences fluctuations in response to market conditions\nthat foster psychological fragility among traders. By employing sophisticated\ntechniques, a classifying decision tree is crafted, enabling clearer\ndecision-making boundaries within the tree structure. By incorporating the\nuser's chronological trade entries, the model becomes adept at identifying\ncritical junctures when psychological risks are heightened. The real-time\nnature of the calculations enhances the model's utility as a proactive tool,\noffering timely alerts to traders about impending moments of psychological\nrisks. The implications of this research extend beyond the confines of currency\ntrading, reaching into the realms of other industries where the judicious\napplication of personalized modeling emerges as an efficient and strategic\napproach. This paper positions itself at the intersection of cutting-edge\ntechnology and the intricate nuances of human psychology, offering a\ntransformative paradigm for decision making support in dynamic and\nhigh-pressure environments.\n"
    },
    {
        "paper_id": 2311.15247,
        "authors": "HyeonJun Kim",
        "title": "Information Content of Financial Youtube Channel: Case Study of 3PROTV\n  and Korean Stock Market",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the information content of 3PROTV, a south Korean financial\nyoutube channel. In our sample we found evidence for the hypothesis that the\nchannel have information content on stock selection, but only on negative\nsentiment. Positively mentioned stock had pre-announcement spike followed by\nsteep fall in stock price around announcement period. Negatively mentioned\nstock started underperforming around the announcement period, with\nunderreaction dynamics in post-announcement period. In the area of market\ntiming, we found that change of sentimental tone of 3PROTV than its historical\naverage predicts the lead value of Korean market portfolio return. Its\npredictive power cannot be explained by future change in news sentiment, future\nshort term interest rate, and future liquidity risk.\n"
    },
    {
        "paper_id": 2311.15333,
        "authors": "St\\'ephane Cr\\'epey, Noufel Frikha, Azar Louzi, Gilles Pag\\`es",
        "title": "Asymptotic Error Analysis of Multilevel Stochastic Approximations for\n  the Value-at-Risk and Expected Shortfall",
        "comments": "56 pages, 1 figure, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cr\\'epey, Frikha, and Louzi (2023) introduced a nested stochastic\napproximation algorithm and its multilevel acceleration to compute the\nvalue-at-risk and expected shortfall of a random financial loss. We hereby\nestablish central limit theorems for the renormalized estimation errors\nassociated with both algorithms as well as their averaged versions. Our\nfindings are substantiated through a numerical example.\n"
    },
    {
        "paper_id": 2311.15355,
        "authors": "Miriam Isabel Seifert",
        "title": "Characterization of valid auxiliary functions for representations of\n  extreme value distributions and their max-domains of attraction",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study two important representations for extreme value\ndistributions and their max-domains of attraction (MDA), namely von Mises\nrepresentation (vMR) and variation representation (VR), which are convenient\nways to gain limit results. Both VR and vMR are defined via so-called auxiliary\nfunctions psi. Up to now, however, the set of valid auxiliary functions for vMR\nhas neither been characterized completely nor separated from those for VR. We\ncontribute to the current literature by introducing ''universal'' auxiliary\nfunctions which are valid for both VR and vMR representations for the entire\nMDA distribution families. Then we identify exactly the sets of valid auxiliary\nfunctions for both VR and vMR. Moreover, we propose a method for finding\nappropriate auxiliary functions with analytically simple structure and provide\nthem for several important distributions.\n"
    },
    {
        "paper_id": 2311.15362,
        "authors": "Hamza Saad",
        "title": "Application of Process Mining and Sequence Clustering in Recognizing an\n  Industrial Issue",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Process mining has become one of the best programs that can outline the event\nlogs of production processes in visualized detail. We have addressed the\nimportant problem that easily occurs in the industrial process called\nBottleneck. The analysis process was focused on extracting the bottlenecks in\nthe production line to improve the flow of production. Given enough stored\nhistory logs, the field of process mining can provide a suitable answer to\noptimize production flow by mitigating bottlenecks in the production stream.\nProcess mining diagnoses the productivity processes by mining event logs, this\ncan help to expose the opportunities to optimize critical production processes.\nWe found that there is a considerable bottleneck in the process because of the\nweaving activities. Through discussions with specialists, it was agreed that\nthe main problem in the weaving processes, especially machines that were\nexhausted in overloading processes. The improvement in the system has measured\nby teamwork; the cycle time for process has improved to 91%, the worker's\nperformance has improved to 96%,product quality has improved by 85%, and lead\ntime has optimized from days and weeks to hours.\n"
    },
    {
        "paper_id": 2311.15548,
        "authors": "Haoqiang Kang and Xiao-Yang Liu",
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination\n  of Hallucination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The hallucination issue is recognized as a fundamental deficiency of large\nlanguage models (LLMs), especially when applied to fields such as finance,\neducation, and law. Despite the growing concerns, there has been a lack of\nempirical investigation. In this paper, we provide an empirical examination of\nLLMs' hallucination behaviors in financial tasks. First, we empirically\ninvestigate LLM model's ability of explaining financial concepts and\nterminologies. Second, we assess LLM models' capacity of querying historical\nstock prices. Third, to alleviate the hallucination issue, we evaluate the\nefficacy of four practical methods, including few-shot learning, Decoding by\nContrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method\nand the prompt-based tool learning method for a function to generate a query\ncommand. Finally, our major finding is that off-the-shelf LLMs experience\nserious hallucination behaviors in financial tasks. Therefore, there is an\nurgent need to call for research efforts in mitigating LLMs' hallucination.\n"
    },
    {
        "paper_id": 2311.15635,
        "authors": "Kerstin Lamert, Benjamin R. Auer and Ralf Wunderlich",
        "title": "Discretization of continuous-time arbitrage strategies in financial\n  markets with fractional Brownian motion",
        "comments": "32 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study evaluates the practical usefulness of continuous-time arbitrage\nstrategies designed to exploit serial correlation in fractional financial\nmarkets. Specifically, we revisit the strategies of \\cite{Shiryaev1998} and\n\\cite{Salopek1998} and transfer them to a real-world setting by distretizing\ntheir dynamics and introducing transaction costs. In Monte Carlo simulations\nwith various market and trading parameter settings, we show that both are\nhighly promising with respect to terminal portfolio values and loss\nprobabilities. These features and complementary sparsity make them valuable\nadditions to the toolkit of quantitative investors.\n"
    },
    {
        "paper_id": 2311.15793,
        "authors": "Francesca Biagini, Andrea Mazzon, Thilo Meyer-Brandis, Katharina\n  Oberpriller",
        "title": "Supplement Liquidity based modeling of asset price bubbles via random\n  matching",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2210.13804",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is a supplement to the paper \"Liquidity based modeling of asset price\nbubbles via random matching\". The supplement is organized as follows. First, we\nprove Theorem 3.13 in [1] which provides the existence of the dynamical system\nD introduced in Definition 3.6 in [1]. Second, we show some properties of D\nwhich are summarized in Theorem 3.14 in [1]. In the following, we only state\nthe basic setting and refer to [1] for definitions.\n"
    },
    {
        "paper_id": 2311.15974,
        "authors": "Colin M. Van Oort, Ethan Ratliff-Crain, Brian F. Tivnan, and Safwan\n  Wshah",
        "title": "Adaptive Agents and Data Quality in Agent-Based Financial Markets",
        "comments": "11 pages, 6 figures, and 1 table. Contains 12 pages of supplemental\n  information with 1 figure and 22 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present our Agent-Based Market Microstructure Simulation (ABMMS), an\nAgent-Based Financial Market (ABFM) that captures much of the complexity\npresent in the US National Market System for equities (NMS). Agent-Based models\nare a natural choice for understanding financial markets. Financial markets\nfeature a constrained action space that should simplify model creation, produce\na wealth of data that should aid model validation, and a successful ABFM could\nstrongly impact system design and policy development processes. Despite these\nadvantages, ABFMs have largely remained an academic novelty. We hypothesize\nthat two factors limit the usefulness of ABFMs. First, many ABFMs fail to\ncapture relevant microstructure mechanisms, leading to differences in the\nmechanics of trading. Second, the simple agents that commonly populate ABFMs do\nnot display the breadth of behaviors observed in human traders or the trading\nsystems that they create. We investigate these issues through the development\nof ABMMS, which features a fragmented market structure, communication\ninfrastructure with propagation delays, realistic auction mechanisms, and more.\nAs a baseline, we populate ABMMS with simple trading agents and investigate\nproperties of the generated data. We then compare the baseline with\nexperimental conditions that explore the impacts of market topology or\nmeta-reinforcement learning agents. The combination of detailed market\nmechanisms and adaptive agents leads to models whose generated data more\naccurately reproduce stylized facts observed in actual markets. These\nimprovements increase the utility of ABFMs as tools to inform design and policy\ndecisions.\n"
    },
    {
        "paper_id": 2311.16004,
        "authors": "Szymon Kubiak, Tillman Weyde, Oleksandr Galkin, Dan Philps and Ram\n  Gopal",
        "title": "Improved Data Generation for Enhanced Asset Allocation: A Synthetic\n  Dataset Approach for the Fixed Income Universe",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a novel process for generating synthetic datasets tailored to\nassess asset allocation methods and construct portfolios within the fixed\nincome universe. Our approach begins by enhancing the CorrGAN model to generate\nsynthetic correlation matrices. Subsequently, we propose an Encoder-Decoder\nmodel that samples additional data conditioned on a given correlation matrix.\nThe resulting synthetic dataset facilitates in-depth analyses of asset\nallocation methods across diverse asset universes. Additionally, we provide a\ncase study that exemplifies the use of the synthetic dataset to improve\nportfolios constructed within a simulation-based asset allocation process.\n"
    },
    {
        "paper_id": 2311.16156,
        "authors": "Adrian Nerja",
        "title": "An efficiency analysis of Spanish airports",
        "comments": null,
        "journal-ref": null,
        "doi": "10.19272/202106701002",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Privatization and commercialization of airports in recent years are drawing a\ndifferent picture in the aeronautical industry. Airport benchmarking shows the\naccommodation and performance of airports in the evolution of the market and\nthe new requirements that they have to face. AENA manages a wide and\nheterogeneous network of airports. There are 46 airports divided into three\ncategories and with particularities due to their geographical location or the\ncompetitive environment where they are located. This paper analyzes the\ntechnical efficiency and its determinants of the 39 commercial airports of the\nAENA network between the years 2011-2014. To do this, two benchmarking\ntechniques, SFA and DEA, are used, with a two-stage analysis. The average\nefficiency of the network is between 75-79\\%. The results with the two\ntechniques are similar with a correlation of 0.67. With regard to the\ncommercial part of the network, AENA has a high margin for improvement because\nit is below the world and European average. AENA must focus on the development\nof the commercial area and the introduction of competition within the network\nto improve the technical efficiency of regional airports mainly.\n"
    },
    {
        "paper_id": 2311.16204,
        "authors": "Tom\\'as de la Rosa",
        "title": "Planning for the Efficient Updating of Mutual Fund Portfolios",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Once there is a decision of rebalancing or updating a portfolio of funds, the\nprocess of changing the current portfolio to the target one, involves a set of\ntransactions that are susceptible of being optimized. This is particularly\nrelevant when managers have to handle the implications of different types of\ninstruments. In this work we present linear programming and heuristic search\napproaches that produce plans for executing the update. The evaluation of our\nproposals shows cost improvements over the compared based strategy. The models\ncan be easily extended to other realistic scenarios in which a holistic\nportfolio management is required\n"
    },
    {
        "paper_id": 2311.1637,
        "authors": "David Ubilava",
        "title": "Climate, Crops, and Postharvest Conflict",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I present new evidence of the effects of climate shocks on conflict. Using\ngranular conflict and weather data covering the entire continent of Africa from\n1997 to 2023, I find that exposure to El Ni\\~no events during the crop-growing\nseason has a negative differential effect on political violence against\ncivilians in croplands during the early postharvest season. A 1{\\deg}C warming\nof the sea surface temperature in the tropical Pacific Ocean relative to its\nlong-run mean, a typical proxy for a moderate-strength El Ni\\~no event, results\nin at least a three percent reduction in political violence with civilian\ntargeting. This main finding, backed by a series of robustness checks, supports\nthe idea that agriculture is the key channel and rapacity is the key motive\nconnecting climatic shocks and political violence. Reassuringly, the magnitude\nof the estimated effect nearly doubles when I use subsets of data that are\nbetter suited to unveiling the proposed mechanism, and the effect only\nmanifests during and after the harvest, but not prior to it, when I examine\nseasonal conflict dynamics in an event-study setting. This study advances the\nknowledge of the relationship between climate and conflict. And because El\nNi\\~no events can be predicted several months in advance, these findings can\ncontribute to creating a platform for early warnings of political violence,\nspecifically in predominantly agrarian societies in Africa.\n"
    },
    {
        "paper_id": 2311.1657,
        "authors": "Daniel Polakow, Tim Gebbie, Emlyn Flint",
        "title": "Epistemic Limits of Empirical Finance: Causal Reductionism and\n  Self-Reference",
        "comments": "9 pages, 2 figures, updated references, enhanced clarification based\n  on community feedback",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The clarion call for causal reduction in the study of capital markets is\nintensifying. However, in self-referencing and open systems such as capital\nmarkets, the idea of unidirectional causation (if applicable) may be limiting\nat best, and unstable or fallacious at worst. In this research, we critically\nassess the use of scientific deduction and causal inference within the study of\nempirical finance and econometrics. We then demonstrate the idea of competing\ncausal chains using a toy model adapted from ecological predator/prey\nrelationships. From this, we develop the alternative view that the study of\nempirical finance, and the risks contained therein, may be better appreciated\nonce we admit that our current arsenal of quantitative finance tools may be\nlimited to ex post causal inference under popular assumptions. Where these\nassumptions are challenged, for example in a recognizable reflexive context,\nthe prescription of unidirectional causation proves deeply problematic.\n"
    },
    {
        "paper_id": 2311.16705,
        "authors": "Reis Castigo Intupo",
        "title": "Development of a Bankruptcy Prediction Model for the Banking Sector in\n  Mozambique Using Linear Discriminant Analysis",
        "comments": null,
        "journal-ref": "10,2023,26-40",
        "doi": "10.11114/aef.v10i4.6539",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In Mozambique there is no evidence of a bankruptcy prediction model developed\nin the national economic context, yet, back in 2016, the national banking\nsector suffered a financial shock that resulted in Mozambique Central Bank\nintervention in two banks (Moza Banco, S.A. and Nosso Banco, S.A.). This was a\nresult of the deterioration of their financial and prudential indicators,\nalthough Mozambique had been adhering to the Basel Accords since 1994. The\nBasel Accords provides recommendations on banking sector supervision worldwide\nwith the aim to enhance financial system stability. While it does not predict\nbankruptcy, the prediction model can be used as an auxiliary tool to manage\nthat risk, but this has to be built in the national economic context. This\npaper develops for Mozambique banking sector a bankruptcy prediction model in\nthe Mozambican context through the linear discriminant analyses method,\nfollowing two assumptions: (i) composition of the sample and (ii) robustness of\nthe financial prediction indicators (the capital structure, profitability asset\nconcentration and asset quality) from 2012 to 2020. The developed model\nattained an accuracy level of 84% one year before Central Bank intervention\n(2015) with the entire population of 19 banks of the sector, which makes it\nrecommendable as a risk management tool for this sector.\n"
    },
    {
        "paper_id": 2311.16762,
        "authors": "Matteo Gambara, Giulia Livieri, Andrea Pallavicini",
        "title": "Machine learning methods for American-style path-dependent contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work, we introduce and compare state-of-the-art algorithms,\nthat are now classified under the name of machine learning, to price Asian and\nlook-back products with early-termination features. These include randomized\nfeed-forward neural networks, randomized recurrent neural networks, and a novel\nmethod based on signatures of the underlying price process. Additionally, we\nexplore potential applications on callable certificates. Furthermore, we\npresent an innovative approach for calculating sensitivities, specifically\nDelta and Gamma, leveraging Chebyshev interpolation techniques.\n"
    },
    {
        "paper_id": 2311.17011,
        "authors": "Alexis Anagnostakis",
        "title": "Pricing and hedging for a sticky diffusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a financial market model featuring a risky asset with a sticky\ngeometric Brownian motion price dynamic and a constant interest rate $r \\in\n\\mathbb R$. We prove that the model is arbitrage-free if and only if $r =0 $.\nIn this case, we find the unique riskless replication strategy, derive the\nassociated pricing equation. We also identify a class of replicable payoffs\nthat coincides with the replicable payoffs in the standard Black-Scholes model.\nLast, we numerically evaluate discrete-time hedging and the hedging error\nincurred from misrepresenting price stickiness.\n"
    },
    {
        "paper_id": 2311.17193,
        "authors": "Guillermo Jos\\'e Navarro del Toro",
        "title": "El tequila para consumo nacional como una ventana de oportunidades para\n  el peque\\~no productor agavero",
        "comments": "20 hojas, in Spanish language",
        "journal-ref": null,
        "doi": "10.23913/ricea.v10i19.159",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The objective of this research was to determine the degree of knowledge that\nthe inhabitants of the Guadalajara Metropolitan Area (made up of the\nmunicipalities Guadalajara, Tlajomulco de Z\\'u\\~niga, Tlaquepaque, Zapopan and\nTonal\\'a) had regarding tequila and the brands produced in Los Altos of\nJalisco. For this, a survey consisting of five questions was designed, which\nwas applied in the central square, center or z\\'ocalo of each municipality. The\nresults show that the big brands, when acquired by international companies,\nfocused their attention on capturing the consumer in international markets,\nsince the prices of the same products that they export have been out of the\npocket of those who like That drink, so it could be considered that the big\nbrands, have left the national market a little behind, of course they did not\nabandon it completely, but it stopped being their main objective. Therefore, it\ncan be concluded that the national market is the window of opportunity to join\nthe small and still unknown producers to work together and in a grouped way,\nthey are able to standardize a series of products that being of the same\nquality and same packaging, they can cover the national market, and perhaps, in\nthe future, become a large company distributed throughout the territory and\nbegin the export process only with national capital.\n"
    },
    {
        "paper_id": 2311.17239,
        "authors": "Andrea Teruzzi",
        "title": "Tail Risk and Systemic Risk Estimation of Cryptocurrencies: an\n  Expectiles and Marginal Expected Shortfall based approach",
        "comments": "16 pages and 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The issue related to the quantification of the tail risk of cryptocurrencies\nis considered in this paper. The statistical methods used in the study are\nthose concerning recent developments in Extreme Value Theory (EVT) for weakly\ndependent data. This research proposes an expectile-based approach for\nassessing the tail risk of dependent data. Expectile is a summary statistic\nthat generalizes the concept of mean, as the quantile generalizes the concept\nof the median. We present the empirical findings for a dataset of\ncryptocurrencies. We propose a method for dynamically evaluating the level of\nthe expectiles by estimating the level of the expectiles of the residuals of a\nheteroscedastic regression, such as a GARCH model. Finally, we introduce the\nMarginal Expected Shortfall (MES) as a tool for measuring the marginal impact\nof single assets on systemic shortfalls. In our case of interest, we are\nfocused on the impact of a single cryptocurrency on the systemic risk of the\nwhole cryptocurrency market. In particular, we present an expectile-based MES\nfor dependent data.\n"
    },
    {
        "paper_id": 2311.17252,
        "authors": "Jialin Dong, Kshama Dwarakanath, Svitlana Vyetrenko",
        "title": "Analyzing the Impact of Tax Credits on Households in Simulated Economic\n  Systems with Learning Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In economic modeling, there has been an increasing investigation into\nmulti-agent simulators. Nevertheless, state-of-the-art studies establish the\nmodel based on reinforcement learning (RL) exclusively for specific agent\ncategories, e.g., households, firms, or the government. It lacks concerns over\nthe resulting adaptation of other pivotal agents, thereby disregarding the\ncomplex interactions within a real-world economic system. Furthermore, we pay\nattention to the vital role of the government policy in distributing tax\ncredits. Instead of uniform distribution considered in state-of-the-art, it\nrequires a well-designed strategy to reduce disparities among households and\nimprove social welfare. To address these limitations, we propose an expansive\nmulti-agent economic model comprising reinforcement learning agents of numerous\ntypes. Additionally, our research comprehensively explores the impact of tax\ncredit allocation on household behavior and captures the spectrum of spending\npatterns that can be observed across diverse households. Further, we propose an\ninnovative government policy to distribute tax credits, strategically\nleveraging insights from tax credit spending patterns. Simulation results\nillustrate the efficacy of the proposed government strategy in ameliorating\ninequalities across households.\n"
    },
    {
        "paper_id": 2311.1727,
        "authors": "Yan Dolinsky",
        "title": "Delayed Semi-static Hedging in the Continuous Time Bachelier Model",
        "comments": "arXiv admin note: text overlap with arXiv:2308.10550",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work we study the continuous time exponential utility maximization\nproblem in the framework of semi-static hedging.\n"
    },
    {
        "paper_id": 2311.17443,
        "authors": "Alice Di Bella, Federico Canti, Matteo Giacomo Prina, Valeria\n  Casalicchio, Giampaolo Manzolini, Wolfram Sparber",
        "title": "Power system investment optimization to identify carbon neutrality\n  scenarios for Italy",
        "comments": "18 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In 2021, the European Commission has adopted the Fit-for-55 policy package,\nlegally binding European countries to reduce their CO2 emissions by 55% with\nrespect to 1990, a first step to achieve carbon neutrality in 2050. In this\ncontext, it is crucial to help national policymakers to choose the most\nappropriate technologies to achieve these goals and energy system modelling can\nbe a valuable tool. This article presents a model of the Italian power system\nrealized employing the open energy modelling framework Oemof. A Linear\nProgramming Optimization is implemented to evaluate how to minimise system\ncosts at decreasing CO2 emissions in 2030. The developed tool is applied to\nevaluate different research questions: i) pathway towards full decarbonization\nand power self-sufficiency of the electricity sector in Italy, ii) relevance of\nflexibility assets in power grids: li-ion batteries, hydrogen storage and\ntransmission lines reinforcement. A 55% CO2 emissions reduction for the actual\nItalian power sector can be achieved through an increase of 30% of the total\nannual system cost. Full decarbonization can be reached with four times today's\nannual costs, which could be lowered with sector coupling and considering more\ntechnologies.\n"
    },
    {
        "paper_id": 2311.17715,
        "authors": "Xihan Xiong, Zhipeng Wang, Tianxiang Cui, William Knottenbelt and\n  Michael Huth",
        "title": "Market Misconduct in Decentralized Finance (DeFi): Analysis, Regulatory\n  Challenges and Policy Implications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Technological advancement drives financial innovation, reshaping the\ntraditional finance landscape and redefining user-market interactions. The rise\nof blockchain and Decentralized Finance (DeFi) underscores this intertwined\nevolution of technology and finance. While DeFi has introduced exciting\nopportunities, it has also exposed the ecosystem to new forms of market\nmisconduct. This paper aims to bridge the academic and regulatory gaps by\naddressing key research questions about market misconduct in DeFi. We begin by\ndiscussing how blockchain technology can potentially enable the emergence of\nnovel forms of market misconduct. We then offer a comprehensive definition and\ntaxonomy for understanding DeFi market misconduct. Through comparative analysis\nand empirical measurements, we examine the novel forms of misconduct in DeFi,\nshedding light on their characteristics and social impact. Subsequently, we\ninvestigate the challenges of building a tailored regulatory framework for\nDeFi. We identify key areas where existing regulatory frameworks may need\nenhancement. Finally, we discuss potential approaches that bring DeFi into the\nregulatory perimeter.\n"
    },
    {
        "paper_id": 2311.17875,
        "authors": "Andrea Auconi",
        "title": "Interaction uncertainty in financial networks",
        "comments": "5 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A minimal stochastic dynamical model of the interbank network is introduced,\nwith linear interactions mediated by an integral of recent variations. Defining\nstress as the variance over the banks' states, the interaction correction to\nthe stress expectation is derived and studied on the short-medium timescale in\nan expansion. It is shown that, while different interaction matrices can\namplify or absorb fluctuations, on average interactions increase the stress\nexpectation. More in general, this analytical framework enables to estimate the\nimpact of uncertainty about financial exposures, and to draw conclusions about\nthe importance of disclosure.\n"
    },
    {
        "paper_id": 2311.17981,
        "authors": "David Kr\\\"oger, Jan Peper, Nils Offermann, Christian Rehtanz",
        "title": "Optimizing the Generation and Transmission Capacity of Offshore Wind\n  Parks under Weather Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Offshore wind power in the North Sea is considered a main pillar in Europe's\nfuture energy system. A key challenge lies in determining the optimal spatial\ncapacity allocation of offshore wind parks in combination with the dimensioning\nand layout of the connecting high-voltage direct current grid infrastructure.\nTo determine economically cost optimal configurations, we apply an integrated\ncapacity and transmission expansion problem within a pan-European electricity\nmarket and transmission grid model with a high spatial and temporal\ngranularity. By conducting scenario analysis for the year 2030 with a gradually\nincreasing CO2 price, possible offshore expansion paths are derived and\npresented. Special emphasis is laid on the effects of weather uncertainty by\nincorporating data from 21 historical weather years in the analysis. Two key\nfindings are (i) an expansion in addition to the existing offshore wind\ncapacity of 0 GW (136 EUR/tCO2), 12 GW (159 EUR/tCO2) and 30 GW (186 EUR/tCO2)\ndependent on the underlying CO2 price. (ii) A strong sensitivity of the results\ntowards the underlying weather data highlighting the importance of\nincorporating multiple weather years.\n"
    },
    {
        "paper_id": 2311.18164,
        "authors": "Agostino Capponi and Ruizhe Jia and Brian Zhu",
        "title": "The Paradox Of Just-in-Time Liquidity in Decentralized Exchanges: More\n  Providers Can Sometimes Mean Less Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study Just-in-time (JIT) liquidity provision in blockchain-based\ndecentralized exchanges. A JIT liquidity provider (LP) monitors pending swap\norders in public mempools of blockchains to sandwich orders of their choice\nwith liquidity, depositing right before and withdrawing right after the order.\nOur game-theoretic model with asymmetrically informed agents reveals that a JIT\nLP's presence does not always enhance liquidity pool depth, as one might\nexpect. While passive LPs face adverse selection by informed arbitrageurs, a\nJIT LP's ability to detect pending orders for toxic order flow prior to\nliquidity provision lets them avoid being adversely selected. JIT LPs thus only\nprovide liquidity to uninformed orders and crowd out passive LPs when order\nvolume is not sufficiently elastic to pool depth, possibly reducing overall\nmarket liquidity. We show that using a two-tiered fee structure which transfers\na part of a JIT LP's fee revenue to passive LPs or allowing for JIT LPs to\ncompete \\`{a} la Cournot are potential solutions to mitigate the negative\neffects of JIT liquidity.\n"
    },
    {
        "paper_id": 2311.18176,
        "authors": "Baishuai Zuo, Narayanaswamy Balakrishnan, Chuancun Yin",
        "title": "An analysis of multivariate measures of skewness and kurtosis of\n  skew-elliptical distributions",
        "comments": "22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines eight measures of skewness and Mardia measure of kurtosis\nfor skew-elliptical distributions. Multivariate measures of skewness considered\ninclude Mardia, Malkovich-Afifi, Isogai, Song, Balakrishnan-Brito-Quiroz,\nM$\\acute{o}$ri, Rohatgi and Sz$\\acute{e}$kely, Kollo and Srivastava measures.\nWe first study the canonical form of skew-elliptical distributions, and then\nderive exact expressions of all measures of skewness and kurtosis for the\nfamily of skew-elliptical distributions, except for Song's measure.\nSpecifically, the formulas of these measures for skew normal, skew $t$, skew\nlogistic, skew Laplace, skew Pearson type II and skew Pearson type VII\ndistributions are obtained. Next, as in Malkovich and Afifi (1973), test\nstatistics based on a random sample are constructed for illustrating the\nusefulness of the established results. In a Monte Carlo simulation study,\ndifferent measures of skewness and kurtosis for $2$-dimensional skewed\ndistributions are calculated and compared. Finally, real data is analyzed to\ndemonstrate all the results.\n"
    },
    {
        "paper_id": 2311.18269,
        "authors": "Julia Zaripova, Ksenia Chuprianova, Irina Polyakova, Daria Semenova,\n  Sofya Kulikova",
        "title": "The impact of sensory characteristics on the willingness to pay for\n  honey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Honey consumption in Russia has been actively growing in recent years due to\nthe increasing interest in healthy and environment-friendly food products.\nHowever, it remains an open question which characteristics of honey are the\nmost significant for consumers and, more importantly, from an economic point of\nview, for which of them consumers are willing to pay. The purpose of this study\nwas to investigate the role of sensory characteristics in assessing consumers'\nwillingness to pay for honey and to determine which properties and\ncharacteristics \"natural\" honey should have to encourage repeated purchases by\ntarget consumers. The study involved a behavioral experiment that included a\npre-test questionnaire, blind tasting of honey samples, an in-room test to\nassess perceived quality, and a closed auction using the\nBecker-DeGroote-Marschak method. As the result, it was revealed that the\ncorrespondence of the expected sensations to the actual taste, taste intensity,\nduration of the aftertaste and the sensations of tickling in the throat had a\npositive effect on both the perceived quality of the product and the\nwillingness to pay for it, while perception of off-flavors or added sugar had a\nnegative impact. Using factor analysis, we have combined 21 sensory\ncharacteristics of honey into eight components that were sufficient to obtain\nthe flavor portrait of honey by Russian consumers.\n"
    },
    {
        "paper_id": 2311.18283,
        "authors": "Bruno Durin and Mathieu Rosenbaum and Gr\\'egoire Szymanski",
        "title": "The two square root laws of market impact and the role of sophisticated\n  market participants",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to disentangle the roles of volume and of\nparticipation rate in the price response of the market to a sequence of\ntransactions. To do so, we are inspired the methodology introduced in\narXiv:1402.1288, arXiv:1805.07134 where price dynamics are derived from order\nflow dynamics using no arbitrage assumptions. We extend this approach by taking\ninto account a sophisticated market participant having superior abilities to\nanalyse market dynamics. Our results lead to the recovery of two square root\nlaws: (i) For a given participation rate, during the execution of a metaorder,\nthe market impact evolves in a square root manner with respect to the cumulated\ntraded volume. (ii) For a given executed volume $Q$, the market impact is\nproportional to $\\sqrt{\\gamma}$, where $\\gamma$ denotes the participation rate,\nfor $\\gamma$ large enough. Smaller participation rates induce a more linear\ndependence of the market impact in the participation rate.\n"
    },
    {
        "paper_id": 2311.18351,
        "authors": "Chenglin Qing and Shanyue Jin",
        "title": "Does ESG and Digital Transformation affects Corporate Sustainability?\n  The Moderating role of Green Innovation",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, environmental, social, and governance (ESG) has become an important\nfactor in companies' sustainable development. Artificial intelligence (AI) is\nalso a core digital technology that can create innovative, sustainable,\ncomprehensive, and resilient environments. ESG- and AI-based digital\ntransformation is a relevant strategy for managing business value and\nsustainability in corporate green management operations. Therefore, this study\nexamines how corporate sustainability relates to ESG- and AI-based digital\ntransformation. Furthermore, it confirms the moderating effect of green\ninnovation on the process of increasing sustainability. To achieve the purpose\nof this study, 359 data points collected for hypothesis testing were used for\nstatistical analysis and for mobile business platform users. The following\nconclusions are drawn. (1) ESG activities have become key variables that enable\nsustainable corporate growth. Companies can implement eco-friendly operating\nprocesses through ESG activities. (2) This study verifies the relationship\nbetween AI-based digital transformation and corporate sustainability and\nconfirms that digital transformation positively affects corporate\nsustainability. In addition, societal problems can be identified and\nenvironmental accidents prevented through technological innovation. (3) This\nstudy does not verify the positive moderating effect of green innovation;\nhowever, it emphasizes its necessity and importance. Although green innovation\nimproves performance only in the long term, it is a key factor for companies\npursuing sustainable growth. This study reveals that ESG- and AI-based digital\ntransformation is an important tool for promoting corporate sustainability,\nbroadening the literature in related fields and providing insights for\ncorporate management and government policymakers to advance corporate\nsustainability.\n"
    },
    {
        "paper_id": 2311.18453,
        "authors": "Dr Mir Hasan Naqvi, Asnan Ahmed, Dr Asif Pervez",
        "title": "Implementing Sustainable Tourism practices in luxury resorts of\n  Maldives: Sustainability principles & Tripple Bottomline Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The aim of the research paper is to understand the sustainability challenges\nfaced by resorts mainly luxury in Maldives and to implement the sustainable\ntourism practices. The Maldives economy is dependent mostly on the fishing,\nboat building, boat repairing and tourism. Over recent years there is a drastic\nchange that has took place in Maldives in tourism industry. Maldives has\nprogressed to be the upper middle-income country and luxury resorts are the\nreason for increased GDP in the country. Although there are some practices\nassociated with the luxury resorts to follow in terms of environmental\nconcerns. Present study focuses on the triple bottom line approach and the 12\nmajor Sustainable Tourism Principles as a framework for sustainability\npractices and its implementation including the challenges associated in\nMaldives. The paper suggests some recommendations on several paradigm of\nenforcing laws and regulations, waste management facilities, fostering\ncollaboration along with promoting local agriculture. The study also\ncontemplates on several other areas such as on the impact of sustainability\ninitiatives, coral restoration, and the use of sustainable supply chains. The\nintent of the current research is to suggest methods to promote the sustainable\npractices in luxury resort in Maldives.\n"
    },
    {
        "paper_id": 2311.18717,
        "authors": "Brett Hemenway Falk, Gerry Tsoukalas, Niuniu Zhang",
        "title": "NFT Wash Trading: Direct vs. Indirect Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Recent studies estimate around 70% of traded value on off-chain crypto\nexchanges like Binance is wash trading. This paper turns to NFT markets, where\nthe on-chain nature of transactions-a key tenet of Web3 innovation-enables more\ndirect estimation methods to be applied. Focusing on three of the largest NFT\nmarketplaces, we find 30-40% of NFT volume and 25-95% of traded value involve\nwash trading. We leverage this direct approach to critically evaluate recent\nindirect estimation methods suggested in the literature, revealing major\ndifferences in effectiveness, with some failing altogether. Trade-roundedness\nfilters, as suggested in Cong et al. (2023), emerge as the most accurate\nindirect estimation method. In fact, we show how direct and indirect approaches\ncan be closely aligned via hyper-parameter fine-tuning. Our findings underscore\nthe crucial role of technological innovation in detecting and regulating\nfinancial misconduct in digital finance.\n"
    },
    {
        "paper_id": 2311.18749,
        "authors": "Jie Shi, Arno P. J. M. Siebes, Siamak Mehrkanoon",
        "title": "TransCORALNet: A Two-Stream Transformer CORAL Networks for Supply Chain\n  Credit Assessment Cold Start",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes an interpretable two-stream transformer CORAL networks\n(TransCORALNet) for supply chain credit assessment under the segment industry\nand cold start problem. The model aims to provide accurate credit assessment\nprediction for new supply chain borrowers with limited historical data. Here,\nthe two-stream domain adaptation architecture with correlation alignment\n(CORAL) loss is used as a core model and is equipped with transformer, which\nprovides insights about the learned features and allow efficient\nparallelization during training. Thanks to the domain adaptation capability of\nthe proposed model, the domain shift between the source and target domain is\nminimized. Therefore, the model exhibits good generalization where the source\nand target do not follow the same distribution, and a limited amount of target\nlabeled instances exist. Furthermore, we employ Local Interpretable\nModel-agnostic Explanations (LIME) to provide more insight into the model\nprediction and identify the key features contributing to supply chain credit\nassessment decisions. The proposed model addresses four significant supply\nchain credit assessment challenges: domain shift, cold start, imbalanced-class\nand interpretability. Experimental results on a real-world data set demonstrate\nthe superiority of TransCORALNet over a number of state-of-the-art baselines in\nterms of accuracy. The code is available on GitHub\nhttps://github.com/JieJieNiu/TransCORALN .\n"
    },
    {
        "paper_id": 2312.00033,
        "authors": "Ravi Kashyap",
        "title": "DeFi Security: Turning The Weakest Link Into The Strongest Attraction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The primary innovation we pioneer -- focused on blockchain information\nsecurity -- is called the Safe-House. The Safe-House is badly needed since\nthere are many ongoing hacks and security concerns in the DeFi space right now.\nThe Safe-House is a piece of engineering sophistication that utilizes existing\nblockchain principles to bring about greater security when customer assets are\nmoved around. The Safe-House logic is easily implemented as smart contracts on\nany decentralized system. The amount of funds at risk from both internal and\nexternal parties -- and hence the maximum one time loss -- is guaranteed to\nstay within the specified limits based on cryptographic fundamentals.\n  To improve the safety of the Safe-House even further, we adapt the one time\npassword (OPT) concept to operate using blockchain technology. Well suited to\nblockchain cryptographic nuances, our secondary advancement can be termed the\none time next time password (OTNTP) mechanism. The OTNTP is designed to\ncomplement the Safe-House making it even more safe.\n  We provide a detailed threat assessment model -- discussing the risks faced\nby DeFi protocols and the specific risks that apply to blockchain fund\nmanagement -- and give technical arguments regarding how these threats can be\novercome in a robust manner. We discuss how the Safe-House can participate with\nother external yield generation protocols in a secure way. We provide reasons\nfor why the Safe-House increases safety without sacrificing the efficiency of\noperation. We start with a high level intuitive description of the landscape,\nthe corresponding problems and our solutions. We then supplement this overview\nwith detailed discussions including the corresponding mathematical formulations\nand pointers for technological implementation. This approach ensures that the\narticle is accessible to a broad audience.\n"
    },
    {
        "paper_id": 2312.00044,
        "authors": "Arisa Ema, Ryo Sato, Tomoharu Hase, Masafumi Nakano, Shinji Kamimura,\n  Hiromu Kitamura",
        "title": "Advancing AI Audits for Enhanced AI Governance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As artificial intelligence (AI) is integrated into various services and\nsystems in society, many companies and organizations have proposed AI\nprinciples, policies, and made the related commitments. Conversely, some have\nproposed the need for independent audits, arguing that the voluntary principles\nadopted by the developers and providers of AI services and systems\ninsufficiently address risk. This policy recommendation summarizes the issues\nrelated to the auditing of AI services and systems and presents three\nrecommendations for promoting AI auditing that contribute to sound AI\ngovernance. Recommendation1.Development of institutional design for AI audits.\nRecommendation2.Training human resources for AI audits. Recommendation3.\nUpdating AI audits in accordance with technological progress.\n  In this policy recommendation, AI is assumed to be that which recognizes and\npredicts data with the last chapter outlining how generative AI should be\naudited.\n"
    },
    {
        "paper_id": 2312.00202,
        "authors": "Zhi Chen",
        "title": "Investigate The ESG Score Methodology",
        "comments": "8 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Whether the Refinitiv provide a reliable and trusted methodology in the\nprocess of aggregating 10 category scores to overall score?\n"
    },
    {
        "paper_id": 2312.00266,
        "authors": "Weixuan Xia",
        "title": "Optimal Consumption--Investment Problems under Time-Varying Incomplete\n  Preferences",
        "comments": "72 pages, 1 table, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main objective of this paper is to develop a martingale-type solution to\noptimal consumption--investment choice problems ([Merton, 1969] and [Merton,\n1971]) under time-varying incomplete preferences driven by externalities such\nas patience, socialization effects, and market volatility. The market is\ncomposed of multiple risky assets and multiple consumption goods, while in\naddition there are multiple fluctuating preference parameters with inexact\nvalues connected to imprecise tastes. Utility maximization is a multi-criteria\nproblem with possibly function-valued criteria. To come up with a complete\ncharacterization of the solutions, first we motivate and introduce a set-valued\nstochastic process for the dynamics of multi-utility indices and formulate the\noptimization problem in a topological vector space. Then, we modify a classical\nscalarization method allowing for infiniteness and randomness in dimensions and\nprove results of equivalence to the original problem. Illustrative examples are\ngiven to demonstrate practical interests and method applicability\nprogressively. The link between the original problem and a dual problem is also\ndiscussed, relatively briefly. Finally, using Malliavin calculus with\nstochastic geometry, we find optimal investment policies to be generally\nset-valued, each of whose selectors admits a four-way decomposition involving\nan additional indecisiveness risk-hedging portfolio. Our results touch on new\ndirections for optimal consumption--investment choices in the presence of\nincomparability and time inconsistency, also signaling potentially testable\nassumptions on the variability of asset prices. Simulation techniques for\nset-valued processes are studied for how solved optimal policies can be\ncomputed in practice.\n"
    },
    {
        "paper_id": 2312.00405,
        "authors": "Mishari Al-Foraih, Jan Posp\\'i\\v{s}il and Josep Vives",
        "title": "Computation of Greeks under rough Volterra stochastic volatility models\n  using the Malliavin calculus approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Malliavin calculus techniques we obtain formulas for computing Greeks\nunder different rough Volterra stochastic volatility models. In particular we\nobtain formulas for rough versions of Stein-Stein, SABR and Bergomi models and\nnumerically demonstrate the convergence.\n"
    },
    {
        "paper_id": 2312.00436,
        "authors": "Phoebe Koundouri, Georgios I. Papayiannis, Electra V. Petracou and\n  Athanasios N. Yannacopoulos",
        "title": "Consensus group decision making under model uncertainty with a view\n  towards environmental policy making",
        "comments": "31 pages, 5 figures",
        "journal-ref": "Environmental and Resource Economics, 2024",
        "doi": "10.1007/s10640-024-00846-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a consensus group decision making scheme under model\nuncertainty consisting of an iterative two-stage procedure and based on the\nconcept of Fr\\'echet barycenter. Each step consists of two stages: the agents\nfirst update their position in the opinion metric space by a local barycenter\ncharacterized by the agents' immediate interactions and then a moderator makes\na proposal in terms of a global barycenter, checking for consensus at each\nstep. In cases of large heterogeneous groups the procedure can be complemented\nby an auxiliary initial homogenization step, consisting of a clustering\nprocedure in opinion space, leading to large homogeneous groups for which the\naforementioned procedure will be applied.\n  The scheme is illustrated in examples motivated from environmental economics.\n"
    },
    {
        "paper_id": 2312.00442,
        "authors": "Alicia Martin-Navarro, Maria Paula Lechuga Sancho, Jose Aurelio\n  Medina-Garrido",
        "title": "BPMS for management: a systematic literature review",
        "comments": null,
        "journal-ref": "Revista Espanola de Documentacion Cientifica, 41(3), e213 (2018)",
        "doi": "10.3989/redc.2018.3.1532",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The aim of this paper is to carry out a systematic analysis of the literature\nto show the state of the art of Business Processes Management Systems (BPMS).\nBPMS represents a technology that automates business processes connecting users\nwith their tasks. For this, a systematic review of the literature of the last\nten years was carried out, using scientific papers indexed in the main\ndatabases of the knowledge area. The papers generated by the search were later\nanalysed and filtered. Among the findings of this study, the academic interest\nand the multidisciplinary nature of the subject, as this type of studies have\nbeen identified in different areas of knowledge. Our research is a starting\npoint for future research eager to develop a more robust theory and broaden the\ninterest of the subject due its economic impact on process management.\n"
    },
    {
        "paper_id": 2312.00443,
        "authors": "Alicia Martin-Navarro, Maria Paula Lechuga Sancho, Jose Aurelio\n  Medina-Garrido",
        "title": "Business process management systems in port processes: a systematic\n  literature review",
        "comments": null,
        "journal-ref": "International Journal of Agile Systems and Management 13:258-278\n  (2020)",
        "doi": "10.1504/ijasm.2020.109245",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Business Process Management Systems (BPMS) represent a technology that\nautomates business processes, connecting users to their tasks. There are many\nbusiness processes within the port activity that can be improved through the\nuse of more efficient technologies and BPMS in particular, which can help to\ncoordinate and automate critical processes such as cargo manifests, customs\ndeclaration the management of scales, or dangerous goods, traditionally\nsupported by EDI technologies. These technologies could be integrated with\nBPMS, modernizing port logistics management. The aim of this work is to\ndemonstrate, through a systematic analysis of the literature, the state of the\nart in BPMS research in the port industry. For this, a systematic review of the\nliterature of the last ten years was carried out. The works generated by the\nsearch were subsequently analysed and filtered. After the investigation, it is\ndiscovered that the relationship between BPMS and the port sector is\npractically non-existent which represents an important gap to be covered and a\nfuture line of research.\n"
    },
    {
        "paper_id": 2312.00506,
        "authors": "Anil R. Doshi and Oliver P. Hauser",
        "title": "Generative artificial intelligence enhances creativity but reduces the\n  diversity of novel content",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Creativity is core to being human. Generative artificial intelligence (GenAI)\nholds promise for humans to be more creative by offering new ideas, or less\ncreative by anchoring on GenAI ideas. We study the causal impact of GenAI on\nthe production of a creative output in an online experimental study where some\nwriters are could obtain ideas for a story from a GenAI platform. Access to\nGenAI ideas causes an increase in the writer's creativity with stories being\nevaluated as better written and more enjoyable, especially among less creative\nwriters. However, GenAI-enabled stories are more similar to each other than\nstories by humans alone. Our results have implications for researchers,\npolicy-makers and practitioners interested in bolstering creativity, but point\nto potential downstream consequences from over-reliance.\n"
    },
    {
        "paper_id": 2312.00517,
        "authors": "Alicia Martin-Navarro, Felix Velicia-Martin, Jose Aurelio\n  Medina-Garrido, Ricardo Gouveia Rodrigues",
        "title": "Causal propensity as an antecedent of entrepreneurial intentions in\n  tourism students",
        "comments": null,
        "journal-ref": "International Entrepreneurship and Management Journal,\n  19(0123456789) 2023",
        "doi": "10.1007/s11365-022-00826-1",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The tourism sector is a sector with many opportunities for business\ndevelopment. Entrepreneurship in this sector promotes economic growth and job\ncreation. Knowing how entrepreneurial intention develops facilitates its\ntransformation into entrepreneurial behaviour. Entrepreneurial behaviour can\nadopt a causal logic, an effectual logic or a combination of both. Considering\nthe causal logic, decision-making is done through prediction. In this way,\nentrepreneurs try to increase their market share by planning strategies and\nanalysing possible deviations from their plans. Previous literature studies\ncausal entrepreneurial behaviour, as well as variables such as creative\ninnovation, proactive decisions and entrepreneurship training when the\nentrepreneur has already created his or her firm. However, there is an obvious\ngap at a stage prior to the start of entrepreneurial activity when the\nentrepreneurial intention is formed. This paper analyses how creativity,\nproactivity, entrepreneurship education and the propensity for causal behaviour\ninfluence entrepreneurial intentions. To achieve the research objective, we\nanalysed a sample of 464 undergraduate tourism students from two universities\nin southern Spain. We used SmartPLS 3 software to apply a structural equation\nmethodology to the measurement model composed of nine hypotheses. The results\nshow, among other relationships, that causal propensity, entrepreneurship\nlearning programmes and proactivity are antecedents of entrepreneurial\nintentions. These findings have implications for theory, as they fill a gap in\nthe field of entrepreneurial intentions. Considering propensity towards causal\nbehaviour before setting up the firm is unprecedented. Furthermore, the results\nof this study have practical implications for the design of public education\npolicies and the promotion of business creation in the tourism sector.\n"
    },
    {
        "paper_id": 2312.00551,
        "authors": "Martin Lnenicka, Anastasija Nikiforova, Mariusz Luterek, Petar Milic,\n  Daniel Rudmark, Sebastian Neumaier, Caterina Santoro, Cesar Casiano Flores,\n  Marijn Janssen, and Manuel Pedro Rodr\\'iguez Bol\\'ivar",
        "title": "Identifying patterns and recommendations of and for sustainable open\n  data initiatives: a benchmarking-driven analysis of open government data\n  initiatives among European countries",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Open government and open (government) data are seen as tools to create new\nopportunities, eliminate or at least reduce information inequalities and\nimprove public services. More than a decade of these efforts has provided much\nexperience, practices, and perspectives to learn how to better deal with them.\nThis paper focuses on benchmarking of open data initiatives over the years and\nattempts to identify patterns observed among European countries that could lead\nto disparities in the development, growth, and sustainability of open data\necosystems. To do this, we studied benchmarks and indices published over the\nlast years (57 editions of 8 artifacts) and conducted a comparative case study\nof eight European countries, identifying patterns among them considering\ndifferent potentially relevant contexts such as e-government, open government\ndata, open data indices and rankings, and others relevant for the country under\nconsideration. Using a Delphi method, we reached a consensus within a panel of\nexperts and validated a final list of 94 patterns, including their frequency of\noccurrence among studied countries and their effects on the respective\ncountries. Finally, we took a closer look at the developments in identified\ncontexts over the years and defined 21 recommendations for more resilient and\nsustainable open government data initiatives and ecosystems and future steps in\nthis area.\n"
    },
    {
        "paper_id": 2312.00613,
        "authors": "Andrea Bovo, Tiziano De Angelis, Jan Palczewski",
        "title": "Stopper vs. singular-controller games with degenerate diffusions",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study zero-sum stochastic games between a singular controller and a\nstopper when the (state-dependent) diffusion matrix of the underlying\ncontrolled diffusion process is degenerate. In particular, we show the\nexistence of a value for the game and determine an optimal strategy for the\nstopper. The degeneracy of the dynamics prevents the use of analytical methods\nbased on solution in Sobolev spaces of suitable variational problems. Therefore\nwe adopt a probabilistic approach based on a perturbation of the underlying\ndiffusion modulated by a parameter $\\gamma>0$. For each $\\gamma>0$ the\napproximating game is non-degenerate and admits a value $u^\\gamma$ and an\noptimal strategy $\\tau^\\gamma_*$ for the stopper. Letting $\\gamma\\to 0$ we\nprove convergence of $u^\\gamma$ to a function $v$, which identifies the value\nof the original game. We also construct explicitly optimal stopping times\n$\\theta^\\gamma_*$ for $u^\\gamma$, related but not equal to $\\tau^\\gamma_*$,\nwhich converge almost surely to an optimal stopping time $\\theta_*$ for the\ngame with degenerate dynamics.\n"
    },
    {
        "paper_id": 2312.00904,
        "authors": "Christoph K\\\"uhn and Christopher Lorenz",
        "title": "Insider trading in discrete time Kyle games",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a new discrete time version of Kyle's (1985) classic model of\ninsider trading, formulated as a generalised extensive form game. The model has\nthree kinds of traders: an insider, random noise traders, and a market maker.\nThe insider aims to exploit her informational advantage and maximise expected\nprofits while the market maker observes the total order flow and sets prices\naccordingly.\n  First, we show how the multi-period model with finitely many pure strategies\ncan be reduced to a (static) social system in the sense of Debreu (1952) and\nprove the existence of a sequential Kyle equilibrium, following Kreps and\nWilson (1982). This works for any probability distribution with finite support\nof the noise trader's demand and the true value, and for any finite information\nflow of the insider. In contrast to Kyle (1985) with normal distributions,\nequilibria exist in general only in mixed strategies and not in pure\nstrategies.\n  In the single-period model we establish bounds for the insider's strategy in\nequilibrium. Finally, we prove the existence of an equilibrium for the game\nwith a continuum of actions, by considering an approximating sequence of games\nwith finitely many actions. Because of the lack of compactness of the set of\nmeasurable price functions, standard infinite-dimensional fixed point theorems\nare not applicable.\n"
    },
    {
        "paper_id": 2312.00916,
        "authors": "Alicia Martin-Navarro, Jose Aurelio Medina-Garrido, Felix\n  Velicia-Martin",
        "title": "How effectual will you be? Development and validation of a scale in\n  higher education",
        "comments": null,
        "journal-ref": "International Journal of Management Education, 19(3), 100547\n  (2021)",
        "doi": "10.1016/j.ijme.2021.100547",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The literature on effectual theory offers validated scales to measure\neffectual or causal logic in entrepreneurs' decision-making. However, there are\nno adequate scales to assess in advance the effectual or causal propensity of\npeople with an entrepreneurial intention before the creation of their\ncompanies. We aim to determine the validity and reliability of an instrument to\nmeasure that propensity by first analysing those works that provide recognised\nvalidated scales with which to measure the effectual or causal logic in people\nwho have already started up companies. Then, considering these scales, we\ndesigned a scale to evaluate the effectual or causal propensity in people who\nhad not yet started up companies using a sample of 230 final-year business\nadministration students to verify its reliability and validity. The validated\nscale has theoretical implications for the literature on potential\nentrepreneurship and entrepreneurial intention and practical implications for\npromoters of entrepreneurship who need to orient the behaviour of\nentrepreneurs, entrepreneurs of established businesses who want to implement a\nspecific strategic orientation, entrepreneurs who want to evaluate the\neffectual propensity of their potential partners and workers, and academic\ninstitutions interested in orienting the entrepreneurial potential of their\nstudents.\n"
    },
    {
        "paper_id": 2312.01018,
        "authors": "Agostino Capponi and Garud Iyengar and Jay Sethuraman",
        "title": "Decentralized Finance: Protocols, Risks, and Governance",
        "comments": "2",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial markets are undergoing an unprecedented transformation.\nTechnological advances have brought major improvements to the operations of\nfinancial services. While these advances promote improved accessibility and\nconvenience, traditional finance shortcomings like lack of transparency and\nmoral hazard frictions continue to plague centralized platforms, imposing\nsocietal costs. In this paper, we argue how these shortcomings and frictions\nare being mitigated by the decentralized finance (DeFi) ecosystem. We delve\ninto the workings of smart contracts, the backbone of DeFi transactions, with\nan emphasis on those underpinning token exchange and lending services. We\nhighlight the pros and cons of the novel form of decentralized governance\nintroduced via the ownership of governance tokens. Despite its potential, the\ncurrent DeFi infrastructure introduces operational risks to users, which we\nsegment into five primary categories: consensus mechanisms, protocol, oracle,\nfrontrunning, and systemic risks. We conclude by emphasizing the need for\nfuture research to focus on the scalability of existing blockchains, the\nimproved design and interoperability of DeFi protocols, and the rigorous\nauditing of smart contracts.\n"
    },
    {
        "paper_id": 2312.01034,
        "authors": "Xia Han, Ruodu Wang, Qinyu Wu",
        "title": "Monotonic mean-deviation risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mean-deviation models, along with the existing theory of coherent risk\nmeasures, are well studied in the literature. In this paper, we characterize\nmonotonic mean-deviation (risk) measures from a general mean-deviation model by\napplying a risk-weighting function to the deviation part. The form is a\ncombination of the deviation-related functional and the expectation, and such\nmeasures belong to the class of consistent risk measures. The monotonic\nmean-deviation measures admit an axiomatic foundation via preference relations.\nBy further assuming the convexity and linearity of the risk-weighting function,\nthe characterizations for convex and coherent risk measures are obtained,\ngiving rise to many new explicit examples of convex and nonconvex consistent\nrisk measures. Further, we specialize in the convex case of the monotonic\nmean-deviation measure and obtain its dual representation. The worst-case\nvalues of the monotonic mean-deviation measures are analyzed under two popular\nsettings of model uncertainty. Further, we establish asymptotic consistency and\nnormality of the natural estimators of the monotonic mean-deviation\nmeasures.Finally, the monotonic mean-deviation measures are applied to a\nproblem of portfolio selection using financial data.\n"
    },
    {
        "paper_id": 2312.01426,
        "authors": "Saad Mouti",
        "title": "Rough volatility: evidence from range volatility estimators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Gatheral et al. 2018, first posted in 2014, volatility is characterized by\nfractional behavior with a Hurst exponent $H < 0.5$, challenging traditional\nviews of volatility dynamics. Gatheral et al. demonstrated this using realized\nvolatility measurements. Our study extends this analysis by employing\nrange-based proxies to confirm their findings across a broader dataset and\nnon-standard assets. Notably, we address the concern that rough volatility\nmight be an artifact of microstructure noise in high-frequency return data. Our\nresults reveal that log-volatility, estimated via range-based methods, behaves\nakin to fractional Brownian motion with an even lower $H$, below $0.1$. We also\naffirm the efficacy of the rough fractional stochastic volatility model (RFSV),\nfinding that its predictive capability surpasses that of AR, HAR, and GARCH\nmodels in most scenarios. This work substantiates the intrinsic nature of rough\nvolatility, independent of the microstructure noise often present in\nhigh-frequency financial data.\n"
    },
    {
        "paper_id": 2312.01442,
        "authors": "Kyle R. Myers and Wei Yang Tham and Jerry Thursby and Marie Thursby\n  and Nina Cohodes and Karim Lakhani and Rachel Mural and Yilun Xu",
        "title": "New Facts and Data about Professors and their Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new survey of professors at roughly 150 of the most\nresearch-intensive institutions of higher education in the US. We document\nseven new features of how research-active professors are compensated, how they\nspend their time, and how they perceive their research pursuits: (1) there is\nmore inequality in earnings within fields than there is across fields; (2)\ninstitutions, ranks, tasks, and sources of earnings can account for roughly\nhalf of the total variation in earnings; (3) there is significant variation\nacross fields in the correlations between earnings and different kinds of\nresearch output, but these account for a small amount of earnings variation;\n(4) measuring professors' productivity in terms of output-per-year versus\noutput-per-research-hour can yield substantial differences; (5) professors'\nbeliefs about the riskiness of their research are best predicted by their\nfundraising intensity, their risk-aversion in their personal lives, and the\ndegree to which their research involves generating new hypotheses; (6) older\nand younger professors have very different research outputs and time\nallocations, but their intended audiences are quite similar; (7) personal\nrisk-taking is highly predictive of professors' orientation towards applied,\ncommercially-relevant research.\n"
    },
    {
        "paper_id": 2312.01668,
        "authors": "Chonghu Guan, Jiacheng Fan, Zuo Quan Xu",
        "title": "Optimal dividend payout with path-dependent drawdown constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal dividend payout problem with drawdown\nconstraint in a Brownian motion model, where the dividend payout rate must be\nno less than a fixed proportion of its historical running maximum. It is a\nstochastic control problem, where the admissible control depends on its past\nvalues, thus is path-dependent. The related Hamilton-Jacobi-Bellman equation\nturns out to be a new type of two-dimensional variational inequality with\ngradient constraint, which has only been studied by viscosity solution\ntechnique in the literature. In this paper, we use delicate PDE methods to\nobtain a strong solution. Different from the viscosity solution, based on our\nsolution, we succeed in deriving an optimal feedback payout strategy, which is\nexpressed in terms of two free boundaries and the running maximum surplus\nprocess. Furthermore, we have obtained many properties of the value function\nand the free boundaries such as the boundedness, continuity etc. Numerical\nexamples are presented as well to verify our theoretical results and give some\nnew but not proved financial insights.\n"
    },
    {
        "paper_id": 2312.0173,
        "authors": "Weixuan Xia",
        "title": "Set-valued stochastic integrals for convoluted L\\'{e}vy processes",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study set-valued Volterra-type stochastic integrals driven\nby L\\'{e}vy processes. Upon extending the classical definitions of set-valued\nstochastic integral functionals to convoluted integrals with square-integrable\nkernels, set-valued convoluted stochastic integrals are defined by taking the\nclosed decomposable hull of the integral functionals for generic time. We show\nthat, aside from well-established results for set-valued It\\^{o} integrals,\nwhile set-valued stochastic integrals with respect to a finite-variation\nPoisson random measure are guaranteed to be integrably bounded for bounded\nintegrands, this is not true when the random measure is of infinite variation.\nFor indefinite integrals, we prove that it is a mutual effect of kernel\nsingularity and jumps that the set-valued convoluted integrals are possibly\nexplosive and take extended vector values. These results have some important\nimplications on how set-valued fractional dynamical systems are to be\nconstructed in general. Two classes of set-monotone processes are studied for\npractical interests in economic and financial modeling.\n"
    },
    {
        "paper_id": 2312.01813,
        "authors": "Tim J. Boonen and Xia Han",
        "title": "Optimal insurance with mean-deviation measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal insurance contracting problem in which the\npreferences of the decision maker given by the sum of the expected loss and a\nconvex, increasing function of a deviation measure. As for the deviation\nmeasure, our focus is on convex signed Choquet integrals (such as the Gini\ncoefficient and a convex distortion risk measure minus the expected value) and\non the standard deviation. We find that if the expected value premium principle\nis used, then stop-loss indemnities are optimal, and we provide a precise\ncharacterization of the corresponding deductible. Moreover, if the premium\nprinciple is based on Value-at-Risk or Expected Shortfall, then a particular\nlayer-type indemnity is optimal, in which there is coverage for small losses up\nto a limit, and additionally for losses beyond another deductible. The\nstructure of these optimal indemnities remains unchanged if there is a limit on\nthe insurance premium budget. If the unconstrained solution is not feasible,\nthen the deductible is increased to make the budget constraint binding. We\nprovide several examples of these results based on the Gini coefficient and the\nstandard deviation.\n"
    },
    {
        "paper_id": 2312.02081,
        "authors": "Alexander Shulzhenko",
        "title": "Copula-based deviation measure of cointegrated financial assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study outlines a comprehensive methodology utilizing copulas to discern\ninconsistencies in the behavior exhibited by pairs of financial assets. It\nintroduces a robust approach to establishing the interrelationship between the\nreturns of these assets, exploring potential measures of dependence among the\nstochastic variables represented by these returns. Special emphasis is placed\non scrutinizing the traditional measure of dependence, namely the correlation\ncoefficient, delineating its limitations. Furthermore, the study articulates an\nalternative methodology that offers enhanced stability and informativeness in\nappraising the relationship between financial instrument returns.\n"
    },
    {
        "paper_id": 2312.02101,
        "authors": "Dylan Possama\\\"i, Chiara Rossato",
        "title": "Golden parachutes under the threat of accidents",
        "comments": "56 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses a continuous-time contracting model that extends the\nproblem introduced by Sannikov and later rigorously analysed by Possama\\\"{i}\nand Touzi. In our model, a principal hires a risk-averse agent to carry out a\nproject. Specifically, the agent can perform two different tasks, namely to\nincrease the instantaneous growth rate of the project's value, and to reduce\nthe likelihood of accidents occurring. In order to compensate for these costly\nactions, the principal offers a continuous stream of payments throughout the\nentire duration of a contract, which concludes at a random time, potentially\nresulting in a lump-sum payment. We examine the consequences stemming from the\nintroduction of accidents, modelled by a compound Poisson process that\nnegatively impact the project's value. Furthermore, we investigate whether\ncertain economic scenarii are still characterised by a golden parachute as in\nSannikov's model. A golden parachute refers to a situation where the agent\nstops working and subsequently receives a compensation, which may be either a\nlump-sum payment leading to termination of the contract or a continuous stream\nof payments, thereby corresponding to a pension.\n"
    },
    {
        "paper_id": 2312.02181,
        "authors": "Zeteng Lin",
        "title": "How Generative-AI can be Effectively used in Government Chatbots",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rapid development of artificial intelligence and breakthroughs in\nmachine learning and natural language processing, intelligent\nquestion-answering robots have become widely used in government affairs. This\npaper conducts a horizontal comparison between Guangdong Province's government\nchatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the\nstrengths and weaknesses of existing government chatbots and AIGC technology.\nThe study finds significant differences between government chatbots and large\nlanguage models. China's government chatbots are still in an exploratory stage\nand have a gap to close to achieve \"intelligence.\" To explore the future\ndirection of government chatbots more deeply, this research proposes targeted\noptimization paths to help generative AI be effectively applied in government\nchatbot conversations.\n"
    },
    {
        "paper_id": 2312.0225,
        "authors": "Yongzhuo Chen, Yixuan Liang, Yiran Liu, Brian Hobbs, Michael Kane",
        "title": "Valuing Post-Revenue Biopharmaceutical Assets with Pfizer's Current\n  Portfolio as a Case Study",
        "comments": "14 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research paper addresses the critical challenge of accurately valuing\npost-revenue drug assets in the biotechnology and pharmaceutical sectors, a key\nfactor influencing a wide range of strategic operations and investment\ndecisions. Recognizing the importance of reliable valuations for stakeholders\nsuch as pharmaceutical companies, venture capitalists, and private equity\nfirms, this study introduces a novel model for forecasting future sales of\npost-revenue biopharmaceutical assets. The proposed model leverages historical\nsales data, a resource known for its high quality and availability in company\nfinancial records, to produce distributional estimates of cumulative sales for\nindividual assets. These estimates are instrumental in calculating the Net\nPresent Value of each asset, thereby facilitating more informed and strategic\ninvestment decisions. A practical application of this model is demonstrated\nthrough its implementation in analyzing Pfizer's portfolio of post-revenue\nassets. This precision highlights the model's potential as a valuable tool in\nthe financial assessment and decision-making processes within the biotech and\npharmaceutical industries, offering a methodical approach to identifying\ninvestment opportunities and optimizing capital allocation.\n"
    },
    {
        "paper_id": 2312.02472,
        "authors": "Bo Li",
        "title": "An explanation for the distribution characteristics of stock returns",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Observations indicate that the distributions of stock returns in financial\nmarkets usually do not conform to normal distributions, but rather exhibit\ncharacteristics of high peaks, fat tails and biases. In this work, we assume\nthat the effects of events or information on prices obey normal distribution,\nwhile financial markets often overreact or underreact to events or information,\nresulting in non normal distributions of stock returns. Based on the above\nassumptions, we propose a reaction function for a financial market reacting to\nevents or information, and a model based on it to describe the distribution of\nreal stock returns. Our analysis of the returns of China Securities Index 300\n(CSI 300), the Standard & Poor's 500 Index (SPX or S&P 500) and the Nikkei 225\nIndex (N225) at different time scales shows that financial markets often\nunderreact to events or information with minor impacts, overreact to events or\ninformation with relatively significant impacts, and react slightly stronger to\npositive events or information than to negative ones. In addition, differences\nin financial markets and time scales of returns can also affect the shapes of\nthe reaction functions.\n"
    },
    {
        "paper_id": 2312.02516,
        "authors": "Yukio Ohsawa, Sae Kondo, Yi Sun and Kaira Sekiguchi",
        "title": "Generating a Map of Well-being Regions using Multiscale Moving Direction\n  Entropy on Mobile Sensors",
        "comments": "6 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The well-being of individuals in a crowd is interpreted as a product of the\ncrossover of individuals from heterogeneous communities, which may occur via\ninteractions with other crowds. The index moving-direction entropy\ncorresponding to the diversity of the moving directions of individuals is\nintroduced to represent such an inter-community crossover. Multi-scale moving\ndirection entropies, composed of various geographical mesh sizes to compute the\nindex values, are used to capture the information flow owing to human movements\nfrom/to various crowds. The generated map of high values of multiscale moving\ndirection entropy is shown to coincide significantly with the preference of\npeople to live in each region.\n"
    },
    {
        "paper_id": 2312.0266,
        "authors": "Nir Chemaya, Lin William Cong, Emma Jorgensen, Dingyue Liu, Luyao\n  Zhang",
        "title": "Uniswap Daily Transaction Indices by Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  DeFi is transforming financial services by removing intermediaries and\nproducing a wealth of open-source data. This transformation is propelled by\nLayer 2 (L2) solutions, aimed at boosting network efficiency and scalability\nbeyond current Layer 1 (L1) capabilities. This study addresses the lack of\ndetailed L2 impact analysis by examining over 50 million transactions from\nUniswap. Our dataset, featuring transactions from L1 and L2 across networks\nlike Ethereum and Polygon, provides daily indices revealing adoption,\nscalability, and decentralization within the DeFi space. These indices help to\nelucidate the complex relationship between DeFi and L2 technologies, advancing\nour understanding of the ecosystem. The dataset is enhanced by an open-source\nPython framework for computing decentralization indices, adaptable for various\nresearch needs. This positions the dataset as a vital resource for machine\nlearning endeavors, particularly deep learning, contributing significantly to\nthe development of Blockchain as Web3's infrastructure.\n"
    },
    {
        "paper_id": 2312.02943,
        "authors": "An Chen, Giorgio Ferrari, Shihao Zhu",
        "title": "Striking the Balance: Life Insurance Timing and Asset Allocation in\n  Financial Planning",
        "comments": "37 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2212.05317",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the consumption and investment decisions of an\nindividual facing uncertain lifespan and stochastic labor income within a\nBlack-Scholes market framework. A key aspect of our study involves the agent's\noption to choose when to acquire life insurance for bequest purposes. We\nexamine two scenarios: one with a fixed bequest amount and another with a\ncontrolled bequest amount. Applying duality theory and addressing free-boundary\nproblems, we analytically solve both cases, and provide explicit expressions\nfor value functions and optimal strategies in both cases. In the first\nscenario, where the bequest amount is fixed, distinct outcomes emerge based on\ndifferent levels of risk aversion parameter $\\gamma$: (i) the optimal time for\nlife insurance purchase occurs when the agent's wealth surpasses a critical\nthreshold if $\\gamma \\in (0,1)$, or (ii) life insurance should be acquired\nimmediately if $\\gamma>1$. In contrast, in the second scenario with a\ncontrolled bequest amount, regardless of $\\gamma$ values, immediate life\ninsurance purchase proves to be optimal.\n"
    },
    {
        "paper_id": 2312.03194,
        "authors": "Alex Kim and Sangwon Yoon",
        "title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT",
        "comments": null,
        "journal-ref": "Proceedings of the Third Workshop on Economics and Natural\n  Language Processing, 2021, 26--36",
        "doi": "10.18653/v1/2021.econlp-1.4",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study performs BERT-based analysis, which is a representative\ncontextualized language model, on corporate disclosure data to predict\nimpending bankruptcies. Prior literature on bankruptcy prediction mainly\nfocuses on developing more sophisticated prediction methodologies with\nfinancial variables. However, in our study, we focus on improving the quality\nof input dataset. Specifically, we employ BERT model to perform sentiment\nanalysis on MD&A disclosures. We show that BERT outperforms dictionary-based\npredictions and Word2Vec-based predictions in terms of adjusted R-square in\nlogistic regression, k-nearest neighbor (kNN-5), and linear kernel support\nvector machine (SVM). Further, instead of pre-training the BERT model from\nscratch, we apply self-learning with confidence-based filtering to corporate\ndisclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate\nthat the domain adaptation procedure brings a significant improvement in\nprediction accuracy.\n"
    },
    {
        "paper_id": 2312.03249,
        "authors": "Andrew Thomas Goheen, Zayon Deshon Mobley-Wright, Rayne Symone\n  Strother, Ryan Thomas Guthrie",
        "title": "The Feasibility of Establishing A Nuclear Presence at Western Michigan\n  University",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research report journal aims to investigate the feasibility of\nestablishing a nuclear presence at Western Michigan University. The report will\nanalyze the potential benefits and drawbacks of introducing nuclear technology\nto WMUs campus. The study will also examine the current state of nuclear\ntechnology and its applications in higher education. The report will conclude\nwith a recommendation on whether WMU should pursue the establishment of a\nnuclear presence on its campus.\n"
    },
    {
        "paper_id": 2312.03294,
        "authors": "Tuoyuan Cheng, Kan Chen",
        "title": "A General Framework for Portfolio Construction Based on Generative\n  Models of Asset Returns",
        "comments": null,
        "journal-ref": "The Journal of Finance and Data Science (2023): 100113",
        "doi": "10.1016/j.jfds.2023.100113",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we present an integrated approach to portfolio construction\nand optimization, leveraging high-performance computing capabilities. We first\nexplore diverse pairings of generative model forecasts and objective functions\nused for portfolio optimization, which are evaluated using\nperformance-attribution models based on LASSO. We illustrate our approach using\nextensive simulations of crypto-currency portfolios, and we show that the\nportfolios constructed using the vine-copula generative model and the\nSharpe-ratio objective function consistently outperform. To accommodate a wide\narray of investment strategies, we further investigate portfolio blending and\npropose a general framework for evaluating and combining investment strategies.\nWe employ an extension of the multi-armed bandit framework and use value models\nand policy models to construct eclectic blended portfolios based on past\nperformance. We consider similarity and optimality measures for value models\nand employ probability-matching (\"blending\") and a greedy algorithm\n(\"switching\") for policy models. The eclectic portfolios are also evaluated\nusing LASSO models. We show that the value model utilizing cosine similarity\nand logit optimality consistently delivers robust superior performances. The\nextent of outperformance by eclectic portfolios over their benchmarks\nsignificantly surpasses that achieved by individual generative model-based\nportfolios over their respective benchmarks.\n"
    },
    {
        "paper_id": 2312.03444,
        "authors": "Christian Bayer, Luca Pelizzari, John Schoenmakers",
        "title": "Primal and dual optimal stopping with signatures",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two signature-based methods to solve the optimal stopping problem\n- that is, to price American options - in non-Markovian frameworks. Both\nmethods rely on a global approximation result for $L^p-$functionals on rough\npath-spaces, using linear functionals of robust, rough path signatures. In the\nprimal formulation, we present a non-Markovian generalization of the famous\nLongstaff-Schwartz algorithm, using linear functionals of the signature as\nregression basis. For the dual formulation, we parametrize the space of\nsquare-integrable martingales using linear functionals of the signature, and\napply a sample average approximation. We prove convergence for both methods and\npresent first numerical examples in non-Markovian and non-semimartingale\nregimes.\n"
    },
    {
        "paper_id": 2312.0351,
        "authors": "Neil Kichler, Sher Afghan, Uwe Naumann",
        "title": "Towards Sobolev Pruning",
        "comments": "11 pages",
        "journal-ref": "Proceedings of the Platform for Advanced Scientific Computing\n  Conference PASC24 (2024)",
        "doi": "10.1145/3659914.3659915",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing use of stochastic models for describing complex phenomena\nwarrants surrogate models that capture the reference model characteristics at a\nfraction of the computational cost, foregoing potentially expensive Monte Carlo\nsimulation. The predominant approach of fitting a large neural network and then\npruning it to a reduced size has commonly neglected shortcomings. The produced\nsurrogate models often will not capture the sensitivities and uncertainties\ninherent in the original model. In particular, (higher-order) derivative\ninformation of such surrogates could differ drastically. Given a large enough\nnetwork, we expect this derivative information to match. However, the pruned\nmodel will almost certainly not share this behavior.\n  In this paper, we propose to find surrogate models by using sensitivity\ninformation throughout the learning and pruning process. We build on work using\nInterval Adjoint Significance Analysis for pruning and combine it with the\nrecent advancements in Sobolev Training to accurately model the original\nsensitivity information in the pruned neural network based surrogate model. We\nexperimentally underpin the method on an example of pricing a multidimensional\nBasket option modelled through a stochastic differential equation with Brownian\nmotion. The proposed method is, however, not limited to the domain of\nquantitative finance, which was chosen as a case study for intuitive\ninterpretations of the sensitivities. It serves as a foundation for building\nfurther surrogate modelling techniques considering sensitivity information.\n"
    },
    {
        "paper_id": 2312.03843,
        "authors": "Lidia Cano Pecharroman and ChangHoon Hahn",
        "title": "Exposing Disparities in Flood Adaptation for Equitable Future\n  Interventions",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As governments race to implement new climate adaptation policies that prepare\nfor more frequent flooding, they must seek policies that are effective for all\ncommunities and uphold climate justice. This requires evaluating policies not\nonly on their overall effectiveness but also on whether their benefits are felt\nacross all communities. We illustrate the importance of considering such\ndisparities for flood adaptation using the FEMA National Flood Insurance\nProgram Community Rating System and its dataset of $\\sim$2.5 million flood\ninsurance claims. We use ${\\rm C{\\scriptsize AUSAL}F{\\scriptsize LOW}}$, a\ncausal inference method based on deep generative models, to estimate the\ntreatment effect of flood adaptation interventions based on a community's\nincome, diversity, population, flood risk, educational attainment, and\nprecipitation. We find that the program saves communities \\$5,000--15,000 per\nhousehold. However, these savings are not evenly spread across communities. For\nexample, for low-income communities savings sharply decline as flood-risk\nincreases in contrast to their high-income counterparts with all else equal.\nEven among low-income communities, there is a gap in savings between\npredominantly white and non-white communities: savings of predominantly white\ncommunities can be higher by more than \\$6000 per household. As communities\nworldwide ramp up efforts to reduce losses inflicted by floods, simply\nprescribing a series flood adaptation measures is not enough. Programs must\nprovide communities with the necessary technical and economic support to\ncompensate for historical patterns of disenfranchisement, racism, and\ninequality. Future flood adaptation efforts should go beyond reducing losses\noverall and aim to close existing gaps to equitably support communities in the\nrace for climate adaptation.\n"
    },
    {
        "paper_id": 2312.03868,
        "authors": "Dongwei Zhao, Vladimir Dvorkin, Stefanos Delikaraoglou, Alberto J.\n  Lamadrid L., Audun Botterud",
        "title": "Uncertainty-Informed Renewable Energy Scheduling: A Scalable Bilevel\n  Framework",
        "comments": "IEEE Transactions on Energy Markets, Policy, and Regulation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work proposes an uncertainty-informed bid adjustment framework for\nintegrating variable renewable energy sources (VRES) into electricity markets.\nThis framework adopts a bilevel model to compute the optimal VRES day-ahead\nbids. It aims to minimize the expected system cost across day-ahead and\nreal-time stages and approximate the cost efficiency of the stochastic market\ndesign. However, solving the bilevel optimization problem is computationally\nchallenging for large-scale systems. To overcome this challenge, we introduce a\nnovel technique based on strong duality and McCormick envelopes, which relaxes\nthe problem to a linear program, enabling large-scale applications. The\nproposed bilevel framework is applied to the 1576-bus NYISO system and\nbenchmarked against a myopic strategy, where the VRES bid is the mean value of\nthe probabilistic power forecast. Results demonstrate that, under high VRES\npenetration levels (e.g., 40%), our framework can significantly reduce system\ncosts and market-price volatility, by optimizing VRES quantities efficiently in\nthe day-ahead market. Furthermore, we find that when transmission capacity\nincreases, the proposed bilevel model will still reduce the system cost,\nwhereas the myopic strategy may incur a much higher cost due to over-scheduling\nof VRES in the day-ahead market and the lack of flexible conventional\ngenerators in real time.\n"
    },
    {
        "paper_id": 2312.03898,
        "authors": "Amir Hosein Afshar Sedigh, Rasoul Haji, Seyed Mehdi Sajadifar",
        "title": "The Cost Function of a Two-Level Inventory System with identical\n  retailers benefitting from Information Sharing",
        "comments": null,
        "journal-ref": "Festschrift for Martin Purvis. An Information Science \"Renaissance\n  Man\", 2022",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates a two-echelon inventory system with a central\nwarehouse and N (N > 2) retailers managed by a centralized information-sharing\nmechanism. In particular, the paper mathematically models an easy-to-implement\ninventory control system that facilitates making use of information. Some\nassumptions of the paper include: a) constant delivery time for retailers and\nthe central warehouse and b) Poisson demand with identical rates for retailers.\nThe inventory policy comprises continuous review (R,Q)-policy on the part of\nretailers and triggering the system with m batches (of a given size Q) at the\ncentral warehouse. Besides, the central warehouse monitors retailers' inventory\nand may order batches sooner than retailers' reorder point, when their\ninventory position reaches R + s. An earlier study proposed the policy and its\ncost function approximation. This paper derives an exact mathematical model of\nthe cost function for the aforementioned problem.\n"
    },
    {
        "paper_id": 2312.03915,
        "authors": "Svetlana Boyarchenko and Sergei Levendorskii",
        "title": "Alternative models for FX, arbitrage opportunities and efficient pricing\n  of double barrier options in L\\'evy models",
        "comments": "arXiv admin note: text overlap with arXiv:2211.07765",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the qualitative differences between prices of double barrier\nno-touch options in the Heston model and pure jump KoBoL model calibrated to\nthe same set of the empirical data, and discuss the potential for arbitrage\nopportunities if the correct model is a pure jump model. We explain and\ndemonstrate with numerical examples that accurate and fast calculations of\nprices of double barrier options in jump models are extremely difficult using\nthe numerical methods available in the literature. We develop a new efficient\nmethod (GWR-SINH method) based of the Gaver-Wynn-Rho acceleration applied to\nthe Bromwich integral; the SINH-acceleration and simplified trapezoid rule are\nused to evaluate perpetual double barrier options for each value of the\nspectral parameter in GWR-algorithm. The program in Matlab running on a Mac\nwith moderate characteristics achieves the precision of the order of E-5 and\nbetter in several several dozen of milliseconds; the precision E-07 is\nachievable in about 0.1 sec. We outline the extension of GWR-SINH method to\nregime-switching models and models with stochastic parameters and stochastic\ninterest rates.\n"
    },
    {
        "paper_id": 2312.03929,
        "authors": "Svetlana Boyarchenko and Sergei Levendorskii",
        "title": "Simulation of a L\\'evy process, its extremum, and hitting time of the\n  extremum via characteristic functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest a general framework for simulation of the triplet $(X_T,\\bar X_\nT,\\tau_T)$ (L\\'evy process, its extremum, and hitting time of the extremum),\nand, separately, $X_T,\\bar X_ T$ and pairs $(X_T,\\bar X_ T)$, $(\\bar X_\nT,\\tau_T)$,\n  $(\\bar X_ T-X_T,\\tau_T)$, via characteristic functions and conditional\ncharacteristic functions. The conformal deformations technique allows one to\nevaluate probability distributions, joint probability distributions and\nconditional probability distributions accurately and fast. For simulations in\nthe far tails of the distribution, we precalculate and store the values of the\n(conditional) characteristic functions on multi-grids on appropriate surfaces\nin $C^n$, and use these values to calculate the quantiles in the tails. For\nsimulation in the central part of a distribution, we precalculate the values of\nthe cumulative distribution at points of a non-uniform (multi-)grid, and use\ninterpolation to calculate quantiles.\n"
    },
    {
        "paper_id": 2312.04045,
        "authors": "Yu-Jui Huang, Li-Hsien Sun",
        "title": "Partial Information Breeds Systemic Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers finitely many investors who perform mean-variance\nportfolio selection under a relative performance criterion. That is, each\ninvestor is concerned about not only her terminal wealth, but how it compares\nto the average terminal wealth of all investors (i.e., the mean field). At the\ninter-personal level, each investor selects a trading strategy in response to\nothers' strategies (which affect the mean field). The selected strategy\nadditionally needs to yield an equilibrium intra-personally, so as to resolve\ntime inconsistency among the investor's current and future selves (triggered by\nthe mean-variance objective). A Nash equilibrium we look for is thus a tuple of\ntrading strategies under which every investor achieves her intra-personal\nequilibrium simultaneously. We derive such a Nash equilibrium explicitly in the\nidealized case of full information (i.e., the dynamics of the underlying stock\nis perfectly known), and semi-explicitly in the realistic case of partial\ninformation (i.e., the stock evolution is observed, but the expected return of\nthe stock is not precisely known). The formula under partial information\ninvolves an additional state process that serves to filter the true state of\nthe expected return. Its effect on trading is captured by two degenerate Cauchy\nproblems, one of which depends on the other, whose solutions are constructed by\nelliptic regularization and a stability analysis of the state process. Our\nresults indicate that partial information alone can reduce investors' wealth\nsignificantly, thereby causing or aggravating systemic risk. Intriguingly, in\ntwo different scenarios of the expected return (i.e., it is constant or\nalternating between two values), our Nash equilibrium formula spells out two\ndistinct manners systemic risk materializes.\n"
    },
    {
        "paper_id": 2312.0418,
        "authors": "Dandan Qiao, Huaxia Rui, and Qian Xiong",
        "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
        "comments": "42 pages, 6 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial intelligence (AI) refers to the ability of machines or software to\nmimic or even surpass human intelligence in a given cognitive task. While\nhumans learn by both induction and deduction, the success of current AI is\nrooted in induction, relying on its ability to detect statistical regularities\nin task input -- an ability learnt from a vast amount of training data using\nenormous computation resources. We examine the performance of such a\nstatistical AI in a human task through the lens of four factors, including task\nlearnability, statistical resource, computation resource, and learning\ntechniques, and then propose a three-phase visual framework to understand the\nevolving relation between AI and jobs. Based on this conceptual framework, we\ndevelop a simple economic model of competition to show the existence of an\ninflection point for each occupation. Before AI performance crosses the\ninflection point, human workers always benefit from an improvement in AI\nperformance, but after the inflection point, human workers become worse off\nwhenever such an improvement occurs. To offer empirical evidence, we first\nargue that AI performance has passed the inflection point for the occupation of\ntranslation but not for the occupation of web development. We then study how\nthe launch of ChatGPT, which led to significant improvement of AI performance\non many tasks, has affected workers in these two occupations on a large online\nlabor platform. Consistent with the inflection point conjecture, we find that\ntranslators are negatively affected by the shock both in terms of the number of\naccepted jobs and the earnings from those jobs, while web developers are\npositively affected by the very same shock. Given the potentially large\ndisruption of AI on employment, more studies on more occupations using data\nfrom different platforms are urgently needed.\n"
    },
    {
        "paper_id": 2312.04332,
        "authors": "Chen Chris Gong, Falko Ueckerdt, Christoph Bertram, Yuxin Yin, David\n  Bantje, Robert Pietzcker, Johanna Hoppe, Michaja Pehl, Gunnar Luderer",
        "title": "Robust CO2-abatement from early end-use electrification under uncertain\n  power transition speed in China's netzero transition",
        "comments": "26 pages 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decarbonizing China's energy system requires both greening the power supply\nand end-use electrification. While the latter speeds up with the electric\nvehicle adoption, a rapid power sector transformation can be technologically\nand institutionally challenging. Using an integrated assessment model, we\nanalyze the synergy between power sector decarbonization and end-use\nelectrification in China's net-zero pathway from a system perspective. We show\nthat even with a slower coal power phase-out, reaching a high electrification\nrate of 60% by 2050 is a robust optimal strategy. Comparing emission intensity\nof typical end-use applications, we find most have reached parity with\nincumbent fossil fuel technologies even under China's current power mix due to\nefficiency gains. Since a 10-year delay in coal power phase-out can result in\nan additional cumulative emission of 28% (4%) of the global 1.5{\\deg}C\n(2{\\deg}C) CO2 budget, policy measures should be undertaken today to ensure a\npower sector transition without unexpected delays.\n"
    },
    {
        "paper_id": 2312.04411,
        "authors": "Eiji Yamamura and Fumio Ohtake",
        "title": "Family Structure, Gender and Subjective Well-being: Effect of Child ren\n  before and after COVID 19 in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Grandparents were anticipated to participated in grand-rearing. The COVID-19\npandemic had detached grandparents from rearing grandchildren. The research\nquestions of this study were as follows: How does the change in family\nrelations impact the well-being (SWB) of grandparents and parents? We examined\nhow family structure influenced subjective SWB before and after COVID-19. We\nfocused on the effects of children, grandchildren, and their gender on\ngrandparents and parents. We found that compared with the happiness level\nbefore COVID-19, (1) granddaughters increased their grandmothers SWB after\nCOVID-19, (2) both daughters and sons reduced their fathers SWB after COVID-19,\nwhereas neither daughters nor sons changed their mothers SWB, and (3) the\nnegative effect of sons reduced substantially if their fathers had younger\nbrothers. Learning from interactions with younger brothers in childhood,\nfathers could avoid the deterioration of relationships with their sons, even\nwhen unexpected events possibly changed the lifestyle of the family and their\nrelationship.\n"
    },
    {
        "paper_id": 2312.04695,
        "authors": "Ummya Salma, Md. Fazlul Huq Khan, Md. Masum Billah",
        "title": "Foreign Capital and Economic Growth: Evidence from Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to examine the relationship between Foreign Direct Investment\n(FDI), personal remittances received, and official development assistance (ODA)\nin the economic growth of Bangladesh. The study utilizes time series data on\nBangladesh from 1976 to 2021. Additionally, this research contributes to the\nexisting literature by introducing the Foreign Capital Depthless Index (FCDI)\nand exploring its impact on Bangladesh's economic growth. The results of the\nVector Error Correction Model (VECM) suggest that the economic growth of\nBangladesh depends on FDI, remittances, and aid in the long run. However, these\nvariables do not exhibit a causal relationship with GDP in the short run. The\nrelationship between FCDI and economic growth is positive in the long run.\nNevertheless, the presence of these three variables has a more significant\nimpact on the economic growth of Bangladesh\n"
    },
    {
        "paper_id": 2312.05013,
        "authors": "Kanis Saengchote",
        "title": "Developers' Leverage, Capital Market Financing, and Fire Sale\n  Externalities Evidence from the Thai Condominium Market",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Leveraged developers facing rollover risk are more likely to engage in fire\nsales. Using COVID-19 as a natural experiment, we find evidence of fire sale\nexternalities in the Thai condominium market. Resales in properties whose\ndevelopers have higher leverage ratios have lower listing prices for listed\ndevelopers (who have access to capital market financing) but not unlisted\ndevelopers (who primarily use bank financing). We attribute this difference to\nthe flexibility of bank loan renegotiation versus the rigidity of debt capital\nmarket repayments and highlight the role of commercial banks in financial\nintermediation in the presence of information asymmetry.\n"
    },
    {
        "paper_id": 2312.05169,
        "authors": "Gabriel Turinici and Pierre Brugiere",
        "title": "Onflow: an online portfolio allocation algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce Onflow, a reinforcement learning technique that enables online\noptimization of portfolio allocation policies based on gradient flows. We\ndevise dynamic allocations of an investment portfolio to maximize its expected\nlog return while taking into account transaction fees. The portfolio allocation\nis parameterized through a softmax function, and at each time step, the\ngradient flow method leads to an ordinary differential equation whose solutions\ncorrespond to the updated allocations. This algorithm belongs to the large\nclass of stochastic optimization procedures; we measure its efficiency by\ncomparing our results to the mathematical theoretical values in a log-normal\nframework and to standard benchmarks from the 'old NYSE' dataset. For\nlog-normal assets, the strategy learned by Onflow, with transaction costs at\nzero, mimics Markowitz's optimal portfolio and thus the best possible asset\nallocation strategy. Numerical experiments from the 'old NYSE' dataset show\nthat Onflow leads to dynamic asset allocation strategies whose performances\nare: a) comparable to benchmark strategies such as Cover's Universal Portfolio\nor Helmbold et al. \"multiplicative updates\" approach when transaction costs are\nzero, and b) better than previous procedures when transaction costs are high.\nOnflow can even remain efficient in regimes where other dynamical allocation\ntechniques do not work anymore. Therefore, as far as tested, Onflow appears to\nbe a promising dynamic portfolio management strategy based on observed prices\nonly and without any assumption on the laws of distributions of the underlying\nassets' returns. In particular it could avoid model risk when building a\ntrading strategy.\n"
    },
    {
        "paper_id": 2312.05222,
        "authors": "Svetlana Boyarchenko and Sergei Levendorskii",
        "title": "Efficient evaluation of joint pdf of a L\\'evy process, its extremum, and\n  hitting time of the extremum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For L\\'evy processes with exponentially decaying tails of the L\\'evy density,\nwe derive integral representations for the joint cpdf $V$ of $(X_T, \\bar\nX_T,\\tau_T)$ (the process, its supremum evaluated at $T<+\\infty$, and the first\ntime at which $X$ attains its supremum). The first representation is a\nRiemann-Stieltjes integral in terms of the (cumulative) probability\ndistribution of the supremum process and joint probability distribution\nfunction of the process and its supremum process. The integral is evaluated\nusing a combination an analog of the trapezoid rule. The second representation\nis amenable to more accurate albeit slower calculations. We calculate\nexplicitly the Laplace-Fourier transform of $V$ w.r.t. all arguments, apply the\ninverse transforms, and reduce the problem to evaluation of the sum of 5D\nintegrals. The integrals can be evaluated using the summation by parts in the\ninfinite trapezoid rule and simplified trapezoid rule; the inverse Laplace\ntransforms can be calculated using the Gaver-Wynn-Rho algorithm. Under\nadditional conditions on the domain of analyticity of the characteristic\nexponent, the speed of calculations is greatly increased using the conformal\ndeformation technique. For processes of infinite variation, the program in\nMatlab running on a Mac with moderate characteristics achieves the precision\nbetter than E-05 in a fraction of a second; the precision better than E-10 is\nachievable in dozens of seconds. As the order of the process (the analog of the\nBlumenthal-Getoor index) decreases, the CPU time increases, and the best\naccuracy achievable with double precision arithmetic decreases.\n"
    },
    {
        "paper_id": 2312.05346,
        "authors": "Mingxuan He",
        "title": "Deep Learning for Dynamic NFT Valuation",
        "comments": "Code available at https://github.com/mingxuan-he/NFT-pred",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the price dynamics of non-fungible tokens (NFTs) and propose a deep\nlearning framework for dynamic valuation of NFTs. I use data from the Ethereum\nblockchain and OpenSea to train a deep learning model on historical trades,\nmarket trends, and traits/rarity features of Bored Ape Yacht Club NFTs. After\nhyperparameter tuning, the model is able to predict the price of NFTs with high\naccuracy. I propose an application framework for this model using\nzero-knowledge machine learning (zkML) and discuss its potential use cases in\nthe context of decentralized finance (DeFi) applications.\n"
    },
    {
        "paper_id": 2312.05475,
        "authors": "Kiyoshi Kanazawa and Didier Sornette",
        "title": "A standard form of master equations for general non-Markovian jump\n  processes: the Laplace-space embedding framework and asymptotic solution",
        "comments": "29 pages, 8 figures",
        "journal-ref": "Phys. Rev. Res. 6, 023270 (2024)",
        "doi": "10.1103/PhysRevResearch.6.023270",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a standard form of master equations (ME) for general\none-dimensional non-Markovian (history-dependent) jump processes, complemented\nby an asymptotic solution derived from an expanded system-size approach. The ME\nis obtained by developing a general Markovian embedding using a suitable set of\nauxiliary field variables. This Markovian embedding uses a Laplace-convolution\noperation applied to the velocity trajectory. We introduce an asymptotic method\ntailored for this ME standard, generalising the system-size expansion for these\njump processes. Under specific stability conditions tied to a single noise\nsource, upon coarse-graining, the Generalized Langevin Equation (GLE) emerges\nas a universal approximate model for point processes in the weak-coupling\nlimit. This methodology offers a unified analytical toolset for general\nnon-Markovian processes, reinforcing the universal applicability of the GLE\nfounded in microdynamics and the principles of statistical physics.\n"
    },
    {
        "paper_id": 2312.0563,
        "authors": "B. F. Oliveira, A. V. M Oliveira",
        "title": "An empirical analysis of the determinants of network construction for\n  Azul Airlines",
        "comments": null,
        "journal-ref": "Journal of Air Transport Management, 101, 102207 (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper describes an econometric model of the Brazilian domestic carrier\nAzul Airlines' network construction. We employed a discrete-choice framework of\nairline route entry to examine the effects of the merger of another regional\ncarrier, Trip Airlines, with Azul in 2012, especially on its entry decisions.\nWe contrasted the estimated entry determinants before and after the merger with\nthe benchmarks of the US carriers JetBlue Airways and Southwest Airlines\nobtained from the literature, and proposed a methodology for comparing\ndifferent airline entry patterns by utilizing the kappa statistic for\ninterrater agreement. Our empirical results indicate a statistically\nsignificant agreement between raters of Azul and JetBlue, but not Southwest,\nand only for entries on previously existing routes during the pre-merger\nperiod. The results suggest that post-merger, Azul has adopted a more\nidiosyncratic entry pattern, focusing on the regional flights segment to\nconquer many monopoly positions across the country, and strengthening its\nprofitability without compromising its distinguished expansion pace in the\nindustry.\n"
    },
    {
        "paper_id": 2312.05633,
        "authors": "Richeng Piao",
        "title": "The New Age of Collusion? An Empirical Study into Airbnb's Pricing\n  Dynamics and Market Behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study investigates the implications of algorithmic pricing in digital\nmarketplaces, focusing on Airbnb's pricing dynamics. With the advent of\nAirbnb's new pricing tool, this research explores how digital tools influence\nhosts' pricing strategies, potentially leading to market dynamics that straddle\nthe line between efficiency and collusion. Utilizing a Regression Discontinuity\nDesign (RDD) and Propensity Score Matching (PSM), the study examines the causal\neffects of the pricing tool on pricing behavior among hosts with different\noperational strategies. The findings aim to provide insights into the evolving\nlandscape of digital economies, examining the balance between competitive\nmarket practices and the risk of tacit collusion facilitated by algorithmic\npricing. This study contributes to the discourse on digital market regulation,\noffering a nuanced understanding of the implications of AI-driven tools in\nmarket dynamics and antitrust analysis.\n"
    },
    {
        "paper_id": 2312.05655,
        "authors": "Marcin Pitera and Thorsten Schmidt and {\\L}ukasz Stettner",
        "title": "A novel scaling approach for unbiased adjustment of risk estimators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The assessment of risk based on historical data faces many challenges, in\nparticular due to the limited amount of available data, lack of stationarity,\nand heavy tails. While estimation on a short-term horizon for less extreme\npercentiles tends to be reasonably accurate, extending it to longer time\nhorizons or extreme percentiles poses significant difficulties. The application\nof theoretical risk scaling laws to address this issue has been extensively\nexplored in the literature.\n  This paper presents a novel approach to scaling a given risk estimator,\nensuring that the estimated capital reserve is robust and conservatively\nestimates the risk. We develop a simple statistical framework that allows\nefficient risk scaling and has a direct link to backtesting performance. Our\nmethod allows time scaling beyond the conventional square-root-of-time rule,\nenables risk transfers, such as those involved in economic capital allocation,\nand could be used for unbiased risk estimation in small sample settings.\n  To demonstrate the effectiveness of our approach, we provide various examples\nrelated to the estimation of value-at-risk and expected shortfall together with\na short empirical study analysing the impact of our method.\n"
    },
    {
        "paper_id": 2312.05718,
        "authors": "Aparajithan Venkateswaran, Jishnu Das, Tyler H. McCormick",
        "title": "Feasible contact tracing",
        "comments": "main paper 37 pages, 4 figures; supplementary 32 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Contact tracing is one of the most important tools for preventing the spread\nof infectious diseases, but as the experience of COVID-19 showed, it is also\nnext-to-impossible to implement when the disease is spreading rapidly. We show\nhow to substantially improve the efficiency of contact tracing by combining\nstandard microeconomic tools that measure heterogeneity in how infectious a\nsick person is with ideas from machine learning about sequential optimization.\nOur contributions are twofold. First, we incorporate heterogeneity in\nindividual infectiousness in a multi-armed bandit to establish optimal\nalgorithms. At the heart of this strategy is a focus on learning. In the\ntypical conceptualization of contact tracing, contacts of an infected person\nare tested to find more infections. Under a learning-first framework, however,\ncontacts of infected persons are tested to ascertain whether the infected\nperson is likely to be a \"high infector\" and to find additional infections only\nif it is likely to be highly fruitful. Second, we demonstrate using three\nadministrative contact tracing datasets from India and Pakistan during COVID-19\nthat this strategy improves efficiency. Using our algorithm, we find 80% of\ninfections with just 40% of contacts while current approaches test twice as\nmany contacts to identify the same number of infections. We further show that a\nsimple strategy that can be easily implemented in the field performs at nearly\noptimal levels, allowing for, what we call, feasible contact tracing. These\nresults are immediately transferable to contact tracing in any epidemic.\n"
    },
    {
        "paper_id": 2312.05827,
        "authors": "\\'Alvaro Cartea, Gerardo Duran-Martin, Leandro S\\'anchez-Betancourt",
        "title": "Detecting Toxic Flow",
        "comments": "27 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper develops a framework to predict toxic trades that a broker\nreceives from her clients. Toxic trades are predicted with a novel online\nBayesian method which we call the projection-based unification of last-layer\nand subspace estimation (PULSE). PULSE is a fast and statistically-efficient\nonline procedure to train a Bayesian neural network sequentially. We employ a\nproprietary dataset of foreign exchange transactions to test our methodology.\nPULSE outperforms standard machine learning and statistical methods when\npredicting if a trade will be toxic; the benchmark methods are logistic\nregression, random forests, and a recursively-updated maximum-likelihood\nestimator. We devise a strategy for the broker who uses toxicity predictions to\ninternalise or to externalise each trade received from her clients. Our\nmethodology can be implemented in real-time because it takes less than one\nmillisecond to update parameters and make a prediction. Compared with the\nbenchmarks, PULSE attains the highest PnL and the largest avoided loss for the\nhorizons we consider.\n"
    },
    {
        "paper_id": 2312.05943,
        "authors": "Wladimir Ostrovsky",
        "title": "Dealer Strategies in Agent-Based Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the utility of agent-based simulations in realistically\nmodelling market structures and sheds light on the nuances of optimal dealer\nstrategies. It underscores the contrast between conclusions drawn from\nprobabilistic modelling and agent-based simulations, but also highlights the\nimportance of employing a realistic test bed to analyse intricate dynamics.\nThis is achieved by extending the agent-based model for auction markets by\n\\cite{Chiarella.2008} to include liquidity providers. By constantly and\npassively quoting, the dealers influence their own wealth but also have\nramifications on the market as a whole and the other participating agents.\nThrough synthetic market simulations, the optimal behaviour of different dealer\nstrategies and their consequences on market dynamics are examined. The analysis\nreveals that dealers exhibiting greater risk aversion tend to yield better\nperformance outcomes. The choice of quote sizes by dealers is\nstrategy-dependent: one strategy demonstrates enhanced performance with larger\nquote sizes, whereas the other strategy show a better results with smaller\nones. Increasing quote size shows positive influence on the market in terms of\nvolatility and kurtosis with both dealer strategies. However, the impact\nstemming from larger risk aversion is mixed. While one of the dealer strategies\nshows no discernible effect, the other strategy results in mixed outcomes,\nencompassing both positive and negative effects.\n"
    },
    {
        "paper_id": 2312.05971,
        "authors": "Marco Gortan, Lorenzo Testa, Giorgio Fagiolo, Francesco Lamperti",
        "title": "A unified repository for pre-processed climate data weighted by gridded\n  economic activity",
        "comments": "11 pages, 8 figures",
        "journal-ref": "Scientific Data 11, 533 (2024)",
        "doi": "10.1038/s41597-024-03304-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although high-resolution gridded climate variables are provided by multiple\nsources, the need for country and region-specific climate data weighted by\nindicators of economic activity is becoming increasingly common in\nenvironmental and economic research. We process available information from\ndifferent climate data sources to provide spatially aggregated data with global\ncoverage for both countries (GADM0 resolution) and regions (GADM1 resolution)\nand for a variety of climate indicators (average precipitations, average\ntemperatures, average SPEI). We weigh gridded climate data by population\ndensity or by night light intensity -- both proxies of economic activity --\nbefore aggregation. Climate variables are measured daily, monthly, and\nannually, covering (depending on the data source) a time window from 1900 (at\nthe earliest) to 2023. We pipeline all the preprocessing procedures in a\nunified framework, which we share in the open-access Weighted Climate Data\nRepository web app. Finally, we validate our data through a systematic\ncomparison with those employed in leading climate impact studies.\n"
    },
    {
        "paper_id": 2312.05973,
        "authors": "Michael Kupper, Max Nendel, Alessandro Sgarabottolo",
        "title": "Risk measures based on weak optimal transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study convex risk measures with weak optimal transport\npenalties. In a first step, we show that these risk measures allow for an\nexplicit representation via a nonlinear transform of the loss function. In a\nsecond step, we discuss computational aspects related to the nonlinear\ntransform as well as approximations of the risk measures using, for example,\nneural networks. Our setup comprises a variety of examples, such as classical\noptimal transport penalties, parametric families of models, uncertainty on path\nspaces, moment constrains, and martingale constraints. In a last step, we show\nhow to use the theoretical results for the numerical computation of worst-case\nlosses in an insurance context and no-arbitrage prices of European contingent\nclaims after quoted maturities in a model-free setting.\n"
    },
    {
        "paper_id": 2312.05977,
        "authors": "Roger J. A. Laeven and Mitja Stadje",
        "title": "A Rank-Dependent Theory for Decision under Risk and Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper axiomatizes, in a two-stage setup, a new theory for decision under\nrisk and ambiguity. The axiomatized preference relation $\\succeq$ on the space\n$\\tilde{V}$ of random variables induces an ambiguity index $c$ on the space\n$\\Delta$ of probabilities, a probability weighting function $\\psi$, generating\nthe measure $\\nu_{\\psi}$ by transforming an objective probability measure, and\na utility function $\\phi$, such that, for all\n$\\tilde{v},\\tilde{u}\\in\\tilde{V}$, \\begin{align*} \\tilde{v}\\succeq\\tilde{u}\n\\Leftrightarrow \\min_{Q \\in \\Delta}\n\\left\\{\\mathbb{E}_Q\\left[\\int\\phi\\left(\\tilde{v}^{\\centerdot}\\right)\\,\\mathrm{d}\\nu_{\\psi}\\right]+c(Q)\\right\\}\n\\geq \\min_{Q \\in \\Delta}\n\\left\\{\\mathbb{E}_Q\\left[\\int\\phi\\left(\\tilde{u}^{\\centerdot}\\right)\\,\\mathrm{d}\\nu_{\\psi}\\right]+c(Q)\\right\\}.\n\\end{align*} Our theory extends the rank-dependent utility model of Quiggin\n(1982) for decision under risk to risk and ambiguity, reduces to the\nvariational preferences model when $\\psi$ is the identity, and is dual to\nvariational preferences when $\\phi$ is affine in the same way as the theory of\nYaari (1987) is dual to expected utility. As a special case, we obtain a\npreference axiomatization of a decision theory that is a rank-dependent\ngeneralization of the popular maxmin expected utility theory. We characterize\nambiguity aversion in our theory.\n"
    },
    {
        "paper_id": 2312.0645,
        "authors": "Evgeny Kuzmin, Maksim Vlasov, Wadim Strielkowski, Marina Faminskaya,\n  Konstantin Kharchenko",
        "title": "Human capital in the sustainable economic development of the energy\n  sector",
        "comments": "12 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the role of human capital investment in driving\nsustainable socio-economic growth within the energy industry. The fuel and\nenergy sector undeniably forms the backbone of contemporary economies,\nsupplying vital resources that underpin industrial activities, transportation,\nand broader societal operations. In the context of the global shift toward\nsustainability, it is crucial to focus not just on technological innovation but\nalso on cultivating human capital within this sector. This is particularly\nrelevant considering the recent shift towards green and renewable energy\nsolutions. In this study, we utilize bibliometric analysis, drawing from a\ndataset of 1933 documents (represented by research papers, conference\nproceedings, and book chapters) indexed in the Web of Science (WoS) database.\nWe conduct a network cluster analysis of the textual and bibliometric data\nusing VOSViewer software. The findings stemming from our analysis indicate that\ninvestments in human capital are perceived as important in achieving long-term\nsustainable economic growth in the energy companies both in Russia and\nworldwide. In addition, it appears that the role of human capital in the energy\nsector is gaining more popularity both among Russian and international\nresearchers and academics.\n"
    },
    {
        "paper_id": 2312.06479,
        "authors": "Kyle Myers, Wei Yang Tham",
        "title": "Money, Time, and Grant Design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The design of research grants has been hypothesized to be a useful tool for\ninfluencing researchers and their science. We test this by conducting two\nthought experiments in a nationally representative survey of academic\nresearchers. First, we offer participants a hypothetical grant with randomized\nattributes and ask how the grant would influence their research strategy.\nLonger grants increase researchers' willingness to take risks, but only among\ntenured professors, which suggests that job security and grant duration are\ncomplements. Both longer and larger grants reduce researchers' focus on speed,\nwhich suggests a significant amount of racing in science is in pursuit of\nresources. But along these and other strategic dimensions, the effect of grant\ndesign is small. Second, we identify researchers' indifference between the two\ngrant design parameters and find they are very unwilling to trade off the\namount of funding a grant provides in order to extend the duration of the grant\n$\\unicode{x2014}$ money is much more valuable than time. Heterogeneity in this\npreference can be explained with a straightforward model of researchers'\nutility. Overall, our results suggest that the design of research grants is\nmore relevant to selection effects on the composition of researchers pursuing\nfunding, as opposed to having large treatment effects on the strategies of\nresearchers that receive funding.\n"
    },
    {
        "paper_id": 2312.0651,
        "authors": "Metin Lamby and Valentin Zieglmeier and Christian Ziegler",
        "title": "Trusting a Smart Contract Means Trusting Its Owners: Understanding\n  Centralization Risk",
        "comments": "Peer-reviewed version accepted for publication in the proceedings of\n  the 5th Conference on Blockchain Research & Applications for Innovative\n  Networks and Services (BRAINS '23)",
        "journal-ref": null,
        "doi": "10.1109/brains59668.2023.10316813",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Smart contract access control mechanisms can introduce centralization into\nsupposedly decentralized ecosystems. In our view, such centralization is an\noverlooked risk of smart contracts that underlies well-known smart contract\nsecurity incidents. Critically, mitigating the known vulnerability of missing\npermission verification by implementing authorization patterns can in turn\nintroduce centralization. To delineate the issue, we define centralization risk\nand describe smart contract source code patterns for Ethereum and Algorand that\ncan introduce it to smart contracts. We explain under which circumstances the\ncentralization can be exploited. Finally, we discuss implications of\ncentralization risk for different smart contract stakeholders.\n"
    },
    {
        "paper_id": 2312.06589,
        "authors": "Alexander Roth",
        "title": "Power sector impacts of a simultaneous European heat pump rollout",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The decarbonization of buildings requires the phase-out of fossil fuel\nheating systems. Heat pumps are considered a crucial technology to supply a\nsubstantial part of heating energy for buildings. Yet, their introduction is\nnot without challenges, as heat pumps generate additional electricity demand as\nwell as peak loads. To better understand these challenges, an ambitious\nsimultaneous heat pump rollout in several central European countries with an\nhourly-resolved capacity expansion model of the power sector is studied. I\nassess the structure of hours and periods of peak heat demands and their\nconcurrence with hours and periods of peak residual load. In a 2030 scenario, I\nfind that meeting 25% of total heat demand in buildings with heat pumps would\nbe covered best with additional wind power generation capacities. I also\nidentify the important role of small thermal energy storage that could reduce\nthe need for additional firm generation capacity. Due to the co-occurrence of\nheat demand, interconnection between countries does not substantially reduce\nthe additional generation capacities needed for heat pump deployment. Based on\nsix different weather years, my analysis cautions against relying on results\nbased on a single weather year.\n"
    },
    {
        "paper_id": 2312.06679,
        "authors": "Rafael Andersson Lipcsey",
        "title": "The Transformative Effects of AI on International Economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As AI adoption accelerates, research on its economic impacts becomes a\nsalient source to consider for stakeholders of AI policy. Such research is\nhowever still in its infancy, and one in need of review. This paper aims to\naccomplish just that and is structured around two main themes. Firstly, the\npath towards transformative AI, and secondly the wealth created by it. It is\nfound that sectors most embedded into global value chains will drive economic\nimpacts, hence special attention is paid to the international trade\nperspective. When it comes to the path towards transformative AI, research is\nheterogenous in its predictions, with some predicting rapid, unhindered\nadoption, and others taking a more conservative view based on potential\nbottlenecks and comparisons to past disruptive technologies. As for wealth\ncreation, while some agreement is to be found in AI's growth boosting\nabilities, predictions on timelines are lacking. Consensus exists however\naround the dispersion of AI induced wealth, which is heavily biased towards\ndeveloped countries due to phenomena such as anchoring and reduced bargaining\npower of developing countries. Finally, a shortcoming of economic growth models\nin failing to consider AI risk is discovered. Based on the review, a\ncalculated, and slower adoption rate of AI technologies is recommended.\n"
    },
    {
        "paper_id": 2312.0669,
        "authors": "Weiye Yang",
        "title": "Backward Stochastic Differential Equations in Financial Mathematics",
        "comments": "33 pages. Essay submitted for Part III of the Cambridge University\n  Mathematical Tripos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A backward stochastic differential equation (BSDE) is an SDE of the form\n$-dY_t = f(t,Y_t,Z_t)dt - Z_t^*dW_t;\\ Y_T = \\xi$. The subject of BSDEs has seen\nextensive attention since their introduction in the linear case by Bismut\n(1973) and in the general case by Pardoux and Peng (1990). In contrast with\ndeterministic differential equations, it is not enough to simply reverse the\ndirection of time and treat the terminal condition as an initial condition, as\nwe would then run into problems with adaptedness. Intuitively, our \"knowledge\"\nat time $t$ consists only of what has happened at all times $s \\in [0,t]$, and\nwe cannot reverse the direction of time whilst keeping this true.\n  The layout of this essay is as follows: In Section 1 we introduce BSDEs and\ngo over the basic results of BSDE theory, including two major theorems: the\nexistence and uniqueness of solutions and the comparison theorem. We also\nintroduce linear BSDEs and the notion of supersolutions of a BSDE. In Section 2\nwe set up the financial framework in which we will price European contingent\nclaims, and prove a result about the fair price of such claims in a dynamically\ncomplete market. In Section 3 we extend the theory of linear BSDEs to include\nconcave BSDEs, and apply this to pricing claims in more complicated market\nmodels. In Section 4 we take a look at utility maximisation problems, and see\nhow utilising BSDE theory allows for a relatively simple and neat solution in\ncertain cases.\n"
    },
    {
        "paper_id": 2312.06694,
        "authors": "Rafael Andersson Lipcsey",
        "title": "An Inquiry Into The Economic Linkages Between The Swedish Air Transport\n  Sector And The Economy As A Whole In The Context Of The Covid-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis aims to assess the importance of the air transport sector for\nSweden's economy in the context of the COVID-19 pandemic. Two complementary\nresearch goals are formulated. Firstly, investigating economic linkages of the\nSwedish air transport sector. Secondly, estimating the effects of the pandemic\non Swedish air transport, and the spin-off effects on the economy as a whole.\nOverview of literature in the field reveals that while a fair amount of\nresearch exists on the importance of air transport, unsurprisingly, pandemic\neffects have yet to be investigated. The methodological framework chosen is\ninput-output analysis, thus, the backbone of data materials used is the Swedish\ninput-output table, complemented by additional datasets. For measuring economic\nlinkages, basic input-output analysis is applied. Meanwhile, to capture the\npandemic's impacts, a combination of inoperability analysis and the partial\nhypothetical extraction model is implemented. It is found that while Swedish\nair transport plays an important role in turning the cogs of the Swedish\neconomy, when compared to all other sectors, it ranks on the lower end.\nFurthermore, while the COVID-19 pandemic has a detrimental short-term impact on\nSwedish air transport, the spin-off effects for the economy as a whole are\nmilder. It is concluded, that out of a value-added perspective, the Swedish\ngovernment, and by extension policy makers globally need not support failing\nairlines, and other air transport actors. Nonetheless, some aspects could not\nbe captured through the methods used, such as the possible importance of\nairlines to national security at times of dodgy global cooperation. Lastly, to\naccurately capture not only short-term, but also long-term effects of the\npandemic, future research using alternative, causality-based frameworks and\ndynamic input-output models will be highly beneficial.\n"
    },
    {
        "paper_id": 2312.06711,
        "authors": "Ashish Dhiman and Yibei Hu",
        "title": "Physics Informed Neural Network for Option Pricing",
        "comments": "7 pages + references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We apply a physics-informed deep-learning approach the PINN approach to the\nBlack-Scholes equation for pricing American and European options. We test our\napproach on both simulated as well as real market data, compare it to\nanalytical/numerical benchmarks. Our model is able to accurately capture the\nprice behaviour on simulation data, while also exhibiting reasonable\nperformance for market data. We also experiment with the architecture and\nlearning process of our PINN model to provide more understanding of convergence\nand stability issues that impact performance.\n"
    },
    {
        "paper_id": 2312.06793,
        "authors": "Edward T.A. Mitchard, Harry Carstairs, Riccardo Cosenza, Sassan S.\n  Saatchi, Jason Funk, Paula Nieto Quintano, Thom Brade, Iain M. McNicol,\n  Patrick Meir, Murray B. Collins, Eric Nowak",
        "title": "Serious errors impair an assessment of forest carbon projects: A\n  rebuttal of West et al. (2023)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Independent retrospective analyses of the effectiveness of reducing\ndeforestation and forest degradation (REDD) projects are vital to ensure\nclimate change benefits are being delivered. A recent study in Science by West\net al. (1) appeared therefore to be a timely alert that the majority of\nprojects operating in the 2010s failed to reduce deforestation rates.\nUnfortunately, their analysis suffered from major flaws in the choice of\nunderlying data, resulting in poorly matched and unstable counterfactual\nscenarios. These were compounded by calculation errors, biasing the study\nagainst finding that projects significantly reduced deforestation. This flawed\nanalysis of 24 projects unfairly condemned all 100+ REDD projects, and risks\ncutting off finance for protecting vulnerable tropical forests from destruction\nat a time when funding needs to grow rapidly.\n"
    },
    {
        "paper_id": 2312.06842,
        "authors": "Yuri Kabanov and Aleksei Kozhevnikov",
        "title": "Optimal pair trading: consumption-investment problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We expose a simple solution of the consumption-investment problem pair\ntrading. The proof is based on the remark that the HJB equation can be reduced\nto a linear parabolic equation solvable explicitly.\n"
    },
    {
        "paper_id": 2312.06994,
        "authors": "Le Tung Bach",
        "title": "The behavioral intention to adopt Proptech services in Vietnam real\n  estate market",
        "comments": "PhD thesis, 127 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.5281/zenodo.10360520",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the main stages for achieving success is the adoption of new\ntechnology by its users. Several studies show that Property technology is\nadvantageous for real estate stakeholders. Hence, the purpose of this paper is\nto investigate the users' engagement behavior to adopt Property technology in\nthe Vietnamese real estate market. To that end, a purposive sample of 142\nparticipants was recruited to complete an online quantitative approach based\nsurvey. The survey consisted of a modified and previously validated measure of\nacceptance based on the extended demographic version of the unified theory of\nacceptance and use of technology), as well as usage scale items. The empirical\nfindings confirm that participants were generally accepting of Property\ntechnology in the Vietnamese real estate market. The highest mean values were\nassociated with the subscales of effort expectancy and performance expectancy,\nwhile we can easily identify the lowest mean value in the social influence\nsubscale. The usage of Property technology was slightly more concerned with the\ngathering of information on properties and markets than transactions or\nportfolio management. This study provides an in depth understanding of Property\ntechnology for firms' managers and marketers. Online social interactions might\nbe either harmful or fruitful for firms depending on the type of interaction\nand engagement behavior. This is especially true of property portals and social\nmedia forums that would help investors to connect, communicate and learn.\nFindings can help users to improve their strategies for digital marketing. By\nproviding robust findings by addressing issues like omitted variables and\nendogeneity, the findings of this study are promising for developing new\nhypotheses and theoretical models in the context of the Vietnamese real estate\nmarket.\n"
    },
    {
        "paper_id": 2312.07065,
        "authors": "Roberto Savona, Cristina Maria Alberini, Lucia Alessi, Iacopo\n  Baussano, Petros Dellaportas, Ranieri Guerra, Sean Khozin, Andrea Modena,\n  Sergio Pecorelli, Guido Rasi, Paolo Daniele Siviero, Roger M. Stein",
        "title": "Towards a Framework for a New Research Ecosystem",
        "comments": "20 pages, 1 table, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major gap exists between the conceptual suggestion of how much a nation\nshould invest in science, innovation, and technology, and the practical\nimplementation of what is done. We identify 4 critical challenges that must be\naddress in order to develop an environment conducive to collaboration across\norganizations and governments, while also preserving commercial rewards for\ninvestors and innovators, in order to move towards a new Research Ecosystem.\n"
    },
    {
        "paper_id": 2312.0724,
        "authors": "Gordon Rausser, Galina Chebotareva, Wadim Strielkowski, Lubos Smutka",
        "title": "Would Russian solar energy projects be feasible independent of state\n  subsidies?",
        "comments": "9 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the critical question of the sustainability of Russian\nsolar energy initiatives in the absence of governmental financial support. The\nstudy aims to determine if Russian energy companies can maintain operations in\nthe solar energy sector without relying on direct state subsidies.\nMethodologically, the analysis utilizes established investment metrics such as\nNet Present Value (NPV), Internal Rate of Return (IRR), and Discounted Payback\nPeriod (DPP), tailored to reflect the unique technical and economic aspects of\nRussian solar energy facilities and to evaluate the influence of\nsector-specific risks on project efficiency, using a rating approach. We\nexamined eleven solar energy projects under ten different scenarios to\nunderstand the dynamics of direct state support, exploring variations in\nsupport cessation, reductions in financial assistance, and the projects'\nresilience to external risk factors. Our multi-criteria scenario assessment\nindicates that, under the prevailing market conditions, the Russian solar\nenergy sector is not yet equipped to operate efficiently without ongoing state\nfinancial subsidies. Interestingly, our findings also suggest that the solar\nenergy sector in Russia has a greater potential to reduce its dependence on\nstate support compared to the wind energy sector. Based on these insights, we\npropose energy policy recommendations aimed at gradually minimizing direct\ngovernment funding in the Russian renewable energy market. This strategy is\ndesigned to foster self-sufficiency and growth in the solar energy sector.\n"
    },
    {
        "paper_id": 2312.07328,
        "authors": "Maria Shishanina and Anatoly Sidorov",
        "title": "Knowledge Management in Socio-Economic Development of Municipal Units:\n  Basic Concepts",
        "comments": "3 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The article discusses the basic concepts of strategic planning in the Russian\nFederation, highlights the legal, financial and resource features that act as\nrestrictions in decision making in the field of socio-economic development of\nmunicipalities. The analysis concluded that to design an adequate model of\nsocio-economic development of municipalities is a very difficult task,\nparticularly when the traditional approaches are applied. To solve the task, we\nproposed to use the semantic modeling as well as cognitive maps which are able\nto point out the set of dependencies that arise between factors having a direct\nimpact on socio-economic development.\n"
    },
    {
        "paper_id": 2312.07469,
        "authors": "Ben-Hur Francisco Cardoso, Eva Yamila da Silva Catela, Guilherme\n  Viegas, Fl\\'avio L. Pinheiro, Dominik Hartmann",
        "title": "Export complexity, industrial complexity and regional economic growth in\n  Brazil",
        "comments": "13 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Research on productive structures has shown that economic complexity\nconditions economic growth. However, little is known about which type of\ncomplexity, e.g., export or industrial complexity, matters more for regional\neconomic growth in a large emerging country like Brazil. Brazil exports natural\nresources and agricultural goods, but a large share of the employment derives\nfrom services, non-tradables, and within-country manufacturing trade. Here, we\nuse a large dataset on Brazil's formal labor market, including approximately\n100 million workers and 581 industries, to reveal the patterns of export\ncomplexity, industrial complexity, and economic growth of 558 micro-regions\nbetween 2003 and 2019. Our results show that export complexity is more evenly\nspread than industrial complexity. Only a few -- mainly developed urban places\n-- have comparative advantages in sophisticated services. Regressions show that\na region's industrial complexity is a significant predictor for 3-year growth\nprospects, but export complexity is not. Moreover, economic complexity in\nneighboring regions is significantly associated with economic growth. The\nresults show export complexity does not appropriately depict Brazil's knowledge\nbase and growth opportunities. Instead, promoting the sophistication of the\nheterogeneous regional industrial structures and development spillovers is a\nkey to growth.\n"
    },
    {
        "paper_id": 2312.07614,
        "authors": "Christian P. Fries, Lennart Quante",
        "title": "Intergenerational Equitable Climate Change Mitigation: Negative Effects\n  of Stochastic Interest Rates; Positive Effects of Financing",
        "comments": "36 pages, 10 figures, arXiv admin note: text overlap with\n  arXiv:2309.16186",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Today's decisions on climate change mitigation affect the damage that future\ngenerations will bear. Discounting future benefits and costs of climate change\nmitigation is one of the most critical components of assessing efficient\nclimate mitigation pathways. We extend the DICE model with stochastic discount\nrates to reflect the uncertain nature of discount rates. Stochastic rates give\nrise to a stochastic mitigation strategy, resulting in all model quantities\nbecoming stochastic.\n  We show that the classical calibration of the DICE model induces\nintergenerational inequality: future generations have to bear higher costs\nrelative to GDP. Further, we show that considering stochastic discount rates\nand stochastic abatement policies, which can be interpreted as successive\nre-calibration, increases intergenerational inequality (and adds additional\nrisks). Motivated by this, we consider additional financing risks by\ninvestigating two modifications of DICE. We find that allowing financing of\nabatement costs and considering non-linear financing effects for large damages\nimproves intergenerational effort sharing. To conclude our discussion of\noptions to improve intergenerational equity in an IAM, we propose a modified\noptimization to keep costs below 3% of GDP, resulting in more equal\ndistribution of efforts between generations.\n"
    },
    {
        "paper_id": 2312.07703,
        "authors": "Tiziano De Angelis, Fabien Gensbittel, St\\'ephane Villeneuve",
        "title": "Nash equilibria for dividend distribution with competition",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a Nash equilibrium in feedback form for a class of two-person\nstochastic games with absorption arising from corporate finance. More\nprecisely, the paper focusses on a strategic dynamic game in which two\nfinancially-constrained firms operate in the same market. The firms distribute\ndividends and are faced with default risk. The strategic interaction arises\nfrom the fact that if one firm defaults, the other one becomes a monopolist and\nincreases its profitability.\n  To determine a Nash equilibrium in feedback form, we develop two different\nconcepts depending on the initial endowment of each firm. If one firm is richer\nthan the other one, then we use a notion of control vs.\\ strategy equilibrium.\nIf the two firms have the same initial endowment (hence they are symmetric in\nour setup) then we need mixed strategies in order to construct a symmetric\nequilibrium.\n"
    },
    {
        "paper_id": 2312.07733,
        "authors": "Mike Ludkovski and Saad Mouti and Glen Swindle",
        "title": "Least-Cost Structuring of 24/7 Carbon-Free Electricity Procurements",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the construction of renewable portfolios targeting specified\ncarbon-free (CFE) hourly performance scores. We work in a probabilistic\nframework that uses a collection of simulation scenarios and imposes\nprobability constraints on achieving the desired CFE score. In our approach\nthere is a fixed set of available CFE generators and a given load customer who\nseeks to minimize annual procurement costs. We illustrate results using a\nrealistic dataset of jointly calibrated solar and wind assets, and compare\ndifferent approaches to handling multiple loads.\n"
    },
    {
        "paper_id": 2312.07878,
        "authors": "Martin Semmann and Mahei Manhei Li",
        "title": "New Kids on the Block: On the impact of information retrieval on\n  contextual resource integration patterns",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rise of new modes of interaction with AI skyrocketed the popularity,\napplicability, and amount of use cases. Despite this evolution, conceptual\nintegration is falling behind. Studies suggest that there is hardly a\nsystematization in using AI in organizations. Thus, by taking a\nservice-dominant logic perspective, specifically, the concept of resource\nintegration patterns, the most potent application of AI for organizational use\n- namely information retrieval - is analyzed. In doing so, we propose a\nsystematization that can be applied to deepen understanding of core technical\nconcepts, further investigate AI in contexts, and help explore research\ndirections guided by SDL.\n"
    },
    {
        "paper_id": 2312.08141,
        "authors": "L\\'aszl\\'o Sipos and Kolos Csaba \\'Agoston and P\\'eter Bir\\'o and\n  S\\'andor Boz\\'oki and L\\'aszl\\'o Csat\\'o",
        "title": "How to measure consumer's inconsistency in sensory testing?",
        "comments": "26 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard methods, standard test conditions and the use of good sensory\npractices are key elements of sensory testing. However, while compliance\nassessment by trained and expert assessors is well developed, few information\nis available on testing consumer consistency. Therefore, we aim to answer the\nfollowing research questions: What type of metrics can be used to characterise\n(in)consistent evaluation? How to rank assessors and attributes' evaluations\nbased on (in)consistency? Who can be considered an (in)consistent assessor?\nWhat is the difference between consistent and inconsistent assessors'\nevaluations and use of scale? What is the impact of consistent and inconsistent\nassessors' evaluations on the overall liking estimate? The proposed detection\nof (in)consistency requires evaluations on two connected scales. We reveal how\nassessors and attributes can be ranked according to the level of inconsistency,\nas well as how inconsistent assessors can be identified. The suggested approach\nis illustrated by data from sensory tests of biscuits enriched with three\npollens at different levels. Future consumer tests are recommended to account\nfor possible inconsistent evaluations.\n"
    },
    {
        "paper_id": 2312.08784,
        "authors": "Ulrich Horst and Wei Xu and Rouyi Zhang",
        "title": "Convergence of Heavy-Tailed Hawkes Processes and the Microstructure of\n  Rough Volatility",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish the weak convergence of the intensity of a nearly-unstable\nHawkes process with heavy-tailed kernel. Our result is used to derive a scaling\nlimit for a financial market model where orders to buy or sell an asset arrive\naccording to a Hawkes process with power-law kernel. After suitable rescaling\nthe price-volatility process converges weakly to a rough Heston model. Our\nconvergence result is much stronger than previously established ones that have\neither focused on light-tailed kernels or the convergence of integrated\nvolatility process. The key is to establish the tightness of the family of\nrescaled volatility processes. This is achieved by introducing a new methods to\nestablish the $C$-tightness of c\\`adl\\`ag processes based on the classical\nKolmogorov-Chentsov tightness criterion for continuous processes.\n"
    },
    {
        "paper_id": 2312.08927,
        "authors": "Konark Jain, Nick Firoozye, Jonathan Kochems and Philip Treleaven",
        "title": "Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes\n  Process",
        "comments": "Presented at Market Microstructure 2023, Quantitative Finance\n  Workshop 2024. Oxford SML Finance Seminar 2024 and Submitted to Finance\n  Research Letters journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Hawkes Process has been used to model Limit Order Book (LOB) dynamics in\nseveral ways in the literature however the focus has been limited to capturing\nthe inter-event times while the order size is usually assumed to be constant.\nWe propose a novel methodology of using Compound Hawkes Process for the LOB\nwhere each event has an order size sampled from a calibrated distribution. The\nprocess is formulated in a novel way such that the spread of the process always\nremains positive. Further, we condition the model parameters on time of day to\nsupport empirical observations. We make use of an enhanced non-parametric\nmethod to calibrate the Hawkes kernels and allow for inhibitory\ncross-excitation kernels. We showcase the results and quality of fits for an\nequity stock's LOB in the NASDAQ exchange and compare them against several\nbaselines. Finally, we conduct a market impact study of the simulator and show\nthe empirical observation of a concave market impact function is indeed\nreplicated.\n"
    },
    {
        "paper_id": 2312.09081,
        "authors": "Niklas Valentin Lehmann",
        "title": "Forecasting skill of a crowd-prediction platform: A comparison of\n  exchange rate forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Open online crowd-prediction platforms are increasingly used to forecast\ntrends and complex events. Despite the large body of research on\ncrowd-prediction and forecasting tournaments, online crowd-prediction platforms\nhave never been directly compared to other forecasting methods. In this\nanalysis, exchange rate crowd-predictions made on Metaculus are compared to\npredictions made by the random-walk, a statistical model considered extremely\nhard-to-beat. The random-walk provides less erroneous forecasts, but the\ncrowd-prediction does very well. By using the random-walk as a benchmark, this\nanalysis provides a rare glimpse into the forecasting skill displayed on open\nonline crowd-prediction platforms.\n"
    },
    {
        "paper_id": 2312.09201,
        "authors": "Alexander M. G. Cox and Annemarie M. Grass",
        "title": "Robust option pricing with volatility term structure -- An empirical\n  study for variance options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The robust option pricing problem is to find upper and lower bounds on fair\nprices of financial claims using only the most minimal assumptions. It\ncontrasts with the classical, model-based approach and gained prominence in the\nwake of the 2008 financial crisis, and can be used to understand the extent to\nwhich a model-based price is sensitive to the underlying model assumptions.\nCommon approaches involve pricing exotic derivatives such as variance options\nby incorporating market data through implied volatility. The existing\nliterature focuses largely on incorporating implied volatility information\ncorresponding to the maturity of the exotic option. In this paper, we aim to\nexplain how intermediate data can and should be incorporated. It is natural to\nexpect that this additional information will improve the robust pricing bounds.\nTo investigate this question, we consider variance options, where the bounds of\nthe informed robust pricing problem are known. We proceed to conduct an\nempirical study uncovering a surprising finding: Contrary to common belief, the\nincorporation of more information does not lead to an improvement of the robust\npricing bounds.\n"
    },
    {
        "paper_id": 2312.09353,
        "authors": "Andrew Na and Justin Wan",
        "title": "Residual U-net with Self-Attention to Solve Multi-Agent Time-Consistent\n  Optimal Trade Execution",
        "comments": "This is an initial draft of the work",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore the use of a deep residual U-net with\nself-attention to solve the the continuous time time-consistent mean variance\noptimal trade execution problem for multiple agents and assets. Given a finite\nhorizon we formulate the time-consistent mean-variance optimal trade execution\nproblem following the Almgren-Chriss model as a Hamilton-Jacobi-Bellman (HJB)\nequation. The HJB formulation is known to have a viscosity solution to the\nunknown value function. We reformulate the HJB to a backward stochastic\ndifferential equation (BSDE) to extend the problem to multiple agents and\nassets. We utilize a residual U-net with self-attention to numerically\napproximate the value function for multiple agents and assets which can be used\nto determine the time-consistent optimal control. In this paper, we show that\nthe proposed neural network approach overcomes the limitations of finite\ndifference methods. We validate our results and study parameter sensitivity.\nWith our framework we study how an agent with significant price impact\ninteracts with an agent without any price impact and the optimal strategies\nused by both types of agents. We also study the performance of multiple sellers\nand buyers and how they compare to a holding strategy under different economic\nconditions.\n"
    },
    {
        "paper_id": 2312.09654,
        "authors": "Umberto Natale and Michael Moser",
        "title": "The cost of artificial latency in the PBS context",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a comprehensive analysis of the implications of artificial latency\nin the Proposer-Builder Separation framework on the Ethereum network. Focusing\non the MEV-Boost auction system, we analyze how strategic latency manipulation\naffects Maximum Extractable Value yields and network integrity. Our findings\nreveal both increased profitability for node operators and significant systemic\nchallenges, including heightened network inefficiencies and centralization\nrisks. We empirically validates these insights with a pilot that Chorus One has\nbeen operating on Ethereum mainnet. We demonstrate the nuanced effects of\nlatency on bid selection and validator dynamics. Ultimately, this research\nunderscores the need for balanced strategies that optimize Maximum Extractable\nValue capture while preserving the Ethereum network's decentralization ethos.\n"
    },
    {
        "paper_id": 2312.09707,
        "authors": "Francesco Cesarone, Rosella Giacometti, Manuel Luis Martino, Fabio\n  Tardella",
        "title": "A return-diversification approach to portfolio selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a general bi-objective model for portfolio\nselection, aiming to maximize both a diversification measure and the portfolio\nexpected return. Within this general framework, we focus on maximizing a\ndiversification measure recently proposed by Choueifaty and Coignard for the\ncase of volatility as a risk measure. We first show that the maximum\ndiversification approach is actually equivalent to the Risk Parity approach\nusing volatility under the assumption of equicorrelated assets. Then, we extend\nthe maximum diversification approach formulated for general risk measures.\nFinally, we provide explicit formulations of our bi-objective model for\ndifferent risk measures, such as volatility, Mean Absolute Deviation,\nConditional Value-at-Risk, and Expectiles, and we present extensive\nout-of-sample performance results for the portfolios obtained with our model.\nThe empirical analysis, based on five real-world data sets, shows that the\nreturn-diversification approach provides portfolios that tend to outperform the\nstrategies based only on a diversification method or on the classical\nrisk-return approach.\n"
    },
    {
        "paper_id": 2312.09843,
        "authors": "Stefano Bianchini, Moritz M\\\"uller and Pierre Pelletier",
        "title": "Drivers and Barriers of AI Adoption and Use in Scientific Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  New technologies have the power to revolutionize science. It has happened in\nthe past and is happening again with the emergence of new computational tools,\nsuch as artificial intelligence and machine learning. Despite the documented\nimpact of these technologies, there remains a significant gap in understanding\nthe process of their adoption within the scientific community. In this paper,\nwe draw on theories of scientific and technical human capital to study the\nintegration of AI in scientific research, focusing on the human capital of\nscientists and the external resources available within their network of\ncollaborators and institutions. We validate our hypotheses on a large sample of\npublications from OpenAlex, covering all sciences from 1980 to 2020, and\nidentify a set key drivers and inhibitors of AI adoption and use in science.\nOur results suggest that AI is pioneered by domain scientists with a `taste for\nexploration' and who are embedded in a network rich of computer scientists,\nexperienced AI scientists and early-career researchers; they come from\ninstitutions with high citation impact and a relatively strong publication\nhistory on AI. The access to computing resources only matters for a few\nscientific disciplines, such as chemistry and medical sciences. Once AI is\nintegrated into research, most adoption factors continue to influence its\nsubsequent reuse. Implications for the organization and management of science\nin the evolving era of AI-driven discovery are discussed.\n"
    },
    {
        "paper_id": 2312.10084,
        "authors": "Aarush Pratik Sheth, Jonah Riley Weinbaum, and Kevin Javier Zvonarek",
        "title": "A Decadal Analysis of the Lead-Lag Effect in the NYSE",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As is widely known, the stock market is a complex system in which a multitude\nof factors influence the performance of individual stocks and the market as a\nwhole. One method for comprehending -- and potentially predicting -- stock\nmarket behavior is through network analysis, which can offer insights into the\nrelationships between stocks and the overall market structure. In this paper,\nwe seek to address the question: Can network analysis of the stock market,\nspecifically in observation of the lead-lag effect, provide valuable insights\nfor investors and market analysts?\n"
    },
    {
        "paper_id": 2312.10388,
        "authors": "Yijun Li, Cheuk Hang Leung, Xiangqian Sun, Chaoqun Wang, Yiyan Huang,\n  Xing Yan, Qi Wu, Dongdong Wang, Zhixiang Huang",
        "title": "The Causal Impact of Credit Lines on Spending Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consumer credit services offered by e-commerce platforms provide customers\nwith convenient loan access during shopping and have the potential to stimulate\nsales. To understand the causal impact of credit lines on spending, previous\nstudies have employed causal estimators, based on direct regression (DR),\ninverse propensity weighting (IPW), and double machine learning (DML) to\nestimate the treatment effect. However, these estimators do not consider the\nnotion that an individual's spending can be understood and represented as a\ndistribution, which captures the range and pattern of amounts spent across\ndifferent orders. By disregarding the outcome as a distribution, valuable\ninsights embedded within the outcome distribution might be overlooked. This\npaper develops a distribution-valued estimator framework that extends existing\nreal-valued DR-, IPW-, and DML-based estimators to distribution-valued\nestimators within Rubin's causal framework. We establish their consistency and\napply them to a real dataset from a large e-commerce platform. Our findings\nreveal that credit lines positively influence spending across all quantiles;\nhowever, as credit lines increase, consumers allocate more to luxuries (higher\nquantiles) than necessities (lower quantiles).\n"
    },
    {
        "paper_id": 2312.10405,
        "authors": "Hao Wang",
        "title": "The Fallacy of Borda Count Method -- Why it is Useless with Group\n  Intelligence and Shouldn't be Used with Big Data including Banking Customer\n  Services",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1051/shsconf/202317904008",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Borda Count Method is an important theory in the field of voting theory. The\nbasic idea and implementation methodology behind the approach is simple and\nstraight forward. Borda Count Method has been used in sports award evaluations\nand many other scenarios, and therefore is an important aspect of our society.\nAn often ignored ground truth is that online cultural rating platforms such as\nDouban.com and Goodreads.com often adopt integer rating values for large scale\npublic audience, and therefore leading to Poisson/Pareto behavior. In this\npaper, we rely on the theory developed by Wang from 2021 to 2023 to demonstrate\nthat online cultural rating platform rating data often evolve into\nPoisson/Pareto behavior, and individualistic voting preferences are predictable\nwithout any data input, so Borda Count Method (or, Range Voting Method) has\nintrinsic fallacy and should not be used as a voting theory method.\n"
    },
    {
        "paper_id": 2312.10476,
        "authors": "Pierre Pelletier and Kevin Wirtz",
        "title": "Sails and Anchors: The Complementarity of Exploratory and Exploitative\n  Scientists in Knowledge Creation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the relationship between scientists' cognitive\nprofile and their ability to generate innovative ideas and gain scientific\nrecognition. We propose a novel author-level metric based on the semantic\nrepresentation of researchers' past publications to measure cognitive diversity\nboth at individual and team levels. Using PubMed Knowledge Graph (PKG), we\nanalyze the impact of cognitive diversity on novelty, as measured by\ncombinatorial novelty indicators and peer labels on Faculty Opinion. We\nassessed scientific impact through citations and disruption indicators. We show\nthat the presence of exploratory individuals (i.e., cognitively diverse) is\nbeneficial in generating distant knowledge combinations, but only when balanced\nby a significant proportion of exploitative individuals (i.e., cognitively\nspecialized). Furthermore, teams with a high proportion of exploitative\nprofiles tend to consolidate science, whereas those with a significant share of\nboth profiles tend to disrupt it. Cognitive diversity between team members\nappears to be always beneficial to combining more distant knowledge. However,\nto maximize the relevance of these distant combinations of knowledge,\nmaintaining a limited number of exploratory individuals is essential, as\nexploitative individuals must question and debate their novel perspectives.\nThese specialized individuals are the most qualified to extract the full\npotential of novel ideas and integrate them within the existing scientific\nparadigm.\n"
    },
    {
        "paper_id": 2312.10739,
        "authors": "Francesco Cesarone, Manuel Luis Martino, Federica Ricca, Andrea\n  Scozzari",
        "title": "Managing ESG Ratings Disagreement in Sustainable Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sustainable Investing identifies the approach of investors whose aim is\ntwofold: on the one hand, they want to achieve the best compromise between\nportfolio risk and return, but they also want to take into account the\nsustainability of their investment, assessed through some Environmental,\nSocial, and Governance (ESG) criteria. The inclusion of sustainable goals in\nthe portfolio selection process may have an actual impact on financial\nportfolio performance. ESG indices provided by the rating agencies are\ngenerally considered good proxies for the performance in sustainability of an\ninvestment, as well as, appropriate measures for Socially Responsible\nInvestments (SRI) in the market. In this framework of analysis, the lack of\nalignment between ratings provided by different agencies is a crucial issue\nthat inevitably undermines the robustness and reliability of these evaluation\nmeasures. In fact, the ESG rating disagreement may produce conflicting\ninformation, implying a difficulty for the investor in the portfolio ESG\nevaluation. This may cause underestimation or overestimation of the market\nopportunities for a sustainable investment. In this paper, we deal with a\nmulti-criteria portfolio selection problem taking into account risk, return,\nand ESG criteria. For the ESG evaluation of the securities in the market, we\nconsider more than one agency and propose a new approach to overcome the\nproblem related to the disagreement between the ESG ratings by different\nagencies. We propose a nonlinear optimization model for our three-criteria\nportfolio selection problem. We show that it can be reformulated as an\nequivalent convex quadratic program by exploiting a technique known in the\nliterature as the k-sum optimization strategy. An extensive empirical analysis\nof the performance of this model is provided on real-world financial data sets.\n"
    },
    {
        "paper_id": 2312.10749,
        "authors": "Francesco Cesarone, Massimiliano Corradini, Lorenzo Lampariello,\n  Jessica Riccioni",
        "title": "A new behavioral model for portfolio selection using the\n  Half-Full/Half-Empty approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We focus on a behavioral model, that has been recently proposed in the\nliterature, whose rational can be traced back to the Half-Full/Half-Empty glass\nmetaphor. More precisely, we generalize the Half-Full/Half-Empty approach to\nthe context of positive and negative lotteries and give financial and\nbehavioral interpretations of the Half-Full/Half-Empty parameters. We develop a\nportfolio selection model based on the Half-Full/Half-Empty strategy, resulting\nin a nonconvex optimization problem, which, nonetheless, is proven to be\nequivalent to an alternative Mixed-Integer Linear Programming formulation. By\nmeans of the ensuing empirical analysis, based on three real-world datasets,\nthe Half-Full/Half-Empty model is shown to be very versatile by appropriately\nvarying its parameters, and to provide portfolios displaying promising\nperformances in terms of risk and profitability, compared with Prospect Theory,\nrisk minimization approaches and Equally-Weighted portfolios.\n"
    },
    {
        "paper_id": 2312.10883,
        "authors": "Masaaki Fukasawa, Takashi Sato, Jun Sekine",
        "title": "Backward stochastic difference equations on lattices with application to\n  market equilibrium analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study backward stochastic difference equations (BS{\\Delta}E) driven by a\nd-dimensional stochastic process on a lattice whose increments have only d + 1\npossible values that generates the lattice. Regarding the driving process as a\nd dimensional asset price process, we give applications to an optimal\ninvestment problem and a market equilibrium analysis, where utility functionals\nare defined through BS{\\Delta}E.\n"
    },
    {
        "paper_id": 2312.1101,
        "authors": "Fangzhi Wang and Hua Liao and Richard S.J. Tol and Changjing Ji",
        "title": "Endogenous preference for non-market goods in carbon abatement decision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Carbon abatement decisions are usually based on the implausible assumption of\nconstant social preference. This paper focuses on a specific case of market and\nnon-market goods, and investigates the optimal climate policy when social\npreference for them is also changed by climate policy in the DICE model. The\nrelative price of non-market goods grows over time due to increases in both\nrelative scarcity and appreciation of it. Therefore, climbing relative price\nbrings upward the social cost of carbon denominated in terms of market goods.\nBecause abatement decision affects the valuation of non-market goods in the\nutility function, unlike previous climate-economy models, we solve the model\niteratively by taking the obtained abatement rates from the last run as inputs\nin the current run. The results in baseline calibration advocate a more\nstringent climate policy, where endogenous social preference to climate policy\nraises the social cost of carbon further by roughly 12%-18% this century.\nMoreover, neglecting changing social preference leads to an underestimate of\nnon-market goods damages by 15%. Our results support that climate policy is\nself-reinforced if it favors more expensive consumption type.\n"
    },
    {
        "paper_id": 2312.11132,
        "authors": "Adil Rengim Cetingoz, Olivier Gu\\'eant",
        "title": "Asset and Factor Risk Budgeting: A Balanced Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio optimization methods have evolved significantly since Markowitz\nintroduced the mean-variance framework in 1952. While the theoretical appeal of\nthis approach is undeniable, its practical implementation poses important\nchallenges, primarily revolving around the intricate task of estimating\nexpected returns. As a result, practitioners and scholars have explored\nalternative methods that prioritize risk management and diversification. One\nsuch approach is Risk Budgeting, where portfolio risk is allocated among assets\naccording to predefined risk budgets. The effectiveness of Risk Budgeting in\nachieving true diversification can, however, be questioned, given that asset\nreturns are often influenced by a small number of risk factors. From this\nperspective, one question arises: is it possible to allocate risk at the factor\nlevel using the Risk Budgeting approach? First, we introduce a comprehensive\nframework to address this question by introducing risk measures directly\nassociated with risk factor exposures and demonstrating the desirable\nmathematical properties of these risk measures, making them suitable for\noptimization. Then, we propose a novel framework to find portfolios that\neffectively balance the risk contributions from both assets and factors.\nLeveraging standard stochastic algorithms, our framework enables the use of a\nwide range of risk measures to construct diversified portfolios.\n"
    },
    {
        "paper_id": 2312.11481,
        "authors": "Christian M{\\o}ller Dahl, Nadja van 't Hoff, Giovanni Mellace, Sinne\n  Smed",
        "title": "Nudging Nutrition: Lessons from the Danish \"Fat Tax\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In October 2011, Denmark introduced the world's first and, to date, only tax\ntargeting saturated fat. However, this tax was subsequently abolished in\nJanuary 2013. Leveraging exogenous variation from untaxed Northern-German\nconsumers, we employ a difference-in-differences approach to estimate the\ncausal effects of both the implementation and repeal of the tax on consumption\nand expenditure behavior across eight product categories targeted by the tax.\nOur findings reveal significant heterogeneity in the tax's impact across these\nproducts. During the taxed period, there was a notable decline in consumption\nof bacon, liver sausage, salami, and cheese, particularly among low-income\nhouseholds. In contrast, expenditure on butter, cream, margarine, and sour\ncream increased as prices rose. Interestingly, we do not observe any difference\nin expenditure increases between high and low-income households, suggesting\nthat the latter were disproportionately affected by the tax. After the repeal\nof the tax, we do not observe any significant decline in consumption. On the\ncontrary, there was an overall increase in consumption for certain products,\nprompting concerns about unintended consequences resulting from the brief\nimplementation of the tax.\n"
    },
    {
        "paper_id": 2312.11484,
        "authors": "Ningyi Li",
        "title": "Effects of Daily Exercise Time on the Academic Performance of Students:\n  An Empirical Analysis Based on CEPS Data",
        "comments": "the content is incomplete",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the effects of daily exercise time on the academic\nperformance of junior high school students in China, with an attempt to figure\nout the most appropriate daily exercise time for students from the perspective\nof improving students' test scores. By dividing the daily exercise time into\nfive sections to construct a categorical variable in a linear regression model\nas well as using another model to draw intuitive figures, I find that spending\nboth too little and too much time on physical activity every day would have\nadverse impacts on students' academic performance, with differences existing in\nthe impacts by gender, grade, city scale, and location type of the school. The\nfindings of this paper carry implications for research, school health and\neducation policy and physical and general education practice. The results also\nprovide recommendations for students, parents and teachers.\n"
    },
    {
        "paper_id": 2312.11496,
        "authors": "Nicholas I Fisher and Alan J Lee",
        "title": "A Hedonic Index for Collectables Arising from Modelling Diamond Prices",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This article describes a case study concerned with modelling the price of\nwholesale diamonds, as part of a project to develop an online diamond auction\nplatform. The work was extended to exploring how to develop an index that could\nbe used to track market trends of wholesale diamond prices. The approach we\nused is readily generalised to defining market indices for so-called\nCollectables, and can provide the basis for construction of derivatives. With\nthe burgeoning interest in new markets of collectables such as those generated\nby the concept of a Non-Fungible Token, it is reasonable to suppose that there\nwill be concomitant increasing interest in developing derivatives for these\nmarkets.\n"
    },
    {
        "paper_id": 2312.1153,
        "authors": "Ana Fern\\'andez Vilas, Rebeca P. D\\'iaz Redondo, Keeley Crockett,\n  Majdi Owda, Lewis Evans",
        "title": "Twitter Permeability to financial events: an experiment towards a model\n  for sensing irregularities",
        "comments": null,
        "journal-ref": "Multimed Tools Appl 78, 2019",
        "doi": "10.1007/s11042-018-6388-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a general consensus of the good sensing and novelty characteristics\nof Twitter as an information media for the complex financial market. This paper\ninvestigates the permeability of Twittersphere, the total universe of Twitter\nusers and their habits, towards relevant events in the financial market.\nAnalysis shows that a general purpose social media is permeable to\nfinancial-specific events and establishes Twitter as a relevant feeder for\ntaking decisions regarding the financial market and event fraudulent activities\nin that market. However, the provenance of contributions, their different\nlevels of credibility and quality and even the purpose or intention behind them\nshould to be considered and carefully contemplated if Twitter is used as a\nsingle source for decision taking. With the overall aim of this research, to\ndeploy an architecture for real-time monitoring of irregularities in the\nfinancial market, this paper conducts a series of experiments on the level of\npermeability and the permeable features of Twitter in the event of one of these\nirregularities. To be precise, Twitter data is collected concerning an event\ncomprising of a specific financial action on the 27th January 2017:{~ }the\nannouncement about the merge of two companies Tesco PLC and Booker Group PLC,\nlisted in the main market of the London Stock Exchange (LSE), to create the\nUK's Leading Food Business. The experiment attempts to answer five key research\nquestions which aim to characterize the features of Twitter permeability to the\nfinancial market. The experimental results confirm that a far-impacting\nfinancial event, such as the merger considered, caused apparent disturbances in\nall the features considered, that is, information volume, content and sentiment\nas well as geographical provenance. Analysis shows that despite, Twitter not\nbeing a specific financial forum, it is permeable to financial events.\n"
    },
    {
        "paper_id": 2312.11531,
        "authors": "Ana Fern\\'andez Vilas and Rebeca D\\'iaz Redondo and Ant\\'on Lorenzo\n  Garc\\'ia",
        "title": "The irruption of cryptocurrencies into Twitter cashtags: a classifying\n  solution",
        "comments": null,
        "journal-ref": "EEE Access, vol. 8, 2020",
        "doi": "10.1109/ACCESS.2020.2973735",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a consensus about the good sensing characteristics of Twitter to\nmine and uncover knowledge in financial markets, being considered a relevant\nfeeder for taking decisions about buying or holding stock shares and even for\ndetecting stock manipulation. Although Twitter hashtags allow to aggregate\ntopic-related content, a specific mechanism for financial information also\nexists: Cashtag. However, the irruption of cryptocurrencies has resulted in a\nsignificant degradation on the cashtag-based aggregation of posts.\nUnfortunately, Twitter' users may use homonym tickers to refer to\ncryptocurrencies and to companies in stock markets, which means that filtering\nby cashtag may result on both posts referring to stock companies and\ncryptocurrencies. This research proposes automated classifiers to distinguish\nconflicting cashtags and, so, their container tweets by analyzing the\ndistinctive features of tweets referring to stock companies and\ncryptocurrencies. As experiment, this paper analyses the interference between\ncryptocurrencies and company tickers in the London Stock Exchange (LSE),\nspecifically, companies in the main and alternative market indices FTSE-100 and\nAIM-100. Heuristic-based as well as supervised classifiers are proposed and\ntheir advantages and drawbacks, including their ability to self-adapt to\nTwitter usage changes, are discussed. The experiment confirms a significant\ndistortion in collected data when colliding or homonym cashtags exist, i.e.,\nthe same \\$ acronym to refer to company tickers and cryptocurrencies. According\nto our results, the distinctive features of posts including cryptocurrencies or\ncompany tickers support accurate classification of colliding tweets (homonym\ncashtags) and Independent Models, as the most detached classifiers from\ntraining data, have the potential to be trans-applicability (in different stock\nmarkets) while retaining performance.\n"
    },
    {
        "paper_id": 2312.11711,
        "authors": "Wadim Strielkowski, Larisa Gorina, Elena Korneeva, Olga Kovaleva",
        "title": "Energy-saving technologies and energy efficiency in the post-pandemic\n  world",
        "comments": "19 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the role of energy-saving technologies and energy\nefficiency in the post-COVID era. The pandemic meant major rethinking of the\nentrenched patterns in energy saving and efficiency. It also provided\nopportunities for reevaluating energy consumption for households and\nindustries. In addition, it highlighted the importance of employing digital\ntools and technologies in energy networks and smart grids (e.g. Internet of\nEnergy (IoE), peer-to-peer (P2P) prosumer networks, or the AI-powered\nautonomous power systems (APS)). In addition, the pandemic added novel legal\naspects to the energy efficiency and energy saving and enhanced inter-national\ncollaborations and partnerships. The paper highlights the importance of energy\nefficiency measures and examines various technologies that can contribute to a\nsustainable and resilient energy future. Using the bibliometric network\nanalysis of 12960 publications indexed in Web of Science databases, it\ndemonstrates the potential benefits and challenges associated with implementing\nenergy-saving technologies and autonomic power systems in a post-COVID world.\nOur findings emphasize the need for robust policies, technological\nadvancements, and public engagement to foster energy efficiency and mitigate\nthe environmental impacts of energy consumption.\n"
    },
    {
        "paper_id": 2312.11767,
        "authors": "Jessica K. Wallingford and William A. Masters",
        "title": "Least-cost diets to teach optimization and consumer behavior, with\n  applications to health equity, poverty measurement and international\n  development",
        "comments": "The associated Excel workbook and Word template for student exercises\n  is here:\n  https://sites.tufts.edu/foodecon/least-cost-diet-exercise-for-nutrient-adequacy",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The least-cost diet problem introduces students to optimization and linear\nprogramming, using the health consequences of food choice. We provide a\ngraphical example, Excel workbook and Word template using actual data on item\nprices, food composition and nutrient requirements for a brief exercise in\nwhich students guess at and then solve for nutrient adequacy at lowest cost,\nbefore comparing modeled diets to actual consumption which has varying degrees\nof nutrient adequacy. The graphical example is a 'three sisters' diet of corn,\nbeans and squash, and the full multidimensional model is compared to current\nfood consumption in Ethiopia. This updated Stigler diet shows how cost\nminimization relates to utility maximization, and links to ongoing research and\npolicy debates about the affordability of healthy diets worldwide.\n"
    },
    {
        "paper_id": 2312.11797,
        "authors": "Min Dai, Yuchao Dong, Yanwei Jia, and Xun Yu Zhou",
        "title": "Learning Merton's Strategies in an Incomplete Market: Recursive Entropy\n  Regularization and Biased Gaussian Exploration",
        "comments": "43 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study Merton's expected utility maximization problem in an incomplete\nmarket, characterized by a factor process in addition to the stock price\nprocess, where all the model primitives are unknown. We take the reinforcement\nlearning (RL) approach to learn optimal portfolio policies directly by\nexploring the unknown market, without attempting to estimate the model\nparameters. Based on the entropy-regularization framework for general\ncontinuous-time RL formulated in Wang et al. (2020), we propose a recursive\nweighting scheme on exploration that endogenously discounts the current\nexploration reward by the past accumulative amount of exploration. Such a\nrecursive regularization restores the optimality of Gaussian exploration.\nHowever, contrary to the existing results, the optimal Gaussian policy turns\nout to be biased in general, due to the interwinding needs for hedging and for\nexploration. We present an asymptotic analysis of the resulting errors to show\nhow the level of exploration affects the learned policies. Furthermore, we\nestablish a policy improvement theorem and design several RL algorithms to\nlearn Merton's optimal strategies. At last, we carry out both simulation and\nempirical studies with a stochastic volatility environment to demonstrate the\nefficiency and robustness of the RL algorithms in comparison to the\nconventional plug-in method.\n"
    },
    {
        "paper_id": 2312.11806,
        "authors": "Yuxin Hu",
        "title": "Managing Demographic Transitions: A Comprehensive Analysis of China's\n  Path to Economic Sustainability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article presents an analysis of China's economic evolution amidst\ndemographic changes from 1990 to 2050, offering valuable insights for academia\nand policymakers. It uniquely intertwines various economic theories with\nempirical data, examining the impact of an aging population, urbanization, and\nfamily dynamics on labor, demand, and productivity. The study's novelty lies in\nits integration of Classical, Neoclassical, and Endogenous Growth theories,\nalongside models like Barro and Sala-i-Martin, to contextualize China's\neconomic trajectory. It provides a forward-looking perspective, utilizing\neconometric methods to predict future trends, and suggests practical policy\nimplications. This comprehensive approach sheds light on managing demographic\ntransitions in a global context, making it a significant contribution to the\nfield of demographic economics.\n"
    },
    {
        "paper_id": 2312.11942,
        "authors": "Eugenia Gonzalez Ehlinger, Fabian Stephany",
        "title": "Skills or Degree? The Rise of Skill-Based Hiring for AI and Green Jobs",
        "comments": "38 pages, 13 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For emerging professions, such as jobs in the field of Artificial\nIntelligence (AI) or sustainability (green), labour supply does not meet\nindustry demand. In this scenario of labour shortages, our work aims to\nunderstand whether employers have started focusing on individual skills rather\nthan on formal qualifications in their recruiting. By analysing a large time\nseries dataset of around one million online job vacancies between 2019 and 2022\nfrom the UK and drawing on diverse literature on technological change and\nlabour market signalling, we provide evidence that employers have started\nso-called \"skill-based hiring\" for AI and green roles, as more flexible hiring\npractices allow them to increase the available talent pool. In our observation\nperiod the demand for AI roles grew twice as much as average labour demand. At\nthe same time, the mention of university education for AI roles declined by\n23%, while AI roles advertise five times as many skills as job postings on\naverage. Our regression analysis also shows that university degrees no longer\nshow an educational premium for AI roles, while for green positions the\neducational premium persists. In contrast, AI skills have a wage premium of\n16%, similar to having a PhD (17%). Our work recommends making use of\nalternative skill building formats such as apprenticeships, on-the-job\ntraining, MOOCs, vocational education and training, micro-certificates, and\nonline bootcamps to use human capital to its full potential and to tackle\ntalent shortages.\n"
    },
    {
        "paper_id": 2312.12139,
        "authors": "Francesca Biagini, Andrea Mazzon, Katharina Oberpriller",
        "title": "Multi-dimensional fractional Brownian motion in the G-setting",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a definition of a multi-dimensional fractional\nBrownian motion of Hurst index $H \\in (0, 1)$ under volatility uncertainty (in\nshort G-fBm). We study the properties of such a process and provide first\nresults about stochastic calculus with respect to a fractional G-Brownian\nmotion for a Hurst index $H >\\frac{1}{2}$ .\n"
    },
    {
        "paper_id": 2312.12305,
        "authors": "Richard J. Martin",
        "title": "Root-finding: from Newton to Halley and beyond",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We give a new improvement over Newton's method for root-finding, when the\nfunction in question is doubly differentiable. It generally exhibits faster and\nmore reliable convergence. It can be also be thought of as a correction to\nHalley's method, as this can exhibit undesirable behaviour.\n"
    },
    {
        "paper_id": 2312.12788,
        "authors": "Radhika Prosad Datta",
        "title": "Leveraging Sample Entropy for Enhanced Volatility Measurement and\n  Prediction in International Oil Price Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the application of Sample Entropy (SampEn) as a\nsophisticated tool for quantifying and predicting volatility in international\noil price returns. SampEn, known for its ability to capture underlying patterns\nand predict periods of heightened volatility, is compared with traditional\nmeasures like standard deviation. The study utilizes a comprehensive dataset\nspanning 27 years (1986-2023) and employs both time series regression and\nmachine learning methods. Results indicate SampEn's efficacy in predicting\ntraditional volatility measures, with machine learning algorithms outperforming\nstandard regression techniques during financial crises. The findings underscore\nSampEn's potential as a valuable tool for risk assessment and decision-making\nin the realm of oil price investments.\n"
    },
    {
        "paper_id": 2312.13057,
        "authors": "Alessandro Gnoatto and Silvia Lavagnini",
        "title": "Cross-Currency Heath-Jarrow-Morton Framework in the Multiple-Curve\n  Setting",
        "comments": "52 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a general HJM framework for forward contracts written on abstract\nmarket indices with arbitrary fixing and payment adjustments. We allow for\nindices on any asset class, featuring collateralization in arbitrary currency\ndenominations. The framework is pivotal for describing portfolios of interest\nrate products which are denominated in multiple currencies. The benchmark\ntransition has created significant discrepancies among the market conventions\nof different currency areas: our framework simultaneously covers\nforward-looking risky IBOR rates, such as EURIBOR, and backward-looking rates\nbased on overnight rates, such as SOFR. In view of this, we provide a thorough\nstudy of cross-currency markets in the presence of collateral, where the cash\nflows of the contract and the margin account can be denominated in arbitrary\ncombinations of currencies. We finally consider cross-currency swap contracts\nas an example of a contract simultaneously depending on all the risk factors\nthat we describe within our framework.\n"
    },
    {
        "paper_id": 2312.13195,
        "authors": "K.B. Gubbels, J.Y. Ypma, C.W. Oosterlee",
        "title": "Principal Component Copulas for Capital Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of copulas that we call Principal Component Copulas.\nThis class intends to combine the strong points of copula-based techniques with\nprincipal component-based models, which results in flexibility when modelling\ntail dependence along the most important directions in multivariate data. The\nproposed techniques have conceptual similarities and technical differences with\nthe increasingly popular class of factor copulas. Such copulas can generate\ncomplex dependence structures and also perform well in high dimensions. We show\nthat Principal Component Copulas give rise to practical and technical\nadvantages compared to other techniques. We perform a simulation study and\napply the copula to multivariate return data. The copula class offers the\npossibility to avoid the curse of dimensionality when estimating very large\ncopula models and it performs particularly well on aggregate measures of tail\nrisk, which is of importance for capital modeling.\n"
    },
    {
        "paper_id": 2312.13432,
        "authors": "Michael Callen and Miguel Fajardo-Steinh\\\"auser and Michael G. Findley\n  and Tarek Ghani",
        "title": "Can Digital Aid Deliver During Humanitarian Crises?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Can digital payments systems help reduce extreme hunger? Humanitarian needs\nare at their highest since 1945, aid budgets are falling behind, and hunger is\nconcentrating in fragile states where repression and aid diversion present\nmajor obstacles. In such contexts, partnering with governments is often neither\nfeasible nor desirable, making private digital platforms a potentially useful\nmeans of delivering assistance. We experimentally evaluated digital payments to\nextremely poor, female-headed households in Afghanistan, as part of a\npartnership between community, nonprofit, and private organizations. The\npayments led to substantial improvements in food security and mental\nwell-being. Despite beneficiaries' limited tech literacy, 99.75\\% used the\npayments, and stringent checks revealed no evidence of diversion. Before seeing\nour results, policymakers and experts are uncertain and skeptical about digital\naid, consistent with the lack of prior evidence on digital payments for\nhumanitarian response. Delivery costs are under 7 cents per dollar, which is 10\ncents per dollar less than the World Food Programme's global figure for\ncash-based transfers. These savings can help reduce hunger without additional\nresources, demonstrating how hybrid partnerships utilizing digital platform\ntechnologies can help address grand challenges in difficult contexts.\n"
    },
    {
        "paper_id": 2312.13448,
        "authors": "Christian P. Fries",
        "title": "Implied CO$_{\\textbf{2}}$-Price and Interest Rate of Carbon",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  By its nature, the so-called social cost of carbon (SCC(t)) will likely not\ncover the cost induced by climate change (damage cost and abatement cost) if it\nis used as a CO$_2$-price. It is a marginal price only. We define an implied\nCO$_2$-price that covers the climate change-induced costs. The price can be\ninterpreted as a \\textit{polluter pays principle}. A numerical analysis using a\nclassical DICE model reveals that the cost-implied CO$_2$ price is around 500\n\\$/tCO$_2$, while the corresponding price associated with the SCC is about 50\n\\$/tCO$_2$. In addition, we define the internal rate of return of carbon\nabatement and calculate it for the classical DICE model. This rate is much\nhigher than the model's discount rate, which may suggest the advantage of\nfinancing abatement by loans.\n"
    },
    {
        "paper_id": 2312.13515,
        "authors": "Marie-Chantale Pelletier, Claire Horner, Mathew Vickers, Aliya Gul,\n  Eren Turak and Christine Turner",
        "title": "Recognising natural capital on the balance sheet: options for water\n  utilities",
        "comments": "26 pages, 4 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Purpose: The aim of this study was to explore the feasibility of natural\ncapital accounting for the purpose of strengthening sustainability claims by\nreporting entities. The study linked riparian land improvement to ecosystem\nservices and tested options for incorporating natural capital into financial\naccounting practices, specifically on the balance sheet. Methodology: To test\nthe approach, the study used a public asset manager (a water utility) with\naccountabilities to protect the environment including maintaining and enhancing\nriparian land assets. Research activities included stakeholder engagement,\nphysical asset measurement, monetary valuation and financial recognition of\nnatural capital income and assets. Natural capital income was estimated by\nmodelling and valuing ecosystem services relating to stormwater filtration and\ncarbon storage. Findings: This research described how a water utility could\ndisclose changes in the natural capital assets they manage either through\nvoluntary disclosures, in notes to the financial statements or as balance sheet\nitems. We found that current accounting standards allowed the recognition of\nsome types of environmental income and assets where ecosystem services were\nassociated with cost savings. The proof-of-concept employed to estimate\nenvironmental income through ecosystem service modelling proved useful to\nstrengthen sustainability claims or report financial returns on natural capital\ninvestment. Originality/value: This study applied financial accounting\nprocesses and principles to a realistic public asset management scenario with\ndirect participation by the asset manager working together with academic\nresearchers and a sub-national government environment management agency.\nImportantly it established that natural assets could be included in financial\nstatements, proposing a new approach to measuring and reporting on natural\ncapital.\n"
    },
    {
        "paper_id": 2312.13564,
        "authors": "Wentian Zhang",
        "title": "The Effect of Antitrust Enforcement on Venture Capital Investments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the effect of antitrust enforcement on venture capital\n(VC) investments and VC-backed companies. To establish causality, I exploit the\nDOJ's decision to close several antitrust field offices in 2013, which reduced\nthe antitrust enforcement in areas near the closed offices. I find that the\nreduction in antitrust enforcement causes a significant decrease in VC\ninvestments in startups located in the affected areas. Furthermore, these\naffected VC-backed startups exhibit a reduced likelihood of successful exits\nand diminished innovation performance. These negative results are mainly driven\nby startups in concentrated industries, where incumbents tend to engage in\nanticompetitive behaviors more frequently. To mitigate the adverse effect,\nstartups should innovate more to differentiate their products. This paper sheds\nlight on the importance of local antitrust enforcement in fostering competition\nand innovation.\n"
    },
    {
        "paper_id": 2312.13719,
        "authors": "Ju-Hong Lee, Bayartsetseg Kalina, KwangTek Na",
        "title": "Market-Adaptive Ratio for Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Traditional risk-adjusted returns, such as the Treynor, Sharpe, Sortino, and\nInformation ratios, have been pivotal in portfolio asset allocation, focusing\non minimizing risk while maximizing profit. Nevertheless, these metrics often\nfail to account for the distinct characteristics of bull and bear markets,\nleading to sub-optimal investment decisions. This paper introduces a novel\napproach called the Market-adaptive Ratio, which was designed to adjust risk\npreferences dynamically in response to market conditions. By integrating the\n$\\rho$ parameter, which differentiates between bull and bear markets, this new\nratio enables a more adaptive portfolio management strategy. The $\\rho$\nparameter is derived from historical data and implemented within a\nreinforcement learning framework, allowing the method to learn and optimize\nportfolio allocations based on prevailing market trends. Empirical analysis\nshowed that the Market-adaptive Ratio outperformed the Sharpe Ratio by\nproviding more robust risk-adjusted returns tailored to the specific market\nenvironment. This advance enhances portfolio performance by aligning investment\nstrategies with the inherent dynamics of bull and bear markets, optimizing risk\nand return outcomes.\n"
    },
    {
        "paper_id": 2312.13774,
        "authors": "Giulio Caldarelli",
        "title": "Investigating Assumptions and Proposals for Blockchain Integration in\n  the Circular Economy. A Delphi Study",
        "comments": "The name of experts involved are not mentioned in this version, but\n  will be public in the journal one as to avoid unwanted representation of\n  their thoughts and ideas",
        "journal-ref": null,
        "doi": "10.1016/j.jclepro.2024.142781",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given the rising interest in the circular economy and blockchain hype,\nnumerous integrations were proposed. However, studies on the practical\nfeasibility were scarce, and the assumptions of blockchain potential in the\ncircular economy were rarely questioned. With the help of eleven of the most\nprominent blockchain experts, the present study critically analyzed technology\nintegration in many areas of the circular economy to forecast their possible\noutcomes. Delphi's technique is leveraged to reach a consensus among experts'\nvisions and opinions. Results support the view that some circular economy\nintegrations are unlikely to succeed, while others if specific conditions are\nmet, may prove to be successful in the long run.\n"
    },
    {
        "paper_id": 2312.13884,
        "authors": "Gregor Svindland and Alexander Vo{\\ss}",
        "title": "Measures of Resilience to Cyber Contagion -- An Axiomatic Approach for\n  Complex Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel class of risk measures designed for the management of\nsystemic risk in networks. In contrast to prevailing approaches, these risk\nmeasures target the topological configuration of the network in order to\nmitigate the propagation risk of contagious threats. While our discussion\nprimarily revolves around the management of systemic cyber risks in digital\nnetworks, we concurrently draw parallels to risk management of other complex\nsystems where analogous approaches may be adequate.\n"
    },
    {
        "paper_id": 2312.13896,
        "authors": "Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Li\\^en Doan and\n  Fabrice Daniel",
        "title": "Comparative Evaluation of Anomaly Detection Methods for Fraud Detection\n  in Online Credit Card Payments",
        "comments": "Accepted at ICICT 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the application of anomaly detection (AD) methods in\nimbalanced learning tasks, focusing on fraud detection using real online credit\ncard payment data. We assess the performance of several recent AD methods and\ncompare their effectiveness against standard supervised learning methods.\nOffering evidence of distribution shift within our dataset, we analyze its\nimpact on the tested models' performances. Our findings reveal that LightGBM\nexhibits significantly superior performance across all evaluated metrics but\nsuffers more from distribution shifts than AD methods. Furthermore, our\ninvestigation reveals that LightGBM also captures the majority of frauds\ndetected by AD methods. This observation challenges the potential benefits of\nensemble methods to combine supervised, and AD approaches to enhance\nperformance. In summary, this research provides practical insights into the\nutility of these techniques in real-world scenarios, showing LightGBM's\nsuperiority in fraud detection while highlighting challenges related to\ndistribution shifts.\n"
    },
    {
        "paper_id": 2312.14044,
        "authors": "Roberto Daluiso, Marco Pinciroli, Michele Trapletti and Edoardo\n  Vittori",
        "title": "CVA Hedging by Risk-Averse Stochastic-Horizon Reinforcement Learning",
        "comments": "35 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies the dynamic risk management of the risk-neutral value of\nthe potential credit losses on a portfolio of derivatives. Sensitivities-based\nhedging of such liability is sub-optimal because of bid-ask costs, pricing\nmodels which cannot be completely realistic, and a discontinuity at default\ntime. We leverage recent advances on risk-averse Reinforcement Learning\ndeveloped specifically for option hedging with an ad hoc practice-aligned\nobjective function aware of pathwise volatility, generalizing them to\nstochastic horizons. We formalize accurately the evolution of the hedger's\nportfolio stressing such aspects. We showcase the efficacy of our approach by a\nnumerical study for a portfolio composed of a single FX forward contract.\n"
    },
    {
        "paper_id": 2312.1412,
        "authors": "Eiji Yamamura",
        "title": "Name order and the top elite: Long-term effects of a hidden cur-riculum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Japanese primary and secondary schools, an alphabetical name list is used\nin various situations. Generally, students are called on by the teacher during\nclass and in the cer-emony if their family name is early on the list.\nTherefore, students whose surname ini-tials are earlier in the Japanese\nalphabet acquire more experience. Surname advantages are considered to have a\nlong-term positive effect on life in adulthood. This study ex-amines the\nsurname effect. The data set is constructed by gathering lists of\nrepresentative figures from various fields. Based on the list, we calculate the\nproportion of surname groups according to Japanese alphabetical column lists.\nThe major findings are as follows: (1) people whose surnames are in the\nA-column (the first column among 10 Japanese name col-umns) are 20% more likely\nto appear among the ruling elite but are less likely to ap-pear in\nentertainment and sports lists. (2) This tendency is rarely observed in the\nUni-versity of Tokyo entrance examination pass rate. Consequently, the A-column\nsur-names are advantageous in helping students succeed as part of the elite\nafter graduating from universities but not when gaining entry into\nuniversities. The surname helps form non-cognitive skills that help students\nbecome part of the ruling elite instead of specif-ic cognitive skills that help\nstudents enter elite universities. Keywords: Surname, Education, Name order,\nHidden curriculum, Cognitive skill, Non-cognitive skill, Elite, Vice Minister,\nAcademic, Prestigious university, Enter-tainment, Sports.\n"
    },
    {
        "paper_id": 2312.14203,
        "authors": "Zhongyang Guo, Guanran Jiang, Zhongdan Zhang, Peng Li, Zhefeng Wang,\n  and Yinchun Wang",
        "title": "Shai: A large language model for asset management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces \"Shai\" a 10B level large language model specifically\ndesigned for the asset management industry, built upon an open-source\nfoundational model. With continuous pre-training and fine-tuning using a\ntargeted corpus, Shai demonstrates enhanced performance in tasks relevant to\nits domain, outperforming baseline models. Our research includes the\ndevelopment of an innovative evaluation framework, which integrates\nprofessional qualification exams, tailored tasks, open-ended question\nanswering, and safety assessments, to comprehensively assess Shai's\ncapabilities. Furthermore, we discuss the challenges and implications of\nutilizing large language models like GPT-4 for performance assessment in asset\nmanagement, suggesting a combination of automated evaluation and human\njudgment. Shai's development, showcasing the potential and versatility of\n10B-level large language models in the financial sector with significant\nperformance and modest computational requirements, hopes to provide practical\ninsights and methodologies to assist industry peers in their similar endeavors.\n"
    },
    {
        "paper_id": 2312.14289,
        "authors": "Matt Clancy",
        "title": "The Returns to Science in the Presence of Technological Risk",
        "comments": "119 pages. Revision changes two model parameters. The lag between\n  science and health/income benefits is increased, to account for international\n  diffusion. The lag between science and technology-related mortality risks is\n  reduced, to more closely match forecasts. Quantitative estimates of returns\n  to science remain strongly positive, but much reduced compared to previous\n  version",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scientific and technological progress has historically been very beneficial\nto humanity but this does not always need to be true. Going forward, science\nmay enable bad actors to cause genetically engineered pandemics that are more\nfrequent and deadly than prior pandemics. I develop a quantitative economic\nmodel to assess the social returns to science, taking into account benefits to\nhealth and income, and forecast damages from new biological capabilities\nenabled by science. I set parameters for this model based on historical trends\nand forecasts from a large forecasting tournament of domain experts and\nsuperforecasters, which included forecasts about genetically engineered\npandemic events. The results depend on the forecast likelihood that new\nscientific capabilities might lead to the end of our advanced civilization -\nthere is substantial disagreement about this probability from participants in\nthe forecasting tournament I use. If I set aside this remote possibility, I\nfind the expected future social returns to science are strongly positive.\nOtherwise, the desirability of accelerating science depends on the value placed\non the long-run future, in addition to which set of (quite different) forecasts\nof extinction risk are preferred. I also explore the sensitivity of these\nconclusions to a range of alternative assumptions.\n"
    },
    {
        "paper_id": 2312.14324,
        "authors": "Oliver Lunding Sandqvist",
        "title": "A multistate approach to disability insurance reserving with information\n  delays",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Disability insurance claims are often affected by lengthy reporting delays\nand adjudication processes. The classic multistate life insurance modeling\nframework is ill-suited to handle such information delays since the cash flow\nand available information can no longer be based on the biometric multistate\nprocess determining the contractual payments. We propose a new individual\nreserving model for disability insurance schemes which describes the claim\nevolution in real-time. Under suitable independence assumptions between the\navailable information and the underlying biometric multistate process, we show\nthat these new reserves may be calculated as natural modifications of the\nclassic reserves. We propose suitable parametric estimators for the model\nconstituents and a real data application shows the practical relevance of our\nconcepts and results.\n"
    },
    {
        "paper_id": 2312.14355,
        "authors": "Benjamin Avanzi and Lewis de Felice",
        "title": "Optimal Strategies for the Decumulation of Retirement Savings under\n  Differing Appetites for Liquidity and Investment Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A retiree's appetite for risk is a common input into the lifetime utility\nmodels that are traditionally used to find optimal strategies for the\ndecumulation of retirement savings.\n  In this work, we consider a retiree with potentially differing appetites for\nthe key financial risks of decumulation: liquidity risk and investment risk. We\nset out to determine whether these differing risk appetites have a significant\nimpact on the retiree's optimal choice of decumulation strategy. To do so, we\ndesign and implement a framework which selects the optimal decumulation\nstrategy from a general set of admissible strategies in line with a retiree's\ngoals, and under differing appetites for the key risks of decumulation.\n  Overall, we find significant evidence to suggest that a retiree's differing\nappetites for different decumulation risks will impact their optimal choice of\nstrategy at retirement. Through an illustrative example calibrated to the\nAustralian context, we find results which are consistent with actual behaviours\nin this jurisdiction (in particular, a shallow market for annuities), which\nlends support to our framework and may provide some new insight into the\nso-called annuity puzzle.\n"
    },
    {
        "paper_id": 2312.14437,
        "authors": "Zongxia Liang and Keyu Zhang",
        "title": "Time-inconsistent mean field and n-agent games under relative\n  performance criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a time-inconsistent portfolio optimization problem for\ncompetitive agents with CARA utilities and non-exponential discounting. The\nutility of each agent depends on her own wealth and consumption as well as the\nrelative wealth and consumption to her competitors. Due to the presence of a\nnon-exponential discount factor, each agent's optimal strategy becomes\ntime-inconsistent. In order to resolve time-inconsistency, each agent makes a\ndecision in a sophisticated way, choosing open-loop equilibrium strategy in\nresponse to the strategies of all the other agents. We construct explicit\nsolutions for the $n$-agent games and the corresponding mean field games (MFGs)\nwhere the limit of former yields the latter. This solution is unique in a\nspecial class of equilibria.\n"
    },
    {
        "paper_id": 2312.14515,
        "authors": "Mathieu Gomes (CleRMa, UCA, IAE - UCA), Sylvain Marsat (CleRMa, UCA,\n  IAE - UCA), Jonathan Peillex, Guillaume Pijourlet (CleRMa, UCA, IAE - UCA)",
        "title": "Does religiosity influence corporate greenwashing behavior?",
        "comments": null,
        "journal-ref": "Journal of Cleaner Production, 2025, 434, pp.140151",
        "doi": "10.1016/j.jclepro.2023.140151",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the influence of religious social norms on corporate greenwashing\nbehavior. Specifically, we focus on a specific form of greenwashing: selective\ndisclosure. Using a large sample of US firms between 2005 and 2019, we show\nthat firms located in counties where religious adherence is high are less\nlikely to engage in greenwashing. We also find that a stronger religious\nadherence within the county in which a company is located reduces the magnitude\nof greenwashing, when observed. We further analyze the mechanism underlying\nthis relationship and show that religious adherence impacts greenwashing\nbehaviors through the channel of risk aversion. A comprehensive set of\nrobustness tests aimed at addressing potential endogeneity concerns confirms\nthat religion is a relevant driver of corporate greenwashing behavior.\n"
    },
    {
        "paper_id": 2312.14565,
        "authors": "Johann Laux, Fabian Stephany, Alice Liefgreen",
        "title": "The Economics of Human Oversight: How Norms and Incentives Affect Costs\n  and Performance of AI Workers",
        "comments": "35 pages, 8 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The global surge in AI applications is transforming industries, leading to\ndisplacement and complementation of existing jobs, while also giving rise to\nnew employment opportunities. Human oversight of AI is an emerging task in\nwhich human workers interact with an AI model to improve its performance,\nsafety, and compliance with normative principles. Data annotation, encompassing\nthe labelling of images or annotating of texts, serves as a critical human\noversight process, as the quality of a dataset directly influences the quality\nof AI models trained on it. Therefore, the efficiency of human oversight work\nstands as an important competitive advantage for AI developers. This paper\ndelves into the foundational economics of human oversight, with a specific\nfocus on the impact of norm design and monetary incentives on data quality and\ncosts. An experimental study involving 307 data annotators examines six groups\nwith varying task instructions (norms) and monetary incentives. Results reveal\nthat annotators provided with clear rules exhibit higher accuracy rates,\noutperforming those with vague standards by 14%. Similarly, annotators\nreceiving an additional monetary incentive perform significantly better, with\nthe highest accuracy rate recorded in the group working with both clear rules\nand incentives (87.5% accuracy). However, both groups require more time to\ncomplete tasks, with a 31% increase in average task completion time compared to\nthose working with standards and no incentives. These empirical findings\nunderscore the trade-off between data quality and efficiency in data curation,\nshedding light on the nuanced impact of norm design and incentives on the\neconomics of AI development. The paper contributes experimental insights to\ndiscussions on the economical, ethical, and legal considerations of AI\ntechnologies.\n"
    },
    {
        "paper_id": 2312.14765,
        "authors": "Patrick Kurth, Max Nendel, Jan Streicher",
        "title": "A hypothesis test for the long-term calibration in rating systems with\n  overlapping time windows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a statistical test that can be used to verify supervisory\nrequirements concerning overlapping time windows for the long-term calibration\nin rating systems. In a first step, we show that the long-run default rate is\napproximately normally distributed with respect to random effects in default\nrealization. We then perform a detailed analysis of the correlation effects\ncaused by the overlapping time windows and solve the problem of an unknown\ndistribution of default probabilities for the long-run default rate. In this\ncontext, we present several methods for a conservative calibration test that\ncan deal with the unknown variance in the test statistic. We present a test for\nindividual rating grades, and then pass to the portfolio level by suitably\nadapting the test statistic. We conclude with comparative statics analysing the\neffect of persisting customers and the number of customers per reference date.\n"
    },
    {
        "paper_id": 2312.14853,
        "authors": "Zahra Ghasemi Kooktapeh, Hakimeh Dustmohammadloo, Hooman Mehrdoost,\n  Farivar Fatehi",
        "title": "In the Line of Fire: A Systematic Review and Meta-Analysis of Job\n  Burnout Among Nurses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a systematic review and meta-analysis, this study investigates the\nimpact of the COVID-19 pandemic on job burnout among nurses. We review\nhealthcare articles following the PRISMA 2020 guidelines and identify the main\naspects and factors of burnout among nurses during the pandemic. Using the\nMaslach Burnout questionnaire, we searched PubMed, ScienceDirect, and Google\nScholar, three open-access databases, for relevant sources measuring emotional\nburnout, personal failure, and nurse depersonalization. Two reviewers extract\nand screen data from the sources and evaluate the risk of bias. The analysis\nreveals that 2.75% of nurses experienced job burnout during the pandemic, with\na 95% confidence interval and rates varying from 1.87% to 7.75%. These findings\nemphasize the need for interventions to address the pandemic's effect on job\nburnout among nurses and enhance their well-being and healthcare quality. We\nrecommend considering individual, organizational, and contextual factors\ninfluencing healthcare workers' burnout. Future research should focus on\nidentifying effective interventions to lower burnout in nurses and other\nhealthcare professionals during pandemics and high-stress situations.\n"
    },
    {
        "paper_id": 2312.14903,
        "authors": "Aaron Wheeler, Jeffrey D. Varner",
        "title": "Scalable Agent-Based Modeling for Complex Financial Market Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we developed a computational framework for simulating\nlarge-scale agent-based financial markets. Our platform supports trading\nmultiple simultaneous assets and leverages distributed computing to scale the\nnumber and complexity of simulated agents. Heterogeneous agents make decisions\nin parallel, and their orders are processed through a realistic, continuous\ndouble auction matching engine. We present a baseline model implementation and\nshow that it captures several known statistical properties of real financial\nmarkets (i.e., stylized facts). Further, we demonstrate these results without\nfitting models to historical financial data. Thus, this framework could be used\nfor direct applications such as human-in-the-loop machine learning or to\nexplore theoretically exciting questions about market microstructure's role in\nforming the statistical regularities of real markets. To the best of our\nknowledge, this study is the first to implement multiple assets, parallel agent\ndecision-making, a continuous double auction mechanism, and intelligent agent\ntypes in a scalable real-time environment.\n"
    },
    {
        "paper_id": 2312.15119,
        "authors": "Andreas S{\\o}jmark, Fabrice Wunderlich",
        "title": "Functional CLTs for subordinated L\\'evy models in physics, finance, and\n  econometrics",
        "comments": "13 pages. arXiv admin note: text overlap with arXiv:2309.12197",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple unifying treatment of a broad class of applications from\nstatistical mechanics, econometrics, mathematical finance, and insurance\nmathematics, where (possibly subordinated) L\\'evy noise arises as a scaling\nlimit of some form of continuous-time random walk (CTRW). For each application,\nit is natural to rely on weak convergence results for stochastic integrals on\nSkorokhod space in Skorokhod's J1 or M1 topologies. As compared to earlier and\nentirely separate works, we are able to give a more streamlined account while\nalso allowing for greater generality and providing important new insights. For\neach application, we first elucidate how the fundamental conclusions for J1\nconvergent CTRWs emerge as special cases of the same general principles, and we\nthen illustrate how the specific settings give rise to different results for\nstrictly M1 convergent CTRWs.\n"
    },
    {
        "paper_id": 2312.15173,
        "authors": "Zongxia Liang, Jianming Xia, Keyu Zhang",
        "title": "Equilibrium stochastic control with implicitly defined objective\n  functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a class of stochastic control problems with implicitly\ndefined objective functions, which are the sources of time-inconsistency. We\nstudy the closed-loop equilibrium solutions in a general controlled diffusion\nframework. First, we provide a sufficient and necessary condition for a\nstrategy to be an equilibrium. Then, we apply the result to discuss two\nproblems of dynamic portfolio selection for a class of betweenness preferences,\nallowing for closed convex constraints on portfolio weights and borrowing cost,\nrespectively. The equilibrium portfolio strategies are explicitly characterized\nin terms of the solutions of some first-order ordinary differential equations\nfor the case of deterministic market coefficients.\n"
    },
    {
        "paper_id": 2312.15198,
        "authors": "Yan Leng, Yuan Yuan",
        "title": "Do LLM Agents Exhibit Social Behavior?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The advances of Large Language Models (LLMs) are expanding their utility in\nboth academic research and practical applications. Recent social science\nresearch has explored the use of these ``black-box'' LLM agents for simulating\ncomplex social systems and potentially substituting human subjects in\nexperiments. Our study delves into this emerging domain, investigating the\nextent to which LLMs exhibit key social interaction principles, such as social\nlearning, social preference, and cooperative behavior (indirect reciprocity),\nin their interactions with humans and other agents. We develop a framework for\nour study, wherein classical laboratory experiments involving human subjects\nare adapted to use LLM agents. This approach involves step-by-step reasoning\nthat mirrors human cognitive processes and zero-shot learning to assess the\ninnate preferences of LLMs. Our analysis of LLM agents' behavior includes both\nthe primary effects and an in-depth examination of the underlying mechanisms.\nFocusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a\nrange of human-like social behaviors such as distributional and reciprocity\npreferences, responsiveness to group identity cues, engagement in indirect\nreciprocity, and social learning capabilities. However, our analysis also\nreveals notable differences: LLMs demonstrate a pronounced fairness preference,\nweaker positive reciprocity, and a more calculating approach in social learning\ncompared to humans. These insights indicate that while LLMs hold great promise\nfor applications in social science research, such as in laboratory experiments\nand agent-based modeling, the subtle behavioral differences between LLM agents\nand humans warrant further investigation. Careful examination and development\nof protocols in evaluating the social behaviors of LLMs are necessary before\ndirectly applying these models to emulate human behavior.\n"
    },
    {
        "paper_id": 2312.15221,
        "authors": "Sania Ashraf, Cristina Bicchieri, Upasak Das, Tanu Gupta, Alex Shpenev",
        "title": "Learning from diversity: jati fractionalization, social expectations and\n  improved sanitation practices in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Prevalence of open defecation is associated with adverse health effects,\ndetrimental not only for the individual but also the community. Therefore,\nneighborhood characteristics can influence collective progressive behavior such\nas improved sanitation practices. This paper uses primary data collected from\nrural and urban areas of Bihar to study the relationship between jati\n(sub-castes) level fractionalization within the community and toilet ownership\nand its usage for defecation. The findings indicate a diversity dividend\nwherein jati fractionalization is found to improve toilet ownership and usage\nsignificantly. While exploring the channels, we find social expectations to\nplay an important role, where individuals from diverse communities tend to\nbelieve that there is a higher prevalence of toilet usage within the community.\nTo assess the reasons for the existence of these social expectations, we use\ndata from an egocentric network survey on a sub-sample of the households. The\nfindings reveal that in fractionalized communities, the neighbors with whom our\nrespondents interacted are more likely to be from different jatis. They are\nalso more likely to use toilets and approve of its usage due to health reasons.\nDiscussions about toilets are more common among neighbors from fractionalized\ncommunities, which underscore the discernible role of social learning. The\ninferences drawn from the paper have significant implications for community\nlevel behavioral change interventions that aim at reducing open defecation.\n"
    },
    {
        "paper_id": 2312.15385,
        "authors": "Xiangyu Cui, Xun Li, Yun Shi and Si Zhao",
        "title": "Discrete-Time Mean-Variance Strategy Based on Reinforcement Learning",
        "comments": "arXiv admin note: text overlap with arXiv:1904.11392 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper studies a discrete-time mean-variance model based on reinforcement\nlearning. Compared with its continuous-time counterpart in \\cite{zhou2020mv},\nthe discrete-time model makes more general assumptions about the asset's return\ndistribution. Using entropy to measure the cost of exploration, we derive the\noptimal investment strategy, whose density function is also Gaussian type.\nAdditionally, we design the corresponding reinforcement learning algorithm.\nBoth simulation experiments and empirical analysis indicate that our\ndiscrete-time model exhibits better applicability when analyzing real-world\ndata than the continuous-time model.\n"
    },
    {
        "paper_id": 2312.15535,
        "authors": "Soheila Khajoui, Saeid Dehyadegari, Sayyed Abdolmajid Jalaee",
        "title": "Forecasting exports in selected OECD countries and Iran using MLP\n  Artificial Neural Network",
        "comments": "23 pages,8 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study aimed to forecast the exports of a select group of\nOrganization for Economic Co-operation and Development (OECD) countries and\nIran using the neural networks. The data concerning the exports of the above\ncountries from 1970 to 2019 were collected. The collected data were implemented\nto forecast the exports of the investigated countries for 2021 to 2025. The\nanalysis was performed using the Multi-Layer-Perceptron (MLP) neural network in\nPython. Out of the total number, 75 percent were used as training data, and 25\npercent were used as the test data. The findings of the study were evaluated\nwith 99% accuracy, which indicated the reliability of the output of the\nnetwork. The Results show that Covid-19 has affected exports over time.\nHowever, long-term export contracts are less affected by tensions and crises,\ndue to the effect of exports on economic growth, per capita income and it is\nbetter for economic policies of countries to use long-term export contracts.\n"
    },
    {
        "paper_id": 2312.15563,
        "authors": "Yongyang Cai, Khyati Malik, Hyeseon Shin",
        "title": "Dynamics of Global Emission Permit Prices and Regional Social Cost of\n  Carbon under Noncooperation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We build a dynamic multi-region model of climate and economy with emission\npermit trading among 12 aggregated regions in the world. We solve for the\ndynamic Nash equilibrium under noncooperation, wherein each region adheres to\nthe emission cap constraints following commitments that were first outlined in\nthe 2015 Paris Agreement and updated in subsequent years. Our model shows that\nthe emission permit price reaches $811 per ton of carbon by 2050. We\ndemonstrate that a regional carbon tax is complementary to the global\ncap-and-trade system, and the optimal regional carbon tax is equal to the\ndifference between the regional marginal abatement cost and the permit price.\n"
    },
    {
        "paper_id": 2312.15646,
        "authors": "Javad Eshtiyagh, Baotong Zhang, Yujing Sun, Linhui Wu, Zhao Wang",
        "title": "A graph-based multimodal framework to predict gentrification",
        "comments": null,
        "journal-ref": "International Conference on Urban Informatics 2023 - Best Paper\n  Award 3rd Place",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Gentrification--the transformation of a low-income urban area caused by the\ninflux of affluent residents--has many revitalizing benefits. However, it also\nposes extremely concerning challenges to low-income residents. To help\npolicymakers take targeted and early action in protecting low-income residents,\nresearchers have recently proposed several machine learning models to predict\ngentrification using socioeconomic and image features. Building upon previous\nstudies, we propose a novel graph-based multimodal deep learning framework to\npredict gentrification based on urban networks of tracts and essential\nfacilities (e.g., schools, hospitals, and subway stations). We train and test\nthe proposed framework using data from Chicago, New York City, and Los Angeles.\nThe model successfully predicts census-tract level gentrification with 0.9\nprecision on average. Moreover, the framework discovers a previously unexamined\nstrong relationship between schools and gentrification, which provides a basis\nfor further exploration of social factors affecting gentrification.\n"
    },
    {
        "paper_id": 2312.1573,
        "authors": "Maochun Xu, Zixun Lan, Zheng Tao, Jiawei Du, Zongao Ye",
        "title": "Deep Reinforcement Learning for Quantitative Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Intelligence (AI) and Machine Learning (ML) are transforming the\ndomain of Quantitative Trading (QT) through the deployment of advanced\nalgorithms capable of sifting through extensive financial datasets to pinpoint\nlucrative investment openings. AI-driven models, particularly those employing\nML techniques such as deep learning and reinforcement learning, have shown\ngreat prowess in predicting market trends and executing trades at a speed and\naccuracy that far surpass human capabilities. Its capacity to automate critical\ntasks, such as discerning market conditions and executing trading strategies,\nhas been pivotal. However, persistent challenges exist in current QT methods,\nespecially in effectively handling noisy and high-frequency financial data.\nStriking a balance between exploration and exploitation poses another challenge\nfor AI-driven trading agents. To surmount these hurdles, our proposed solution,\nQTNet, introduces an adaptive trading model that autonomously formulates QT\nstrategies through an intelligent trading agent. Incorporating deep\nreinforcement learning (DRL) with imitative learning methodologies, we bolster\nthe proficiency of our model. To tackle the challenges posed by volatile\nfinancial datasets, we conceptualize the QT mechanism within the framework of a\nPartially Observable Markov Decision Process (POMDP). Moreover, by embedding\nimitative learning, the model can capitalize on traditional trading tactics,\nnurturing a balanced synergy between discovery and utilization. For a more\nrealistic simulation, our trading agent undergoes training using\nminute-frequency data sourced from the live financial market. Experimental\nfindings underscore the model's proficiency in extracting robust market\nfeatures and its adaptability to diverse market conditions.\n"
    },
    {
        "paper_id": 2312.15865,
        "authors": "Saleh Ghobbe, Mahdi Nohekhan",
        "title": "Mental Perception of Quality: Green Marketing as a Catalyst for Brand\n  Quality Enhancement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The environmental conservation issue has led consumers to rethink the\nproducts they purchase. Nowadays, many consumers are willing to pay more for\nproducts that genuinely adhere to environmental standards to support the\nenvironment. Consequently, concepts like green marketing have gradually\ninfiltrated marketing literature, making environmental considerations one of\nthe most important activities for companies. Accordingly, this research\ninvestigates the impacts of green marketing strategy on perceived brand quality\n(case study: food exporting companies). The study population comprises 345\nemployees and managers from companies such as Kalleh, Solico, Pemina, Sorbon,\nMac, Pol, and Casel. Using Cochran's formula, a sample of 182 individuals was\nrandomly selected. This research is practical; the required data were collected\nthrough surveys and questionnaires. The findings indicate that (1) green\nmarketing strategy has a significant positive effect on perceived brand\nquality, (2) green products have a significant positive effect on perceived\nbrand quality, (3) green promotion has a significant positive effect on\nperceived brand quality, (4) green distribution has a significant positive\neffect on perceived brand quality, and (5) green pricing has a significant\npositive effect on perceived brand quality.\n"
    },
    {
        "paper_id": 2312.1595,
        "authors": "Herv\\'e Andr\\`es (CERMICS), Alexandre Boumezoued, Benjamin Jourdain\n  (CERMICS, MATHRISK)",
        "title": "Implied volatility (also) is path-dependent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for the coherent forecasting of both the implied\nvolatility surfaces and the underlying asset returns.In the spirit of Guyon and\nLekeufack (2023) who are interested in the dependence of volatility indices\n(e.g. the VIX) on the paths of the associated equity indices (e.g. the S&P\n500), we first study how implied volatility can be predicted using the past\ntrajectory of the underlying asset price. Our empirical study reveals that a\nlarge part of the movements of the at-the-money-forward implied volatility for\nup to two years maturities can be explained using the past returns and their\nsquares. Moreover, we show that up to four years of the past evolution of the\nunderlying price should be used for the prediction and that this feedback\neffect gets weaker when the maturity increases. Building on this new stylized\nfact, we fit to historical data a parsimonious version of the SSVI\nparameterization (Gatheral and Jacquier, 2014) of the implied volatility\nsurface relying on only four parameters and show that the two parameters ruling\nthe at-the-money-forward implied volatility as a function of the maturity\nexhibit a path-dependent behavior with respect to the underlying asset price.\nFinally, we propose a model for the joint dynamics of the implied volatility\nsurface and the underlying asset price. The latter is modelled using a variant\nof the path-dependent volatility model of Guyon and Lekeufack and the former is\nobtained by adding a feedback effect of the underlying asset price onto the two\nparameters ruling the at-the-money-forward implied volatility in the\nparsimonious SSVI parameterization and by specifying a hidden semi-Markov\ndiffusion model for the residuals of these two parameters and the two other\nparameters. Thanks to this model, we are able to simulate highly realistic\npaths of implied volatility surfaces that are arbitrage-free.\n"
    },
    {
        "paper_id": 2312.16161,
        "authors": "Johanna Fink",
        "title": "Can the creation of separate bidding zones within countries create\n  imbalances in PV uptake? Evidence from Sweden",
        "comments": "70 pages, 19 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper estimates how electricity price divergence within Sweden has\naffected incentives to invest in photovoltaic (PV) generation between 2016 and\n2022 based on a synthetic control approach. Sweden is chosen as the research\nsubject since it is together with Italy the only EU country with multiple\nbidding zones and is facing dramatic divergence in electricity prices between\nlow-tariff bidding zones in Northern and high-tariff bidding zones in Southern\nSweden since 2020. The results indicate that PV uptake in municipalities\nlocated north of the bidding zone border is reduced by 40.9-48% compared to\ntheir Southern counterparts. Based on these results, the creation of separate\nbidding zones within countries poses a threat to the expansion of PV generation\nand other renewables since it disincentivizes investment in areas with low\nelectricity prices.\n"
    },
    {
        "paper_id": 2312.16173,
        "authors": "Pierre Gosselin (IF), A\\\"ileen Lotz",
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2205.03087",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a general method to translate a standard economic model\nwith a large number of agents into a field-formalism model. This formalism\npreserves the system's interactions and microeconomic features at the\nindividual level but reveals the emergence of collective states.We apply this\nmethod to a simple microeconomic framework of investors and firms. Both macro\nand micro aspects of the formalism are studied.At the macro-scale, the field\nformalism shows that, in each sector, three patterns of capital accumulation\nmay emerge. A distribution of patterns across sectors constitute a collective\nstate. Any change in external parameters or expectations in one sector will\naffect neighbouring sectors, inducing transitions between collective states and\ngenerating permanent fluctuations in patterns and flows of capital. Although\nchanges in expectations can cause abrupt changes in collective states,\ntransitions may be slow to occur. Due to its relative inertia, the real economy\nis bound to be more affected by these constant variations than the financial\nmarkets.At the micro-scale we compute the transition functions of individual\nagents and study their probabilistic dynamics in a given collective state, as a\nfunction of their initial state. We show that capital accumulation of an\nindividual agent depends on various factors. The probability associated with\neach firm's trajectories is the result of several contradictory effects: the\nfirm tends to shift towards sectors with the greatest long-term return, but\nmust take into account the impact of its shift on its attractiveness for\ninvestors throughout its trajectory. Since this trajectory depends largely on\nthe average capital of transition sectors, a firm's attractiveness during its\nrelocation depends on the relative level of capital in those sectors. Moreover,\nthe firm must also consider the effects of competition in the intermediate\nsectors that tends to oust under-capitalized firm towards sectors with lower\naverage capital. For investors, capital allocation depends on their short and\nlong-term returns and investors will tend to reallocate their capital to\nmaximize both. The higher their level of capital, the stronger the\nre-allocation will be.\n"
    },
    {
        "paper_id": 2312.16179,
        "authors": "Albert Cohen, Jimmy Risk",
        "title": "European Football Player Valuation: Integrating Financial Models and\n  Network Theory",
        "comments": "15 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a new framework for player valuation in European football\nby fusing principles from financial mathematics and network theory. The\nvaluation model leverages a \"passing matrix\" to encapsulate player interactions\non the field, utilizing centrality measures to quantify individual influence.\nUnlike traditional approaches, this model is both metric-driven and\ncohort-free, providing a dynamic and individualized framework for ascertaining\na player's fair market value. The methodology is empirically validated through\na case study in European football, employing real-world match and financial\ndata. The paper advances the disciplines of sports analytics and financial\nmathematics by offering a cross-disciplinary mechanism for player valuation,\nand also links together two well-known econometric methods in marginal revenue\nproduct and expected present valuation.\n"
    },
    {
        "paper_id": 2312.16185,
        "authors": "Haochun Ma, Davide Prosperino, Alexander Haluszczynski, Christoph\n  R\\\"ath",
        "title": "Linear and nonlinear causality in financial markets",
        "comments": "14 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying and quantifying co-dependence between financial instruments is a\nkey challenge for researchers and practitioners in the financial industry.\nLinear measures such as the Pearson correlation are still widely used today,\nalthough their limited explanatory power is well known. In this paper we\npresent a much more general framework for assessing co-dependencies by\nidentifying and interpreting linear and nonlinear causalities in the complex\nsystem of financial markets. To do so, we use two different causal inference\nmethods, transfer entropy and convergent cross-mapping, and employ Fourier\ntransform surrogates to separate their linear and nonlinear contributions. We\nfind that stock indices in Germany and the U.S. exhibit a significant degree of\nnonlinear causality and that correlation, while a very good proxy for linear\ncausality, disregards nonlinear effects and hence underestimates causality\nitself. The presented framework enables the measurement of nonlinear causality,\nthe correlation-causality fallacy, and motivates how causality can be used for\ninferring market signals, pair trading, and risk management of portfolios. Our\nresults suggest that linear and nonlinear causality can be used as early\nwarning indicators of abnormal market behavior, allowing for better trading\nstrategies and risk management.\n"
    },
    {
        "paper_id": 2312.1619,
        "authors": "Raffaele Giuseppe Cestari, Filippo Barchi, Riccardo Busetto, Daniele\n  Marazzina, Simone Formentin",
        "title": "Hawkes-based cryptocurrency forecasting via Limit Order Book data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurately forecasting the direction of financial returns poses a formidable\nchallenge, given the inherent unpredictability of financial time series. The\ntask becomes even more arduous when applied to cryptocurrency returns, given\nthe chaotic and intricately complex nature of crypto markets. In this study, we\npresent a novel prediction algorithm using limit order book (LOB) data rooted\nin the Hawkes model, a category of point processes. Coupled with a continuous\noutput error (COE) model, our approach offers a precise forecast of return\nsigns by leveraging predictions of future financial interactions. Capitalizing\non the non-uniformly sampled structure of the original time series, our\nstrategy surpasses benchmark models in both prediction accuracy and cumulative\nprofit when implemented in a trading environment. The efficacy of our approach\nis validated through Monte Carlo simulations across 50 scenarios. The research\ndraws on LOB measurements from a centralized cryptocurrency exchange where the\nstablecoin Tether is exchanged against the U.S. dollar.\n"
    },
    {
        "paper_id": 2312.16205,
        "authors": "Sania Ashraf, Cristina Bicchieri, Upasak Das. Alex Shpenev",
        "title": "Valuing Open Defecation Free Surroundings: Experimental Evidence from a\n  Norm-Based Intervention in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Open defecation, which is linked with poor health outcomes, lower cognitive\nability and productivity, has been widespread in India. This paper assesses the\nimpact of a randomized norm-centric intervention implemented in peri-urban\nareas of Tamil Nadu in India on raising the value attached to residence in\nareas with a lower prevalence of open defecation, measured through Willingness\nto Pay (WTP). The intervention aimed to change social expectations about toilet\nusage through audio announcements, wall paintings, household visits, and\ncommunity meetings. The findings indicate a significant increase in the WTP for\nrelocating to areas with lower prevalence of open defecation. The results are\nconsistent when using local average treatment effect estimations wherein the\npossibility of spillovers in the control areas is accounted for. They are also\nrobust to potential bias due to local socio-political events during the study\nperiod and COVID-led attrition. We further observe a significant increase in\ntoilet ownership and usage. While assessing the mechanism, we find that change\nin empirical expectations through the intervention (what one believes about the\nprevalence of toilet usage in the community) is one of the primary mediating\nchannels. Normative expectations (what one believes about community approval of\ntoilet usage) are found to have limited effect. The findings underscore the\nneed for norm-centric interventions to propel change in beliefs and achieve\nlong-term and sustainable sanitation behavior.\n"
    },
    {
        "paper_id": 2312.16223,
        "authors": "Sahar Arshad, Seemab Latif, Ahmad Salman, Rabia Latif",
        "title": "Enhancing Profitability and Investor Confidence through Interpretable AI\n  Models for Investment Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial forecasting plays an important role in making informed decisions\nfor financial stakeholders, specifically in the stock exchange market. In a\ntraditional setting, investors commonly rely on the equity research department\nfor valuable reports on market insights and investment recommendations. The\nequity research department, however, faces challenges in effectuating\ndecision-making do to the demanding cognitive effort required for analyzing the\ninherently volatile nature of market dynamics. Furthermore, financial\nforecasting systems employed by analysts pose potential risks in terms of\ninterpretability and gaining the trust of all stakeholders. This paper presents\nan interpretable decision-making model leveraging the SHAP-based explainability\ntechnique to forecast investment recommendations. The proposed solution not\nonly provides valuable insights into the factors that influence forecasted\nrecommendations but also caters the investors of varying types, including those\ninterested in daily and short-term investment opportunities. To ascertain the\nefficacy of the proposed model, a case study is devised that demonstrates a\nnotable enhancement in investor's portfolio value, employing our trading\nstrategies. The results highlight the significance of incorporating\ninterpretability in forecasting models to boost stakeholders' confidence and\nfoster transparency in the stock exchange domain.\n"
    },
    {
        "paper_id": 2312.16448,
        "authors": "Erdinc Akyildirim, Matteo Gambara, Josef Teichmann, Syang Zhou",
        "title": "Randomized Signature Methods in Optimal Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present convincing empirical results on the application of Randomized\nSignature Methods for non-linear, non-parametric drift estimation for a\nmulti-variate financial market. Even though drift estimation is notoriously ill\ndefined due to small signal to noise ratio, one can still try to learn optimal\nnon-linear maps from data to future returns for the purposes of portfolio\noptimization. Randomized Signatures, in contrast to classical signatures, allow\nfor high dimensional market dimension and provide features on the same scale.\nWe do not contribute to the theory of Randomized Signatures here, but rather\npresent our empirical findings on portfolio selection in real world settings\nincluding real market data and transaction costs.\n"
    },
    {
        "paper_id": 2312.16637,
        "authors": "Andrey Shternshis and Stefano Marmi",
        "title": "Price predictability at ultra-high frequency: Entropy-based randomness\n  test",
        "comments": "28 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use the statistical properties of Shannon entropy estimator and\nKullback-Leibler divergence to study the predictability of ultra-high frequency\nfinancial data. We develop a statistical test for the predictability of a\nsequence based on empirical frequencies. We show that the degree of randomness\ngrows with the increase of aggregation level in transaction time. We also find\nthat predictable days are usually characterized by high trading activity, i.e.,\ndays with unusually high trading volumes and the number of price changes. We\nfind a group of stocks for which predictability is caused by a frequent change\nof price direction. We study stylized facts that cause price predictability\nsuch as persistence of order signs, autocorrelation of returns, and volatility\nclustering. We perform multiple testing for sub-intervals of days to identify\nwhether there is predictability at a specific time period during the day.\n"
    },
    {
        "paper_id": 2312.16698,
        "authors": "Erfan Mohammadi, MohammadMahdi Barzegar, Mahdi Nohekhan",
        "title": "The Green Advantage: Analyzing the Effects of Eco-Friendly Marketing on\n  Consumer Loyalty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The idea that marketing, in addition to profitability and sales, should also\nconsider the consumer's health is not and has not been a far-fetched concept.\nIt can be stated that there is no longer a way back to producing\nenvironmentally harmful products, and gradually, governmental pressures,\ncompetition, and changing customer attitudes are obliging companies to adopt\nand implement a green marketing approach. Over time, concepts such as green\nmarketing have penetrated marketing literature, making environmental\nconsiderations one of the most important activities of companies. For this\npurpose, this research examines the effects of green marketing strategy on\nbrand loyalty (case study: food exporting companies). The population of this\nstudy consists of 345 employees and managers of companies like Kalleh, Solico,\nPemina, Sorben, Mac, Pol, and Casel, out of which 182 were randomly selected as\na sample using Cochran's formula. This research is practical; the required data\nwere collected through a survey and questionnaire. The research results\nindicate that (1) green marketing strategy significantly affects brand loyalty.\n(2) Green products have a significant positive effect on brand loyalty. (3)\nGreen promotion has a significant positive effect on brand loyalty. (4) Green\ndistribution has a significant positive effect on brand loyalty. (5) Green\npricing has a significant positive effect on brand loyalty.\n"
    },
    {
        "paper_id": 2312.1671,
        "authors": "Lucas Gonzalez, Andrea Alcaraz, Carolina Gabay, Monica Castro, Silvina\n  Vigo, Eduardo Carinci, Federico Augustovski",
        "title": "Health-related Quality of life, Financial Toxicity, Productivity Loss\n  and Catastrophic Health Expenditures After Lung Cancer Diagnosis in Argentina",
        "comments": "16 pages, 4 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Objective: About 12,000 people are diagnosed with lung cancer (LC) each year\nin Argentina, and the diagnosis has a significant personal and family. The\nobjective of this study was to characterize the Health-Related Quality of Life\n(HRQoL) and the economic impact in patients with LC and in their households.\nMethods: Observational cross-sectional study, through validated structured\nquestionnaires to patients with a diagnosis of Non-Small Cell Lung Cancer\n(NSCLC) in two referral public health care centers in Argentina. Questionnaries\nused: Health-Related Quality of life (EuroQol EQ-5D-3L questionnaire);\nfinancial toxicity (COST questionnaire); productivity loss (WPAI-GH, Work\nProductivity and Activity Impairment Questionnaire: General Health), and\nout-of-pocket expenses. Results: We included 101 consecutive patients (mean age\n67.5 years; 55.4% men; 57.4% with advanced disease -stage III/IV-). The mean\nEQ-VAS was 68.8 (SD:18.3), with 82.2% describing fair or poor health. The most\naffected dimensions were anxiety/depression, pain, and activities of daily\nliving. Patients reported an average 59% decrease in their ability to perform\nregular daily activities as measured by WPAI-GH. 54.5% reported a reduction in\nincome due to the disease, and 19.8% lost their jobs. The annual economic\nproductivity loss was estimated at USD 2,465 per person. 70.3%) of patients\nreported financial toxicity. The average out-of-pocket expenditure was USD\n100.38 per person per month, which represented 18.5% of household income.\nCatastrophic expenditures were present in 37.1% of households. When performing\nsubgroup analysis by disease severity, all outcomes were worse in the\nsubpopulation with advanced disease. Conclusions Patients with NSCLC treated in\npublic hospitals in Argentina have significant health-related quality of life\nand economic impact, worsening in patients with advanced disease.\n"
    },
    {
        "paper_id": 2312.17061,
        "authors": "Parley R Yang and Alexander Y Shestopaloff",
        "title": "Bayesian Analysis of High Dimensional Vector Error Correction Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Vector Error Correction Model (VECM) is a classic method to analyse\ncointegration relationships amongst multivariate non-stationary time series. In\nthis paper, we focus on high dimensional setting and seek for\nsample-size-efficient methodology to determine the level of cointegration. Our\ninvestigation centres at a Bayesian approach to analyse the cointegration\nmatrix, henceforth determining the cointegration rank. We design two algorithms\nand implement them on simulated examples, yielding promising results\nparticularly when dealing with high number of variables and relatively low\nnumber of observations. Furthermore, we extend this methodology to empirically\ninvestigate the constituents of the S&P 500 index, where low-volatility\nportfolios can be found during both in-sample training and out-of-sample\ntesting periods.\n"
    },
    {
        "paper_id": 2312.17123,
        "authors": "Pauline Leung and Zhuan Pei",
        "title": "Further Education During Unemployment",
        "comments": "Minor expositional edits. No change to results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evidence on the effectiveness of retraining U.S. unemployed workers primarily\ncomes from evaluations of training programs, which represent one narrow avenue\nfor skill acquisition. We use high-quality records from Ohio and a matching\nmethod to estimate the effects of retraining, broadly defined as enrollment in\npostsecondary institutions. Our simple method bridges two strands of the\ndynamic treatment effect literature that estimate the\ntreatment-now-versus-later and treatment-versus-no-treatment effects. We find\nthat enrollees experience earnings gains of six percent three to four years\nafter enrolling, after depressed earnings during the first two years. The\nearnings effects are driven by industry-switchers, particularly to healthcare.\n"
    },
    {
        "paper_id": 2312.17157,
        "authors": "J. Doyne Farmer, John Geanakoplos, Matteo G. Richiardi, Miquel\n  Montero, Josep Perell\\'o, Jaume Masoliver",
        "title": "Discounting the distant future: What do historical bond prices imply\n  about the long term discount rate?",
        "comments": "27 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a thorough empirical study on real interest rates by also\nincluding risk aversion through the introduction of the market price of risk.\nWith the view of complex systems science and its multidisciplinary approach, we\nuse the theory of bond pricing to study the long term discount rate.\nCentury-long historical records of 3 month bonds, 10 year bonds, and inflation\nallow us to estimate real interest rates for the UK and the US. Real interest\nrates are negative about a third of the time and the real yield curves are\ninverted more than a third of the time, sometimes by substantial amounts. This\nrules out most of the standard bond pricing models, which are designed for\nnominal rates that are assumed to be positive. We therefore use the\nOrnstein-Uhlenbeck model which allows negative rates and gives a good match to\ninversions of the yield curve. We derive the discount function using the method\nof Fourier transforms and fit it to the historical data. The estimated long\nterm discount rate is $1.7$ \\% for the UK and $2.2$ \\% for the US. The value of\n$1.4$ \\% used by Stern is less than a standard deviation from our estimated\nlong run return rate for the UK, and less than two standard deviations of the\nestimated value for the US. All of this once more reinforces the support for\nimmediate and substantial spending to combat climate change.\n"
    },
    {
        "paper_id": 2312.17337,
        "authors": "Tobias Schimanski, Chiara Colesanti Senni, Glen Gostlow, Jingwei Ni,\n  Tingyu Yu, Markus Leippold",
        "title": "Exploring Nature: Datasets and Models for Analyzing Nature-Related\n  Disclosures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nature is an amorphous concept. Yet, it is essential for the planet's\nwell-being to understand how the economy interacts with it. To address the\ngrowing demand for information on corporate nature disclosure, we provide\ndatasets and classifiers to detect nature communication by companies. We ground\nour approach in the guidelines of the Taskforce on Nature-related Financial\nDisclosures (TNFD). Particularly, we focus on the specific dimensions of water,\nforest, and biodiversity. For each dimension, we create an expert-annotated\ndataset with 2,200 text samples and train classifier models. Furthermore, we\nshow that nature communication is more prevalent in hotspot areas and directly\neffected industries like agriculture and utilities. Our approach is the first\nto respond to calls to assess corporate nature communication on a large scale.\n"
    },
    {
        "paper_id": 2312.17375,
        "authors": "Agathe Sadeghi, Achintya Gopal, Mohammad Fesanghary",
        "title": "Causal Discovery in Financial Markets: A Framework for Nonstationary\n  Time-Series Data",
        "comments": "35 pages, 28 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new causal structure learning method for\nnonstationary time series data, a common data type found in fields such as\nfinance, economics, healthcare, and environmental science. Our work builds upon\nthe constraint-based causal discovery from nonstationary data algorithm\n(CD-NOD). We introduce a refined version (CD-NOTS) which is designed\nspecifically to account for lagged dependencies in time series data. We compare\nthe performance of different algorithmic choices, such as the type of\nconditional independence test and the significance level, to help select the\nbest hyperparameters given various scenarios of sample size, problem\ndimensionality, and availability of computational resources. Using the results\nfrom the simulated data, we apply CD-NOTS to a broad range of real-world\nfinancial applications in order to identify causal connections among\nnonstationary time series data, thereby illustrating applications in\nfactor-based investing, portfolio diversification, and comprehension of market\ndynamics.\n"
    },
    {
        "paper_id": 2401.00001,
        "authors": "Runjia Yang and Beining Shi",
        "title": "Sector Rotation by Factor Model and Fundamental Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This study presents an analytical approach to sector rotation, leveraging\nboth factor models and fundamental metrics. We initiate with a systematic\nclassification of sectors, followed by an empirical investigation into their\nreturns. Through factor analysis, the paper underscores the significance of\nmomentum and short-term reversion in dictating sectoral shifts. A subsequent\nin-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA,\nDividend Yield, among others. Our primary contribution lies in developing a\npredictive framework based on these fundamental indicators. The constructed\nmodels, post rigorous training, exhibit noteworthy predictive capabilities. The\nfindings furnish a nuanced understanding of sector rotation strategies, with\nimplications for asset management and portfolio construction in the financial\ndomain.\n"
    },
    {
        "paper_id": 2401.00081,
        "authors": "Vamsi K. Potluru, Daniel Borrajo, Andrea Coletta, Niccol\\`o Dalmasso,\n  Yousef El-Laham, Elizabeth Fons, Mohsen Ghassemi, Sriram Gopalakrishnan,\n  Vikesh Gosai, Eleonora Krea\\v{c}i\\'c, Ganapathy Mani, Saheed Obitayo, Deepak\n  Paramanand, Natraj Raman, Mikhail Solonin, Srijan Sood, Svitlana Vyetrenko,\n  Haibei Zhu, Manuela Veloso, Tucker Balch",
        "title": "Synthetic Data Applications in Finance",
        "comments": "50 pages, journal submission; updated 6 privacy levels",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Synthetic data has made tremendous strides in various commercial settings\nincluding finance, healthcare, and virtual reality. We present a broad overview\nof prototypical applications of synthetic data in the financial sector and in\nparticular provide richer details for a few select ones. These cover a wide\nvariety of data modalities including tabular, time-series, event-series, and\nunstructured arising from both markets and retail financial applications. Since\nfinance is a highly regulated industry, synthetic data is a potential approach\nfor dealing with issues related to privacy, fairness, and explainability.\nVarious metrics are utilized in evaluating the quality and effectiveness of our\napproaches in these applications. We conclude with open directions in synthetic\ndata in the context of the financial domain.\n"
    },
    {
        "paper_id": 2401.00103,
        "authors": "Gechun Liang, Yifan Sun, Thaleia Zariphopoulou",
        "title": "Representation of forward performance criteria with random endowment via\n  FBSDE and application to forward optimized certainty equivalent",
        "comments": "50 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We extend the notion of forward performance criteria to settings with random\nendowment in incomplete markets. Building on these results, we introduce and\ndevelop the novel concept of forward optimized certainty equivalent (forward\nOCE), which offers a genuinely dynamic valuation mechanism that accommodates\nprogressively adaptive market model updates, stochastic risk preferences, and\nincoming claims with arbitrary maturities.\n  In parallel, we develop a new methodology to analyze the emerging stochastic\noptimization problems by directly studying the candidate optimal control\nprocesses for both the primal and dual problems. Specifically, we derive two\nnew systems of forward-backward stochastic differential equations (FBSDEs) and\nestablish necessary and sufficient conditions for optimality, and various\nequivalences between the two problems. This new approach is general and\ncomplements the existing one based on backward stochastic partial differential\nequations (backward SPDEs) for the related value functions. We, also, consider\nrepresentative examples for both forward performance criteria with random\nendowment and forward OCE, and for the case of exponential criteria, we\ninvestigate the connection between forward OCE and forward entropic risk\nmeasures.\n"
    },
    {
        "paper_id": 2401.00188,
        "authors": "Davide Lauria, W. Brent Lindquist and Svetlozar T. Rachev",
        "title": "Enhancing CVaR portfolio optimisation performance with GAM factor models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a discrete-time econometric model that combines autoregressive\nfilters with factor regressions to predict stock returns for portfolio\noptimisation purposes. In particular, we test both robust linear regressions\nand general additive models on two different investment universes composed of\nthe Dow Jones Industrial Average and the Standard & Poor's 500 indexes, and we\ncompare the out-of-sample performances of mean-CVaR optimal portfolios over a\nhorizon of six years. The results show a substantial improvement in portfolio\nperformances when the factor model is estimated with general additive models.\n"
    },
    {
        "paper_id": 2401.00227,
        "authors": "Bhaso Ndzendze",
        "title": "Does the World Bank's Ease of Doing Business Index Matter for FDI?\n  Findings from Africa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates whether foreign investment (FDI) into Africa is at\nleast partially responsive to World Bank-measured market friendliness.\nSpecifically, I conducted analyses of four countries between 2009 and 2017,\nusing cases that represent two of the highest scorers on the bank's Doing\nBusiness index as of 2008 (Mauritius and South Africa) and the two lowest\nscorers (DRC and CAR), and subsequently traced all four for growths or declines\nin FDI in relation to their scores in the index. The findings show that there\nis a moderate association between decreased costs of starting a business and\ngrowth of FDI. Mauritius, South Africa and the DRC reduced their total cost of\nstarting a business by 71.7%, 143.7% and 122.9% for the entire period, and saw\ninward FDI increases of 167.6%, 79.8% and 152.21%, respectively. The CAR\nincreased the cost of starting businesses but still saw increases in FDI.\nHowever, the country also saw the least amount of growth in FDI at only 13.3%.\n"
    },
    {
        "paper_id": 2401.00263,
        "authors": "Christoph Moehr",
        "title": "A framework for the valuation of insurance liabilities by production\n  cost",
        "comments": "35 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper sets out a framework for the valuation of insurance liabilities\nthat is intended to be economically realistic, elementary, reasonably\npractically applicable, and as a special case to provide a basis for the\nvaluation in regulatory solvency systems such as Solvency II and the SST. The\nvaluation framework is based on the cost of producing the liabilities to an\ninsurance company that is subject to solvency regulation (regulatory solvency\ncapital requirements) and insolvency laws (consequences of failure) in finite\ndiscrete time. Starting from the replication approach of classical no-arbitrage\ntheory, the framework additionally considers the nature and cost of capital\n(expressed by a ``financiability condition\"), that the liabilities may be\nrequired to be fulfilled only ``in sufficiently many cases\" (expressed by a\n``fulfillment condition\"), production using ``fully illiquid\" assets in\naddition to tradables, and the asymmetry between assets and liabilities. We\nidentify necessary and sufficient conditions on the capital investment under\nwhich the framework recovers the market prices of tradables, investigate\nextending production to take account of insolvency, implications of using\nilliquid assets in the production, and show how Solvency II and SST valuation\ncan be derived with specific assumptions.\n"
    },
    {
        "paper_id": 2401.00307,
        "authors": "Tayfun S\\\"onmez",
        "title": "Minimalist Market Design: A Framework for Economists with Policy\n  Aspirations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Earlier in my career, prevalent approaches in the emerging field of market\ndesign largely represented the experiences and perspectives of leaders who were\ncommissioned to design or reform various institutions. Since being commissioned\nfor a similar task seemed unlikely for me as an aspiring design economist, I\ndeveloped my own minimalist approach to market design. Using the policy\nobjectives of stakeholders, my approach creates a new institution from the\nexisting one with minimal interference with its elements that compromise the\nobjectives. Minimalist market design initially evolved through my integrated\nresearch and policy efforts in school choice from 1997 to 2005 and in kidney\nexchange from 2003 to 2007. Given its success in school choice and kidney\nexchange, I systematically followed this approach in many other, often unusual\nreal-world settings. In recent years, my efforts in minimalist market design\nled to the 2021 reform of the US Army's branching system for its cadets to\nmilitary specialties, the adoption of reserve systems during the Covid-19\npandemic for vaccine allocation in 15 states and therapies in 2 states, and the\ndeployment of a highly efficient liver exchange system in T\\\"urkiye. This same\nmethodology also predicted the rescission of a 1995 Supreme Court judgment in\nIndia, resulting in countless litigations and interruptions of public\nrecruitment for 25 years, as well as the mandates of its replacement. In this\nmonograph, I describe the philosophy, evolution, and successful applications of\nminimalist market design, contrasting it with the mainstream paradigm for the\nfield. In doing so, I also provide a paradigm for economists who want to\ninfluence policy and change institutions through their research.\n"
    },
    {
        "paper_id": 2401.00313,
        "authors": "Daniel Huttenlocher, Hannah Li, Liang Lyu, Asuman Ozdaglar and James\n  Siderius",
        "title": "Matching of Users and Creators in Two-Sided Markets with Departures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many online platforms of today, including social media sites, are two-sided\nmarkets bridging content creators and users. Most of the existing literature on\nplatform recommendation algorithms largely focuses on user preferences and\ndecisions, and does not simultaneously address creator incentives. We propose a\nmodel of content recommendation that explicitly focuses on the dynamics of\nuser-content matching, with the novel property that both users and creators may\nleave the platform permanently if they do not experience sufficient engagement.\nIn our model, each player decides to participate at each time step based on\nutilities derived from the current match: users based on alignment of the\nrecommended content with their preferences, and creators based on their\naudience size. We show that a user-centric greedy algorithm that does not\nconsider creator departures can result in arbitrarily poor total engagement,\nrelative to an algorithm that maximizes total engagement while accounting for\ntwo-sided departures. Moreover, in stark contrast to the case where only users\nor only creators leave the platform, we prove that with two-sided departures,\napproximating maximum total engagement within any constant factor is NP-hard.\nWe present two practical algorithms, one with performance guarantees under mild\nassumptions on user preferences, and another that tends to outperform\nalgorithms that ignore two-sided departures in practice.\n"
    },
    {
        "paper_id": 2401.00507,
        "authors": "Vahidin Jeleskovic, Claudio Latini, Zahid I. Younas, Mamdouh A. S.\n  Al-Faryan",
        "title": "Optimization of portfolios with cryptocurrencies: Markowitz and\n  GARCH-Copula model approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The growing interest in cryptocurrencies has drawn the attention of the\nfinancial world to this innovative medium of exchange. This study aims to\nexplore the impact of cryptocurrencies on portfolio performance. We conduct our\nanalysis retrospectively, assessing the performance achieved within a specific\ntime frame by three distinct portfolios: one consisting solely of equities,\nbonds, and commodities; another composed exclusively of cryptocurrencies; and a\nthird, which combines both 'traditional' assets and the best-performing\ncryptocurrency from the second portfolio.To achieve this, we employ the classic\nvariance-covariance approach, utilizing the GARCH-Copula and GARCH-Vine Copula\nmethods to calculate the risk structure. The optimal asset weights within the\noptimized portfolios are determined through the Markowitz optimization problem.\nOur analysis predominantly reveals that the portfolio comprising both\ncryptocurrency and traditional assets exhibits a higher Sharpe ratio from a\nretrospective viewpoint and demonstrates more stable performances from a\nprospective perspective. We also provide an explanation for our choice of\nportfolio optimization based on the Markowitz approach rather than CVaR and ES.\n"
    },
    {
        "paper_id": 2401.00534,
        "authors": "Shun Liu, Kexin Wu, Chufeng Jiang, Bin Huang, Danqing Ma",
        "title": "Financial Time-Series Forecasting: Towards Synergizing Performance And\n  Interpretability Within a Hybrid Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the realm of cryptocurrency, the prediction of Bitcoin prices has garnered\nsubstantial attention due to its potential impact on financial markets and\ninvestment strategies. This paper propose a comparative study on hybrid machine\nlearning algorithms and leverage on enhancing model interpretability.\nSpecifically, linear regression(OLS, LASSO), long-short term memory(LSTM),\ndecision tree regressors are introduced. Through the grounded experiments, we\nobserve linear regressor achieves the best performance among candidate models.\nFor the interpretability, we carry out a systematic overview on the\npreprocessing techniques of time-series statistics, including decomposition,\nauto-correlational function, exponential triple forecasting, which aim to\nexcavate latent relations and complex patterns appeared in the financial\ntime-series forecasting. We believe this work may derive more attention and\ninspire more researches in the realm of time-series analysis and its realistic\napplications.\n"
    },
    {
        "paper_id": 2401.00535,
        "authors": "Theodoros Chatzivasileiadis and Ignasi Cortes Arbues and Jochen Hinkel\n  and Daniel Lincke and Richard S.J. Tol",
        "title": "Actualised and future changes in regional economic growth through sea\n  level rise",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the long-term economic impact of sea-level rise (SLR)\non coastal regions in Europe, focusing on Gross Domestic Product (GDP). Using a\nnovel dataset covering regional SLR and economic growth from 1900 to 2020, we\nquantify the relationships between SLR and regional GDP per capita across 79\ncoastal EU & UK regions. Our results reveal that the current SLR has already\nnegatively influenced GDP of coastal regions, leading to a cumulative 4.7% loss\nat 39 cm of SLR. Over the 120 year period studied, the actualised impact of SLR\non the annual growth rate is between -0.02% and 0.04%. Extrapolating these\nfindings to future climate and socio-economic scenarios, we show that in the\nabsence of additional adaptation measures, GDP losses by 2100 could range\nbetween -6.3% and -20.8% under the most extreme SLR scenario (SSP5-RCP8.5\nHigh-end Ice, or -4.0% to -14.1% in SSP5-RCP8.5 High Ice). This statistical\nanalysis utilising a century-long dataset, provides an empirical foundation for\ndesigning region-specific climate adaptation strategies to mitigate economic\ndamages caused by SLR. Our evidence supports the argument for strategically\nrelocating assets and establishing coastal setback zones when it is\neconomically preferable and socially agreeable, given that protection\ninvestments have an economic impact.\n"
    },
    {
        "paper_id": 2401.00539,
        "authors": "Elisa Al\\`os, Eulalia Nualart, Makar Pravosud",
        "title": "On the implied volatility of Inverse and Quanto Inverse options under\n  stochastic volatility models",
        "comments": "arXiv admin note: text overlap with arXiv:2308.15341,\n  arXiv:2208.01353",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study short-time behavior of the at-the-money implied\nvolatility for Inverse and Quanto Inverse European options with fixed strike\nprice. The asset price is assumed to follow a general stochastic volatility\nprocess. Using techniques of the Malliavin calculus such as the anticipating\nIto's formula we first compute the level of the implied volatility of the\noption when the maturity converges to zero. Then, we find a short maturity\nasymptotic formula for the skew of the implied volatility that depends on the\nroughness of the volatility model. We apply our general results to the SABR and\nfractional Bergomi models, and provide some numerical simulations that confirm\nthe accurateness of the asymptotic formula for the skew.\n"
    },
    {
        "paper_id": 2401.00603,
        "authors": "Vahidin Jeleskovic and Stephen Mackay",
        "title": "Intraday Trading Algorithm for Predicting Cryptocurrency Price Movements\n  Using Twitter Big Data Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Cryptocurrencies have emerged as a novel financial asset garnering\nsignificant attention in recent years. A defining characteristic of these\ndigital currencies is their pronounced short-term market volatility, primarily\ninfluenced by widespread sentiment polarization, particularly on social media\nplatforms such as Twitter. Recent research has underscored the correlation\nbetween sentiment expressed in various networks and the price dynamics of\ncryptocurrencies. This study delves into the 15-minute impact of informative\ntweets disseminated through foundation channels on trader behavior, with a\nfocus on potential outcomes related to sentiment polarization. The primary\nobjective is to identify factors that can predict positive price movements and\npotentially be leveraged through a trading algorithm. To accomplish this\nobjective, we conduct a conditional examination of return and excess return\nrates within the 15 minutes following tweet publication. The empirical findings\nreveal statistically significant increases in return rates, particularly within\nthe initial three minutes following tweet publication. Notably, adverse effects\nresulting from the messages were not observed. Surprisingly, sentiments were\nfound to have no discerni-ble impact on cryptocurrency price movements. Our\nanalysis further identifies that inves-tors are primarily influenced by the\nquality of tweet content, as reflected in the choice of words and tweet volume.\nWhile the basic trading algorithm presented in this study does yield some\nbenefits within the 15-minute timeframe, these benefits are not statistically\nsignificant. Nevertheless, it serves as a foundational framework for potential\nenhance-ments and further investigations.\n"
    },
    {
        "paper_id": 2401.00919,
        "authors": "Francisco Estrada, Veronica Lupi, Wouter Botzen, Richard S.J. Tol",
        "title": "Urban and non-urban contributions to the social cost of carbon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The social cost of carbon (SCC) serves as a concise gauge of climate change's\neconomic impact, often reported at the global and country level. SCC values are\ndisproportionately high for less-developed, populous countries. Assessing the\ncontributions of urban and non-urban areas to the SCC can provide additional\ninsights for climate policy. Cities are essential for defining global\nemissions, influencing warming levels and associated damages. High exposure and\nconcurrent socioenvironmental problems exacerbate climate change risks in\ncities. Using a spatially explicit integrated assessment model, the SCC is\nestimated at USD$137-USD$579/tCO2, rising to USD$262-USD$1,075/tCO2 when\nincluding urban heat island (UHI) warming. Urban SCC dominates, with both urban\nexposure and the UHI contributing significantly. A permanent 1% reduction of\nthe UHI in urban areas yields net present benefits of USD$484-USD$1,562 per\nurban dweller. Global cities have significant leverage and incentives for a\nswift transition to a low-carbon economy, and for reducing local warming.\n"
    },
    {
        "paper_id": 2401.00949,
        "authors": "Alejandro Rodriguez Dominguez",
        "title": "A Portfolio's Common Causal Conditional Risk-neutral PDE",
        "comments": "6 pages, 4 figures, Mathematical and Statistical Methods for\n  Actuarial Sciences and Finance - MAF2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio's optimal drivers for diversification are common causes of the\nconstituents' correlations. A closed-form formula for the conditional\nprobability of the portfolio given its optimal common drivers is presented,\nwith each pair constituent-common driver joint distribution modelled by\nGaussian copulas. A conditional risk-neutral PDE is obtained for this\nconditional probability as a system of copulas' PDEs, allowing for dynamical\nrisk management of a portfolio as shown in the experiments. Implied conditional\nportfolio volatilities and implied weights are new risk metrics that can be\ndynamically monitored from the PDEs or obtained from their solution.\n"
    },
    {
        "paper_id": 2401.0097,
        "authors": "Eberhard Mayerhofer",
        "title": "Almost Perfect Shadow Prices",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Shadow prices simplify the derivation of optimal trading strategies in\nmarkets with transaction costs by transferring optimization into a more\ntractable, frictionless market. This paper establishes that a na\\\"ive shadow\nprice Ansatz for maximizing long term returns given average volatility yields a\nstrategy that is, for small bid-ask-spreads, asymptotically optimal at third\norder. Considering the second-order impact of transaction costs, such a\nstrategy is essentially optimal. However, for risk aversion different from one,\nwe devise alternative strategies that outperform the shadow market at fourth\norder. Finally, it is shown that the risk-neutral objective rules out the\nexistence of shadow prices.\n"
    },
    {
        "paper_id": 2401.0108,
        "authors": "Leah Costlow, Anna Herforth, Timothy B. Sulser, Nicola Cenacchi,\n  William A. Masters",
        "title": "How and where global food supplies fall short of healthy diets: Past\n  trends and future projections, 1961-2020 and 2010-2050",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most of the world still lacks access to sufficient quantities of all food\ngroups needed for an active and healthy life. This study traces historical and\nprojected changes in global food systems toward alignment with the new Healthy\nDiet Basket (HDB) used by UN agencies and the World Bank to monitor the cost\nand affordability of healthy diets worldwide. We use HDB as a standard to\nmeasure adequacy of national, regional and global supply-demand balances,\nfinding substantial but inconsistent progress toward closer alignment with\ndietary guidelines, with large global shortfalls in fruits, vegetables, and\nlegumes, nuts, and seeds, and large disparities among regions in use of animal\nsource foods. Projections show that additional investments in the supply of\nagricultural products would modestly accelerate improvements in adequacy where\nshortfalls are greatest, revealing the need for complementary investments to\nincrease purchasing power and demand for under-consumed food groups especially\nin low-income countries.\n"
    },
    {
        "paper_id": 2401.01411,
        "authors": "Geoff Boeing, Clemens Pilgram, Yougeng Lu",
        "title": "Urban Street Network Design and Transport-Related Greenhouse Gas\n  Emissions around the World",
        "comments": null,
        "journal-ref": "Transportation Research Part D: Transport and Environment, 127,\n  103961 (2024)",
        "doi": "10.1016/j.trd.2023.103961",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study estimates the relationships between street network characteristics\nand transport-sector CO2 emissions across every urban area in the world and\ninvestigates whether they are the same across development levels and urban\ndesign paradigms. The prior literature has estimated relationships between\nstreet network design and transport emissions -- including greenhouse gases\nimplicated in climate change -- primarily through case studies focusing on\ncertain world regions or relatively small samples of cities, complicating\ngeneralizability and applicability for evidence-informed practice. Our\nworldwide study finds that straighter, more-connected, and less-overbuilt\nstreet networks are associated with lower transport emissions, all else equal.\nImportantly, these relationships vary across development levels and design\nparadigms -- yet most prior literature reports findings from urban areas that\nare outliers by global standards. Planners need a better empirical base for\nevidence-informed practice in under-studied regions, particularly the rapidly\nurbanizing Global South.\n"
    },
    {
        "paper_id": 2401.01427,
        "authors": "Liam Welsh and Sebastian Jaimungal",
        "title": "Nash Equilibria in Greenhouse Gas Offset Credit Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  One approach to reducing greenhouse gas (GHG) emissions is to incentivize\ncarbon capturing and carbon reducing projects while simultaneously penalising\nexcess GHG output. In this work, we present a novel market framework and\ncharacterise the optimal behaviour of GHG offset credit (OC) market\nparticipants in both single-player and two-player settings. The single player\nsetting is posed as an optimal stopping and control problem, while the\ntwo-player setting is posed as optimal stopping and mixed-Nash equilibria\nproblem. We demonstrate the importance of acting optimally using numerical\nsolutions and Monte Carlo simulations and explore the differences between the\nhomogeneous and heterogeneous players. In both settings, we find that market\nparticipants benefit from optimal OC trading and OC generation.\n"
    },
    {
        "paper_id": 2401.01526,
        "authors": "Joseph Najnudel, Shen-Ning Tung, Kazutoshi Yamazaki, Ju-Yi Yen",
        "title": "An arbitrage driven price dynamics of Automated Market Makers in the\n  presence of fees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a model for price dynamics in the Automated Market Makers (AMM)\nsetting. Within this framework, we propose a reference market price following a\ngeometric Brownian motion. The AMM price is constrained by upper and lower\nbounds, determined by constant multiplications of the reference price. Through\nthe utilization of local times and excursion-theoretic approaches, we derive\nseveral analytical results, including its time-changed representation and\nlimiting behavior.\n"
    },
    {
        "paper_id": 2401.01622,
        "authors": "Lioba Heimbach, Vabuk Pahari, Eric Schertenleib",
        "title": "Non-Atomic Arbitrage in Decentralized Finance",
        "comments": "In Proceedings of IEEE Symposium on Security and Privacy (S&P) 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem\nhas led to a characterization of the latter as a dark forest. Studies of MEV\nhave thus far largely been restricted to purely on-chain MEV, i.e., sandwich\nattacks, cyclic arbitrage, and liquidations. In this work, we shed light on the\nprevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the\nEthereum blockchain. Importantly, non-atomic arbitrage exploits price\ndifferences between DEXes on the Ethereum blockchain as well as exchanges\noutside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other\nblockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions\non and off the Ethereum blockchain.\n  In our study of non-atomic arbitrage, we uncover that more than a fourth of\nthe volume on Ethereum's biggest five DEXes from the merge until 31 October\n2023 can likely be attributed to this type of MEV. We further highlight that\nonly eleven searchers are responsible for more than 80% of the identified\nnon-atomic arbitrage volume sitting at a staggering $132 billion and draw a\nconnection between the centralization of the block construction market and\nnon-atomic arbitrage. Finally, we discuss the security implications of these\nhigh-value transactions that account for more than 10% of Ethereum's total\nblock value and outline possible mitigations.\n"
    },
    {
        "paper_id": 2401.01751,
        "authors": "Michele Leonardo Bianchi",
        "title": "Text mining arXiv: a look through quantitative finance papers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores articles hosted on the arXiv preprint server with the aim\nto uncover valuable insights hidden in this vast collection of research.\nEmploying text mining techniques and through the application of natural\nlanguage processing methods, we examine the contents of quantitative finance\npapers posted in arXiv from 1997 to 2022. We extract and analyze crucial\ninformation from the entire documents, including the references, to understand\nthe topics trends over time and to find out the most cited researchers and\njournals on this domain. Additionally, we compare numerous algorithms to\nperform topic modeling, including state-of-the-art approaches.\n"
    },
    {
        "paper_id": 2401.01758,
        "authors": "Fabien Le Floc'h",
        "title": "Notes on the SWIFT method based on Shannon Wavelets for Option Pricing\n  -- Revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This note revisits the SWIFT method based on Shannon wavelets to price\nEuropean options under models with a known characteristic function in 2023. In\nparticular, it discusses some possible improvements and exposes some concrete\ndrawbacks of the method.\n"
    },
    {
        "paper_id": 2401.02042,
        "authors": "Mahdi Nohekhan, Mohammadmahdi Barzegar",
        "title": "Impact of Green Marketing Strategy on Brand Awareness: Business,\n  Management, and Human Resources Aspects",
        "comments": "arXiv admin note: text overlap with arXiv:2312.16698",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given the move towards industrialization in societies, the increase in\ndynamism and competition among companies to capture market share, raising\nconcerns about the environment, government, and international regulations and\nobligations, increased consumer awareness, pressure from nature-loving groups,\netc., organizations have become more attentive to issues related to\nenvironmental management. Over time, concepts such as green marketing have\npermeated marketing literature, making environmental considerations one of the\nmost important activities of companies. To this end, this research examines the\nimpact of green marketing strategy on brand awareness (case study: food\nexporting companies). The population of this research consists of 345 employees\nand managers of companies like Kalleh, Solico, Pemina, Sorbon, Mac, Pol, and\nCastle, from which 182 individuals were randomly selected as the sample using\nCochran's formula. This research is practical, and the required data have been\ncollected through a survey and a questionnaire. The research results indicate\nthat (1) green marketing strategy significantly affects brand awareness. (2)\nGreen products have a significant positive effect on brand awareness. (3) Green\npromotions have a significant positive effect on brand awareness. (4) Green\ndistribution has a significant positive effect on brand awareness. (5) Green\npricing has a significant positive effect on brand awareness.\n"
    },
    {
        "paper_id": 2401.02049,
        "authors": "Cristina Chinazzo and Vahidin Jeleskovic",
        "title": "Forecasting Bitcoin Volatility: A Comparative Analysis of Volatility\n  Approaches",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper conducts an extensive analysis of Bitcoin return series, with a\nprimary focus on three volatility metrics: historical volatility (calculated as\nthe sample standard deviation), forecasted volatility (derived from GARCH-type\nmodels), and implied volatility (computed from the emerging Bitcoin options\nmarket). These measures of volatility serve as indicators of market\nexpectations for conditional volatility and are compared to elucidate their\ndifferences and similarities. The central finding of this study underscores a\nnotably high expected level of volatility, both on a daily and annual basis,\nacross all the methodologies employed. However, it's crucial to emphasize the\npotential challenges stemming from suboptimal liquidity in the Bitcoin options\nmarket. These liquidity constraints may lead to discrepancies in the computed\nvalues of implied volatility, particularly in scenarios involving extreme\nmoneyness or maturity. This analysis provides valuable insights into Bitcoin's\nvolatility landscape, shedding light on the unique characteristics and dynamics\nof this cryptocurrency within the context of financial markets.\n"
    },
    {
        "paper_id": 2401.02134,
        "authors": "Ruiying Xiao",
        "title": "Female Entrepreneur on Board:Assessing the Effect of Gender on Corporate\n  Financial Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study investigates the impact of female leadership on the financial\nconstraints of firms, which are publicly listed entrepreneurial enterprises in\nChina. Utilizing data from 938 companies on the China Growth Enterprise Market\n(GEM) over a period of 2013-2022, this paper explores how the female presence\nin CEO positions, senior management, and board membership influences a firm's\nability to manage financial constraints. Our analysis employs the\nKaplan-Zingales (KZ) Index to measure these constraints, encompassing some key\nfinancial factors such as cash flow, dividends, and leverage. The findings\nreveal that companies with female CEOs or a higher proportion of women in top\nmanagement are associated with reduced financial constraints. However, the\ninfluence of female board members is less clear-cut. Our study also delves into\nthe variances of these effects between high-tech and low-tech industry sectors,\nemphasizing how internal gender biases in high-tech industries may impede the\nalleviation of financing constraints on firms. This research contributes to a\nnuanced understanding of the role of gender dynamics in corporate financial\nmanagement, especially in the context of China's evolving economic landscape.\nIt underscores the importance of promoting female leadership not only for\ngender equity but also for enhancing corporate financial resilience.\n"
    },
    {
        "paper_id": 2401.02139,
        "authors": "Alessandro V. M. Oliveira, Bruno F. Oliveira, Moises D. Vassallo",
        "title": "Airport service quality perception and flight delays: examining the\n  influence of psychosituational latent traits of respondents in passenger\n  satisfaction surveys",
        "comments": null,
        "journal-ref": "Research in Transportation Economics (2023), 102, 101371",
        "doi": "10.1016/j.retrec.2023.101371",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The service quality of a passenger transport operator can be measured through\nface-to-face surveys at the terminals or on board. However, the resulting\nresponses may suffer from the influence of the intrinsic aspects of the\nrespondent's personality and emotional context at the time of the interview.\nThis study proposes a methodology to generate and select control variables for\nthese latent psychosituational traits, thus mitigating the risk of omitted\nvariable bias. We developed an econometric model of the determinants of\npassenger satisfaction in a survey conducted at the largest airport in Latin\nAmerica, S\\~ao Paulo GRU Airport. Our focus was on the role of flight delays in\nthe perception of quality. The results of this study confirm the existence of a\nrelationship between flight delays and the global satisfaction of passengers\nwith airports. In addition, favorable evaluations regarding airports'\nfood/beverage concessions and Wi-Fi services, but not their retail options,\nhave a relevant moderating effect on that relationship. Furthermore,\ndissatisfaction arising from passengers' interaction with the airline can have\nnegative spillover effects on their satisfaction with the airport. We also\nfound evidence of blame-attribution behavior, in which only delays of internal\norigin, such as failures in flight management, are significant, indicating that\npassengers overlook weather-related flight delays. Finally, the results suggest\nthat an empirical specification that does not consider the latent\npsychosituational traits of passengers produces a relevant overestimation of\nthe absolute effect of flight delays on passenger satisfaction.\n"
    },
    {
        "paper_id": 2401.02353,
        "authors": "James W. Bono, David H. Wolpert",
        "title": "Game Mining: How to Make Money from those about to Play a Game",
        "comments": "25 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  It is known that a player in a noncooperative game can benefit by publicly\nrestricting his possible moves before play begins. We show that, more\ngenerally, a player may benefit by publicly committing to pay an external party\nan amount that is contingent on the game's outcome. We explore what happens\nwhen external parties -- who we call ``game miners'' -- discover this fact and\nseek to profit from it by entering an outcome-contingent contract with the\nplayers. We analyze various structured bargaining games between miners and\nplayers for determining such an outcome-contingent contract. These bargaining\ngames include playing the players against one another, as well as allowing the\nplayers to pay the miner(s) for exclusivity and first-mover advantage. We\nestablish restrictions on the strategic settings in which a game miner can\nprofit and bounds on the game miner's profit. We also find that game miners can\nlead to both efficient and inefficient equilibria.\n"
    },
    {
        "paper_id": 2401.02378,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages, and Dima L. Shepelyansky",
        "title": "Opinion formation in the world trade network",
        "comments": "16 pages, 19 figures (including 9 figures present in Appendix\n  section) and 1 table",
        "journal-ref": "Coquid\\'{e} C., Lages J. and Shepelyansky D.L., Opinion formation\n  in the world trade network. Entropy 2024, 26(2), 141",
        "doi": "10.3390/e26020141",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend the opinion formation approach to probe the world influence of\neconomical organizations. Our opinion formation model mimics a battle between\ncurrencies within the international trade network. Based on the United Nations\nComtrade database, we construct the world trade network for the years of the\nlast decade from 2010 to 2020. We consider different core groups constituted by\ncountries preferring to trade in a specific currency. We will consider\nprincipally two core groups, namely, 5 Anglo-Saxon countries which prefer to\ntrade in US dollar and the 11 BRICS+ which prefer to trade in a hypothetical\ncurrency, hereafter called BRI, pegged to their economies. We determine the\ntrade currency preference of the other countries via a Monte Carlo process\ndepending on the direct transactions between the countries. The results\nobtained in the frame of this mathematical model show that starting from year\n2014 the majority of the world countries would have preferred to trade in BRI\nthan USD. The Monte Carlo process reaches a steady state with 3 distinct\ngroups: two groups of countries preferring, whatever is the initial\ndistribution of the trade currency preferences, to trade, one in BRI and the\nother in USD, and a third group of countries swinging as a whole between USD\nand BRI depending on the initial distribution of the trade currency\npreferences. We also analyze the battle between USD, EUR and BRI, and present\nthe reduced Google matrix description of the trade relations between the\nAnglo-Saxon countries and the BRICS+.\n"
    },
    {
        "paper_id": 2401.02428,
        "authors": "Manuel de Mier and Fernando Delbianco",
        "title": "Cu\\'anto es demasiada inflaci\\'on? Una clasificaci\\'on de reg\\'imenes\n  inflacionarios",
        "comments": "in Spanish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classifications of inflationary regimes proposed in the literature have\nmostly been based on arbitrary characterizations, subject to value judgments by\nresearchers. The objective of this study is to propose a new methodological\napproach that reduces subjectivity and improves accuracy in the construction of\nsuch regimes. The method is built upon a combination of clustering techniques\nand classification trees, which allows for an historical periodization of\nArgentina's inflationary history for the period 1943-2022. Additionally, two\nprocedures are introduced to smooth out the classification over time: a measure\nof temporal contiguity of observations and a rolling method based on the simple\nmajority rule. The obtained regimes are compared against the existing\nliterature on the inflation-relative price variability relationship, revealing\na better performance of the proposed regimes.\n"
    },
    {
        "paper_id": 2401.02601,
        "authors": "Albert Dorador",
        "title": "Constrained Max Drawdown: a Fast and Robust Portfolio Optimization\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose an alternative linearization to the classical Markowitz quadratic\nportfolio optimization model, based on maximum drawdown. This model, which\nminimizes maximum portfolio drawdown, is particularly appealing during times of\nfinancial distress, like during the COVID-19 pandemic. In addition, we will\npresent a Mixed-Integer Linear Programming variation of our new model that,\nbased on our out-of-sample results and sensitivity analysis, delivers a more\nprofitable and robust solution with a 200 times faster solving time compared to\nthe standard Markowitz quadratic formulation.\n"
    },
    {
        "paper_id": 2401.02681,
        "authors": "Alessandra Mainini, Enrico Moretto, Daniela Visetti",
        "title": "Displaying risk in mergers: a diagrammatic approach for exchange ratio\n  determination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article extends, in a stochastic setting, previous results in the\ndetermination of feasible exchange ratios for merging companies. A first\noutcome is that shareholders of the companies involved in the merging process\nface both an upper and a lower bounds for acceptable exchange ratios. Secondly,\nin order for the improved `bargaining region' to be intelligibly displayed, the\ndiagrammatic approach developed by Kulpa is exploited.\n"
    },
    {
        "paper_id": 2401.02963,
        "authors": "St\\'ephane Goria (Crem)",
        "title": "L'utilit\\'e de l'\\'echelle op\\'eratique pour consid\\'erer des\n  strat\\'egies d'intelligence et de guerre \\'economique",
        "comments": "in French language",
        "journal-ref": "Revue Internationale d'Intelligence {\\'E}conomique, 2021, 13 (2),\n  pp.43-60",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 20th century saw the emergence of an intermediate level of consideration,\nsituated between the tactical and strategic levels: the operational level. This\nlevel of scale is that of a large area of operations, that is to say\ncoordinated engagements bringing together forces belonging to different corps\nover a considerable geographical distance. In this article, we approach\noperatics and its implementation, called operative art or operational art,\nfirst from its contributions to military thinking, then we transpose what can\nbe transposed beyond this domain. We consider operative art as a new point of\nview generating a particular thought and solutions based on agile systems, but\nwhich may have reached their limits. We will limit ourselves here to the\nquestion of the potential contributions of the operatic perspective to\nunderstand or implement a set of actions related to competitive or economic\nwarfare. To do this, we will begin by providing some answers to the question of\nwhat operatic military is in terms of its characteristics. Then, we propose a\nway to transpose them in the context of companies preferably confronted with a\ncontext of competitive or economic warfare by taking as an example the\nacquisition of the essential activities of the company Alstom by the company\nGeneral Electric (GE).\n"
    },
    {
        "paper_id": 2401.03096,
        "authors": "MS Hosen, SM Hossain, MN Mia, MR Chowdhury",
        "title": "The Effects of COVID-19 and the Russia-Ukraine War on Inward Foreign\n  Direct Investment",
        "comments": "Volume 2 Issue 4 November 2023 / Pg. No: 408-417",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol. 2, No. 4, pp. 408-417, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inward Foreign Direct Investment (IFDI) into Europe and Asian developing\ncountries like Bangladesh is experimentally examined in this study. IFDI in\nemerging markets has been boosted by global investment and inflow influenced by\nresource availability and public policy. The economic policy uncertainty on\nIFDI in 13 countries is explored at a time when the crisis between Russia and\nUkraine war is having a global impact. Microeconomic factors affected Gross\nDomestic Product (GDP) growth, inflation, interest rates, and the currency rate\nfluctuated with IFDI, which mostly shocked during COVID-19 and the\nRussia-Ukraine war. With data from the World Bank and the United Nations\nConference on Trade and Development (UNCTAD) database, we compile a panel\ndataset covering 2018-2022. The researchers used a mixture of panel and linear\nregression analysis using a random effect model. Our findings show that the\nimpact of global rates hurts IFDI in 13 selected countries. There is a\ncorrelation between a country's ability to enforce contracts and the amount of\nInward FDI it receives. Using the top 13 hosts of incoming FDI flows COVID-19\nand Russia-Ukraine wartime series analysis gives valuable information for\npolicymakers in the remaining countries chosen to attract IFDI inflows.\n"
    },
    {
        "paper_id": 2401.03305,
        "authors": "Xue Cheng, Peng Guo and Tai-ho Wang",
        "title": "Optimal Order Execution subject to Reservation Strategies under\n  Execution Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper addresses the problem of meta order execution from a\nbroker-dealer's point of view in Almgren-Chriss model under order fill\nuncertainty. A broker-dealer agency is authorized to execute an order of\ntrading on client's behalf. The strategies that the agent is allowed to deploy\nis subject to a benchmark, referred to as the reservation strategy, regulated\nby the client. We formulate the broker's problem as a utility maximization\nproblem in which the broker seeks to maximize his utility of excess\nprofit-and-loss at the execution horizon. Optimal strategy in feedback form is\nobtained in closed form. In the absence of execution risk, the optimal\nstrategies subject to reservation strategies are deterministic. We establish an\naffine structure among the trading trajectories under optimal strategies\nsubject to general reservation strategies using implementation shortfall and\ntarget close orders as basis. We conclude the paper with numerical experiments\nillustrating the trading trajectories as well as histograms of terminal wealth\nand utility at investment horizon under optimal strategies versus those under\nTWAP strategies.\n"
    },
    {
        "paper_id": 2401.03328,
        "authors": "Jean-Gabriel Lauzier, Liyuan Lin and Ruodu Wang",
        "title": "Negatively dependent optimal risk sharing",
        "comments": "35 pages, 1 figure, Keywords: Pareto optimality, Risk sharing,\n  Counter-monotonicity, Risk seeking, Rank-dependent expected utility,\n  Cryptocurrency mining pools",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the problem of optimally sharing risk using allocations that\nexhibit counter-monotonicity, the most extreme form of negative dependence.\nCounter-monotonic allocations take the form of either \"winner-takes-all\"\nlotteries or \"loser-loses-all\" lotteries, and we respectively refer to these\n(normalized) cases as jackpot or scapegoat allocations. Our main theorem, the\ncounter-monotonic improvement theorem, states that for a given set of random\nvariables that are either all bounded from below or all bounded from above, one\ncan always find a set of counter-monotonic random variables such that each\ncomponent is greater or equal than its counterpart in the convex order. We show\nthat Pareto optimal allocations, if they exist, must be jackpot allocations\nwhen all agents are risk seeking. We essentially obtain the opposite when all\nagents have discontinuous Bernoulli utility functions, as scapegoat allocations\nmaximize the probability of being above the discontinuity threshold. We also\nconsider the case of rank-dependent expected utility (RDU) agents and find\nconditions which guarantee that RDU agents prefer jackpot allocations. We\nprovide an application for the mining of cryptocurrencies and show that in\ncontrast to risk-averse miners, RDU miners with small computing power never\njoin a mining pool. Finally, we characterize the competitive equilibria with\nrisk-seeking agents, providing a first and second fundamental theorem of\nwelfare economics where all equilibrium allocations are jackpot allocations.\n"
    },
    {
        "paper_id": 2401.03345,
        "authors": "Eduardo Abi Jaber and Shaun (Xiaoyuan) Li",
        "title": "Volatility models in practice: Rough, Path-dependent or Markovian?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  An extensive empirical study of the class of Volterra Bergomi models using\nSPX options data between 2011 and 2022 reveals the following fact-check on two\nfundamental claims echoed in the rough volatility literature:\n  Do rough volatility models with Hurst index $H \\in (0,1/2)$ really capture\nwell SPX implied volatility surface with very few parameters? No, rough\nvolatility models are inconsistent with the global shape of SPX smiles. They\nsuffer from severe structural limitations imposed by the roughness component,\nwith the Hurst parameter $H \\in (0,1/2)$ controlling the smile in a poor way.\nIn particular, the SPX at-the-money skew is incompatible with the power-law\nshape generated by rough volatility models. The skew of rough volatility models\nincreases too fast on the short end, and decays too slow on the longer end\nwhere \"negative\" $H$ is sometimes needed.\n  Do rough volatility models really outperform consistently their classical\nMarkovian counterparts? No, for short maturities they underperform their\none-factor Markovian counterpart with the same number of parameters. For longer\nmaturities, they do not systematically outperform the one-factor model and\nsignificantly underperform when compared to an under-parametrized two-factor\nMarkovian model with only one additional calibratable parameter.\n  On the positive side: our study identifies a (non-rough) path-dependent\nBergomi model and an under-parametrized two-factor Markovian Bergomi model that\nconsistently outperform their rough counterpart in capturing SPX smiles between\none week and three years with only 3 to 4 calibratable parameters.\n\\end{abstract}\n"
    },
    {
        "paper_id": 2401.03393,
        "authors": "Dennis Koch and Vahidin Jeleskovic and Zahid I. Younas",
        "title": "Modelling and Predicting the Conditional Variance of Bitcoin Daily\n  Returns: Comparsion of Markov Switching GARCH and SV Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a unique and valuable research design aimed at\nanalyzing Bitcoin price volatility. To achieve this, a range of models from the\nMarkov Switching-GARCH and Stochastic Autoregressive Volatility (SARV) model\nclasses are considered and their out-of-sample forecasting performance is\nthoroughly examined. The paper provides insights into the rationale behind the\nrecommendation for a two-stage estimation approach, emphasizing the separate\nestimation of coefficients in the mean and variance equations. The results\npresented in this paper indicate that Stochastic Volatility models,\nparticularly SARV models, outperform MS-GARCH models in forecasting Bitcoin\nprice volatility. Moreover, the study suggests that in certain situations,\npersistent simple GARCH models may even outperform Markov-Switching GARCH\nmodels in predicting the variance of Bitcoin log returns. These findings offer\nvaluable guidance for risk management experts, highlighting the potential\nadvantages of SARV models in managing and forecasting Bitcoin price volatility.\n"
    },
    {
        "paper_id": 2401.03443,
        "authors": "Hoang Nguyen, Audron\\.e Virbickait\\.e, M. Concepci\\'on Aus\\'in, Pedro\n  Galeano",
        "title": "Structured factor copulas for modeling the systemic risk of European and\n  United States banks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we employ Credit Default Swaps (CDS) to model the joint and\nconditional distress probabilities of banks in Europe and the U.S. using factor\ncopulas. We propose multi-factor, structured factor, and factor-vine models\nwhere the banks in the sample are clustered according to their geographic\nlocation. We find that within each region, the co-dependence between banks is\nbest described using both, systematic and idiosyncratic, financial contagion\nchannels. However, if we consider the banking system as a whole, then the\nsystematic contagion channel prevails, meaning that the distress probabilities\nare driven by a latent global factor and region-specific factors. In all cases,\nthe co-dependence structure of bank CDS spreads is highly correlated in the\ntail. The out-of-sample forecasts of several measures of systematic risk allow\nus to identify the periods of distress in the banking sector over the recent\nyears including the COVID-19 pandemic, the interest rate hikes in 2022, and the\nbanking crisis in 2023.\n"
    },
    {
        "paper_id": 2401.03658,
        "authors": "Saleh Ghobbeh, Armita Atrian",
        "title": "Spiritual Intelligence's Role in Reducing Technostress through Ethical\n  Work Climates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the impact of spiritual intelligence (SI) on\ntechnostress, with a focus on the mediating role of the ethical environment. In\nan era where technological advancements continually reshape our work and\npersonal lives, understanding the interplay between human intelligence,\nwell-being, and ethics within organizations is increasingly significant.\nSpiritual intelligence, transcending traditional cognitive and emotional\nintelligences, emphasizes understanding personal meaning and values. This paper\ninvestigates how higher levels of SI enable individuals to integrate technology\ninto their lives without undue stress, and how a robust ethical environment\nwithin organizations supports and amplifies these benefits. Through a\ncomprehensive review of literature, empirical research, and detailed analysis,\nthe study highlights the protective role of SI against technostress and the\nsignificant influence of an ethical climate in enhancing this effect. The\nfindings offer valuable insights for organizational strategies aimed at\npromoting a harmonious, stress-free workplace environment.\n"
    },
    {
        "paper_id": 2401.03737,
        "authors": "Georgios Fatouros, Konstantinos Metaxas, John Soldatos, Dimosthenis\n  Kyriazis",
        "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of\n  AI in Stock Selection",
        "comments": "17 pages, 12 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper introduces MarketSenseAI, an innovative framework leveraging\nGPT-4's advanced reasoning for selecting stocks in financial markets. By\nintegrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes\ndiverse data sources, including market trends, news, fundamentals, and\nmacroeconomic factors, to emulate expert investment decision-making. The\ndevelopment, implementation, and validation of the framework are elaborately\ndiscussed, underscoring its capability to generate actionable and interpretable\ninvestment signals. A notable feature of this work is employing GPT-4 both as a\npredictive mechanism and signal evaluator, revealing the significant impact of\nthe AI-generated explanations on signal accuracy, reliability and acceptance.\nThrough empirical testing on the competitive S&P 100 stocks over a 15-month\nperiod, MarketSenseAI demonstrated exceptional performance, delivering excess\nalpha of 10% to 30% and achieving a cumulative return of up to 72% over the\nperiod, while maintaining a risk profile comparable to the broader market. Our\nfindings highlight the transformative potential of Large Language Models in\nfinancial decision-making, marking a significant leap in integrating generative\nAI into financial analytics and investment strategies.\n"
    },
    {
        "paper_id": 2401.0374,
        "authors": "Paulo M. M. Rodrigues, Mirjam Salish, Nazarii Salish",
        "title": "Saving for sunny days: The impact of climate (change) on consumer prices\n  in the euro area",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate (change) affects the prices of goods and services in different\ncountries or regions differently. Simply relying on aggregate measures or\nsummary statistics, such as the impact of average country temperature changes\non HICP headline inflation, conceals a large heterogeneity across (sub-)sectors\nof the economy. Additionally, the impact of a weather anomaly on consumer\nprices depends not only on its sign and magnitude, but also on its location and\nthe size of the area affected by the shock. This is especially true for larger\ncountries or regions with diverse climate zones, since the geographical\ndistribution of climatic effects plays a role in shaping economic outcomes.\nUsing time series data of geolocations, we demonstrate that relying solely on\ncountry averages fails to adequately capture and explain the influence of\nweather on consumer prices in the euro area. We conclude that the information\ncontent hidden in rich and complex surface data can provide valuable insights\ninto the role of weather and climate variables for price stability, and more\ngenerally may help to inform economic policy.\n"
    },
    {
        "paper_id": 2401.03776,
        "authors": "Liexin Cheng, Xue Cheng",
        "title": "Approximating Smiles: A Time Change Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new method for approximating the shape of implied volatility\nsmiles. The method is applicable to common semimartingale models, such as\njump-diffusion, rough volatility, and infinite activity models. We firstly\nadopt an Edgeworth expansion method to approximate the at-the-money skew and\ncurvature and propose conditions under which the approximation errors converge.\nThen, we explicitly approximate the volatility skew and curvature under a time\nchange framework using moment-based formula. Additionally, we derive the\ncharacteristics of volatility skew and curvature and explain their implications\nfor model selection based on the approximation. The accuracy of the short-term\napproximation results on models is tested via numerical methods and on\nempirical data. The method is then applied to the calibration problem.\n"
    },
    {
        "paper_id": 2401.04046,
        "authors": "Osiris Jorge Parcero, Elissaios Papyrakis",
        "title": "Income inequality and the oil resource curse",
        "comments": "10 tables, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.reseneeco.2016.06.001",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Surprisingly, there has been little research conducted about the\ncross-country relationship between oil dependence/abundance and income\ninequality. At the same time, there is some tentative evidence suggesting that\noil rich nations tend to under-report data on income inequality, which can\npotentially influence the estimated empirical relationships between oil\nrichness and income inequality. In this paper we contribute to the literature\nin a twofold manner. First, we explore in depth the empirical relationship\nbetween oil and income inequality by making use of the Standardized World\nIncome Inequality Database; the most comprehensive dataset on income inequality\nproviding comparable data for the broadest set of country-year observations.\nSecond, this is the first study to our knowledge that adopts an empirical\nframework to examine whether oil rich nations tend to under-report data on\nincome inequality and the possible implications thereof. We make use of Heckman\nselection models to validate the tendency of oil rich countries to under-report\nand correct for the bias that might arise as a result of this; we find that oil\nis associated with lower income inequality with the exception of the very\noil-rich economies.\n"
    },
    {
        "paper_id": 2401.04132,
        "authors": "Yue Chen and Mohan Li",
        "title": "Economic Forces in Stock Returns",
        "comments": "11 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When analyzing the components influencing the stock prices, it is commonly\nbelieved that economic activities play an important role. More specifically,\nasset prices are more sensitive to the systematic economic news that impose a\npervasive effect on the whole market. Moreover, the investors will not be\nrewarded for bearing idiosyncratic risks as such risks are diversifiable. In\nthe paper Economic Forces and the Stock Market 1986, the authors introduced an\nattribution model to identify the specific systematic economic forces\ninfluencing the market. They first defined and examined five classic factors\nfrom previous research papers: Industrial Production, Unanticipated Inflation,\nChange in Expected Inflation, Risk Premia, and The Term Structure. By adding in\nnew factors, the Market Indices, Consumptions and Oil Prices, one by one, they\nexamined the significant contribution of each factor to the stock return. The\npaper concluded that the stock returns are exposed to the systematic economic\nnews, and they are priced with respect to their risk exposure. Also, the\nsignificant factors can be identified by simply adopting their model. Driven by\nsuch motivation, we conduct an attribution analysis based on the general\nframework of their model to further prove the importance of the economic\nfactors and identify the specific identity of significant factors.\n"
    },
    {
        "paper_id": 2401.04214,
        "authors": "Osiris Parcero, James Christopher Ryan",
        "title": "Becoming a Knowledge Economy: the Case of Qatar, UAE and 17 Benchmark\n  Countries",
        "comments": "Pages 48, 8 tables, 5 charts",
        "journal-ref": null,
        "doi": "10.1007/s13132-016-0355-y",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper assesses the performance of Qatar and the United Arab Emirates\n(UAE) in terms of their achievements towards becoming knowledge-based\neconomies. This is done through a comparison against 17 benchmark countries\nusing a four pillars' framework comprising; (1) information and communication\ntechnology, (2) education, (3) innovation and (4) economy and regime. Results\nindicate that the UAE ranks slightly better than the median rank of the 19\ncompared countries while Qatar ranks somewhat below. Results also indicate that\nboth countries lag considerably behind knowledge economy leaders; particularly\nevidenced in the innovation pillar. Policy recommendations are mainly addressed\nat further developing the two countries' research culture as well as improving\nthe incentives to attract top quality researchers and highly talented workers.\n"
    },
    {
        "paper_id": 2401.04243,
        "authors": "Osiris Jorge Parcero",
        "title": "Optimal National policies towards multinationals when local regions can\n  choose between firm-specific and non-firm-specific policies",
        "comments": "Pages 26, 3 tables, 1 figure",
        "journal-ref": null,
        "doi": "10.1628/001522117X14932991128985",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper looks at a country's optimal central-government optimal policy in\na setting where its two identical local jurisdictions compete to attract\nfootloose multinationals to their sites, and where the considered\nmultinationals strictly prefer this country to the rest of the world. For the\nsake of realism the model allows the local jurisdictions to choose between\nfirm-specific and non-firm-specific policies. We show that the implementation\nof the jurisdictional firm-specific policy is weakly welfare dominant. Hence\nthe frequent calls for the central government to ban the former type of\npolicies go against the advice of this paper.\n"
    },
    {
        "paper_id": 2401.04253,
        "authors": "N. Y. Al Saiqal, James. C. Ryabn, Osiris Jorge Parcero",
        "title": "Entrepreneurial Intention and UAE Youth: Unique Influencers of\n  Entrepreneurial Intentions in an Emerging Country Context",
        "comments": "Pages 33, 4 tables, 3 figure",
        "journal-ref": null,
        "doi": "10.1080/10669868.2018.1536012",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The United Arab Emirates (UAE) is a young, oil-rich country, where national\nyouth display a clear preference for public sector employment. Growing youth\nunemployment reinforces the importance of non-government employment, including\nentrepreneurship. This study investigates UAE national youth intentions towards\nentrepreneurship through the Theory of Planned Behavior and the Entrepreneurial\nIntention Questionnaire (EIQ). Analysis (N=544) identifies the direct influence\nof attitude and perceived behavioral control, and indirect influence of\nsubjective norms on entrepreneurship intention. Results also examine several\ndemographic variables and highlight the potential importance of family and\nsocial groups in promoting entrepreneurial intentions in this emerging country\ncontext.\n"
    },
    {
        "paper_id": 2401.04289,
        "authors": "Kenan Wood, Maurice Herlihy, Hammurabi Mendes, Jonad Pulaj",
        "title": "Expiring Assets in Automated Market Makers",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An automated market maker (AMM) is a state machine that manages pools of\nassets, allowing parties to buy and sell those assets according to a fixed\nmathematical formula. AMMs are typically implemented as smart contracts on\nblockchains, and its prices are kept in line with the overall market price by\narbitrage: if the AMM undervalues an asset with respect to the market, an\n\"arbitrageur\" can make a risk-free profit by buying just enough of that asset\nto bring the AMM's price back in line with the market.\n  AMMs, however, are not designed for assets that expire: that is, assets that\ncannot be produced or resold after a specified date. As assets approach\nexpiration, arbitrage may not be able to reconcile supply and demand, and the\nliquidity providers that funded the AMM may have excessive exposure to risk due\nto rapid price variations.\n  This paper formally describes the design of a decentralized exchange (DEX)\nfor assets that expire, combining aspects of AMMs and limit-order books. We\nensure liveness and market clearance, providing mechanisms for liquidity\nproviders to control their exposure to risk and adjust prices dynamically in\nresponse to situations where arbitrage may fail.\n"
    }
]